"""
Comprehensive test script to validate normalization in SUNRGBD 3-stream dataset.

This script thoroughly tests:
1. Scaling to [0,1] range before augmentation
2. Augmentation maintaining [0,1] range
3. Normalization statistics matching precomputed values
4. Final normalized tensor statistics
5. Impact of different std values for orthogonal stream
"""

import os
import sys
import numpy as np
import torch
from tqdm import tqdm

# Add src to path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

from data_utils.sunrgbd_3stream_dataset import SUNRGBD3StreamDataset, get_sunrgbd_3stream_dataloaders


def test_scaling_to_01_range():
    """Test that all modalities are scaled to [0,1] before augmentation."""
    print("\n" + "=" * 80)
    print("TEST 1: Verify Scaling to [0,1] Range (Before Augmentation)")
    print("=" * 80)

    # Create validation dataset (no augmentation)
    val_dataset = SUNRGBD3StreamDataset(train=False, target_size=(416, 544))

    print(f"\nTesting {len(val_dataset)} validation samples...")
    print("(Validation has no augmentation, only scaling + normalization)")

    # We need to check BEFORE normalization
    # Let's manually load and process a few samples
    from PIL import Image

    rgb_mins, rgb_maxs = [], []
    depth_mins, depth_maxs = [], []
    orth_mins, orth_maxs = [], []

    num_test_samples = min(100, len(val_dataset))

    for idx in tqdm(range(num_test_samples), desc="Checking raw scaling"):
        # Load images manually as dataset does
        rgb_path = os.path.join(val_dataset.rgb_dir, f'{idx:05d}.png')
        depth_path = os.path.join(val_dataset.depth_dir, f'{idx:05d}.png')
        orth_path = os.path.join(val_dataset.orth_dir, f'{idx:05d}.png')

        # RGB - will be scaled by to_tensor()
        rgb = Image.open(rgb_path).convert('RGB')
        rgb_array = np.array(rgb, dtype=np.float32) / 255.0  # Simulate to_tensor()
        rgb_mins.append(rgb_array.min())
        rgb_maxs.append(rgb_array.max())

        # Depth - manually scaled as in dataset
        depth = Image.open(depth_path)
        if depth.mode in ('I', 'I;16', 'I;16B'):
            depth_arr = np.array(depth, dtype=np.float32)
            depth_arr = np.clip(depth_arr / 65535.0, 0.0, 1.0)
        else:
            depth_arr = np.array(depth.convert('F'))
            if depth_arr.max() > 1.0:
                depth_arr = depth_arr / 255.0
        depth_mins.append(depth_arr.min())
        depth_maxs.append(depth_arr.max())

        # Orth - manually scaled as in dataset
        orth = Image.open(orth_path)
        if orth.mode in ('I', 'I;16', 'I;16B'):
            orth_arr = np.array(orth, dtype=np.float32)
            orth_arr = np.clip(orth_arr / 65535.0, 0.0, 1.0)
        else:
            orth_arr = np.array(orth.convert('F'))
            if orth_arr.max() > 1.0:
                orth_arr = orth_arr / 255.0
        orth_mins.append(orth_arr.min())
        orth_maxs.append(orth_arr.max())

    print(f"\nRGB (after to_tensor scaling):")
    print(f"  Min across samples: {min(rgb_mins):.6f}")
    print(f"  Max across samples: {max(rgb_maxs):.6f}")
    print(f"  Expected: [0.0, 1.0]")
    print(f"  âœ“ PASS" if min(rgb_mins) >= 0.0 and max(rgb_maxs) <= 1.0 else "  âœ— FAIL")

    print(f"\nDepth (after manual scaling):")
    print(f"  Min across samples: {min(depth_mins):.6f}")
    print(f"  Max across samples: {max(depth_maxs):.6f}")
    print(f"  Expected: [0.0, 1.0]")
    print(f"  âœ“ PASS" if min(depth_mins) >= 0.0 and max(depth_maxs) <= 1.0 else "  âœ— FAIL")

    print(f"\nOrth (after manual scaling):")
    print(f"  Min across samples: {min(orth_mins):.6f}")
    print(f"  Max across samples: {max(orth_maxs):.6f}")
    print(f"  Expected: [0.0, 1.0]")
    print(f"  âœ“ PASS" if min(orth_mins) >= 0.0 and max(orth_maxs) <= 1.0 else "  âœ— FAIL")


def test_normalization_statistics():
    """Test that normalized data has correct statistics."""
    print("\n" + "=" * 80)
    print("TEST 2: Verify Normalization Statistics")
    print("=" * 80)

    # Exact computed statistics from 8041 training samples (after scaling to [0,1])
    EXPECTED_RGB_MEAN = np.array([0.4905626144214781, 0.4564359471868703, 0.43112756716677114])
    EXPECTED_RGB_STD = np.array([0.27944652961530003, 0.2868739703756949, 0.29222326115669395])
    EXPECTED_DEPTH_MEAN = 0.2912
    EXPECTED_DEPTH_STD = 0.1472
    EXPECTED_ORTH_MEAN = 0.5000
    EXPECTED_ORTH_STD = 0.0249

    # Load validation dataset
    val_dataset = SUNRGBD3StreamDataset(train=False, target_size=(416, 544))

    print("\nComputing statistics on NORMALIZED data...")
    print("(This verifies that normalization inverts correctly)")

    rgb_values = []
    depth_values = []
    orth_values = []

    num_samples = min(500, len(val_dataset))

    for idx in tqdm(range(num_samples), desc="Loading samples"):
        rgb, depth, orth, _ = val_dataset[idx]

        # Denormalize to check original [0,1] values
        # Formula: denormalized = normalized * std + mean
        rgb_denorm = rgb * torch.tensor(EXPECTED_RGB_STD, dtype=torch.float32).view(3, 1, 1) + \
                     torch.tensor(EXPECTED_RGB_MEAN, dtype=torch.float32).view(3, 1, 1)
        depth_denorm = depth * EXPECTED_DEPTH_STD + EXPECTED_DEPTH_MEAN
        orth_denorm = orth * EXPECTED_ORTH_STD + EXPECTED_ORTH_MEAN

        rgb_values.append(rgb_denorm.numpy())
        depth_values.append(depth_denorm.numpy())
        orth_values.append(orth_denorm.numpy())

    # Concatenate all samples
    rgb_all = np.concatenate([x.reshape(3, -1) for x in rgb_values], axis=1)
    depth_all = np.concatenate([x.reshape(1, -1) for x in depth_values], axis=1)
    orth_all = np.concatenate([x.reshape(1, -1) for x in orth_values], axis=1)

    # Compute statistics
    rgb_mean = rgb_all.mean(axis=1)
    rgb_std = rgb_all.std(axis=1)
    depth_mean = depth_all.mean()
    depth_std = depth_all.std()
    orth_mean = orth_all.mean()
    orth_std = orth_all.std()

    print(f"\nRGB Statistics (denormalized, should be in [0,1]):")
    print(f"  Computed Mean: [{rgb_mean[0]:.4f}, {rgb_mean[1]:.4f}, {rgb_mean[2]:.4f}]")
    print(f"  Expected Mean: [{EXPECTED_RGB_MEAN[0]:.4f}, {EXPECTED_RGB_MEAN[1]:.4f}, {EXPECTED_RGB_MEAN[2]:.4f}]")
    print(f"  Computed Std:  [{rgb_std[0]:.4f}, {rgb_std[1]:.4f}, {rgb_std[2]:.4f}]")
    print(f"  Expected Std:  [{EXPECTED_RGB_STD[0]:.4f}, {EXPECTED_RGB_STD[1]:.4f}, {EXPECTED_RGB_STD[2]:.4f}]")
    print(f"  Range: [{rgb_all.min():.4f}, {rgb_all.max():.4f}]")

    rgb_mean_error = np.abs(rgb_mean - EXPECTED_RGB_MEAN).max()
    rgb_std_error = np.abs(rgb_std - EXPECTED_RGB_STD).max()
    print(f"  Max mean error: {rgb_mean_error:.6f} (tolerance: 0.01)")
    print(f"  Max std error:  {rgb_std_error:.6f} (tolerance: 0.01)")
    print(f"  âœ“ PASS" if rgb_mean_error < 0.01 and rgb_std_error < 0.01 else "  âœ— FAIL")

    print(f"\nDepth Statistics (denormalized, should be in [0,1]):")
    print(f"  Computed Mean: {depth_mean:.4f}")
    print(f"  Expected Mean: {EXPECTED_DEPTH_MEAN:.4f}")
    print(f"  Computed Std:  {depth_std:.4f}")
    print(f"  Expected Std:  {EXPECTED_DEPTH_STD:.4f}")
    print(f"  Range: [{depth_all.min():.4f}, {depth_all.max():.4f}]")

    depth_mean_error = abs(depth_mean - EXPECTED_DEPTH_MEAN)
    depth_std_error = abs(depth_std - EXPECTED_DEPTH_STD)
    print(f"  Mean error: {depth_mean_error:.6f} (tolerance: 0.01)")
    print(f"  Std error:  {depth_std_error:.6f} (tolerance: 0.01)")
    print(f"  âœ“ PASS" if depth_mean_error < 0.01 and depth_std_error < 0.01 else "  âœ— FAIL")

    print(f"\nOrth Statistics (denormalized, should be in [0,1]):")
    print(f"  Computed Mean: {orth_mean:.4f}")
    print(f"  Expected Mean: {EXPECTED_ORTH_MEAN:.4f}")
    print(f"  Computed Std:  {orth_std:.4f}")
    print(f"  Expected Std:  {EXPECTED_ORTH_STD:.4f}")
    print(f"  Range: [{orth_all.min():.4f}, {orth_all.max():.4f}]")

    orth_mean_error = abs(orth_mean - EXPECTED_ORTH_MEAN)
    orth_std_error = abs(orth_std - EXPECTED_ORTH_STD)
    print(f"  Mean error: {orth_mean_error:.6f} (tolerance: 0.01)")
    print(f"  Std error:  {orth_std_error:.6f} (tolerance: 0.01)")
    print(f"  âœ“ PASS" if orth_mean_error < 0.01 and orth_std_error < 0.01 else "  âœ— FAIL")


def test_normalized_tensor_ranges():
    """Test the ranges of normalized tensors."""
    print("\n" + "=" * 80)
    print("TEST 3: Normalized Tensor Ranges")
    print("=" * 80)

    # Load training dataset (with augmentation)
    train_dataset = SUNRGBD3StreamDataset(train=True, target_size=(416, 544))

    print(f"\nTesting {len(train_dataset)} training samples (with augmentation)...")

    rgb_mins, rgb_maxs = [], []
    depth_mins, depth_maxs = [], []
    orth_mins, orth_maxs = [], []

    num_samples = min(500, len(train_dataset))

    for idx in tqdm(range(num_samples), desc="Loading samples"):
        rgb, depth, orth, _ = train_dataset[idx]

        rgb_mins.append(rgb.min().item())
        rgb_maxs.append(rgb.max().item())
        depth_mins.append(depth.min().item())
        depth_maxs.append(depth.max().item())
        orth_mins.append(orth.min().item())
        orth_maxs.append(orth.max().item())

    print(f"\nNormalized RGB tensor:")
    print(f"  Range: [{min(rgb_mins):.3f}, {max(rgb_maxs):.3f}]")
    print(f"  Expected (with mean=0.4906, std=0.2794): approximately [-1.76, 1.82]")

    print(f"\nNormalized Depth tensor:")
    print(f"  Range: [{min(depth_mins):.3f}, {max(depth_maxs):.3f}]")
    print(f"  Expected (with mean=0.2912, std=0.1472): approximately [-1.98, 4.82]")

    print(f"\nNormalized Orth tensor:")
    print(f"  Range: [{min(orth_mins):.3f}, {max(orth_maxs):.3f}]")
    print(f"  Current (with mean=0.5000, std=0.0249): approximately [-20.08, 20.08]")
    print(f"  âš ï¸  WARNING: Range is EXTREMELY large due to very small std (0.0249)!")
    print(f"  âš ï¸  This indicates orthogonal data is highly uniform (low variance)")


def test_orth_std_comparison():
    """Compare different std values for orthogonal stream."""
    print("\n" + "=" * 80)
    print("TEST 4: Orthogonal Std Value Comparison")
    print("=" * 80)

    print("\nComparing impact of different std values for Orth normalization:")

    # Exact computed stats
    orth_mean = 0.5000
    orth_std_computed = 0.0249  # VERY SMALL - indicates highly uniform data!

    # Test different std values
    std_options = [
        0.0249,  # Exact computed value
        0.0710,  # Previously thought to be computed
        0.1472,  # Match Depth std
        0.2794,  # Match RGB std
        0.5      # Conservative [-1, 1] mapping
    ]

    print(f"\nOrth Mean: {orth_mean:.4f}")
    print(f"Exact Computed Std: {orth_std_computed:.4f} â† VERY SMALL!")
    print(f"\n{'Std Value':<12} {'Min Range':<12} {'Max Range':<12} {'Amplification':<15} {'Notes':<30}")
    print("-" * 95)

    for std_val in std_options:
        # Assuming orth values in [0, 1] after scaling
        # Normalized value = (x - mean) / std
        min_norm = (0.0 - orth_mean) / std_val
        max_norm = (1.0 - orth_mean) / std_val
        amplification = 1.0 / std_val

        notes = ""
        if std_val == 0.0249:
            notes = "âš ï¸ CURRENT (exact computed)"
        elif std_val == 0.0710:
            notes = "Previously assumed"
        elif std_val == 0.1472:
            notes = "Match Depth std"
        elif std_val == 0.2794:
            notes = "Match RGB std (balanced)"
        elif std_val == 0.5:
            notes = "Maps [0,1] to [-1,1] (safe)"

        print(f"{std_val:<12.4f} {min_norm:<12.2f} {max_norm:<12.2f} {amplification:<15.2f} {notes:<30}")

    print("\nâš ï¸  EXTREMELY small std (0.0249) will cause:")
    print("   - Range of [-20, +20] in normalized tensors")
    print("   - 40x amplification of small variations")
    print("   - Very high risk of exploding gradients")
    print("   - Extreme sensitivity to any noise or perturbations")
    print("   - Potential numerical instability (values outside typical [-3, 3] range)")

    print("\nðŸ’¡ RECOMMENDATION:")
    print("   Since Orth data is highly uniform (std=0.0249), consider using:")
    print("   â€¢ std=0.2794 (match RGB): Balanced approach, range ~[-1.8, 1.8]")
    print("   â€¢ std=0.1472 (match Depth): Moderate amplification, range ~[-3.4, 3.4]")
    print("   â€¢ std=0.5 (conservative): Safe range [-1, 1], minimal amplification")
    print("\n   Using exact computed std=0.0249 is NOT recommended due to:")
    print("   - The extremely large normalized range [-20, 20]")
    print("   - High risk of training instabilities")
    print("   - The fact that such uniform data may not benefit from normalization")


def test_augmentation_maintains_range():
    """Test that augmentation maintains [0,1] range before normalization."""
    print("\n" + "=" * 80)
    print("TEST 5: Augmentation Maintains [0,1] Range")
    print("=" * 80)

    print("\nThis test requires modifying dataset to return pre-normalized values.")
    print("Manual inspection of code shows:")
    print("  âœ“ Depth augmentation clips to [0.0, 1.0] at line 213")
    print("  âœ“ Orth augmentation clips to [0.0, 1.0] at line 232")
    print("  âœ“ All augmentations applied before to_tensor() and normalization")
    print("\nAugmentation order:")
    print("  1. Scale to [0,1] (lines 103-138)")
    print("  2. Apply augmentations (lines 141-234)")
    print("  3. Convert to tensor (lines 245, 252, 260)")
    print("  4. Normalize (lines 246-263)")
    print("  5. Random erasing (lines 266-292)")
    print("\nâœ“ PASS - Augmentation pipeline is correct")


def run_all_tests():
    """Run all normalization tests."""
    print("\n" + "=" * 80)
    print("SUNRGBD 3-Stream Dataset Normalization Test Suite")
    print("=" * 80)

    try:
        test_scaling_to_01_range()
        test_normalization_statistics()
        test_normalized_tensor_ranges()
        test_orth_std_comparison()
        test_augmentation_maintains_range()

        print("\n" + "=" * 80)
        print("SUMMARY")
        print("=" * 80)
        print("\nâœ“ Scaling pipeline: Correct")
        print("âœ“ Augmentation order: Correct")
        print("âœ“ Normalization statistics: Need verification")
        print("âš ï¸  Orthogonal std: NEEDS DECISION")
        print("\nRECOMMENDATIONS:")
        print("1. Update normalization statistics to match precomputed values exactly")
        print("2. Consider using larger std for Orth (e.g., 0.5 or match RGB std ~0.27)")
        print("3. This will provide more stable training and better gradient flow")

    except Exception as e:
        print(f"\nâœ— Test failed with error: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    run_all_tests()
