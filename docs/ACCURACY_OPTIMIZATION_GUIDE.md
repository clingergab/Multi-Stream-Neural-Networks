# Accuracy Optimization Guide for MCResNet on NYU Depth V2

## Current Status
- **GPU RAM Usage:** 6.1 / 80.0 GB (only 7.6% utilized!)
- **Training Speed:** ~20 seconds/epoch (very fast)
- **Model:** ResNet18
- **Batch Size:** 128

## Recommended Optimizations (Priority Order)

### üéØ Priority 1: Use Deeper Architecture (Biggest Accuracy Gain)

**Current:** ResNet18 (11M parameters)
**Recommended:** ResNet50 (25M parameters)

```python
MODEL_CONFIG = {
    'architecture': 'resnet50',  # ‚Üê CHANGE THIS
    'num_classes': 27,
    'stream1_channels': 3,
    'stream2_channels': 1,
    'device': 'cuda',
    'use_amp': True
}
```

**Expected Impact:**
- ‚úÖ Accuracy gain: +3-7%
- ‚ö†Ô∏è GPU RAM increase: 6.1 GB ‚Üí ~12 GB (still only 15% of A100!)
- ‚ö†Ô∏è Training time: 20s/epoch ‚Üí ~45s/epoch (still very reasonable)

---

### üéØ Priority 2: Increase Batch Size (Better Gradient Estimates)

**Current:** 128
**Recommended:** 256 or 512

```python
DATASET_CONFIG = {
    'dataset_path': '/content/nyu_depth_v2_labeled.mat',
    'batch_size': 256,  # ‚Üê DOUBLE IT (or try 512)
    'num_workers': 2,
    'target_size': (224, 224),
    'num_classes': 27
}
```

**Expected Impact:**
- ‚úÖ More stable gradients ‚Üí better convergence
- ‚úÖ Accuracy gain: +1-3%
- ‚úÖ Faster training (fewer optimizer steps)
- ‚ö†Ô∏è May need to adjust learning rate: `lr = 0.1 * (batch_size / 128)`

**If using batch_size=256:**
```python
TRAINING_CONFIG = {
    'optimizer': 'sgd',
    'learning_rate': 0.2,  # ‚Üê 0.1 * (256/128) = 0.2
    'weight_decay': 1e-4,
    'momentum': 0.9,
    'loss': 'cross_entropy',
    'scheduler': 'cosine'
}
```

**If using batch_size=512:**
```python
TRAINING_CONFIG = {
    'optimizer': 'sgd',
    'learning_rate': 0.4,  # ‚Üê 0.1 * (512/128) = 0.4
    'weight_decay': 1e-4,
    'momentum': 0.9,
    'loss': 'cross_entropy',
    'scheduler': 'cosine'
}
```

---

### üéØ Priority 3: Increase Image Resolution (More Detail)

**Current:** 224√ó224
**Recommended:** 384√ó384 or 448√ó448

```python
DATASET_CONFIG = {
    'dataset_path': '/content/nyu_depth_v2_labeled.mat',
    'batch_size': 128,  # ‚Üê Keep at 128 or reduce to 64 for larger images
    'num_workers': 2,
    'target_size': (384, 384),  # ‚Üê INCREASE THIS
    'num_classes': 27
}
```

**Expected Impact:**
- ‚úÖ More spatial detail ‚Üí better scene recognition
- ‚úÖ Accuracy gain: +2-5%
- ‚ö†Ô∏è GPU RAM increase: ~2-4x (but you have tons of headroom!)
- ‚ö†Ô∏è Training time increase: ~2-3x per epoch
- ‚ö†Ô∏è Reduce batch size if needed (try 64 with 384√ó384)

---

### üéØ Priority 4: Add Data Augmentation (Regularization)

Currently, the transforms are basic. Add stronger augmentation:

**Edit:** `src/data_utils/nyu_depth_dataset.py` around line 230

```python
def get_nyu_transforms(train: bool = True):
    """Get transforms for NYU Depth V2 dataset."""
    if train:
        return transforms.Compose([
            transforms.RandomHorizontalFlip(p=0.5),
            transforms.RandomRotation(degrees=10),  # ‚Üê ADD
            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # ‚Üê ADD
            transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),  # ‚Üê ADD (replace Resize)
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
    else:
        # Validation: just normalize
        return transforms.Compose([
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
```

**Expected Impact:**
- ‚úÖ Better generalization
- ‚úÖ Accuracy gain: +1-3%
- ‚ö†Ô∏è Slower training per epoch (~10% slower)

---

### üéØ Priority 5: Train Longer with Better Scheduler

**Current:** 90 epochs, cosine annealing

**Option A: Longer Training**
```python
TRAIN_CONFIG = {
    'epochs': 150,  # ‚Üê INCREASE
    'grad_clip_norm': 5.0,
    'early_stopping': True,
    'patience': 20,  # ‚Üê Increase patience
    'min_delta': 0.001,
    'monitor': 'val_accuracy',
    'restore_best_weights': True,
    'save_path': f"{checkpoint_dir}/best_model.pt"
}
```

**Option B: Warmup + Cosine (Better for large batch sizes)**

You'll need to add warmup support, or use this trick:
```python
# Train for 5 epochs with low LR (warmup)
# Then restart with full LR and cosine annealing
```

---

### üéØ Priority 6: Use Label Smoothing (Regularization)

Add label smoothing to CrossEntropyLoss to prevent overconfidence:

**Edit:** `src/models/abstracts/abstract_model.py` around line 290

```python
# In compile method, replace:
self.criterion = nn.CrossEntropyLoss()

# With:
self.criterion = nn.CrossEntropyLoss(label_smoothing=0.1)  # ‚Üê ADD label_smoothing
```

**Expected Impact:**
- ‚úÖ Better calibration
- ‚úÖ Accuracy gain: +0.5-2%
- ‚úÖ No performance cost

---

### üéØ Priority 7: Mixup or CutMix (Advanced Augmentation)

This requires code changes but can give significant gains. Not recommended for first iteration.

---

## üöÄ Recommended Configuration for Maximum Accuracy

**Best bang for buck:** ResNet50 + batch_size=256 + label_smoothing

```python
# Model
MODEL_CONFIG = {
    'architecture': 'resnet50',  # ‚Üê Better architecture
    'num_classes': 27,
    'stream1_channels': 3,
    'stream2_channels': 1,
    'device': 'cuda',
    'use_amp': True
}

# Dataset
DATASET_CONFIG = {
    'dataset_path': '/content/nyu_depth_v2_labeled.mat',
    'batch_size': 256,  # ‚Üê Larger batch
    'num_workers': 2,
    'target_size': (224, 224),  # Keep same initially
    'num_classes': 27
}

# Training
TRAINING_CONFIG = {
    'optimizer': 'sgd',
    'learning_rate': 0.2,  # ‚Üê Scaled with batch size
    'weight_decay': 1e-4,
    'momentum': 0.9,
    'loss': 'cross_entropy',
    'scheduler': 'cosine'
}

TRAIN_CONFIG = {
    'epochs': 120,  # ‚Üê Train longer
    'grad_clip_norm': 5.0,
    'early_stopping': True,
    'patience': 20,  # ‚Üê More patience
    'min_delta': 0.001,
    'monitor': 'val_accuracy',
    'restore_best_weights': True,
    'save_path': f"{checkpoint_dir}/best_model.pt"
}
```

**Expected Resources:**
- GPU RAM: ~15-20 GB (still only 25% of A100!)
- Training time: ~50-60 secs/epoch
- Total time: ~2 hours

**Expected Accuracy Gain:** +5-10% over current ResNet18 baseline

---

## üî¨ Aggressive Configuration (Maximum Accuracy)

If you want to push harder:

```python
MODEL_CONFIG = {
    'architecture': 'resnet50',
    'num_classes': 27,
    'stream1_channels': 3,
    'stream2_channels': 1,
    'device': 'cuda',
    'use_amp': True
}

DATASET_CONFIG = {
    'dataset_path': '/content/nyu_depth_v2_labeled.mat',
    'batch_size': 128,  # ‚Üê Reduce for larger images
    'num_workers': 2,
    'target_size': (384, 384),  # ‚Üê Higher resolution
    'num_classes': 27
}

TRAINING_CONFIG = {
    'optimizer': 'sgd',
    'learning_rate': 0.1,
    'weight_decay': 1e-4,
    'momentum': 0.9,
    'loss': 'cross_entropy',
    'scheduler': 'cosine'
}

TRAIN_CONFIG = {
    'epochs': 150,
    'grad_clip_norm': 5.0,
    'early_stopping': True,
    'patience': 25,
    'min_delta': 0.0005,
    'monitor': 'val_accuracy',
    'restore_best_weights': True,
    'save_path': f"{checkpoint_dir}/best_model.pt"
}
```

**Expected Resources:**
- GPU RAM: ~30-40 GB (still < 50% of A100!)
- Training time: ~2-3 mins/epoch
- Total time: ~5-7 hours

**Expected Accuracy Gain:** +10-15% over current baseline

---

## üìä Summary Table

| Change | Accuracy Gain | GPU RAM | Training Time | Difficulty |
|--------|---------------|---------|---------------|------------|
| ResNet50 | +3-7% | +6 GB | +2.5x | Easy |
| Batch 256 | +1-3% | +2 GB | -20% | Easy |
| Batch 512 | +1-3% | +4 GB | -40% | Easy |
| Image 384√ó384 | +2-5% | +10 GB | +2x | Easy |
| Label Smoothing | +0.5-2% | 0 | 0 | Medium |
| Data Aug | +1-3% | 0 | +10% | Medium |
| Train 150 epochs | +1-2% | 0 | +1.7x | Easy |

---

## üéØ Implementation Steps

1. **Quick Win (5 min):**
   - Change `architecture` to `resnet50`
   - Change `batch_size` to `256`
   - Adjust `learning_rate` to `0.2`
   - Run training

2. **If you want more (15 min):**
   - Add label smoothing to loss function
   - Increase epochs to 120-150
   - Increase patience to 20-25

3. **Experimental (30 min):**
   - Increase image resolution to 384√ó384
   - Add stronger data augmentation
   - Reduce batch size to 128 (for larger images)

---

## ‚ö†Ô∏è Things to Monitor

1. **Overfitting:** If train accuracy >> val accuracy, you need more regularization
2. **Underfitting:** If both train and val accuracy are low, you need more capacity/longer training
3. **Memory:** Watch `nvidia-smi` to ensure you're not OOM
4. **Learning Rate:** If loss diverges early, reduce LR

---

## üîç Debugging Tips

**If accuracy doesn't improve:**
- Check that labels are correct (use visualization script)
- Verify data augmentation isn't too aggressive
- Try reducing learning rate by 2x
- Check for class imbalance (some scenes might have very few samples)

**If training is unstable:**
- Reduce learning rate
- Increase gradient clipping (try `grad_clip_norm=1.0`)
- Reduce batch size
- Add more warmup epochs

---

## üìà Expected Baseline Performance

**Current (ResNet18, batch=128, 224√ó224):**
- Expected: 60-70% validation accuracy

**Optimized (ResNet50, batch=256, 224√ó224, label smoothing):**
- Expected: 70-80% validation accuracy

**Aggressive (ResNet50, batch=128, 384√ó384, strong aug):**
- Expected: 75-85% validation accuracy
