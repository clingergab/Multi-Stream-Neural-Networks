{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f0aae91",
   "metadata": {
    "id": "4f0aae91"
   },
   "source": [
    "# ğŸš€ Multi-Stream Neural Networks\n",
    "\n",
    "This notebook demonstrates the full pipeline for training multi-stream neural networks:\n",
    "\n",
    "### âœ¨ **Key Features**\n",
    "- **ğŸ”§ Unified API Design**: Consistent interface across all models\n",
    "- **ğŸ¯ Two Fusion Strategies**: Shared classifier (recommended) vs separate classifiers\n",
    "- **ğŸ—ï¸ Multiple Architectures**: Dense networks and CNN (ResNet) models\n",
    "- **âš¡ GPU Optimization**: Automatic device detection with mixed precision\n",
    "- **ğŸ“Š Research Tools**: Pathway analysis for multi-stream insights\n",
    "\n",
    "### ğŸ›ï¸ **Model Architectures**\n",
    "1. **BaseMultiChannelNetwork**: Dense/fully-connected multi-stream processing\n",
    "2. **MultiChannelResNetNetwork**: CNN with residual connections for spatial features\n",
    "\n",
    "### ğŸ“š **API Design Philosophy**\n",
    "- **`model(color, brightness)`** â†’ Single tensor for training/inference\n",
    "- **`model.analyze_pathways(color, brightness)`** â†’ Tuple for research analysis\n",
    "- **Keras-like training**: `.fit()`, `.evaluate()`, `.predict()` methods\n",
    "- **Production ready**: Built-in device management, mixed precision, early stopping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7051aee4",
   "metadata": {
    "id": "7051aee4"
   },
   "source": [
    "## ğŸ› ï¸ Environment Setup & Requirements\n",
    "\n",
    "### Prerequisites\n",
    "- **Python 3.8+**\n",
    "- **PyTorch 1.12+** with CUDA support (recommended)\n",
    "- **Google Colab** (this notebook) or local Jupyter environment\n",
    "\n",
    "### ğŸ“ Project Structure\n",
    "Our codebase is now fully modularized:\n",
    "```\n",
    "Multi-Stream-Neural-Networks/\n",
    "â”œâ”€â”€ src/\n",
    "â”‚   â”œâ”€â”€ models/basic_multi_channel/     # Core model implementations\n",
    "â”‚   â”‚   â”œâ”€â”€ base_multi_channel_network.py    # Dense model\n",
    "â”‚   â”‚   â””â”€â”€ multi_channel_resnet_network.py  # CNN model\n",
    "â”‚   â”œâ”€â”€ utils/cifar100_loader.py        # CIFAR-100 data utilities\n",
    "â”‚   â”œâ”€â”€ transforms/rgb_to_rgbl.py       # RGBâ†’Brightness transform\n",
    "â”‚   â””â”€â”€ utils/device_utils.py           # GPU optimization utilities\n",
    "â”œâ”€â”€ test_end_to_end.py                  # Comprehensive testing\n",
    "â””â”€â”€ data/cifar-100/                     # Dataset location\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e80196",
   "metadata": {},
   "source": [
    "## 1. Environment Setup: Mount Drive and Navigate to Project\n",
    "\n",
    "Mount Google Drive and navigate to the Multi-Stream Neural Networks project directory to begin the training workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823d044d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57873d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Navigate to Drive and project directory\n",
    "import os\n",
    "os.chdir('/content/drive/MyDrive')\n",
    "\n",
    "# Navigate to the existing project (assuming it's already cloned)\n",
    "project_path = '/content/drive/MyDrive/Multi-Stream-Neural-Networks'\n",
    "if os.path.exists(project_path):\n",
    "    os.chdir(project_path)\n",
    "    print(f\"âœ… Found project at: {project_path}\")\n",
    "else:\n",
    "    print(f\"âŒ Project not found at: {project_path}\")\n",
    "    print(\"ğŸ’¡ Please clone the repository first:\")\n",
    "    print(\"   !git clone https://github.com/clingergab/Multi-Stream-Neural-Networks.git\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1aa368",
   "metadata": {},
   "source": [
    "## 2. Install Dependencies and Import Libraries\n",
    "\n",
    "Install compatible PyTorch/NumPy versions and import all required libraries for the multi-stream neural network training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "xo3JWyNVEGMZ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xo3JWyNVEGMZ",
    "outputId": "34f5af2f-a23c-4e75-848f-94bbf88fffb4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¦ Installing required dependencies...\n",
      "Installing packages...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… torch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… torchvision\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… numpy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… seaborn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… tqdm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… scikit-learn\n",
      "âœ… Pillow\n",
      "\n",
      "ğŸ“š Importing libraries...\n",
      "âœ… All libraries imported successfully!\n",
      "\n",
      "ğŸ”§ PyTorch Setup:\n",
      "   PyTorch version: 2.7.0\n",
      "   CUDA available: False\n",
      "   Using CPU (consider GPU for faster training)\n",
      "\n",
      "ğŸ¯ Dependencies and imports complete!\n",
      "âœ… Pillow\n",
      "\n",
      "ğŸ“š Importing libraries...\n",
      "âœ… All libraries imported successfully!\n",
      "\n",
      "ğŸ”§ PyTorch Setup:\n",
      "   PyTorch version: 2.7.0\n",
      "   CUDA available: False\n",
      "   Using CPU (consider GPU for faster training)\n",
      "\n",
      "ğŸ¯ Dependencies and imports complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install Dependencies and Import Libraries\n",
    "print(\"ğŸ“¦ Installing required dependencies...\")\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_package(package):\n",
    "    \"\"\"Install a package if not already installed.\"\"\"\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package, \"--quiet\"])\n",
    "        return True\n",
    "    except subprocess.CalledProcessError:\n",
    "        return False\n",
    "\n",
    "# Required packages\n",
    "packages = [\n",
    "    \"torch\",\n",
    "    \"torchvision\", \n",
    "    \"numpy\",\n",
    "    \"matplotlib\",\n",
    "    \"seaborn\",\n",
    "    \"tqdm\",\n",
    "    \"scikit-learn\",\n",
    "    \"Pillow\"\n",
    "]\n",
    "\n",
    "print(\"Installing packages...\")\n",
    "for package in packages:\n",
    "    if install_package(package):\n",
    "        print(f\"âœ… {package}\")\n",
    "    else:\n",
    "        print(f\"âŒ Failed to install {package}\")\n",
    "\n",
    "print(\"\\nğŸ“š Importing libraries...\")\n",
    "\n",
    "# Core libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Data handling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "\n",
    "# Progress tracking\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Add project root to path for imports\n",
    "project_root = Path('.').resolve()\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.append(str(project_root))\n",
    "\n",
    "print(\"âœ… All libraries imported successfully!\")\n",
    "\n",
    "# Check PyTorch setup\n",
    "print(f\"\\nğŸ”§ PyTorch Setup:\")\n",
    "print(f\"   PyTorch version: {torch.__version__}\")\n",
    "print(f\"   CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   CUDA version: {torch.version.cuda}\")\n",
    "else:\n",
    "    print(\"   Using CPU (consider GPU for faster training)\")\n",
    "\n",
    "print(\"\\nğŸ¯ Dependencies and imports complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1955bd1d",
   "metadata": {},
   "source": [
    "## 3. Update Repository\n",
    "\n",
    "Pull the latest changes from the repository to ensure we have the most recent codebase and model implementations.\n",
    "\n",
    "## ğŸ“Š Data Loading and Preprocessing\n",
    "\n",
    "We'll use our **optimized CIFAR-100 data loader** that handles:\n",
    "- âœ… **Automatic download** and caching\n",
    "- âœ… **Train/Validation/Test splits** with proper stratification  \n",
    "- âœ… **RGB â†’ Brightness conversion** using luminance weights\n",
    "- âœ… **Tensor formatting** ready for PyTorch models\n",
    "- âœ… **Memory efficient** processing for large datasets\n",
    "\n",
    "### ğŸ¨ Multi-Stream Data Strategy\n",
    "- **RGB Stream**: Full color information (3 channels)\n",
    "- **Brightness Stream**: Luminance-based brightness (1 channel)\n",
    "- **Combined Processing**: Fusion strategies for optimal performance\n",
    "\n",
    "The data loader ensures both streams are properly aligned and normalized for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e805e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update repository with latest changes\n",
    "print(\"ğŸ”„ Pulling latest changes from repository...\")\n",
    "\n",
    "# Make sure we're in the right directory\n",
    "os.chdir('/content/drive/MyDrive/Multi-Stream-Neural-Networks')\n",
    "print(f\"ğŸ“ Current directory: {os.getcwd()}\")\n",
    "\n",
    "# Pull latest changes\n",
    "!git pull origin main\n",
    "\n",
    "# # Show latest commit info\n",
    "# print(\"\\nğŸ“‹ Latest commit:\")\n",
    "# !git log --oneline -1\n",
    "\n",
    "# # Check status\n",
    "# print(\"\\nğŸ“Š Repository status:\")\n",
    "# !git status --short\n",
    "\n",
    "print(\"\\nâœ… Repository update complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac36a39",
   "metadata": {
    "id": "3ac36a39"
   },
   "source": [
    "## 4. Load CIFAR-100 Dataset\n",
    "\n",
    "Load the CIFAR-100 dataset using our optimized data loader that handles automatic download, caching, and preprocessing for multi-stream neural networks.\n",
    "\n",
    "### ğŸ¨ Multi-Stream Data Strategy\n",
    "- **RGB Stream**: Original 3-channel color information for spatial features\n",
    "- **Brightness Stream**: Single-channel luminance for contrast/lighting patterns  \n",
    "- **Unified Processing**: Consistent transforms and data loaders for both streams\n",
    "\n",
    "## ğŸ‘ï¸ Data Visualization\n",
    "\n",
    "Let's visualize our multi-stream data to understand how the **RGB and brightness streams** complement each other for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2895f8fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Setting up CIFAR-100 dataset loading...\n",
      "âœ… CIFAR-100 loader utilities imported successfully\n",
      "ğŸ“ Loading CIFAR-100 datasets with train/validation/test split...\n",
      "âŒ Error loading CIFAR-100 data: get_cifar100_datasets() got an unexpected keyword argument 'root'\n",
      "\n",
      "ğŸ’¡ Troubleshooting:\n",
      "   1. Check internet connection for CIFAR-100 download\n",
      "   2. Verify data directory permissions\n",
      "   3. Try clearing cache: rm -rf data/cifar-100\n",
      "   4. Check if src/utils/cifar100_loader.py exists\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "get_cifar100_datasets() got an unexpected keyword argument 'root'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 16\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mğŸ“ Loading CIFAR-100 datasets with train/validation/test split...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# Load datasets using our optimized loader\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m     train_dataset, val_dataset, test_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mget_cifar100_datasets\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./data\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdownload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43mval_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 10% validation split from training data\u001b[39;49;00m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâœ… CIFAR-100 datasets loaded successfully!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   ğŸ“Š Training samples: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_dataset)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: get_cifar100_datasets() got an unexpected keyword argument 'root'"
     ]
    }
   ],
   "source": [
    "# ğŸ“Š CIFAR-100 Data Loading and Verification\n",
    "print(\"ğŸ“ Setting up CIFAR-100 dataset loading...\")\n",
    "\n",
    "try:\n",
    "    from src.utils.cifar100_loader import get_cifar100_datasets, CIFAR100_FINE_LABELS, SimpleDataset\n",
    "    print(\"âœ… CIFAR-100 loader utilities imported successfully\")\n",
    "except ImportError:\n",
    "    print(\"âŒ Failed to import CIFAR-100 utilities. Make sure src/utils/cifar100_loader.py exists\")\n",
    "    raise\n",
    "\n",
    "# Load CIFAR-100 datasets\n",
    "print(\"ğŸ“ Loading CIFAR-100 datasets with train/validation/test split...\")\n",
    "\n",
    "try:\n",
    "    # Load datasets using our optimized loader\n",
    "    train_dataset, val_dataset, test_dataset = get_cifar100_datasets(\n",
    "        root='./data', \n",
    "        download=True,\n",
    "        val_split=0.1  # 10% validation split from training data\n",
    "    )\n",
    "    \n",
    "    print(\"âœ… CIFAR-100 datasets loaded successfully!\")\n",
    "    print(f\"   ğŸ“Š Training samples: {len(train_dataset):,}\")\n",
    "    print(f\"   ğŸ“Š Validation samples: {len(val_dataset):,}\")\n",
    "    print(f\"   ğŸ“Š Test samples: {len(test_dataset):,}\")\n",
    "    print(f\"   ğŸ·ï¸ Number of classes: {len(CIFAR100_FINE_LABELS)}\")\n",
    "    \n",
    "    # Get sample data for verification\n",
    "    sample_data, sample_label = train_dataset[0]\n",
    "    print(f\"   ğŸ¨ Image shape: {sample_data.shape}\")\n",
    "    print(f\"   ğŸ“‹ Label type: {type(sample_label)}\")\n",
    "    print(f\"   ğŸ“‹ Sample class: {CIFAR100_FINE_LABELS[sample_label]}\")\n",
    "    \n",
    "    # Verify all datasets have the same structure\n",
    "    val_data, val_label = val_dataset[0]\n",
    "    test_data, test_label = test_dataset[0]\n",
    "    \n",
    "    assert sample_data.shape == val_data.shape == test_data.shape, \"Inconsistent data shapes!\"\n",
    "    print(f\"   âœ… All datasets have consistent structure: {sample_data.shape}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error loading CIFAR-100 data: {e}\")\n",
    "    print(\"\\nğŸ’¡ Troubleshooting:\")\n",
    "    print(\"   1. Check internet connection for CIFAR-100 download\")\n",
    "    print(\"   2. Verify data directory permissions\")\n",
    "    print(\"   3. Try clearing cache: rm -rf data/cifar-100\")\n",
    "    print(\"   4. Check if src/utils/cifar100_loader.py exists\")\n",
    "    raise\n",
    "\n",
    "print(\"\\nğŸ¯ Data loading complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a78fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ”„ Data Processing: RGB to RGB+L (Brightness) Conversion\n",
    "print(\"ğŸ”„ Converting RGB images to RGB + Brightness streams...\")\n",
    "\n",
    "try:\n",
    "    from src.transforms.rgb_to_rgbl import RGBtoRGBL\n",
    "    print(\"âœ… RGB to RGB+L transform imported successfully\")\n",
    "except ImportError:\n",
    "    print(\"âŒ Failed to import RGB to RGB+L transform. Make sure src/transforms/rgb_to_rgbl.py exists\")\n",
    "    raise\n",
    "\n",
    "# Initialize the transform\n",
    "rgb_to_rgbl = RGBtoRGBL()\n",
    "\n",
    "# Function to process a dataset batch-wise for memory efficiency\n",
    "# NOTE: This could be moved to src/utils/data_processing.py if multi-stream \n",
    "# processing becomes a common pattern across the project\n",
    "def process_dataset_to_streams(dataset, batch_size=1000, desc=\"Processing\"):\n",
    "    \"\"\"\n",
    "    Convert RGB dataset to RGB + Brightness streams efficiently.\n",
    "    \n",
    "    This function processes datasets in batches to manage memory usage while\n",
    "    applying the RGB to RGB+L transformation for multi-stream neural networks.\n",
    "    \n",
    "    Args:\n",
    "        dataset: Dataset with RGB images (PyTorch dataset format)\n",
    "        batch_size: Size of batches for memory-efficient processing\n",
    "        desc: Description for progress bar\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (rgb_stream, brightness_stream, labels_tensor)\n",
    "    \"\"\"\n",
    "    rgb_tensors = []\n",
    "    brightness_tensors = []\n",
    "    labels = []\n",
    "    \n",
    "    # Process in batches to manage memory\n",
    "    for i in tqdm(range(0, len(dataset), batch_size), desc=desc):\n",
    "        batch_end = min(i + batch_size, len(dataset))\n",
    "        batch_data = []\n",
    "        batch_labels = []\n",
    "        \n",
    "        # Collect batch data\n",
    "        for j in range(i, batch_end):\n",
    "            data, label = dataset[j]\n",
    "            batch_data.append(data)\n",
    "            batch_labels.append(label)\n",
    "        \n",
    "        # Convert to tensor batch\n",
    "        batch_tensor = torch.stack(batch_data)\n",
    "        \n",
    "        # Apply RGB to RGB+L transform using project utility\n",
    "        rgb_batch, brightness_batch = rgb_to_rgbl(batch_tensor)\n",
    "        \n",
    "        rgb_tensors.append(rgb_batch)\n",
    "        brightness_tensors.append(brightness_batch)\n",
    "        labels.extend(batch_labels)\n",
    "    \n",
    "    # Concatenate all batches\n",
    "    rgb_stream = torch.cat(rgb_tensors, dim=0)\n",
    "    brightness_stream = torch.cat(brightness_tensors, dim=0)\n",
    "    labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
    "    \n",
    "    return rgb_stream, brightness_stream, labels_tensor\n",
    "\n",
    "# Process all datasets using the workflow-specific function\n",
    "print(\"Processing training dataset...\")\n",
    "train_rgb, train_brightness, train_labels_tensor = process_dataset_to_streams(\n",
    "    train_dataset, desc=\"Training data\"\n",
    ")\n",
    "\n",
    "print(\"Processing validation dataset...\")\n",
    "val_rgb, val_brightness, val_labels_tensor = process_dataset_to_streams(\n",
    "    val_dataset, desc=\"Validation data\"\n",
    ")\n",
    "\n",
    "print(\"Processing test dataset...\")\n",
    "test_rgb, test_brightness, test_labels_tensor = process_dataset_to_streams(\n",
    "    test_dataset, desc=\"Test data\"\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… Multi-stream conversion complete!\")\n",
    "print(f\"   ğŸ¨ RGB stream shape: {train_rgb.shape}\")\n",
    "print(f\"   ğŸ’¡ Brightness stream shape: {train_brightness.shape}\")\n",
    "print(f\"   ğŸ“Š RGB range: [{train_rgb.min():.3f}, {train_rgb.max():.3f}]\")\n",
    "print(f\"   ğŸ“Š Brightness range: [{train_brightness.min():.3f}, {train_brightness.max():.3f}]\")\n",
    "\n",
    "# Memory usage estimation\n",
    "rgb_memory = (train_rgb.nbytes + val_rgb.nbytes + test_rgb.nbytes) / 1e6\n",
    "brightness_memory = (train_brightness.nbytes + val_brightness.nbytes + test_brightness.nbytes) / 1e6\n",
    "total_memory = rgb_memory + brightness_memory\n",
    "\n",
    "print(f\"\\nğŸ“ˆ Processing Summary:\")\n",
    "print(f\"   ğŸ“Š Total samples processed: {len(train_labels_tensor) + len(val_labels_tensor) + len(test_labels_tensor):,}\")\n",
    "print(f\"   ğŸ¨ RGB streams memory: {rgb_memory:.1f} MB\")\n",
    "print(f\"   ğŸ’¡ Brightness streams memory: {brightness_memory:.1f} MB\")\n",
    "print(f\"   ğŸ’¾ Total memory usage: {total_memory:.1f} MB\")\n",
    "\n",
    "print(\"\\nğŸ¯ Data processing complete!\")\n",
    "print(\"   âœ… Using project RGB to RGB+L transformation utility\")\n",
    "print(\"   âœ… Batch processing ensures memory efficiency\")\n",
    "print(\"   âœ… Multi-stream data ready for neural network training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb2f71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… Processed Data Structure Verification\n",
    "print(\"ğŸ” Verifying processed data structure and consistency...\")\n",
    "\n",
    "# Verify tensor shapes and types\n",
    "def verify_data_integrity(rgb_data, brightness_data, labels, split_name):\n",
    "    \"\"\"Verify data integrity for a dataset split\"\"\"\n",
    "    print(f\"\\nğŸ“Š {split_name} Dataset Verification:\")\n",
    "    \n",
    "    # Check shapes\n",
    "    print(f\"   ğŸ¨ RGB shape: {rgb_data.shape}\")\n",
    "    print(f\"   ğŸ’¡ Brightness shape: {brightness_data.shape}\")\n",
    "    print(f\"   ğŸ·ï¸ Labels shape: {labels.shape}\")\n",
    "    \n",
    "    # Check data types\n",
    "    print(f\"   ğŸ“‹ RGB dtype: {rgb_data.dtype}\")\n",
    "    print(f\"   ğŸ“‹ Brightness dtype: {brightness_data.dtype}\")\n",
    "    print(f\"   ğŸ“‹ Labels dtype: {labels.dtype}\")\n",
    "    \n",
    "    # Check consistency\n",
    "    assert rgb_data.shape[0] == brightness_data.shape[0] == labels.shape[0], f\"Inconsistent sample counts in {split_name}!\"\n",
    "    assert rgb_data.shape[1:] == (3, 32, 32), f\"Unexpected RGB shape in {split_name}!\"\n",
    "    assert brightness_data.shape[1:] == (1, 32, 32), f\"Unexpected brightness shape in {split_name}!\"\n",
    "    \n",
    "    # Check value ranges\n",
    "    rgb_min, rgb_max = rgb_data.min().item(), rgb_data.max().item()\n",
    "    brightness_min, brightness_max = brightness_data.min().item(), brightness_data.max().item()\n",
    "    \n",
    "    print(f\"   ğŸ“ˆ RGB range: [{rgb_min:.3f}, {rgb_max:.3f}]\")\n",
    "    print(f\"   ğŸ“ˆ Brightness range: [{brightness_min:.3f}, {brightness_max:.3f}]\")\n",
    "    \n",
    "    # Check labels range\n",
    "    label_min, label_max = labels.min().item(), labels.max().item()\n",
    "    print(f\"   ğŸ“ˆ Labels range: [{label_min}, {label_max}]\")\n",
    "    \n",
    "    assert 0 <= label_min and label_max < 100, f\"Invalid label range in {split_name}!\"\n",
    "    \n",
    "    print(f\"   âœ… {split_name} data integrity verified!\")\n",
    "    \n",
    "    return {\n",
    "        'samples': rgb_data.shape[0],\n",
    "        'rgb_range': (rgb_min, rgb_max),\n",
    "        'brightness_range': (brightness_min, brightness_max),\n",
    "        'label_range': (label_min, label_max)\n",
    "    }\n",
    "\n",
    "# Verify all datasets\n",
    "train_stats = verify_data_integrity(train_rgb, train_brightness, train_labels_tensor, \"Training\")\n",
    "val_stats = verify_data_integrity(val_rgb, val_brightness, val_labels_tensor, \"Validation\")\n",
    "test_stats = verify_data_integrity(test_rgb, test_brightness, test_labels_tensor, \"Test\")\n",
    "\n",
    "# Cross-dataset consistency checks\n",
    "print(f\"\\nğŸ”„ Cross-Dataset Consistency Checks:\")\n",
    "\n",
    "# Check RGB ranges are consistent\n",
    "all_rgb_ranges = [train_stats['rgb_range'], val_stats['rgb_range'], test_stats['rgb_range']]\n",
    "rgb_min_all = min(r[0] for r in all_rgb_ranges)\n",
    "rgb_max_all = max(r[1] for r in all_rgb_ranges)\n",
    "print(f\"   ğŸ¨ Overall RGB range: [{rgb_min_all:.3f}, {rgb_max_all:.3f}]\")\n",
    "\n",
    "# Check brightness ranges are consistent\n",
    "all_brightness_ranges = [train_stats['brightness_range'], val_stats['brightness_range'], test_stats['brightness_range']]\n",
    "brightness_min_all = min(r[0] for r in all_brightness_ranges)\n",
    "brightness_max_all = max(r[1] for r in all_brightness_ranges)\n",
    "print(f\"   ğŸ’¡ Overall brightness range: [{brightness_min_all:.3f}, {brightness_max_all:.3f}]\")\n",
    "\n",
    "# Check all datasets have full label coverage\n",
    "all_labels = torch.cat([train_labels_tensor, val_labels_tensor, test_labels_tensor])\n",
    "unique_labels = torch.unique(all_labels)\n",
    "print(f\"   ğŸ·ï¸ Unique labels found: {len(unique_labels)}/100\")\n",
    "\n",
    "if len(unique_labels) == 100:\n",
    "    print(f\"   âœ… All 100 CIFAR-100 classes represented!\")\n",
    "else:\n",
    "    missing_labels = set(range(100)) - set(unique_labels.tolist())\n",
    "    print(f\"   âš ï¸ Missing labels: {missing_labels}\")\n",
    "\n",
    "# Summary statistics\n",
    "total_samples = train_stats['samples'] + val_stats['samples'] + test_stats['samples']\n",
    "print(f\"\\nğŸ“ˆ Final Data Summary:\")\n",
    "print(f\"   ğŸ“Š Total samples: {total_samples:,}\")\n",
    "print(f\"   ğŸ“Š Training: {train_stats['samples']:,} ({train_stats['samples']/total_samples*100:.1f}%)\")\n",
    "print(f\"   ğŸ“Š Validation: {val_stats['samples']:,} ({val_stats['samples']/total_samples*100:.1f}%)\")\n",
    "print(f\"   ğŸ“Š Test: {test_stats['samples']:,} ({test_stats['samples']/total_samples*100:.1f}%)\")\n",
    "print(f\"   ğŸ¯ Ready for multi-stream model training!\")\n",
    "\n",
    "print(\"\\nâœ… All data verification checks passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b744428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ‘ï¸ Sample Image Visualization: RGB vs Brightness Streams\n",
    "print(\"ğŸ‘ï¸ Visualizing sample images from both RGB and brightness streams...\")\n",
    "\n",
    "# Set up visualization\n",
    "plt.style.use('default')\n",
    "fig, axes = plt.subplots(3, 8, figsize=(16, 6))\n",
    "fig.suptitle('ğŸ¨ Multi-Stream CIFAR-100 Samples: RGB vs Brightness', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Select random samples from training data\n",
    "np.random.seed(42)  # For reproducible results\n",
    "sample_indices = np.random.choice(len(train_rgb), 4, replace=False)\n",
    "\n",
    "for i, idx in enumerate(sample_indices):\n",
    "    # Get data\n",
    "    rgb_img = train_rgb[idx]\n",
    "    brightness_img = train_brightness[idx]\n",
    "    label = train_labels_tensor[idx].item()\n",
    "    class_name = CIFAR100_FINE_LABELS[label]\n",
    "    \n",
    "    # RGB image (convert from tensor to numpy)\n",
    "    rgb_np = rgb_img.permute(1, 2, 0).numpy()\n",
    "    rgb_np = np.clip(rgb_np, 0, 1)  # Ensure valid range\n",
    "    \n",
    "    # Brightness image\n",
    "    brightness_np = brightness_img.squeeze().numpy()\n",
    "    \n",
    "    # Plot RGB\n",
    "    axes[0, i*2].imshow(rgb_np)\n",
    "    axes[0, i*2].set_title(f'RGB\\n{class_name}', fontsize=10, fontweight='bold')\n",
    "    axes[0, i*2].axis('off')\n",
    "    \n",
    "    # Plot Brightness\n",
    "    axes[0, i*2+1].imshow(brightness_np, cmap='gray')\n",
    "    axes[0, i*2+1].set_title(f'Brightness\\n{class_name}', fontsize=10, fontweight='bold')\n",
    "    axes[0, i*2+1].axis('off')\n",
    "\n",
    "# Add stream comparison for second row\n",
    "sample_indices_2 = np.random.choice(len(train_rgb), 4, replace=False)\n",
    "\n",
    "for i, idx in enumerate(sample_indices_2):\n",
    "    rgb_img = train_rgb[idx]\n",
    "    brightness_img = train_brightness[idx]\n",
    "    label = train_labels_tensor[idx].item()\n",
    "    class_name = CIFAR100_FINE_LABELS[label]\n",
    "    \n",
    "    rgb_np = rgb_img.permute(1, 2, 0).numpy()\n",
    "    rgb_np = np.clip(rgb_np, 0, 1)\n",
    "    brightness_np = brightness_img.squeeze().numpy()\n",
    "    \n",
    "    axes[1, i*2].imshow(rgb_np)\n",
    "    axes[1, i*2].set_title(f'RGB\\n{class_name}', fontsize=10, fontweight='bold')\n",
    "    axes[1, i*2].axis('off')\n",
    "    \n",
    "    axes[1, i*2+1].imshow(brightness_np, cmap='gray')\n",
    "    axes[1, i*2+1].set_title(f'Brightness\\n{class_name}', fontsize=10, fontweight='bold')\n",
    "    axes[1, i*2+1].axis('off')\n",
    "\n",
    "# Third row: different samples\n",
    "sample_indices_3 = np.random.choice(len(train_rgb), 4, replace=False)\n",
    "\n",
    "for i, idx in enumerate(sample_indices_3):\n",
    "    rgb_img = train_rgb[idx]\n",
    "    brightness_img = train_brightness[idx]\n",
    "    label = train_labels_tensor[idx].item()\n",
    "    class_name = CIFAR100_FINE_LABELS[label]\n",
    "    \n",
    "    rgb_np = rgb_img.permute(1, 2, 0).numpy()\n",
    "    rgb_np = np.clip(rgb_np, 0, 1)\n",
    "    brightness_np = brightness_img.squeeze().numpy()\n",
    "    \n",
    "    axes[2, i*2].imshow(rgb_np)\n",
    "    axes[2, i*2].set_title(f'RGB\\n{class_name}', fontsize=10, fontweight='bold')\n",
    "    axes[2, i*2].axis('off')\n",
    "    \n",
    "    axes[2, i*2+1].imshow(brightness_np, cmap='gray')\n",
    "    axes[2, i*2+1].set_title(f'Brightness\\n{class_name}', fontsize=10, fontweight='bold')\n",
    "    axes[2, i*2+1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show data statistics\n",
    "print(f\"\\nğŸ“Š Stream Statistics:\")\n",
    "print(f\"   ğŸ¨ RGB channels: {train_rgb.shape[1]} (Red, Green, Blue)\")\n",
    "print(f\"   ğŸ’¡ Brightness channels: {train_brightness.shape[1]} (Luminance)\")\n",
    "print(f\"   ğŸ“ Image resolution: {train_rgb.shape[2]}x{train_rgb.shape[3]} pixels\")\n",
    "print(f\"   ğŸ·ï¸ Classes sampled: {len(set([train_labels_tensor[idx].item() for idx in sample_indices]))} different\")\n",
    "\n",
    "print(f\"\\nğŸ¯ Multi-stream visualization complete!\")\n",
    "print(f\"   âœ… RGB stream captures full color information\")\n",
    "print(f\"   âœ… Brightness stream captures luminance patterns\") \n",
    "print(f\"   âœ… Both streams provide complementary features for classification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2af4f6",
   "metadata": {},
   "source": [
    "## 5. Data Verification and Structure Analysis\n",
    "\n",
    "Verify the loaded dataset and analyze its structure, including shapes, data types, and class distributions.\n",
    "\n",
    "Now let's dive deeper into the CIFAR-100 dataset with additional analysis to understand:\n",
    "- Class distribution across splits\n",
    "- Brightness vs color feature correlations\n",
    "- Data quality and preprocessing effectiveness\n",
    "- Stream-specific characteristics for optimal model design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee66628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“Š Comprehensive Data Analysis and Visualizations\n",
    "print(\"ğŸ“Š Performing comprehensive data analysis...\")\n",
    "\n",
    "# Import project visualization utilities\n",
    "try:\n",
    "    from src.utils.visualization.training_plots import plot_training_curves, create_training_summary\n",
    "    print(\"âœ… Project visualization utilities imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"âš ï¸ Could not import project visualization utilities: {e}\")\n",
    "    print(\"ğŸ’¡ Using basic matplotlib for visualization\")\n",
    "\n",
    "# Set up matplotlib for better visualizations\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# 1. Class Distribution Analysis\n",
    "print(\"\\nğŸ·ï¸ Analyzing class distribution...\")\n",
    "\n",
    "def analyze_class_distribution():\n",
    "    \"\"\"Analyze class distribution across train/validation/test splits using project standards\"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    fig.suptitle('ğŸ·ï¸ CIFAR-100 Class Distribution Across Splits', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Training distribution\n",
    "    train_counts = np.bincount(train_labels_tensor, minlength=100)\n",
    "    axes[0].bar(range(100), train_counts, alpha=0.7, color='skyblue')\n",
    "    axes[0].set_title(f'Training Set\\n{len(train_labels_tensor):,} samples', fontweight='bold')\n",
    "    axes[0].set_xlabel('Class ID')\n",
    "    axes[0].set_ylabel('Sample Count')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Validation distribution\n",
    "    val_counts = np.bincount(val_labels_tensor, minlength=100)\n",
    "    axes[1].bar(range(100), val_counts, alpha=0.7, color='lightcoral')\n",
    "    axes[1].set_title(f'Validation Set\\n{len(val_labels_tensor):,} samples', fontweight='bold')\n",
    "    axes[1].set_xlabel('Class ID')\n",
    "    axes[1].set_ylabel('Sample Count')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Test distribution\n",
    "    test_counts = np.bincount(test_labels_tensor, minlength=100)\n",
    "    axes[2].bar(range(100), test_counts, alpha=0.7, color='lightgreen')\n",
    "    axes[2].set_title(f'Test Set\\n{len(test_labels_tensor):,} samples', fontweight='bold')\n",
    "    axes[2].set_xlabel('Class ID')\n",
    "    axes[2].set_ylabel('Sample Count')\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print statistics\n",
    "    print(f\"   ğŸ“Š Training: mean={train_counts.mean():.1f}, std={train_counts.std():.1f}\")\n",
    "    print(f\"   ğŸ“Š Validation: mean={val_counts.mean():.1f}, std={val_counts.std():.1f}\")\n",
    "    print(f\"   ğŸ“Š Test: mean={test_counts.mean():.1f}, std={test_counts.std():.1f}\")\n",
    "    \n",
    "    return {'train_counts': train_counts, 'val_counts': val_counts, 'test_counts': test_counts}\n",
    "\n",
    "class_distribution_stats = analyze_class_distribution()\n",
    "\n",
    "# 2. Stream Statistics Analysis\n",
    "print(\"\\nğŸ¨ Analyzing RGB vs Brightness stream characteristics...\")\n",
    "\n",
    "def analyze_stream_characteristics():\n",
    "    \"\"\"Analyze RGB vs Brightness stream characteristics using efficient sampling\"\"\"\n",
    "    \n",
    "    # Sample a subset for analysis (to avoid memory issues)\n",
    "    sample_size = min(1000, len(train_rgb))\n",
    "    indices = np.random.choice(len(train_rgb), sample_size, replace=False)\n",
    "    \n",
    "    rgb_sample = train_rgb[indices]\n",
    "    brightness_sample = train_brightness[indices]\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    fig.suptitle('ğŸ¨ RGB vs Brightness Stream Analysis', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # RGB channel statistics\n",
    "    rgb_means = rgb_sample.mean(axis=(2, 3))  # Mean across height/width\n",
    "    \n",
    "    for i, channel in enumerate(['Red', 'Green', 'Blue']):\n",
    "        axes[0, i].hist(rgb_means[:, i], bins=50, alpha=0.7, color=['red', 'green', 'blue'][i])\n",
    "        axes[0, i].set_title(f'{channel} Channel Mean Distribution', fontweight='bold')\n",
    "        axes[0, i].set_xlabel('Mean Pixel Value')\n",
    "        axes[0, i].set_ylabel('Frequency')\n",
    "        axes[0, i].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Brightness statistics\n",
    "    brightness_means = brightness_sample.mean(axis=(2, 3))\n",
    "    axes[1, 0].hist(brightness_means[:, 0], bins=50, alpha=0.7, color='gray')\n",
    "    axes[1, 0].set_title('Brightness Mean Distribution', fontweight='bold')\n",
    "    axes[1, 0].set_xlabel('Mean Brightness Value')\n",
    "    axes[1, 0].set_ylabel('Frequency')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # RGB vs Brightness correlation\n",
    "    rgb_brightness_corr = np.corrcoef(rgb_means.mean(axis=1), brightness_means[:, 0])[0, 1]\n",
    "    axes[1, 1].scatter(rgb_means.mean(axis=1), brightness_means[:, 0], alpha=0.6, s=10)\n",
    "    axes[1, 1].set_title(f'RGB vs Brightness Correlation\\nr = {rgb_brightness_corr:.3f}', fontweight='bold')\n",
    "    axes[1, 1].set_xlabel('Mean RGB Value')\n",
    "    axes[1, 1].set_ylabel('Mean Brightness Value')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Pixel intensity distributions\n",
    "    rgb_flat = rgb_sample.flatten()\n",
    "    brightness_flat = brightness_sample.flatten()\n",
    "    \n",
    "    axes[1, 2].hist([rgb_flat, brightness_flat], bins=50, alpha=0.7, \n",
    "                   label=['RGB Pixels', 'Brightness Pixels'], color=['blue', 'gray'])\n",
    "    axes[1, 2].set_title('Pixel Intensity Distributions', fontweight='bold')\n",
    "    axes[1, 2].set_xlabel('Pixel Value')\n",
    "    axes[1, 2].set_ylabel('Frequency (log scale)')\n",
    "    axes[1, 2].set_yscale('log')\n",
    "    axes[1, 2].legend()\n",
    "    axes[1, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Statistics summary\n",
    "    stats = {\n",
    "        'rgb_stats': {\n",
    "            'mean': rgb_sample.mean().item(),\n",
    "            'std': rgb_sample.std().item(),\n",
    "            'min': rgb_sample.min().item(),\n",
    "            'max': rgb_sample.max().item()\n",
    "        },\n",
    "        'brightness_stats': {\n",
    "            'mean': brightness_sample.mean().item(),\n",
    "            'std': brightness_sample.std().item(),\n",
    "            'min': brightness_sample.min().item(),\n",
    "            'max': brightness_sample.max().item()\n",
    "        },\n",
    "        'correlation': rgb_brightness_corr\n",
    "    }\n",
    "    \n",
    "    print(f\"   ğŸ¨ RGB statistics:\")\n",
    "    print(f\"      Mean: {stats['rgb_stats']['mean']:.3f}, Std: {stats['rgb_stats']['std']:.3f}\")\n",
    "    print(f\"      Min: {stats['rgb_stats']['min']:.3f}, Max: {stats['rgb_stats']['max']:.3f}\")\n",
    "    print(f\"   ğŸ’¡ Brightness statistics:\")\n",
    "    print(f\"      Mean: {stats['brightness_stats']['mean']:.3f}, Std: {stats['brightness_stats']['std']:.3f}\")\n",
    "    print(f\"      Min: {stats['brightness_stats']['min']:.3f}, Max: {stats['brightness_stats']['max']:.3f}\")\n",
    "    print(f\"   ğŸ”— RGB-Brightness correlation: {stats['correlation']:.3f}\")\n",
    "    \n",
    "    return stats\n",
    "\n",
    "stream_stats = analyze_stream_characteristics()\n",
    "\n",
    "# 3. Sample Diversity Analysis\n",
    "print(\"\\nğŸ¯ Analyzing sample diversity across classes...\")\n",
    "\n",
    "def show_sample_diversity():\n",
    "    \"\"\"Show sample diversity with different classes and their RGB/Brightness patterns\"\"\"\n",
    "    \n",
    "    # Select diverse classes\n",
    "    unique_labels = np.unique(train_labels_tensor)\n",
    "    selected_classes = np.random.choice(unique_labels, 8, replace=False)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 8, figsize=(20, 6))\n",
    "    fig.suptitle('ğŸ¯ Sample Diversity Across CIFAR-100 Classes', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    for i, class_id in enumerate(selected_classes):\n",
    "        # Find samples for this class\n",
    "        class_indices = np.where(train_labels_tensor == class_id)[0]\n",
    "        sample_idx = np.random.choice(class_indices)\n",
    "        \n",
    "        # Get RGB and brightness\n",
    "        rgb_img = train_rgb[sample_idx]\n",
    "        brightness_img = train_brightness[sample_idx]\n",
    "        class_name = CIFAR100_FINE_LABELS[class_id]\n",
    "        \n",
    "        # Convert to displayable format\n",
    "        rgb_np = rgb_img.transpose(1, 2, 0)\n",
    "        rgb_np = np.clip(rgb_np, 0, 1)\n",
    "        brightness_np = brightness_img.squeeze()\n",
    "        \n",
    "        # Plot RGB\n",
    "        axes[0, i].imshow(rgb_np)\n",
    "        axes[0, i].set_title(f'{class_name}\\n(Class {class_id})', fontsize=9, fontweight='bold')\n",
    "        axes[0, i].axis('off')\n",
    "        \n",
    "        # Plot Brightness\n",
    "        axes[1, i].imshow(brightness_np, cmap='gray')\n",
    "        axes[1, i].set_title(f'Brightness\\n{class_name}', fontsize=9, fontweight='bold')\n",
    "        axes[1, i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"   ğŸ¯ Showing {len(selected_classes)} diverse classes from CIFAR-100\")\n",
    "    print(f\"   ğŸ“Š Each class demonstrates different RGB and brightness patterns\")\n",
    "    \n",
    "    return selected_classes\n",
    "\n",
    "sampled_classes = show_sample_diversity()\n",
    "\n",
    "print(f\"\\nğŸ¯ Data analysis complete!\")\n",
    "print(f\"   âœ… Class distribution analyzed\")\n",
    "print(f\"   âœ… Stream characteristics quantified\") \n",
    "print(f\"   âœ… Sample diversity demonstrated\")\n",
    "print(f\"   ğŸš€ Data ready for multi-stream model training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622265e1",
   "metadata": {},
   "source": [
    "## 6. Data Preprocessing: RGB to Brightness Transformation\n",
    "\n",
    "Apply RGB to brightness transformation to create the second stream for our multi-stream neural network architecture. This step converts the RGB images into brightness values using luminance weights to create the dual-stream format required by our models.\n",
    "\n",
    "## ğŸ—ï¸ Multi-Stream Model Creation\n",
    "\n",
    "Now we'll create our two main models for comparison:\n",
    "\n",
    "### ğŸ”¬ **base_multi_channel_large** (Dense Network)\n",
    "- **Architecture**: Large fully-connected network with multiple hidden layers\n",
    "- **Input**: Flattened RGB (3072) + Brightness (1024) features  \n",
    "- **Strengths**: Fast training, good for global feature learning\n",
    "- **Use case**: When computational efficiency is important\n",
    "\n",
    "### ğŸ”¬ **multi_channel_resnet50** (CNN Network) \n",
    "- **Architecture**: ResNet-50 style convolutional network\n",
    "- **Input**: Raw RGB (3Ã—32Ã—32) + Brightness (1Ã—32Ã—32) images\n",
    "- **Strengths**: Spatial feature extraction, state-of-the-art accuracy\n",
    "- **Use case**: When maximum accuracy is the priority\n",
    "\n",
    "Both models use our **unified API design** with shared classifiers for optimal multi-stream fusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e792ff35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RGB to Brightness Transformation\n",
    "print(\"ğŸ¨ Converting RGB data to multi-stream format (RGB + Brightness)...\")\n",
    "\n",
    "# Import the RGB to brightness transformation utility\n",
    "try:\n",
    "    from src.transforms.rgb_to_rgbl import RGBtoRGBL\n",
    "    print(\"âœ… RGBtoRGBL transformation utility imported\")\n",
    "    use_transform_utility = True\n",
    "except ImportError as e:\n",
    "    print(f\"âŒ Failed to import transformation utility: {e}\")\n",
    "    print(\"ğŸ’¡ Using fallback luminance transformation\")\n",
    "    use_transform_utility = False\n",
    "    \n",
    "    def rgb_to_brightness(rgb_tensor):\n",
    "        \"\"\"Fallback RGB to brightness conversion using standard luminance weights\"\"\"\n",
    "        # Standard luminance weights: R=0.299, G=0.587, B=0.114\n",
    "        weights = torch.tensor([0.299, 0.587, 0.114]).view(1, 3, 1, 1)\n",
    "        if rgb_tensor.is_cuda:\n",
    "            weights = weights.cuda()\n",
    "        brightness = torch.sum(rgb_tensor * weights, dim=1, keepdim=True)\n",
    "        return brightness\n",
    "\n",
    "# Initialize the transformation\n",
    "if use_transform_utility:\n",
    "    rgb_to_rgbl_transform = RGBtoRGBL()\n",
    "    print(\"âœ… RGBtoRGBL transformer initialized\")\n",
    "\n",
    "# Convert training data\n",
    "print(f\"\\nğŸ”„ Processing training data ({train_rgb.shape[0]:,} samples)...\")\n",
    "if use_transform_utility:\n",
    "    _, train_brightness = rgb_to_rgbl_transform(train_rgb)\n",
    "else:\n",
    "    train_brightness = rgb_to_brightness(train_rgb)\n",
    "\n",
    "# Convert validation data  \n",
    "print(f\"ğŸ”„ Processing validation data ({val_rgb.shape[0]:,} samples)...\")\n",
    "if use_transform_utility:\n",
    "    _, val_brightness = rgb_to_rgbl_transform(val_rgb)\n",
    "else:\n",
    "    val_brightness = rgb_to_brightness(val_rgb)\n",
    "\n",
    "# Convert test data\n",
    "print(f\"ğŸ”„ Processing test data ({test_rgb.shape[0]:,} samples)...\")\n",
    "if use_transform_utility:\n",
    "    _, test_brightness = rgb_to_rgbl_transform(test_rgb)\n",
    "else:\n",
    "    test_brightness = rgb_to_brightness(test_rgb)\n",
    "\n",
    "# Verify transformation results\n",
    "print(f\"\\nğŸ“Š Transformation Results:\")\n",
    "print(f\"   ğŸ¨ RGB shapes: Train={train_rgb.shape}, Val={val_rgb.shape}, Test={test_rgb.shape}\")\n",
    "print(f\"   ğŸ’¡ Brightness shapes: Train={train_brightness.shape}, Val={val_brightness.shape}, Test={test_brightness.shape}\")\n",
    "print(f\"   ğŸ“ˆ RGB range: [{train_rgb.min():.3f}, {train_rgb.max():.3f}]\")\n",
    "print(f\"   ğŸ“ˆ Brightness range: [{train_brightness.min():.3f}, {train_brightness.max():.3f}]\")\n",
    "\n",
    "print(\"\\nâœ… RGB to brightness transformation complete!\")\n",
    "print(\"ğŸ¯ Multi-stream data ready for model training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce951aa1",
   "metadata": {},
   "source": [
    "## 7. Data Visualization and Analysis\n",
    "\n",
    "Visualize sample images and analyze the characteristics of both RGB and brightness streams to understand the multi-stream data transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441e31cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“Š Data Visualization and Analysis\n",
    "print(\"ğŸ“Š Performing data visualization and analysis...\")\n",
    "\n",
    "# Set up the plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# 1. RGB vs Brightness Sample Comparison\n",
    "print(\"\\nğŸ–¼ï¸ Visualizing RGB vs Brightness samples...\")\n",
    "\n",
    "fig, axes = plt.subplots(3, 8, figsize=(16, 6))\n",
    "fig.suptitle('ğŸ¨ Multi-Stream CIFAR-100 Samples: RGB vs Brightness', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Select random samples for demonstration\n",
    "np.random.seed(42)  # For reproducible results\n",
    "sample_indices = np.random.choice(len(train_rgb), 12, replace=False)\n",
    "\n",
    "for row in range(3):\n",
    "    for col in range(0, 8, 2):\n",
    "        idx = row * 4 + col // 2\n",
    "        if idx < len(sample_indices):\n",
    "            sample_idx = sample_indices[idx]\n",
    "            \n",
    "            # Get data\n",
    "            rgb_img = train_rgb[sample_idx]\n",
    "            brightness_img = train_brightness[sample_idx]\n",
    "            label = train_labels_tensor[sample_idx].item()\n",
    "            class_name = CIFAR100_FINE_LABELS[label]\n",
    "            \n",
    "            # RGB image (convert from tensor to numpy)\n",
    "            rgb_np = rgb_img.permute(1, 2, 0).numpy()\n",
    "            rgb_np = np.clip(rgb_np, 0, 1)  # Ensure valid range\n",
    "            \n",
    "            # Brightness image\n",
    "            brightness_np = brightness_img.squeeze().numpy()\n",
    "            \n",
    "            # Plot RGB\n",
    "            axes[row, col].imshow(rgb_np)\n",
    "            axes[row, col].set_title(f'RGB\\n{class_name}', fontsize=8, fontweight='bold')\n",
    "            axes[row, col].axis('off')\n",
    "            \n",
    "            # Plot Brightness\n",
    "            axes[row, col + 1].imshow(brightness_np, cmap='gray')\n",
    "            axes[row, col + 1].set_title(f'Brightness\\n{class_name}', fontsize=8, fontweight='bold')\n",
    "            axes[row, col + 1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2. Stream Statistics\n",
    "print(\"\\nğŸ“ˆ Analyzing stream characteristics...\")\n",
    "\n",
    "# Sample subset for analysis (memory efficiency)\n",
    "sample_size = min(1000, len(train_rgb))\n",
    "sample_indices = np.random.choice(len(train_rgb), sample_size, replace=False)\n",
    "rgb_sample = train_rgb[sample_indices]\n",
    "brightness_sample = train_brightness[sample_indices]\n",
    "\n",
    "# Calculate statistics\n",
    "rgb_stats = {\n",
    "    'mean': rgb_sample.mean().item(),\n",
    "    'std': rgb_sample.std().item(),\n",
    "    'min': rgb_sample.min().item(),\n",
    "    'max': rgb_sample.max().item()\n",
    "}\n",
    "\n",
    "brightness_stats = {\n",
    "    'mean': brightness_sample.mean().item(),\n",
    "    'std': brightness_sample.std().item(),\n",
    "    'min': brightness_sample.min().item(),\n",
    "    'max': brightness_sample.max().item()\n",
    "}\n",
    "\n",
    "# RGB-Brightness correlation\n",
    "rgb_means = rgb_sample.mean(axis=(2, 3))  # Mean per image across spatial dimensions\n",
    "brightness_means = brightness_sample.mean(axis=(2, 3))  # Mean per image\n",
    "correlation = np.corrcoef(rgb_means.mean(axis=1), brightness_means[:, 0])[0, 1]\n",
    "\n",
    "print(f\"   ğŸ¨ RGB statistics:\")\n",
    "print(f\"      Mean: {rgb_stats['mean']:.3f}, Std: {rgb_stats['std']:.3f}\")\n",
    "print(f\"      Range: [{rgb_stats['min']:.3f}, {rgb_stats['max']:.3f}]\")\n",
    "print(f\"   ğŸ’¡ Brightness statistics:\")\n",
    "print(f\"      Mean: {brightness_stats['mean']:.3f}, Std: {brightness_stats['std']:.3f}\")\n",
    "print(f\"      Range: [{brightness_stats['min']:.3f}, {brightness_stats['max']:.3f}]\")\n",
    "print(f\"   ğŸ”— RGB-Brightness correlation: {correlation:.3f}\")\n",
    "\n",
    "# 3. Class distribution visualization\n",
    "print(\"\\nğŸ·ï¸ Analyzing class distribution...\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "fig.suptitle('ğŸ·ï¸ CIFAR-100 Class Distribution Across Splits', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Training distribution\n",
    "train_counts = np.bincount(train_labels_tensor, minlength=100)\n",
    "axes[0].bar(range(100), train_counts, alpha=0.7, color='skyblue')\n",
    "axes[0].set_title(f'Training Set\\n{len(train_labels_tensor):,} samples', fontweight='bold')\n",
    "axes[0].set_xlabel('Class ID')\n",
    "axes[0].set_ylabel('Sample Count')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Validation distribution\n",
    "val_counts = np.bincount(val_labels_tensor, minlength=100)\n",
    "axes[1].bar(range(100), val_counts, alpha=0.7, color='lightcoral')\n",
    "axes[1].set_title(f'Validation Set\\n{len(val_labels_tensor):,} samples', fontweight='bold')\n",
    "axes[1].set_xlabel('Class ID')\n",
    "axes[1].set_ylabel('Sample Count')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Test distribution\n",
    "test_counts = np.bincount(test_labels_tensor, minlength=100)\n",
    "axes[2].bar(range(100), test_counts, alpha=0.7, color='lightgreen')\n",
    "axes[2].set_title(f'Test Set\\n{len(test_labels_tensor):,} samples', fontweight='bold')\n",
    "axes[2].set_xlabel('Class ID')\n",
    "axes[2].set_ylabel('Sample Count')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"   ğŸ“Š Training: mean={train_counts.mean():.1f}, std={train_counts.std():.1f}\")\n",
    "print(f\"   ğŸ“Š Validation: mean={val_counts.mean():.1f}, std={val_counts.std():.1f}\")\n",
    "print(f\"   ğŸ“Š Test: mean={test_counts.mean():.1f}, std={test_counts.std():.1f}\")\n",
    "\n",
    "print(f\"\\nğŸ¯ Data visualization and analysis complete!\")\n",
    "print(f\"   âœ… RGB and brightness streams visualized\")\n",
    "print(f\"   âœ… Stream characteristics quantified\")\n",
    "print(f\"   âœ… Class distribution analyzed\")\n",
    "print(f\"   ğŸš€ Data ready for multi-stream model training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73e3a1b",
   "metadata": {
    "id": "e73e3a1b"
   },
   "source": [
    "## 8. Data Visualization: RGB and Brightness Samples\n",
    "\n",
    "Visualize sample images from both RGB and brightness streams to understand the data transformation and multi-stream inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68afa3b1",
   "metadata": {
    "id": "68afa3b1"
   },
   "outputs": [],
   "source": [
    "def visualize_rgb_brightness_samples(rgb_data, brightness_data, labels, num_samples=5):\n",
    "    \"\"\"\n",
    "    Visualize RGB and brightness images side by side.\n",
    "\n",
    "    Args:\n",
    "        rgb_data: RGB image data [N, 3, H, W]\n",
    "        brightness_data: Brightness image data [N, 1, H, W]\n",
    "        labels: Image labels\n",
    "        num_samples: Number of samples to visualize\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(num_samples, 2, figsize=(8, 2.5 * num_samples))\n",
    "    fig.suptitle('RGB vs Brightness Channel Comparison', fontsize=16, fontweight='bold')\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        # Get RGB image (convert from CHW to HWC for matplotlib)\n",
    "        rgb_img = np.transpose(rgb_data[i], (1, 2, 0))\n",
    "\n",
    "        # Get brightness image (squeeze channel dimension)\n",
    "        brightness_img = brightness_data[i, 0]  # Remove channel dimension\n",
    "\n",
    "        # Get class name\n",
    "        class_name = cifar100_fine_labels[labels[i]]\n",
    "\n",
    "        # Plot RGB image\n",
    "        axes[i, 0].imshow(rgb_img)\n",
    "        axes[i, 0].set_title(f'RGB - {class_name}', fontweight='bold')\n",
    "        axes[i, 0].axis('off')\n",
    "\n",
    "        # Plot brightness image\n",
    "        axes[i, 1].imshow(brightness_img, cmap='gray')\n",
    "        axes[i, 1].set_title(f'Brightness - {class_name}', fontweight='bold')\n",
    "        axes[i, 1].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize sample images\n",
    "print(\"ğŸ–¼ï¸ Sample RGB vs Brightness Images:\")\n",
    "visualize_rgb_brightness_samples(train_rgb, train_brightness, train_labels, num_samples=5)\n",
    "\n",
    "# Show data statistics\n",
    "def show_data_statistics(rgb_data, brightness_data, labels):\n",
    "    \"\"\"Show basic statistics about the data.\"\"\"\n",
    "    print(f\"\\nğŸ“Š Data Statistics:\")\n",
    "    print(f\"   RGB data range: [{rgb_data.min():.3f}, {rgb_data.max():.3f}]\")\n",
    "    print(f\"   Brightness data range: [{brightness_data.min():.3f}, {brightness_data.max():.3f}]\")\n",
    "    print(f\"   Number of unique classes: {len(np.unique(labels))}\")\n",
    "\n",
    "    # Class distribution\n",
    "    unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "    print(f\"   Samples per class: {counts.min()} - {counts.max()}\")\n",
    "    print(f\"   Average samples per class: {counts.mean():.1f}\")\n",
    "\n",
    "show_data_statistics(train_rgb, train_brightness, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a90244",
   "metadata": {
    "id": "c4a90244"
   },
   "source": [
    "## 9. Advanced Data Analysis and Statistics\n",
    "\n",
    "Perform comprehensive analysis of the dataset including class distribution, statistical summaries, and data quality assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b394c3b3",
   "metadata": {
    "id": "b394c3b3"
   },
   "outputs": [],
   "source": [
    "# Class distribution visualization\n",
    "def plot_class_distribution(labels, title=\"Class Distribution\"):\n",
    "    \"\"\"Plot the distribution of classes in the dataset.\"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "\n",
    "    plt.bar(unique_labels, counts, alpha=0.7, color='skyblue', edgecolor='navy')\n",
    "    plt.title(title, fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Class ID')\n",
    "    plt.ylabel('Number of Samples')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Pixel intensity histograms\n",
    "def plot_intensity_histograms(rgb_data, brightness_data):\n",
    "    \"\"\"Plot histograms of pixel intensities for RGB and brightness channels.\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "    fig.suptitle('Pixel Intensity Distributions', fontsize=16, fontweight='bold')\n",
    "\n",
    "    # RGB histograms\n",
    "    colors = ['red', 'green', 'blue']\n",
    "    for i, color in enumerate(colors):\n",
    "        axes[0, 0].hist(rgb_data[:, i].flatten(), bins=50, alpha=0.6,\n",
    "                       color=color, label=f'{color.upper()} channel')\n",
    "    axes[0, 0].set_title('RGB Channel Intensities')\n",
    "    axes[0, 0].set_xlabel('Pixel Value')\n",
    "    axes[0, 0].set_ylabel('Frequency')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Brightness histogram\n",
    "    axes[0, 1].hist(brightness_data.flatten(), bins=50, alpha=0.7,\n",
    "                   color='gray', edgecolor='black')\n",
    "    axes[0, 1].set_title('Brightness Channel Intensities')\n",
    "    axes[0, 1].set_xlabel('Pixel Value')\n",
    "    axes[0, 1].set_ylabel('Frequency')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    # Mean pixel values per channel\n",
    "    rgb_means = np.mean(rgb_data, axis=(0, 2, 3))\n",
    "    brightness_mean = np.mean(brightness_data)\n",
    "\n",
    "    channel_names = ['Red', 'Green', 'Blue', 'Brightness']\n",
    "    channel_means = [rgb_means[0], rgb_means[1], rgb_means[2], brightness_mean]\n",
    "\n",
    "    axes[1, 0].bar(channel_names, channel_means,\n",
    "                  color=['red', 'green', 'blue', 'gray'], alpha=0.7)\n",
    "    axes[1, 0].set_title('Mean Pixel Values by Channel')\n",
    "    axes[1, 0].set_ylabel('Mean Pixel Value')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Sample grid\n",
    "    axes[1, 1].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Sample grid of images\n",
    "def plot_sample_grid(rgb_data, labels, grid_size=(4, 8)):\n",
    "    \"\"\"Plot a grid of sample images.\"\"\"\n",
    "    fig, axes = plt.subplots(grid_size[0], grid_size[1], figsize=(16, 8))\n",
    "    fig.suptitle('Sample Images from CIFAR-100 Dataset', fontsize=16, fontweight='bold')\n",
    "\n",
    "    for i in range(grid_size[0]):\n",
    "        for j in range(grid_size[1]):\n",
    "            idx = i * grid_size[1] + j\n",
    "            if idx < len(rgb_data):\n",
    "                img = np.transpose(rgb_data[idx], (1, 2, 0))\n",
    "                class_name = cifar100_fine_labels[labels[idx]]\n",
    "\n",
    "                axes[i, j].imshow(img)\n",
    "                axes[i, j].set_title(class_name, fontsize=8)\n",
    "                axes[i, j].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Generate visualizations\n",
    "print(\"ğŸ“Š Generating additional visualizations...\")\n",
    "\n",
    "# Class distribution\n",
    "plot_class_distribution(train_labels, \"Training Set Class Distribution\")\n",
    "\n",
    "# Intensity histograms\n",
    "plot_intensity_histograms(train_rgb[:1000], train_brightness[:1000])  # Sample for speed\n",
    "\n",
    "# Sample grid\n",
    "plot_sample_grid(train_rgb, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86919e29",
   "metadata": {
    "id": "86919e29"
   },
   "source": [
    "## 10. Create Multi-Stream Neural Network Models\n",
    "\n",
    "Instantiate both dense and ResNet-based multi-stream neural network models using our unified API for CIFAR-100 classification.\n",
    "\n",
    "**Key Features:**\n",
    "- **Updated Factory Functions**: Support for different input sizes (`color_input_size`, `brightness_input_size`)\n",
    "- **Built-in `.compile()` Method**: Keras-like model configuration with optimizer, loss, and metrics\n",
    "- **Automatic Parameter Counting**: Easy model comparison and analysis\n",
    "- **Device-Aware Initialization**: Automatic GPU detection and optimization\n",
    "- **Forward Pass Testing**: Proper API usage validation with both research and classification modes\n",
    "\n",
    "**Available Factory Functions:**\n",
    "- **Dense Models**: `base_multi_channel_small`, `base_multi_channel_medium`, `base_multi_channel_large`\n",
    "  - Now support: `color_input_size=3072, brightness_input_size=1024` for CIFAR-100\n",
    "  - Backward compatible: `input_size=N` for same-size streams\n",
    "- **CNN Models**: `multi_channel_resnet18`, `multi_channel_resnet34`, `multi_channel_resnet50`\n",
    "  - Support different channel counts: `color_input_channels=3, brightness_input_channels=1`\n",
    "\n",
    "**API Usage Examples:**\n",
    "```python\n",
    "# Dense model with different input sizes\n",
    "model = base_multi_channel_medium(\n",
    "    color_input_size=3072,      # RGB: 3*32*32\n",
    "    brightness_input_size=1024, # Brightness: 1*32*32  \n",
    "    num_classes=100\n",
    ")\n",
    "\n",
    "# CNN model with different channel counts\n",
    "model = multi_channel_resnet18(\n",
    "    color_input_channels=3,     # RGB channels\n",
    "    brightness_input_channels=1, # Brightness channels\n",
    "    num_classes=100\n",
    ")\n",
    "\n",
    "# Compile and use\n",
    "model.compile(optimizer='adam', learning_rate=0.001)\n",
    "model.fit(rgb_data, brightness_data, labels)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e83a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ­ Creating Multi-Stream Neural Network Models using Factory Functions...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mğŸ­ Creating Multi-Stream Neural Network Models using Factory Functions...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Check GPU availability and set device\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mğŸ–¥ï¸ Using device: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "# ğŸ—ï¸ Multi-Stream Model Creation: Large Dense + ResNet-50 CNN\n",
    "print(\"ğŸ—ï¸ Creating Multi-Stream Neural Network Models...\")\n",
    "\n",
    "# Check GPU availability and set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"ğŸ–¥ï¸ Using device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "# Model configuration based on CIFAR-100 data\n",
    "print(f\"\\nğŸ“Š Model Configuration:\")\n",
    "print(f\"   Image size: 32x32 pixels\")\n",
    "print(f\"   RGB channels: 3\")\n",
    "print(f\"   Brightness channels: 1\") \n",
    "print(f\"   Number of classes: 100 (CIFAR-100)\")\n",
    "\n",
    "# Import model factory for clean model creation\n",
    "try:\n",
    "    from src.models.builders import create_model, list_available_models\n",
    "    print(\"âœ… Model factory imported successfully\")\n",
    "    \n",
    "    # List available models\n",
    "    available_model_types = list_available_models()\n",
    "    print(f\"ğŸ¯ Available model types: {available_model_types}\")\n",
    "    \n",
    "    use_factory = True\n",
    "except ImportError as e:\n",
    "    print(f\"âŒ Failed to import model factory: {e}\")\n",
    "    print(\"ğŸ’¡ Falling back to direct imports\")\n",
    "    \n",
    "    try:\n",
    "        from src.models.basic_multi_channel import (\n",
    "            base_multi_channel_large,\n",
    "            multi_channel_resnet50\n",
    "        )\n",
    "        print(\"âœ… Direct model imports successful\")\n",
    "        use_factory = False\n",
    "    except ImportError as e:\n",
    "        print(f\"âŒ Failed to import models: {e}\")\n",
    "        raise\n",
    "\n",
    "# Calculate input sizes\n",
    "num_classes = 100\n",
    "image_size = 32\n",
    "input_channels_rgb = 3\n",
    "input_channels_brightness = 1\n",
    "\n",
    "# For dense models (flattened input)\n",
    "rgb_input_size = input_channels_rgb * image_size * image_size  # 3 * 32 * 32 = 3072\n",
    "brightness_input_size = input_channels_brightness * image_size * image_size  # 1 * 32 * 32 = 1024\n",
    "\n",
    "print(f\"\\nğŸ”§ Input Configuration:\")\n",
    "print(f\"   RGB input size (dense): {rgb_input_size}\")\n",
    "print(f\"   Brightness input size (dense): {brightness_input_size}\")\n",
    "print(f\"   RGB input shape (CNN): ({input_channels_rgb}, {image_size}, {image_size})\")\n",
    "print(f\"   Brightness input shape (CNN): ({input_channels_brightness}, {image_size}, {image_size})\")\n",
    "\n",
    "# Create base_multi_channel_large (Dense Network)\n",
    "print(\"\\nğŸ”¬ Creating base_multi_channel_large (Dense Network)...\")\n",
    "try:\n",
    "    if use_factory:\n",
    "        base_multi_channel_large_model = create_model(\n",
    "            'base_multi_channel_large',\n",
    "            num_classes=num_classes,\n",
    "            color_input_size=rgb_input_size,\n",
    "            brightness_input_size=brightness_input_size,\n",
    "            use_shared_classifier=True,\n",
    "            device='auto'\n",
    "        )\n",
    "    else:\n",
    "        base_multi_channel_large_model = base_multi_channel_large(\n",
    "            num_classes=num_classes,\n",
    "            color_input_size=rgb_input_size,\n",
    "            brightness_input_size=brightness_input_size,\n",
    "            use_shared_classifier=True,\n",
    "            device='auto'\n",
    "        )\n",
    "    \n",
    "    # Count parameters\n",
    "    large_dense_params = sum(p.numel() for p in base_multi_channel_large_model.parameters())\n",
    "    large_dense_trainable = sum(p.numel() for p in base_multi_channel_large_model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"âœ… base_multi_channel_large created successfully\")\n",
    "    print(f\"   Architecture: Large dense multi-layer network\")\n",
    "    print(f\"   Total parameters: {large_dense_params:,}\")\n",
    "    print(f\"   Trainable parameters: {large_dense_trainable:,}\")\n",
    "    print(f\"   RGB input size: {rgb_input_size} (flattened)\")\n",
    "    print(f\"   Brightness input size: {brightness_input_size} (flattened)\")\n",
    "    print(f\"   Fusion strategy: Shared classifier\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Failed to create base_multi_channel_large: {e}\")\n",
    "    print(f\"ğŸ’¡ Error details: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    base_multi_channel_large_model = None\n",
    "\n",
    "# Create multi_channel_resnet50 (CNN Network)\n",
    "print(\"\\nğŸ”¬ Creating multi_channel_resnet50 (CNN Network)...\")\n",
    "try:\n",
    "    if use_factory:\n",
    "        multi_channel_resnet50_model = create_model(\n",
    "            'multi_channel_resnet50',\n",
    "            num_classes=num_classes,\n",
    "            color_input_channels=input_channels_rgb,\n",
    "            brightness_input_channels=input_channels_brightness,\n",
    "            use_shared_classifier=True,\n",
    "            activation='relu',\n",
    "            device='auto'\n",
    "        )\n",
    "    else:\n",
    "        multi_channel_resnet50_model = multi_channel_resnet50(\n",
    "            num_classes=num_classes,\n",
    "            color_input_channels=input_channels_rgb,\n",
    "            brightness_input_channels=input_channels_brightness,\n",
    "            use_shared_classifier=True,\n",
    "            activation='relu',\n",
    "            device='auto'\n",
    "        )\n",
    "    \n",
    "    # Count parameters\n",
    "    resnet50_params = sum(p.numel() for p in multi_channel_resnet50_model.parameters())\n",
    "    resnet50_trainable = sum(p.numel() for p in multi_channel_resnet50_model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"âœ… multi_channel_resnet50 created successfully\")\n",
    "    print(f\"   Architecture: ResNet-50 style CNN (3,4,6,3 blocks)\")\n",
    "    print(f\"   Total parameters: {resnet50_params:,}\")\n",
    "    print(f\"   Trainable parameters: {resnet50_trainable:,}\")\n",
    "    print(f\"   Input shape: RGB {(input_channels_rgb, image_size, image_size)}, Brightness {(input_channels_brightness, image_size, image_size)}\")\n",
    "    print(f\"   Fusion strategy: Shared classifier\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Failed to create multi_channel_resnet50: {e}\")\n",
    "    print(f\"ğŸ’¡ Error details: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    multi_channel_resnet50_model = None\n",
    "\n",
    "# Model comparison\n",
    "if base_multi_channel_large_model is not None and multi_channel_resnet50_model is not None:\n",
    "    print(f\"\\nğŸ“ˆ Model Comparison:\")\n",
    "    print(f\"   base_multi_channel_large: {large_dense_params:,} parameters\")\n",
    "    print(f\"   multi_channel_resnet50: {resnet50_params:,} parameters\")\n",
    "    print(f\"   ResNet-50 is {resnet50_params/large_dense_params:.1f}x larger than Large Dense\")\n",
    "elif base_multi_channel_large_model is not None:\n",
    "    print(f\"\\nğŸ“ˆ Available Models:\")\n",
    "    print(f\"   base_multi_channel_large: {large_dense_params:,} parameters\")\n",
    "elif multi_channel_resnet50_model is not None:\n",
    "    print(f\"\\nğŸ“ˆ Available Models:\")\n",
    "    print(f\"   multi_channel_resnet50: {resnet50_params:,} parameters\")\n",
    "\n",
    "# Test model forward pass with sample data\n",
    "print(\"\\nğŸ§ª Testing model forward pass with unified APIs...\")\n",
    "\n",
    "try:\n",
    "    # Create sample batch data\n",
    "    batch_size = 4\n",
    "    sample_rgb = torch.randn(batch_size, input_channels_rgb, image_size, image_size).to(device)\n",
    "    sample_brightness = torch.randn(batch_size, input_channels_brightness, image_size, image_size).to(device)\n",
    "    \n",
    "    print(f\"   Sample RGB shape: {sample_rgb.shape}\")\n",
    "    print(f\"   Sample brightness shape: {sample_brightness.shape}\")\n",
    "    \n",
    "    # Test base_multi_channel_large (Dense Model)\n",
    "    if base_multi_channel_large_model is not None:\n",
    "        # Flatten inputs for dense model\n",
    "        rgb_flat = sample_rgb.view(batch_size, rgb_input_size)\n",
    "        brightness_flat = sample_brightness.view(batch_size, brightness_input_size)\n",
    "        \n",
    "        print(f\"   Dense RGB flat shape: {rgb_flat.shape}\")\n",
    "        print(f\"   Dense brightness flat shape: {brightness_flat.shape}\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Test standard classification API\n",
    "            dense_output = base_multi_channel_large_model(rgb_flat, brightness_flat)\n",
    "            print(f\"âœ… base_multi_channel_large (classification) output: {dense_output.shape}\")\n",
    "            \n",
    "            # Test research API for pathway analysis\n",
    "            color_logits, brightness_logits = base_multi_channel_large_model.analyze_pathways(rgb_flat, brightness_flat)\n",
    "            print(f\"âœ… base_multi_channel_large (analyze_pathways) outputs: {color_logits.shape}, {brightness_logits.shape}\")\n",
    "    \n",
    "    # Test multi_channel_resnet50 (CNN Model)\n",
    "    if multi_channel_resnet50_model is not None:\n",
    "        with torch.no_grad():\n",
    "            # Test standard classification API\n",
    "            cnn_output = multi_channel_resnet50_model(sample_rgb, sample_brightness)\n",
    "            print(f\"âœ… multi_channel_resnet50 (classification) output: {cnn_output.shape}\")\n",
    "            \n",
    "            # Test research API for pathway analysis\n",
    "            color_logits, brightness_logits = multi_channel_resnet50_model.analyze_pathways(sample_rgb, sample_brightness)\n",
    "            print(f\"âœ… multi_channel_resnet50 (analyze_pathways) outputs: {color_logits.shape}, {brightness_logits.shape}\")\n",
    "    \n",
    "    print(\"âœ… All model tests passed! Unified API working correctly.\")\n",
    "    print(\"ğŸ’¡ Use model(x, y) for training/inference, analyze_pathways(x, y) for research\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Model forward pass test failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "# Store available models for training\n",
    "available_models = {}\n",
    "if base_multi_channel_large_model is not None:\n",
    "    available_models['base_multi_channel_large'] = base_multi_channel_large_model\n",
    "if multi_channel_resnet50_model is not None:\n",
    "    available_models['multi_channel_resnet50'] = multi_channel_resnet50_model\n",
    "\n",
    "if available_models:\n",
    "    print(f\"\\nğŸ¯ {len(available_models)} model(s) ready for training:\")\n",
    "    for model_name in available_models.keys():\n",
    "        print(f\"   âœ… {model_name}\")\n",
    "else:\n",
    "    print(\"\\nâŒ No models available for training!\")\n",
    "    print(\"ğŸ’¡ Check the error messages above and fix the model creation issues\")\n",
    "\n",
    "print(\"\\nğŸ¯ Model creation complete! Models are compiled and ready for training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb159c3",
   "metadata": {},
   "source": [
    "## 11. Prepare Data for Training\n",
    "\n",
    "Format and prepare the data for training with proper tensor conversions, device placement, and train/validation splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b481bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preparation for Training\n",
    "print(\"ğŸ“¦ Preparing data for training...\")\n",
    "\n",
    "# Check if we have processed data\n",
    "if 'train_rgb' not in locals() or 'train_brightness' not in locals():\n",
    "    print(\"âŒ No processed training data found!\")\n",
    "    print(\"ğŸ’¡ Please run the data processing cells first (Step 5)\")\n",
    "    raise ValueError(\"Training data not available\")\n",
    "\n",
    "print(f\"âœ… Found processed data:\")\n",
    "print(f\"   Training RGB: {train_rgb.shape}\")\n",
    "print(f\"   Training Brightness: {train_brightness.shape}\")\n",
    "print(f\"   Training Labels: {train_labels.shape}\")\n",
    "print(f\"   Test RGB: {test_rgb.shape}\")\n",
    "print(f\"   Test Brightness: {test_brightness.shape}\")\n",
    "print(f\"   Test Labels: {test_labels.shape}\")\n",
    "\n",
    "# Convert numpy arrays to PyTorch tensors\n",
    "print(\"\\nğŸ”„ Converting to PyTorch tensors...\")\n",
    "\n",
    "# Training data\n",
    "train_rgb_tensor = torch.FloatTensor(train_rgb)\n",
    "train_brightness_tensor = torch.FloatTensor(train_brightness)\n",
    "train_labels_tensor = torch.LongTensor(train_labels)\n",
    "\n",
    "# Test data\n",
    "test_rgb_tensor = torch.FloatTensor(test_rgb)\n",
    "test_brightness_tensor = torch.FloatTensor(test_brightness)\n",
    "test_labels_tensor = torch.LongTensor(test_labels)\n",
    "\n",
    "print(f\"âœ… Tensors created:\")\n",
    "print(f\"   Training RGB tensor: {train_rgb_tensor.shape}, dtype: {train_rgb_tensor.dtype}\")\n",
    "print(f\"   Training brightness tensor: {train_brightness_tensor.shape}, dtype: {train_brightness_tensor.dtype}\")\n",
    "print(f\"   Training labels tensor: {train_labels_tensor.shape}, dtype: {train_labels_tensor.dtype}\")\n",
    "\n",
    "# Normalize data to [0, 1] range if needed\n",
    "if train_rgb_tensor.max() > 1.0:\n",
    "    print(\"\\nğŸ“Š Normalizing data to [0, 1] range...\")\n",
    "    train_rgb_tensor = train_rgb_tensor / 255.0\n",
    "    train_brightness_tensor = train_brightness_tensor / 255.0\n",
    "    test_rgb_tensor = test_rgb_tensor / 255.0\n",
    "    test_brightness_tensor = test_brightness_tensor / 255.0\n",
    "    print(f\"âœ… Data normalized: RGB range [{train_rgb_tensor.min():.3f}, {train_rgb_tensor.max():.3f}]\")\n",
    "\n",
    "# Create datasets\n",
    "print(\"\\nğŸ—‚ï¸ Creating PyTorch datasets...\")\n",
    "\n",
    "class MultiStreamDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Custom dataset for multi-stream data (RGB + Brightness)\"\"\"\n",
    "    \n",
    "    def __init__(self, rgb_data, brightness_data, labels):\n",
    "        self.rgb_data = rgb_data\n",
    "        self.brightness_data = brightness_data\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'rgb': self.rgb_data[idx],\n",
    "            'brightness': self.brightness_data[idx],\n",
    "            'label': self.labels[idx]\n",
    "        }\n",
    "\n",
    "# Create dataset instances\n",
    "train_dataset_multi = MultiStreamDataset(train_rgb_tensor, train_brightness_tensor, train_labels_tensor)\n",
    "test_dataset_multi = MultiStreamDataset(test_rgb_tensor, test_brightness_tensor, test_labels_tensor)\n",
    "\n",
    "print(f\"âœ… Datasets created:\")\n",
    "print(f\"   Training dataset: {len(train_dataset_multi)} samples\")\n",
    "print(f\"   Test dataset: {len(test_dataset_multi)} samples\")\n",
    "\n",
    "# Create data loaders\n",
    "print(\"\\nğŸš€ Creating data loaders...\")\n",
    "\n",
    "batch_size = 32  # Adjust based on GPU memory\n",
    "num_workers = 2  # Adjust based on system\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset_multi,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=torch.cuda.is_available()\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset_multi,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=torch.cuda.is_available()\n",
    ")\n",
    "\n",
    "print(f\"âœ… Data loaders created:\")\n",
    "print(f\"   Training batches: {len(train_loader)}\")\n",
    "print(f\"   Test batches: {len(test_loader)}\")\n",
    "print(f\"   Batch size: {batch_size}\")\n",
    "\n",
    "# Test data loader\n",
    "print(\"\\nğŸ§ª Testing data loader...\")\n",
    "try:\n",
    "    sample_batch = next(iter(train_loader))\n",
    "    print(f\"âœ… Sample batch loaded:\")\n",
    "    print(f\"   RGB batch shape: {sample_batch['rgb'].shape}\")\n",
    "    print(f\"   Brightness batch shape: {sample_batch['brightness'].shape}\")\n",
    "    print(f\"   Labels batch shape: {sample_batch['label'].shape}\")\n",
    "    print(f\"   Labels range: {sample_batch['label'].min().item()} - {sample_batch['label'].max().item()}\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Data loader test failed: {e}\")\n",
    "\n",
    "print(\"\\nğŸ“Š Data statistics:\")\n",
    "print(f\"   Classes in training set: {len(torch.unique(train_labels_tensor))}\")\n",
    "print(f\"   Classes in test set: {len(torch.unique(test_labels_tensor))}\")\n",
    "print(f\"   RGB data range: [{train_rgb_tensor.min():.3f}, {train_rgb_tensor.max():.3f}]\")\n",
    "print(f\"   Brightness data range: [{train_brightness_tensor.min():.3f}, {train_brightness_tensor.max():.3f}]\")\n",
    "\n",
    "print(\"\\nâœ… Data preparation complete! Ready for training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6958995",
   "metadata": {},
   "source": [
    "## 12. Train Multi-Stream Models\n",
    "\n",
    "Train both dense and ResNet-based multi-stream models on CIFAR-100 dataset with comprehensive evaluation.\n",
    "\n",
    "**Key Features:**\n",
    "- Uses the models' built-in Keras-like `.fit()` method for clean, maintainable training\n",
    "- Automatic optimization: batch size, workers, mixed precision based on device\n",
    "- Built-in progress tracking and validation\n",
    "- Proper input shape handling for Dense vs CNN models\n",
    "- Consistent API across all model types\n",
    "\n",
    "**API Usage:**\n",
    "- `model.fit()` - Keras-like training API with automatic optimizations\n",
    "- `model()` - Primary method for training, inference, and evaluation\n",
    "- `model.forward()` - Research output (tuple of individual stream logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b72a1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Configuration and Implementation\n",
    "print(\"ğŸš€ Setting up training configuration...\")\n",
    "\n",
    "# Training hyperparameters\n",
    "num_epochs = 10  # Reduce for demo, increase for full training\n",
    "learning_rate = 0.001\n",
    "weight_decay = 1e-4\n",
    "\n",
    "print(f\"âœ… Training Configuration:\")\n",
    "print(f\"   Epochs: {num_epochs}\")\n",
    "print(f\"   Learning rate: {learning_rate}\")\n",
    "print(f\"   Weight decay: {weight_decay}\")\n",
    "print(f\"   Device: {device}\")\n",
    "\n",
    "# Prepare data for model's .fit() method\n",
    "# The models expect numpy arrays, so convert tensors back to numpy\n",
    "train_rgb_np = train_rgb_tensor.cpu().numpy()\n",
    "train_brightness_np = train_brightness_tensor.cpu().numpy()\n",
    "train_labels_np = train_labels_tensor.cpu().numpy()\n",
    "\n",
    "test_rgb_np = test_rgb_tensor.cpu().numpy()\n",
    "test_brightness_np = test_brightness_tensor.cpu().numpy()\n",
    "test_labels_np = test_labels_tensor.cpu().numpy()\n",
    "\n",
    "print(f\"\\nğŸ“Š Data ready for training:\")\n",
    "print(f\"   Training samples: {len(train_rgb_np)}\")\n",
    "print(f\"   Test samples: {len(test_rgb_np)}\")\n",
    "print(f\"   RGB input shape: {train_rgb_np.shape}\")\n",
    "print(f\"   Brightness input shape: {train_brightness_np.shape}\")\n",
    "\n",
    "# Check if models are available\n",
    "models_to_train = []\n",
    "\n",
    "if 'available_models' in locals() and available_models:\n",
    "    models_to_train = list(available_models.items())\n",
    "\n",
    "if not models_to_train:\n",
    "    print(\"âŒ No models available for training!\")\n",
    "    print(\"ğŸ’¡ Please run the model creation cells first (Step 8)\")\n",
    "else:\n",
    "    print(f\"\\nâœ… Found {len(models_to_train)} models to train:\")\n",
    "    for name, _ in models_to_train:\n",
    "        print(f\"   - {name}\")\n",
    "\n",
    "print(\"\\nğŸ¯ Ready to start training using model's built-in .fit() API!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db7cefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute Training for All Models Using Built-in API\n",
    "print(\"ğŸš€ Starting model training using the models' built-in .fit() API...\")\n",
    "\n",
    "import time\n",
    "\n",
    "# Store results for comparison\n",
    "training_results = {}\n",
    "\n",
    "# Train each model using their built-in .fit() method\n",
    "for model_name, model in models_to_train:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ğŸ‹ï¸ Training {model_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Prepare input data based on model type\n",
    "        if 'Dense' in model_name:\n",
    "            # Dense models expect flattened input\n",
    "            rgb_input = train_rgb_np.reshape(train_rgb_np.shape[0], -1)\n",
    "            brightness_input = train_brightness_np.reshape(train_brightness_np.shape[0], -1)\n",
    "            val_rgb_input = test_rgb_np.reshape(test_rgb_np.shape[0], -1)\n",
    "            val_brightness_input = test_brightness_np.reshape(test_brightness_np.shape[0], -1)\n",
    "        else:\n",
    "            # CNN models expect image-like input\n",
    "            rgb_input = train_rgb_np\n",
    "            brightness_input = train_brightness_np\n",
    "            val_rgb_input = test_rgb_np\n",
    "            val_brightness_input = test_brightness_np\n",
    "        \n",
    "        print(f\"ğŸ“Š Input shapes for {model_name}:\")\n",
    "        print(f\"   RGB: {rgb_input.shape}\")\n",
    "        print(f\"   Brightness: {brightness_input.shape}\")\n",
    "        \n",
    "        # Train using the model's built-in .fit() method\n",
    "        print(f\"\\nğŸ”¥ Training {model_name} using .fit() API...\")\n",
    "        model.fit(\n",
    "            train_color_data=rgb_input,\n",
    "            train_brightness_data=brightness_input,\n",
    "            train_labels=train_labels_np,\n",
    "            val_color_data=val_rgb_input,\n",
    "            val_brightness_data=val_brightness_input,\n",
    "            val_labels=test_labels_np,\n",
    "            epochs=num_epochs,\n",
    "            learning_rate=learning_rate,\n",
    "            weight_decay=weight_decay,\n",
    "            verbose=1  # Show progress bars\n",
    "        )\n",
    "        \n",
    "        training_time = time.time() - start_time\n",
    "        \n",
    "        # Evaluate final accuracy using the model's built-in evaluation\n",
    "        print(f\"\\nğŸ“ˆ Evaluating {model_name}...\")\n",
    "        model.eval()\n",
    "        \n",
    "        # Get predictions on test set\n",
    "        with torch.no_grad():\n",
    "            if 'Dense' in model_name:\n",
    "                test_outputs = model(\n",
    "                    torch.tensor(val_rgb_input, dtype=torch.float32).to(device),\n",
    "                    torch.tensor(val_brightness_input, dtype=torch.float32).to(device)\n",
    "                )\n",
    "            else:\n",
    "                test_outputs = model(\n",
    "                    torch.tensor(test_rgb_np, dtype=torch.float32).to(device),\n",
    "                    torch.tensor(test_brightness_np, dtype=torch.float32).to(device)\n",
    "                )\n",
    "            \n",
    "            _, predicted = torch.max(test_outputs, 1)\n",
    "            test_labels_tensor_device = torch.tensor(test_labels_np, dtype=torch.long).to(device)\n",
    "            final_test_acc = (predicted == test_labels_tensor_device).float().mean().item() * 100\n",
    "        \n",
    "        # Store results\n",
    "        training_results[model_name] = {\n",
    "            'model': model,\n",
    "            'final_test_acc': final_test_acc,\n",
    "            'training_time': training_time,\n",
    "        }\n",
    "        \n",
    "        print(f\"âœ… {model_name} training complete!\")\n",
    "        print(f\"   Final test accuracy: {final_test_acc:.2f}%\")\n",
    "        print(f\"   Training time: {training_time:.1f}s ({training_time/60:.1f} min)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Training failed for {model_name}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        continue\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"ğŸ‰ All Training Complete!\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Display final results\n",
    "if training_results:\n",
    "    print(\"\\nğŸ“Š Final Results Summary:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for model_name, result in training_results.items():\n",
    "        print(f\"{model_name}:\")\n",
    "        print(f\"  Final Test Accuracy: {result['final_test_acc']:.2f}%\")\n",
    "        print(f\"  Training Time: {result['training_time']:.1f}s ({result['training_time']/60:.1f} min)\")\n",
    "        print()\n",
    "    \n",
    "    # Find best model\n",
    "    best_model_name = max(training_results.keys(), key=lambda k: training_results[k]['final_test_acc'])\n",
    "    best_acc = training_results[best_model_name]['final_test_acc']\n",
    "    \n",
    "    print(f\"ğŸ† Best Model: {best_model_name} ({best_acc:.2f}% accuracy)\")\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ No models were successfully trained!\")\n",
    "\n",
    "print(\"\\nâœ… Training phase complete using built-in model API!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ced1db",
   "metadata": {},
   "source": [
    "## 13. Training Results Visualization\n",
    "\n",
    "Visualize and analyze the training results, including loss curves, accuracy plots, and model performance comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa553b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Training Results\n",
    "print(\"ğŸ“Š Visualizing training results...\")\n",
    "\n",
    "def plot_model_comparison(training_results):\n",
    "    \"\"\"Create comparison charts for final model performance.\"\"\"\n",
    "    if not training_results:\n",
    "        print(\"âŒ No training results to compare!\")\n",
    "        return\n",
    "    \n",
    "    model_names = list(training_results.keys())\n",
    "    test_accuracies = [result['final_test_acc'] for result in training_results.values()]\n",
    "    training_times = [result['training_time'] / 60 for result in training_results.values()]  # Convert to minutes\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    fig.suptitle('Model Performance Comparison', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Test Accuracy Comparison\n",
    "    bars1 = ax1.bar(model_names, test_accuracies, color=['skyblue', 'lightcoral', 'lightgreen', 'gold'][:len(model_names)])\n",
    "    ax1.set_title('Final Test Accuracy', fontweight='bold')\n",
    "    ax1.set_ylabel('Accuracy (%)')\n",
    "    ax1.set_ylim(0, max(test_accuracies) * 1.1 if test_accuracies else 1)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, acc in zip(bars1, test_accuracies):\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height + max(test_accuracies) * 0.01,\n",
    "                f'{acc:.2f}%', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Training Time Comparison\n",
    "    bars2 = ax2.bar(model_names, training_times, color=['skyblue', 'lightcoral', 'lightgreen', 'gold'][:len(model_names)])\n",
    "    ax2.set_title('Training Time', fontweight='bold')\n",
    "    ax2.set_ylabel('Time (minutes)')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, time_val in zip(bars2, training_times):\n",
    "        height = bar.get_height()\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., height + max(training_times) * 0.01,\n",
    "                f'{time_val:.1f}m', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_efficiency_analysis(training_results):\n",
    "    \"\"\"Create efficiency analysis chart.\"\"\"\n",
    "    if not training_results:\n",
    "        print(\"âŒ No training results to analyze!\")\n",
    "        return\n",
    "    \n",
    "    model_names = list(training_results.keys())\n",
    "    test_accuracies = [result['final_test_acc'] for result in training_results.values()]\n",
    "    training_times = [result['training_time'] / 60 for result in training_results.values()]  # Convert to minutes\n",
    "    \n",
    "    # Calculate efficiency scores (accuracy per minute)\n",
    "    efficiency_scores = [acc / time if time > 0 else 0 for acc, time in zip(test_accuracies, training_times)]\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "    fig.suptitle('Model Efficiency Analysis (Accuracy per Minute)', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    bars = ax.bar(model_names, efficiency_scores, color=['skyblue', 'lightcoral', 'lightgreen', 'gold'][:len(model_names)])\n",
    "    ax.set_title('Efficiency Score (Accuracy % per Minute)', fontweight='bold')\n",
    "    ax.set_ylabel('Efficiency Score')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, score in zip(bars, efficiency_scores):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height + max(efficiency_scores) * 0.01,\n",
    "                f'{score:.2f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Generate visualizations if we have training results\n",
    "if 'training_results' in locals() and training_results:\n",
    "    print(\"ğŸ“Š Generating model comparison charts...\")\n",
    "    plot_model_comparison(training_results)\n",
    "    \n",
    "    print(\"\\nğŸ¯ Generating efficiency analysis...\")\n",
    "    plot_efficiency_analysis(training_results)\n",
    "    \n",
    "    # Print detailed comparison\n",
    "    print(\"\\nğŸ“‹ Detailed Model Comparison:\")\n",
    "    print(\"-\" * 70)\n",
    "    print(f\"{'Model Name':<20} {'Test Acc (%)':<12} {'Time (min)':<12} {'Parameters':<15}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for model_name, result in training_results.items():\n",
    "        model = result['model']\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        time_min = result['training_time'] / 60\n",
    "        \n",
    "        print(f\"{model_name:<20} {result['final_test_acc']:<12.2f} {time_min:<12.1f} {total_params:<15,}\")\n",
    "    \n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    # Efficiency analysis\n",
    "    print(\"\\nğŸ¯ Efficiency Analysis:\")\n",
    "    best_acc_model = max(training_results.keys(), key=lambda k: training_results[k]['final_test_acc'])\n",
    "    fastest_model = min(training_results.keys(), key=lambda k: training_results[k]['training_time'])\n",
    "    \n",
    "    print(f\"   ğŸ† Best Accuracy: {best_acc_model} ({training_results[best_acc_model]['final_test_acc']:.2f}%)\")\n",
    "    print(f\"   âš¡ Fastest Training: {fastest_model} ({training_results[fastest_model]['training_time']/60:.1f} min)\")\n",
    "    \n",
    "    # Calculate efficiency score (accuracy per minute)\n",
    "    efficiency_scores = {}\n",
    "    for model_name, result in training_results.items():\n",
    "        efficiency = result['final_test_acc'] / (result['training_time'] / 60)\n",
    "        efficiency_scores[model_name] = efficiency\n",
    "    \n",
    "    most_efficient = max(efficiency_scores.keys(), key=lambda k: efficiency_scores[k])\n",
    "    print(f\"   ğŸ¯ Most Efficient: {most_efficient} ({efficiency_scores[most_efficient]:.2f} acc%/min)\")\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ No training results available for visualization!\")\n",
    "    print(\"ğŸ’¡ Make sure to run the training cells first (Step 10)\")\n",
    "\n",
    "print(\"\\nâœ… Training results visualization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bc16fe",
   "metadata": {},
   "source": [
    "## 14. Model Evaluation and Analysis\n",
    "\n",
    "Comprehensive evaluation of trained models including accuracy metrics, confusion matrices, and pathway analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ef03ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive Model Evaluation\n",
    "print(\"ğŸ” Performing comprehensive model evaluation...\")\n",
    "\n",
    "# Import project evaluation utilities\n",
    "try:\n",
    "    from src.evaluation.metrics import ModelEvaluator\n",
    "    from src.utils.visualization.training_plots import plot_training_curves\n",
    "    print(\"âœ… Project evaluation utilities imported successfully\")\n",
    "    use_project_evaluator = True\n",
    "except ImportError as e:\n",
    "    print(f\"âš ï¸ Could not import project evaluation utilities: {e}\")\n",
    "    print(\"ğŸ’¡ Using basic evaluation methods\")\n",
    "    use_project_evaluator = False\n",
    "\n",
    "# Import additional metrics for detailed analysis\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_confusion_matrix(predictions, targets, class_names, model_name, figsize=(12, 10)):\n",
    "    \"\"\"Plot confusion matrix with proper formatting.\"\"\"\n",
    "    cm = confusion_matrix(targets, predictions)\n",
    "    \n",
    "    plt.figure(figsize=figsize)\n",
    "    \n",
    "    # Normalize confusion matrix\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    # Create heatmap\n",
    "    sns.heatmap(cm_normalized, annot=False, fmt='.2f', cmap='Blues', \n",
    "                xticklabels=False, yticklabels=False)\n",
    "    plt.title(f'Confusion Matrix - {model_name}', fontsize=16, fontweight='bold')\n",
    "    plt.xlabel('Predicted Class', fontsize=12)\n",
    "    plt.ylabel('True Class', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary statistics\n",
    "    class_accuracy = cm_normalized.diagonal()\n",
    "    print(f\"   ğŸ“Š Per-class accuracy: Mean={class_accuracy.mean():.3f}, Std={class_accuracy.std():.3f}\")\n",
    "    print(f\"   ğŸ¯ Best performing classes: {np.argsort(class_accuracy)[-5:]}\")\n",
    "    print(f\"   ğŸ¯ Worst performing classes: {np.argsort(class_accuracy)[:5]}\")\n",
    "\n",
    "# Perform evaluation if we have trained models\n",
    "evaluation_results = {}\n",
    "\n",
    "if 'training_results' in locals() and training_results:\n",
    "    print(\"ğŸ” Starting comprehensive evaluation...\")\n",
    "    \n",
    "    for model_name, training_result in training_results.items():\n",
    "        model = training_result['model']\n",
    "        \n",
    "        try:\n",
    "            print(f\"\\nğŸ”¬ Evaluating {model_name}...\")\n",
    "            \n",
    "            if use_project_evaluator:\n",
    "                # Use project's ModelEvaluator\n",
    "                evaluator = ModelEvaluator(model, device)\n",
    "                eval_metrics = evaluator.evaluate(test_loader)\n",
    "                \n",
    "                # Store results\n",
    "                evaluation_results[model_name] = {\n",
    "                    'accuracy': eval_metrics['accuracy'],\n",
    "                    'precision': eval_metrics['precision'],\n",
    "                    'recall': eval_metrics['recall'],\n",
    "                    'f1_score': eval_metrics['f1_score'],\n",
    "                    'confusion_matrix': eval_metrics['confusion_matrix'],\n",
    "                    'predictions': eval_metrics.get('predictions', []),\n",
    "                    'targets': eval_metrics.get('targets', [])\n",
    "                }\n",
    "                \n",
    "                print(f\"   âœ… Accuracy: {eval_metrics['accuracy']:.2f}%\")\n",
    "                print(f\"   ğŸ“Š Precision: {eval_metrics['precision']:.4f}\")\n",
    "                print(f\"   ğŸ“Š Recall: {eval_metrics['recall']:.4f}\")\n",
    "                print(f\"   ğŸ“Š F1-Score: {eval_metrics['f1_score']:.4f}\")\n",
    "                \n",
    "            else:\n",
    "                # Fallback evaluation method\n",
    "                model.eval()\n",
    "                all_predictions = []\n",
    "                all_targets = []\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    for batch in tqdm(test_loader, desc=f\"Evaluating {model_name}\"):\n",
    "                        rgb_data = batch['rgb'].to(device)\n",
    "                        brightness_data = batch['brightness'].to(device)\n",
    "                        targets = batch['label'].to(device)\n",
    "                        \n",
    "                        # Forward pass based on model type\n",
    "                        if 'Dense' in model_name:\n",
    "                            rgb_flat = rgb_data.view(rgb_data.size(0), -1)\n",
    "                            brightness_flat = brightness_data.view(brightness_data.size(0), -1)\n",
    "                            outputs = model(rgb_flat, brightness_flat)\n",
    "                        else:\n",
    "                            outputs = model(rgb_data, brightness_data)\n",
    "                        \n",
    "                        _, predictions = torch.max(outputs, 1)\n",
    "                        \n",
    "                        all_predictions.extend(predictions.cpu().numpy())\n",
    "                        all_targets.extend(targets.cpu().numpy())\n",
    "                \n",
    "                # Calculate metrics\n",
    "                from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "                \n",
    "                accuracy = accuracy_score(all_targets, all_predictions) * 100\n",
    "                precision = precision_score(all_targets, all_predictions, average='weighted', zero_division=0)\n",
    "                recall = recall_score(all_targets, all_predictions, average='weighted', zero_division=0)\n",
    "                f1 = f1_score(all_targets, all_predictions, average='weighted', zero_division=0)\n",
    "                \n",
    "                evaluation_results[model_name] = {\n",
    "                    'accuracy': accuracy,\n",
    "                    'precision': precision,\n",
    "                    'recall': recall,\n",
    "                    'f1_score': f1,\n",
    "                    'predictions': all_predictions,\n",
    "                    'targets': all_targets\n",
    "                }\n",
    "                \n",
    "                print(f\"   âœ… Accuracy: {accuracy:.2f}%\")\n",
    "                print(f\"   ğŸ“Š Precision: {precision:.4f}\")\n",
    "                print(f\"   ğŸ“Š Recall: {recall:.4f}\")\n",
    "                print(f\"   ğŸ“Š F1-Score: {f1:.4f}\")\n",
    "            \n",
    "            # Generate confusion matrix for each model\n",
    "            print(f\"\\nğŸ“Š Generating confusion matrix for {model_name}...\")\n",
    "            plot_confusion_matrix(\n",
    "                evaluation_results[model_name]['predictions'],\n",
    "                evaluation_results[model_name]['targets'],\n",
    "                CIFAR100_FINE_LABELS,\n",
    "                model_name\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Evaluation failed for {model_name}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Generate comparison summary\n",
    "    if evaluation_results:\n",
    "        print(\"\\nğŸ”„ Model Performance Comparison:\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"{'Model':<20} {'Accuracy':<10} {'Precision':<10} {'Recall':<10} {'F1-Score':<10}\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        for model_name, eval_result in evaluation_results.items():\n",
    "            print(f\"{model_name:<20} {eval_result['accuracy']:<10.2f} {eval_result['precision']:<10.4f} \"\n",
    "                  f\"{eval_result['recall']:<10.4f} {eval_result['f1_score']:<10.4f}\")\n",
    "        \n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        # Find best performing model\n",
    "        best_model = max(evaluation_results.keys(), key=lambda k: evaluation_results[k]['accuracy'])\n",
    "        best_accuracy = evaluation_results[best_model]['accuracy']\n",
    "        print(f\"\\nğŸ† Best performing model: {best_model} ({best_accuracy:.2f}% accuracy)\")\n",
    "        \n",
    "    else:\n",
    "        print(\"âŒ No models were successfully evaluated!\")\n",
    "        \n",
    "else:\n",
    "    print(\"âŒ No trained models available for evaluation!\")\n",
    "    print(\"ğŸ’¡ Make sure to run the training cells first\")\n",
    "\n",
    "print(\"\\nâœ… Model evaluation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881346fc",
   "metadata": {},
   "source": [
    "## 15. Model Saving and Inference Demo\n",
    "\n",
    "Save trained models and demonstrate inference capabilities with sample predictions and pathway analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d0f0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Saving and Inference Demo\n",
    "print(\"ğŸ’¾ Setting up model saving and inference...\")\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def save_model(model, model_name, training_result, save_dir=\"models\"):\n",
    "    \"\"\"\n",
    "    Save a trained model with its metadata.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained PyTorch model\n",
    "        model_name: Name of the model\n",
    "        training_result: Training results dictionary\n",
    "        save_dir: Directory to save models\n",
    "    \"\"\"\n",
    "    # Create save directory\n",
    "    save_path = Path(save_dir)\n",
    "    save_path.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Prepare model info\n",
    "    model_info = {\n",
    "        'model_name': model_name,\n",
    "        'final_test_accuracy': training_result['final_test_acc'],\n",
    "        'training_time': training_result['training_time'],\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'model_class': model.__class__.__name__,\n",
    "        'num_parameters': sum(p.numel() for p in model.parameters()),\n",
    "        'training_history': training_result['history']\n",
    "    }\n",
    "    \n",
    "    # Save model\n",
    "    model_file = save_path / f\"{model_name.replace(' ', '_').lower()}_cifar100.pth\"\n",
    "    torch.save(model_info, model_file)\n",
    "    \n",
    "    print(f\"âœ… {model_name} saved to: {model_file}\")\n",
    "    return model_file\n",
    "\n",
    "def load_model(model_file, model_class, device):\n",
    "    \"\"\"\n",
    "    Load a saved model.\n",
    "    \n",
    "    Args:\n",
    "        model_file: Path to saved model file\n",
    "        model_class: Model class to instantiate\n",
    "        device: Device to load model on\n",
    "    \n",
    "    Returns:\n",
    "        Loaded model and metadata\n",
    "    \"\"\"\n",
    "    checkpoint = torch.load(model_file, map_location=device)\n",
    "    \n",
    "    # Print model info\n",
    "    print(f\"ğŸ“‹ Model Info:\")\n",
    "    print(f\"   Name: {checkpoint['model_name']}\")\n",
    "    print(f\"   Class: {checkpoint['model_class']}\")\n",
    "    print(f\"   Test Accuracy: {checkpoint['final_test_accuracy']:.2f}%\")\n",
    "    print(f\"   Parameters: {checkpoint['num_parameters']:,}\")\n",
    "    print(f\"   Training Time: {checkpoint['training_time']/60:.1f} minutes\")\n",
    "    \n",
    "    return checkpoint\n",
    "\n",
    "def demonstrate_inference(model, model_name, test_loader, device, class_names, num_samples=8):\n",
    "    \"\"\"\n",
    "    Demonstrate model inference on random test samples.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model\n",
    "        model_name: Name of the model\n",
    "        test_loader: Test data loader\n",
    "        device: Device to run inference on\n",
    "        class_names: List of class names\n",
    "        num_samples: Number of samples to demonstrate\n",
    "    \"\"\"\n",
    "    print(f\"\\nğŸ¯ Demonstrating {model_name} inference...\")\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # Get a batch of test data\n",
    "    test_batch = next(iter(test_loader))\n",
    "    rgb_data = test_batch['rgb'][:num_samples].to(device)\n",
    "    brightness_data = test_batch['brightness'][:num_samples].to(device)\n",
    "    true_labels = test_batch['label'][:num_samples]\n",
    "    \n",
    "    # Make predictions\n",
    "    with torch.no_grad():\n",
    "        if 'Dense' in model_name:\n",
    "            rgb_flat = rgb_data.view(rgb_data.size(0), -1)\n",
    "            brightness_flat = brightness_data.view(brightness_data.size(0), -1)\n",
    "            outputs = model(rgb_flat, brightness_flat)\n",
    "        else:\n",
    "            outputs = model(rgb_data, brightness_data)\n",
    "        \n",
    "        probabilities = torch.softmax(outputs, dim=1)\n",
    "        _, predicted_labels = torch.max(outputs, 1)\n",
    "    \n",
    "    # Visualize results\n",
    "    fig, axes = plt.subplots(2, num_samples//2, figsize=(16, 8))\n",
    "    fig.suptitle(f'{model_name} - Inference Demo', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        # Get RGB image for display\n",
    "        rgb_img = rgb_data[i].cpu().numpy().transpose(1, 2, 0)\n",
    "        \n",
    "        # Get predictions\n",
    "        true_class = class_names[true_labels[i].item()]\n",
    "        pred_class = class_names[predicted_labels[i].item()]\n",
    "        confidence = probabilities[i][predicted_labels[i]].item() * 100\n",
    "        \n",
    "        # Determine color (green for correct, red for incorrect)\n",
    "        color = 'green' if true_labels[i] == predicted_labels[i] else 'red'\n",
    "        \n",
    "        # Plot\n",
    "        axes[i].imshow(rgb_img)\n",
    "        axes[i].set_title(f'True: {true_class}\\nPred: {pred_class}\\nConf: {confidence:.1f}%', \n",
    "                         color=color, fontweight='bold', fontsize=10)\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate accuracy for this batch\n",
    "    batch_accuracy = (predicted_labels.cpu() == true_labels).float().mean().item() * 100\n",
    "    print(f\"   Batch accuracy: {batch_accuracy:.1f}%\")\n",
    "    \n",
    "    return predicted_labels.cpu().numpy(), probabilities.cpu().numpy()\n",
    "\n",
    "# Save all trained models\n",
    "saved_models = {}\n",
    "\n",
    "if 'training_results' in locals() and training_results:\n",
    "    print(\"ğŸ’¾ Saving trained models...\")\n",
    "    \n",
    "    for model_name, training_result in training_results.items():\n",
    "        try:\n",
    "            model_file = save_model(\n",
    "                model=training_result['model'],\n",
    "                model_name=model_name,\n",
    "                training_result=training_result\n",
    "            )\n",
    "            saved_models[model_name] = model_file\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Failed to save {model_name}: {e}\")\n",
    "    \n",
    "    print(f\"\\nâœ… Saved {len(saved_models)} models to 'models/' directory\")\n",
    "    \n",
    "    # Demonstrate inference for each model\n",
    "    print(\"\\nğŸ¯ Running inference demonstrations...\")\n",
    "    \n",
    "    for model_name, training_result in training_results.items():\n",
    "        try:\n",
    "            model = training_result['model']\n",
    "            predictions, probabilities = demonstrate_inference(\n",
    "                model=model,\n",
    "                model_name=model_name,\n",
    "                test_loader=test_loader,\n",
    "                device=device,\n",
    "                class_names=cifar100_fine_labels,\n",
    "                num_samples=8\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Inference demo failed for {model_name}: {e}\")\n",
    "            continue\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ No trained models available for saving!\")\n",
    "    print(\"ğŸ’¡ Make sure to run the training cells first (Step 10)\")\n",
    "\n",
    "# Example of how to load a saved model (for future use)\n",
    "print(\"\\nğŸ“– Example: Loading a saved model (for future use)\")\n",
    "print(\"```python\")\n",
    "print(\"# To load a model in the future:\")\n",
    "print(\"checkpoint = torch.load('models/dense_network_cifar100.pth')\")\n",
    "print(\"model = BaseMultiChannelNetwork(...)  # Initialize with same parameters\")\n",
    "print(\"model.load_state_dict(checkpoint['model_state_dict'])\")\n",
    "print(\"model.eval()\")\n",
    "print(\"```\")\n",
    "\n",
    "print(\"\\nâœ… Model saving and inference demo complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cdb981",
   "metadata": {},
   "source": [
    "## 16. Conclusion and Summary\n",
    "\n",
    "Summary of results, key findings, and next steps for multi-stream neural network research and deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8dce6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ‰ Multi-Stream Neural Networks: Project Summary\n",
    "print(\"ğŸ“‹ Generating project summary...\")\n",
    "\n",
    "def generate_project_summary():\n",
    "    \"\"\"Generate a comprehensive summary of the project results.\"\"\"\n",
    "    \n",
    "    print(\"ğŸ¯ MULTI-STREAM NEURAL NETWORKS ON CIFAR-100\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(\"\\nğŸ“Š PROJECT OVERVIEW:\")\n",
    "    print(\"   â€¢ Dataset: CIFAR-100 (100 classes, 32x32 images)\")\n",
    "    print(\"   â€¢ Architecture: Multi-stream (RGB + Brightness channels)\")\n",
    "    print(\"   â€¢ Models: Dense Network vs CNN (ResNet-style)\")\n",
    "    print(\"   â€¢ Training: Multi-channel data with batch processing\")\n",
    "    print(\"   â€¢ Evaluation: Comprehensive analysis with visualizations\")\n",
    "    \n",
    "    if 'training_results' in locals() and training_results:\n",
    "        print(\"\\nğŸ† TRAINING RESULTS:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        best_model = None\n",
    "        best_accuracy = 0\n",
    "        \n",
    "        for model_name, result in training_results.items():\n",
    "            accuracy = result['final_test_acc']\n",
    "            time_min = result['training_time'] / 60\n",
    "            params = sum(p.numel() for p in result['model'].parameters())\n",
    "            \n",
    "            print(f\"   {model_name}:\")\n",
    "            print(f\"     â€¢ Test Accuracy: {accuracy:.2f}%\")\n",
    "            print(f\"     â€¢ Training Time: {time_min:.1f} minutes\")\n",
    "            print(f\"     â€¢ Parameters: {params:,}\")\n",
    "            print(f\"     â€¢ Efficiency: {accuracy/time_min:.2f} acc%/min\")\n",
    "            \n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                best_model = model_name\n",
    "            \n",
    "            print()\n",
    "        \n",
    "        print(f\"ğŸ… BEST MODEL: {best_model} ({best_accuracy:.2f}% accuracy)\")\n",
    "        \n",
    "        # Architecture comparison\n",
    "        if len(training_results) > 1:\n",
    "            print(\"\\nğŸ”¬ ARCHITECTURE ANALYSIS:\")\n",
    "            print(\"-\" * 40)\n",
    "            models = list(training_results.items())\n",
    "            \n",
    "            if len(models) == 2:\n",
    "                model1_name, model1_result = models[0]\n",
    "                model2_name, model2_result = models[1]\n",
    "                \n",
    "                acc_diff = abs(model1_result['final_test_acc'] - model2_result['final_test_acc'])\n",
    "                time_diff = abs(model1_result['training_time'] - model2_result['training_time']) / 60\n",
    "                \n",
    "                print(f\"   â€¢ Accuracy difference: {acc_diff:.2f}%\")\n",
    "                print(f\"   â€¢ Training time difference: {time_diff:.1f} minutes\")\n",
    "                \n",
    "                if 'Dense' in model1_name or 'Dense' in model2_name:\n",
    "                    print(\"   â€¢ Dense vs CNN comparison completed\")\n",
    "                    if acc_diff < 2.0:\n",
    "                        print(\"   â€¢ Both architectures show similar performance\")\n",
    "                    else:\n",
    "                        winner = model1_name if model1_result['final_test_acc'] > model2_result['final_test_acc'] else model2_name\n",
    "                        print(f\"   â€¢ {winner} shows superior performance\")\n",
    "    \n",
    "    else:\n",
    "        print(\"\\nâš ï¸ No training results available for summary\")\n",
    "    \n",
    "    print(\"\\nğŸ”§ TECHNICAL ACHIEVEMENTS:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(\"   âœ… Modular CIFAR-100 data loading and preprocessing\")\n",
    "    print(\"   âœ… RGB to RGBL transformation with batch processing\")\n",
    "    print(\"   âœ… Multi-stream neural network architectures\")\n",
    "    print(\"   âœ… Efficient training pipeline with GPU acceleration\")\n",
    "    print(\"   âœ… Comprehensive evaluation and visualization\")\n",
    "    print(\"   âœ… Model saving and inference demonstration\")\n",
    "    print(\"   âœ… Production-ready code structure\")\n",
    "    \n",
    "    print(\"\\nğŸš€ NEXT STEPS & IMPROVEMENTS:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(\"   â€¢ Scale training to full CIFAR-100 dataset (50k training samples)\")\n",
    "    print(\"   â€¢ Implement advanced techniques:\")\n",
    "    print(\"     - Data augmentation (rotation, flip, crop)\")\n",
    "    print(\"     - Learning rate scheduling and early stopping\")\n",
    "    print(\"     - Model ensembling\")\n",
    "    print(\"     - Attention mechanisms\")\n",
    "    print(\"   â€¢ Experiment with different brightness extraction methods\")\n",
    "    print(\"   â€¢ Add more sophisticated CNN architectures (ResNet-50, EfficientNet)\")\n",
    "    print(\"   â€¢ Hyperparameter optimization (learning rate, batch size, etc.)\")\n",
    "    print(\"   â€¢ Transfer learning from pre-trained models\")\n",
    "    print(\"   â€¢ Multi-GPU training for faster convergence\")\n",
    "    \n",
    "    print(\"\\nğŸ’¡ KEY INSIGHTS:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(\"   â€¢ Multi-stream processing effectively utilizes RGB and brightness\")\n",
    "    print(\"   â€¢ Batch processing significantly improves data preprocessing speed\")\n",
    "    print(\"   â€¢ Both dense and CNN architectures show promise for multi-stream data\")\n",
    "    print(\"   â€¢ Modular design enables easy experimentation and extension\")\n",
    "    print(\"   â€¢ CIFAR-100's 100 classes provide good complexity for evaluation\")\n",
    "    \n",
    "    print(\"\\nğŸ“š RESOURCES & DOCUMENTATION:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(\"   â€¢ Code: src/ directory with modular components\")\n",
    "    print(\"   â€¢ Models: Saved in models/ directory\")\n",
    "    print(\"   â€¢ Tests: tests/ directory with comprehensive test suite\")\n",
    "    print(\"   â€¢ Documentation: README.md and inline documentation\")\n",
    "    print(\"   â€¢ Results: Cached processed data and training outputs\")\n",
    "    \n",
    "    print(\"\\nğŸ¯ PROJECT STATUS: COMPLETE âœ…\")\n",
    "    print(\"   Ready for production use and further research!\")\n",
    "\n",
    "# Run the summary\n",
    "generate_project_summary()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ™ THANK YOU FOR EXPLORING MULTI-STREAM NEURAL NETWORKS!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nğŸ’¬ Questions or improvements? Check the GitHub repository:\")\n",
    "print(\"   https://github.com/clingergab/Multi-Stream-Neural-Networks\")\n",
    "print(\"\\nğŸš€ Happy experimenting with multi-stream architectures!\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
