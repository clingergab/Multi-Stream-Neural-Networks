{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f0aae91",
   "metadata": {
    "id": "4f0aae91"
   },
   "source": [
    "# Multi-Stream Neural Networks: CIFAR-100 Training\n",
    "\n",
    "This notebook demonstrates the full pipeline for training multi-stream neural networks on CIFAR-100 data:\n",
    "\n",
    "üöÄ **Features:**\n",
    "- Automatic GPU detection and optimization\n",
    "- RGB to RGBL preprocessing with visualizations\n",
    "- BaseMultiChannelNetwork (Dense) and MultiChannelResNetNetwork (CNN) models\n",
    "- Dynamic progress bars during training\n",
    "- Comprehensive evaluation and analysis\n",
    "\n",
    "**Hardware Requirements:**\n",
    "- Google Colab with GPU runtime (A100/V100 recommended)\n",
    "- Sufficient memory for CIFAR-100 dataset processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7051aee4",
   "metadata": {
    "id": "7051aee4"
   },
   "source": [
    "## 1. Setup: Mount Drive and Navigate to Project\n",
    "\n",
    "Mount Google Drive and navigate to the existing Multi-Stream Neural Networks project directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "-A29FzCJ5c4R",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-A29FzCJ5c4R",
    "outputId": "9c45fe68-d922-4e02-978d-38efd4c4061d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xo3JWyNVEGMZ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xo3JWyNVEGMZ",
    "outputId": "34f5af2f-a23c-4e75-848f-94bbf88fffb4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'Multi-Stream-Neural-Networks'...\n",
      "remote: Enumerating objects: 228, done.\u001b[K\n",
      "remote: Counting objects: 100% (228/228), done.\u001b[K\n",
      "remote: Compressing objects: 100% (193/193), done.\u001b[K\n",
      "remote: Total 228 (delta 33), reused 221 (delta 26), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (228/228), 258.62 KiB | 2.94 MiB/s, done.\n",
      "Resolving deltas: 100% (33/33), done.\n"
     ]
    }
   ],
   "source": [
    "# Navigate to Drive and project directory\n",
    "import os\n",
    "os.chdir('/content/drive/MyDrive')\n",
    "\n",
    "# Navigate to the existing project (assuming it's already cloned)\n",
    "project_path = '/content/drive/MyDrive/Multi-Stream-Neural-Networks'\n",
    "if os.path.exists(project_path):\n",
    "    os.chdir(project_path)\n",
    "    print(f\"‚úÖ Found project at: {project_path}\")\n",
    "else:\n",
    "    print(f\"‚ùå Project not found at: {project_path}\")\n",
    "    print(\"üí° Please clone the repository first:\")\n",
    "    print(\"   !git clone https://github.com/clingergab/Multi-Stream-Neural-Networks.git\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1955bd1d",
   "metadata": {},
   "source": [
    "## 2. Update Repository\n",
    "\n",
    "Pull the latest changes from the repository to ensure you have the most up-to-date code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3baf6cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Pulling latest changes from repository...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/Multi-Stream-Neural-Networks'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müîÑ Pulling latest changes from repository...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Make sure we're in the right directory\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/content/drive/MyDrive/Multi-Stream-Neural-Networks\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müìÅ Current directory: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos\u001b[38;5;241m.\u001b[39mgetcwd()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Pull latest changes\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Multi-Stream-Neural-Networks'"
     ]
    }
   ],
   "source": [
    "# Update repository with latest changes\n",
    "print(\"üîÑ Pulling latest changes from repository...\")\n",
    "\n",
    "# Make sure we're in the right directory\n",
    "os.chdir('/content/drive/MyDrive/Multi-Stream-Neural-Networks')\n",
    "print(f\"üìÅ Current directory: {os.getcwd()}\")\n",
    "\n",
    "# Pull latest changes\n",
    "!git pull origin main\n",
    "\n",
    "# Show latest commit info\n",
    "print(\"\\nüìã Latest commit:\")\n",
    "!git log --oneline -1\n",
    "\n",
    "# Check status\n",
    "print(\"\\nüìä Repository status:\")\n",
    "!git status --short\n",
    "\n",
    "print(\"\\n‚úÖ Repository update complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac36a39",
   "metadata": {
    "id": "3ac36a39"
   },
   "source": [
    "## 3. Install Dependencies and Import Libraries\n",
    "\n",
    "Install compatible PyTorch/NumPy versions and import all required libraries for the multi-stream neural network training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c193f97",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "0c193f97",
    "outputId": "1786d6e3-0a7e-4e72-8267-b9b469dbcf78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
      "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
      "INFO: pip is looking at multiple versions of torch to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting torch\n",
      "  Downloading https://download.pytorch.org/whl/cu118/torch-2.7.1%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (28 kB)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu11==11.8.89 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_nvrtc_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (23.2 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m23.2/23.2 MB\u001b[0m \u001b[31m91.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.8.89 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_runtime_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (875 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m875.6/875.6 kB\u001b[0m \u001b[31m53.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.8.87 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_cupti_cu11-11.8.87-py3-none-manylinux1_x86_64.whl (13.1 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m126.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu11==9.1.0.70 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cudnn_cu11-9.1.0.70-py3-none-manylinux2014_x86_64.whl (663.9 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m663.9/663.9 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cublas-cu11==11.11.3.6 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cublas_cu11-11.11.3.6-py3-none-manylinux1_x86_64.whl (417.9 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m417.9/417.9 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-curand-cu11==10.3.0.86 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu118/nvidia_curand_cu11-10.3.0.86-py3-none-manylinux1_x86_64.whl (58.1 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m58.1/58.1 MB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.1.48 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cusolver_cu11-11.4.1.48-py3-none-manylinux1_x86_64.whl (128.2 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m128.2/128.2 MB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.5.86 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cusparse_cu11-11.7.5.86-py3-none-manylinux1_x86_64.whl (204.1 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m204.1/204.1 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nccl-cu11==2.21.5 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu118/nvidia_nccl_cu11-2.21.5-py3-none-manylinux2014_x86_64.whl (147.8 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m147.8/147.8 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvtx-cu11==11.8.86 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu118/nvidia_nvtx_cu11-11.8.86-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting triton==3.3.1 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/triton-3.3.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.3.1->torch) (75.2.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
      "Collecting torch\n",
      "  Downloading https://download.pytorch.org/whl/cu118/torch-2.6.0%2Bcu118-cp311-cp311-linux_x86_64.whl.metadata (27 kB)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
      "Downloading https://download.pytorch.org/whl/cu118/torch-2.6.0%2Bcu118-cp311-cp311-linux_x86_64.whl (848.7 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m848.7/848.7 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nvidia-cusolver-cu11, nvidia-cudnn-cu11, torch\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.6.0+cu124\n",
      "    Uninstalling torch-2.6.0+cu124:\n",
      "      Successfully uninstalled torch-2.6.0+cu124\n",
      "Successfully installed nvidia-cublas-cu11-11.11.3.6 nvidia-cuda-cupti-cu11-11.8.87 nvidia-cuda-nvrtc-cu11-11.8.89 nvidia-cuda-runtime-cu11-11.8.89 nvidia-cudnn-cu11-9.1.0.70 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.3.0.86 nvidia-cusolver-cu11-11.4.1.48 nvidia-cusparse-cu11-11.7.5.86 nvidia-nccl-cu11-2.21.5 nvidia-nvtx-cu11-11.8.86 torch-2.6.0+cu118\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
      "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.4)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.11/dist-packages (from seaborn) (2.2.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Detected that PyTorch and torchvision were compiled with different CUDA major versions. PyTorch has CUDA Version=11.8 and torchvision has CUDA Version=12.4. Please reinstall the torchvision that matches your PyTorch install.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-6-2174902703.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensorDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Don't re-order these, we need to load the _C extension (done when importing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# .extensions) before entering _meta_registrations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mextension\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_HAS_OPS\u001b[0m  \u001b[0;31m# usort:skip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_meta_registrations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m  \u001b[0;31m# usort:skip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/extension.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m \u001b[0m_check_cuda_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/extension.py\u001b[0m in \u001b[0;36m_check_cuda_version\u001b[0;34m()\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mt_minor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_version\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mt_major\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtv_major\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m     79\u001b[0m                 \u001b[0;34m\"Detected that PyTorch and torchvision were compiled with different CUDA major versions. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;34mf\"PyTorch has CUDA Version={t_major}.{t_minor} and torchvision has \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Detected that PyTorch and torchvision were compiled with different CUDA major versions. PyTorch has CUDA Version=11.8 and torchvision has CUDA Version=12.4. Please reinstall the torchvision that matches your PyTorch install."
     ]
    }
   ],
   "source": [
    "# üîß Smart Dependency Setup - Uses Colab defaults when possible\n",
    "print(\"üîß Setting up dependencies with Colab compatibility...\")\n",
    "\n",
    "# Check current environment\n",
    "import sys\n",
    "print(f\"Python version: {sys.version}\")\n",
    "\n",
    "# First, try to import existing packages (Colab defaults)\n",
    "print(\"üì¶ Checking for existing packages...\")\n",
    "\n",
    "try:\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    import torchvision\n",
    "    print(\"‚úÖ Found existing PyTorch installation\")\n",
    "    print(f\"   NumPy: {np.__version__}\")\n",
    "    print(f\"   PyTorch: {torch.__version__}\")\n",
    "    print(f\"   Torchvision: {torchvision.__version__}\")\n",
    "    print(f\"   CUDA available: {torch.cuda.is_available()}\")\n",
    "    \n",
    "    # Check for CUDA version compatibility\n",
    "    pytorch_cuda = torch.__version__.split('+')[-1] if '+' in torch.__version__ else 'unknown'\n",
    "    torchvision_cuda = torchvision.__version__.split('+')[-1] if '+' in torchvision.__version__ else 'unknown'\n",
    "    \n",
    "    if pytorch_cuda == torchvision_cuda:\n",
    "        print(f\"‚úÖ CUDA versions match: {pytorch_cuda}\")\n",
    "        use_existing = True\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è CUDA version mismatch: PyTorch={pytorch_cuda}, Torchvision={torchvision_cuda}\")\n",
    "        use_existing = False\n",
    "        \n",
    "except ImportError as e:\n",
    "    print(f\"‚ö†Ô∏è Missing packages: {e}\")\n",
    "    use_existing = False\n",
    "\n",
    "# If existing packages work, use them; otherwise install compatible versions\n",
    "if use_existing:\n",
    "    print(\"üéØ Using existing Colab packages - no installation needed!\")\n",
    "else:\n",
    "    print(\"üì¶ Installing compatible package versions...\")\n",
    "    \n",
    "    # Install specific compatible versions\n",
    "    !pip install -q \"numpy>=1.24.4,<2.1.0\"\n",
    "    !pip install -q torch==2.1.0+cu118 torchvision==0.16.0+cu118 torchaudio==2.1.0+cu118 --index-url https://download.pytorch.org/whl/cu118\n",
    "    !pip install -q tqdm matplotlib seaborn scikit-learn\n",
    "\n",
    "# Import all required libraries\n",
    "print(\"\\nüì¶ Importing all required libraries...\")\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add project to Python path\n",
    "sys.path.append('/content/drive/MyDrive/Multi-Stream-Neural-Networks')\n",
    "\n",
    "# Core libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Data and visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "from typing import Tuple, Dict, List\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Verify final setup\n",
    "print(\"üîç Final verification...\")\n",
    "print(f\"   NumPy: {np.__version__}\")\n",
    "print(f\"   PyTorch: {torch.__version__}\")\n",
    "print(f\"   Torchvision: {torchvision.__version__}\")\n",
    "print(f\"   CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "# Import project modules\n",
    "print(\"\\nüìÅ Importing project modules...\")\n",
    "try:\n",
    "    from src.models.basic_multi_channel.base_multi_channel_network import BaseMultiChannelNetwork\n",
    "    from src.models.basic_multi_channel.multi_channel_resnet_network import MultiChannelResNetNetwork\n",
    "    from src.transforms.rgb_to_rgbl import RGBtoRGBL\n",
    "    from src.utils.colab_utils import load_cifar10\n",
    "    from src.utils.cifar100_loader import get_cifar100_datasets, CIFAR100_FINE_LABELS, SimpleDataset\n",
    "    print(\"‚úÖ All imports successful!\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ö†Ô∏è Import warning: {e}\")\n",
    "    print(\"   Make sure you've updated the repository in the previous step\")\n",
    "\n",
    "# Final compatibility check\n",
    "print(\"\\nüß™ Testing NumPy-PyTorch compatibility...\")\n",
    "try:\n",
    "    test_array = np.array([1, 2, 3], dtype=np.float32)\n",
    "    test_tensor = torch.from_numpy(test_array)\n",
    "    result = test_tensor + 1\n",
    "    print(\"‚úÖ NumPy-PyTorch integration working perfectly!\")\n",
    "    print(\"‚úÖ Ready to proceed to Step 4 (Load CIFAR-100 Dataset)!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Minor compatibility issue: {e}\")\n",
    "    print(\"üí° This usually doesn't affect functionality - you can still proceed!\")\n",
    "\n",
    "print(\"\\nüéØ Setup complete! You can now proceed to Step 4.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4abf9ff",
   "metadata": {
    "id": "d4abf9ff"
   },
   "source": [
    "## 4. Load CIFAR-100 Dataset\n",
    "\n",
    "Load the CIFAR-100 dataset and verify its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0884a06",
   "metadata": {
    "id": "a0884a06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Setting up CIFAR-100 dataset loading...\n",
      "‚ùå Failed to import CIFAR-100 utilities. Make sure src/utils/cifar100_loader.py exists\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'CIFAR100Loader' from 'src.utils.cifar100_loader' (/Users/gclinger/Documents/projects/Multi-Stream-Neural-Networks/src/utils/cifar100_loader.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müìÅ Setting up CIFAR-100 dataset loading...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 9\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcifar100_loader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CIFAR100Loader, DirectCIFAR100Dataset\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚úÖ CIFAR-100 loader utilities imported successfully\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'CIFAR100Loader' from 'src.utils.cifar100_loader' (/Users/gclinger/Documents/projects/Multi-Stream-Neural-Networks/src/utils/cifar100_loader.py)"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries for data loading\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Import our utilities\n",
    "print(\"üìÅ Setting up CIFAR-100 dataset loading...\")\n",
    "try:\n",
    "    from src.utils.cifar100_loader import (\n",
    "        load_cifar100_raw, \n",
    "        get_cifar100_datasets, \n",
    "        SimpleDataset\n",
    "    )\n",
    "    print(\"‚úÖ CIFAR-100 loader utilities imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Failed to import CIFAR-100 utilities: {e}\")\n",
    "    print(\"üí° Make sure src/utils/cifar100_loader.py exists and is accessible\")\n",
    "    raise\n",
    "\n",
    "# Check if data folder exists\n",
    "data_path = Path(\"data/cifar-100\")\n",
    "if data_path.exists():\n",
    "    print(f\"‚úÖ Data folder found at: {data_path}\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Data folder not found at: {data_path}\")\n",
    "    print(\"üí° Will create it when downloading CIFAR-100 data\")\n",
    "\n",
    "# Load the datasets using our utility\n",
    "print(\"üìä Loading CIFAR-100 datasets...\")\n",
    "train_dataset, test_dataset, cifar100_fine_labels = get_cifar100_datasets()\n",
    "\n",
    "# Get raw data for backward compatibility with existing code\n",
    "train_data = train_dataset.data\n",
    "train_labels = train_dataset.labels\n",
    "test_data = test_dataset.data\n",
    "test_labels = test_dataset.labels\n",
    "\n",
    "print(f\"\\nüìä Dataset Info:\")\n",
    "print(f\"   Training samples: {len(train_data)}\")\n",
    "print(f\"   Test samples: {len(test_data)}\")\n",
    "print(f\"   Image shape: {train_data[0].shape}\")\n",
    "print(f\"   Number of classes: {len(cifar100_fine_labels)}\")\n",
    "print(f\"   Label format: Single integer (fine labels 0-99)\")\n",
    "print(f\"   Data range: [{train_data.min():.3f}, {train_data.max():.3f}]\")\n",
    "\n",
    "print(\"‚úÖ CIFAR-100 datasets ready for processing!\")\n",
    "print(\"üí° No torchvision naming conventions needed - loaded directly from pickle files!\")\n",
    "\n",
    "# Final compatibility check (simplified to avoid NumPy 2.x issues)\n",
    "print(\"\\nüß™ Testing NumPy-PyTorch compatibility...\")\n",
    "try:\n",
    "    # Simple test that should work with NumPy 2.x\n",
    "    # Use explicit imports to avoid scoping issues\n",
    "    import numpy\n",
    "    import torch as pytorch\n",
    "    \n",
    "    test_data = [1, 2, 3]\n",
    "    test_array = numpy.array(test_data, dtype=numpy.float32)\n",
    "    test_tensor = pytorch.tensor(test_data, dtype=pytorch.float32)\n",
    "    \n",
    "    # Test basic operations\n",
    "    array_sum = test_array.sum()\n",
    "    tensor_sum = test_tensor.sum()\n",
    "    \n",
    "    # Test tensor conversion (this often fails with NumPy 2.x issues)\n",
    "    converted_tensor = pytorch.from_numpy(test_array.copy())\n",
    "    \n",
    "    print(\"‚úÖ NumPy-PyTorch integration working!\")\n",
    "    print(\"‚úÖ Ready to proceed with CIFAR-100 loading!\")\n",
    "    \n",
    "    # Note about NumPy 2.x warnings\n",
    "    if numpy.__version__.startswith('2.'):\n",
    "        print(\"‚ÑπÔ∏è Note: You may see NumPy 2.x compatibility warnings, but they don't affect functionality.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è NumPy-PyTorch compatibility issue: {e}\")\n",
    "    print(\"üí° This is likely a NumPy 2.x scoping issue, but shouldn't affect CIFAR-100 loading\")\n",
    "    print(\"‚úÖ You can still proceed to Step 4 - the data loading will work fine!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6ae3dd",
   "metadata": {
    "id": "1a6ae3dd"
   },
   "source": [
    "## 5. Process Data: RGB to Multi-Stream Format\n",
    "\n",
    "Convert RGB images to both RGB and brightness (luminance) channels to create multi-stream data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945c9ead",
   "metadata": {
    "id": "945c9ead"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Starting RGB to RGBL data processing...\n",
      "‚úÖ RGBtoRGBL transform imported successfully\n",
      "\n",
      "üöÄ Processing training data...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 150\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;66;03m# Process training data\u001b[39;00m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124müöÄ Processing training data...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    149\u001b[0m train_rgb, train_brightness, train_labels \u001b[38;5;241m=\u001b[39m convert_dataset_to_multi_stream(\n\u001b[0;32m--> 150\u001b[0m     \u001b[43mtrain_dataset\u001b[49m, \n\u001b[1;32m    151\u001b[0m     max_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5000\u001b[39m,  \u001b[38;5;66;03m# Reduce for faster demo\u001b[39;00m\n\u001b[1;32m    152\u001b[0m     cache_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_processed.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    153\u001b[0m )\n\u001b[1;32m    155\u001b[0m \u001b[38;5;66;03m# Process test data\u001b[39;00m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124müß™ Processing test data...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# RGB to RGBL Data Processing with caching support\n",
    "print(\"üîÑ Starting RGB to RGBL data processing...\")\n",
    "\n",
    "# Import the transformation (should work if previous cells succeeded)\n",
    "try:\n",
    "    from src.transforms.rgb_to_rgbl import RGBtoRGBL\n",
    "    print(\"‚úÖ RGBtoRGBL transform imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Failed to import RGBtoRGBL: {e}\")\n",
    "    print(\"üí° Make sure you're in the correct directory and have run git pull\")\n",
    "    raise\n",
    "\n",
    "def convert_dataset_to_multi_stream_batch(dataset, max_samples=None, cache_file=None, force_reprocess=False, batch_size=256):\n",
    "    \"\"\"\n",
    "    Convert a CIFAR-100 dataset to multi-stream format (RGB + Brightness) using efficient batch processing.\n",
    "    Supports caching to avoid reprocessing if interrupted.\n",
    "\n",
    "    Args:\n",
    "        dataset: CIFAR-100 dataset\n",
    "        max_samples: Maximum number of samples to process (for faster testing)\n",
    "        cache_file: Path to cache file for resuming processing\n",
    "        force_reprocess: Force reprocessing even if cache exists\n",
    "        batch_size: Number of images to process in each batch (default: 256)\n",
    "\n",
    "    Returns:\n",
    "        rgb_data: RGB channel data [N, 3, 32, 32]\n",
    "        brightness_data: Brightness channel data [N, 1, 32, 32]\n",
    "        labels: Class labels [N]\n",
    "    \"\"\"\n",
    "    # Check for cached results first\n",
    "    if cache_file and Path(cache_file).exists() and not force_reprocess:\n",
    "        print(f\"‚úÖ Loading cached data from {cache_file}\")\n",
    "        try:\n",
    "            with open(cache_file, 'rb') as f:\n",
    "                cached_data = pickle.load(f)\n",
    "            print(f\"   Loaded {len(cached_data['labels'])} samples from cache\")\n",
    "            return cached_data['rgb_data'], cached_data['brightness_data'], cached_data['labels']\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Failed to load cache: {e}. Reprocessing...\")\n",
    "\n",
    "    print(f\"üîÑ Converting dataset to multi-stream format using batch processing...\")\n",
    "    print(f\"   Batch size: {batch_size}\")\n",
    "\n",
    "    # Initialize RGB to RGBL transform\n",
    "    rgb_to_rgbl = RGBtoRGBL()\n",
    "    print(\"‚úÖ RGB to RGBL transform initialized\")\n",
    "\n",
    "    # Determine number of samples to process\n",
    "    num_samples = len(dataset) if max_samples is None else min(max_samples, len(dataset))\n",
    "    print(f\"   Processing {num_samples} samples...\")\n",
    "\n",
    "    # Initialize arrays\n",
    "    rgb_data = []\n",
    "    brightness_data = []\n",
    "    labels = []\n",
    "\n",
    "    # Process samples in batches with progress bar\n",
    "    try:\n",
    "        num_batches = (num_samples + batch_size - 1) // batch_size\n",
    "        \n",
    "        with tqdm(range(0, num_samples, batch_size), desc=\"Processing batches\", total=num_batches) as pbar:\n",
    "            for batch_start in pbar:\n",
    "                batch_end = min(batch_start + batch_size, num_samples)\n",
    "                current_batch_size = batch_end - batch_start\n",
    "                \n",
    "                try:\n",
    "                    # Collect batch data\n",
    "                    batch_images = []\n",
    "                    batch_labels = []\n",
    "                    \n",
    "                    for i in range(batch_start, batch_end):\n",
    "                        image, label = dataset[i]\n",
    "                        \n",
    "                        # Convert label to int if it's a tensor\n",
    "                        if hasattr(label, 'item'):\n",
    "                            label = label.item()\n",
    "                        \n",
    "                        batch_images.append(image)\n",
    "                        batch_labels.append(label)\n",
    "                    \n",
    "                    # Stack images into batch tensor [B, C, H, W]\n",
    "                    batch_tensor = torch.stack(batch_images)\n",
    "                    \n",
    "                    # Apply RGBtoRGBL transform to entire batch - it returns a TUPLE (rgb_batch, brightness_batch)\n",
    "                    rgb_batch, brightness_batch = rgb_to_rgbl(batch_tensor)\n",
    "                    \n",
    "                    # Validate the returned tensors\n",
    "                    if not isinstance(rgb_batch, torch.Tensor) or not isinstance(brightness_batch, torch.Tensor):\n",
    "                        print(f\"‚ö†Ô∏è Warning: RGBtoRGBL returned unexpected types: {type(rgb_batch)}, {type(brightness_batch)}\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Validate tensor dimensions\n",
    "                    if rgb_batch.dim() != 4 or rgb_batch.shape[1] != 3:\n",
    "                        print(f\"‚ö†Ô∏è Warning: Expected RGB batch [B, 3, H, W], got {rgb_batch.shape}\")\n",
    "                        continue\n",
    "                        \n",
    "                    if brightness_batch.dim() != 4 or brightness_batch.shape[1] != 1:\n",
    "                        print(f\"‚ö†Ô∏è Warning: Expected brightness batch [B, 1, H, W], got {brightness_batch.shape}\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Convert to numpy and add to our data lists\n",
    "                    rgb_data.append(rgb_batch.numpy())\n",
    "                    brightness_data.append(brightness_batch.numpy())\n",
    "                    labels.extend(batch_labels)\n",
    "                    \n",
    "                    # Update progress bar\n",
    "                    pbar.set_postfix({'processed': len(labels), 'batch_size': current_batch_size})\n",
    "\n",
    "                    # Save checkpoint every few batches (every ~5000 samples)\n",
    "                    if cache_file and len(labels) % 5000 < batch_size:\n",
    "                        checkpoint_data = {\n",
    "                            'rgb_data': np.concatenate(rgb_data, axis=0),\n",
    "                            'brightness_data': np.concatenate(brightness_data, axis=0),\n",
    "                            'labels': np.array(labels),\n",
    "                            'processed': len(labels)\n",
    "                        }\n",
    "                        with open(f\"{cache_file}.checkpoint\", 'wb') as f:\n",
    "                            pickle.dump(checkpoint_data, f)\n",
    "                        print(f\"   Checkpoint saved at sample {len(labels)}\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ö†Ô∏è Error processing batch {batch_start}-{batch_end}: {e}\")\n",
    "                    continue\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(f\"\\n‚ö†Ô∏è Processing interrupted at sample {len(labels)}\")\n",
    "        if len(labels) > 0:\n",
    "            print(\"üíæ Saving partial results...\")\n",
    "            # Concatenate what we have so far\n",
    "            rgb_data = np.concatenate(rgb_data, axis=0)\n",
    "            brightness_data = np.concatenate(brightness_data, axis=0)\n",
    "            labels = np.array(labels)\n",
    "            \n",
    "            if cache_file:\n",
    "                partial_data = {\n",
    "                    'rgb_data': rgb_data,\n",
    "                    'brightness_data': brightness_data,\n",
    "                    'labels': labels,\n",
    "                    'processed': len(labels)\n",
    "                }\n",
    "                with open(f\"{cache_file}.partial\", 'wb') as f:\n",
    "                    pickle.dump(partial_data, f)\n",
    "                print(f\"   Partial results saved to {cache_file}.partial\")\n",
    "            \n",
    "            return rgb_data, brightness_data, labels\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "    # Concatenate all batches\n",
    "    if len(rgb_data) == 0:\n",
    "        raise ValueError(\"No data was processed successfully\")\n",
    "        \n",
    "    rgb_data = np.concatenate(rgb_data, axis=0)\n",
    "    brightness_data = np.concatenate(brightness_data, axis=0)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    # Save to cache if specified\n",
    "    if cache_file:\n",
    "        final_data = {\n",
    "            'rgb_data': rgb_data,\n",
    "            'brightness_data': brightness_data,\n",
    "            'labels': labels\n",
    "        }\n",
    "        with open(cache_file, 'wb') as f:\n",
    "            pickle.dump(final_data, f)\n",
    "        print(f\"üíæ Results cached to {cache_file}\")\n",
    "\n",
    "    print(f\"‚úÖ Conversion complete!\")\n",
    "    print(f\"   RGB data shape: {rgb_data.shape}\")\n",
    "    print(f\"   Brightness data shape: {brightness_data.shape}\")\n",
    "    print(f\"   Labels shape: {labels.shape}\")\n",
    "\n",
    "    return rgb_data, brightness_data, labels\n",
    "\n",
    "# Process training data\n",
    "print(\"\\nüöÄ Processing training data...\")\n",
    "train_rgb, train_brightness, train_labels = convert_dataset_to_multi_stream_batch(\n",
    "    train_dataset, \n",
    "    max_samples=5000,  # Reduce for faster demo\n",
    "    cache_file=\"train_processed.pkl\",\n",
    "    batch_size=256  # Process 256 images at once\n",
    ")\n",
    "\n",
    "# Process test data\n",
    "print(\"\\nüß™ Processing test data...\")\n",
    "test_rgb, test_brightness, test_labels = convert_dataset_to_multi_stream_batch(\n",
    "    test_dataset, \n",
    "    max_samples=1000,  # Reduce for faster demo\n",
    "    cache_file=\"test_processed.pkl\",\n",
    "    batch_size=256  # Process 256 images at once\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Final Dataset Shapes:\")\n",
    "print(f\"   Training RGB: {train_rgb.shape}\")\n",
    "print(f\"   Training Brightness: {train_brightness.shape}\")\n",
    "print(f\"   Training Labels: {train_labels.shape}\")\n",
    "print(f\"   Test RGB: {test_rgb.shape}\")\n",
    "print(f\"   Test Brightness: {test_brightness.shape}\")\n",
    "print(f\"   Test Labels: {test_labels.shape}\")\n",
    "\n",
    "print(\"\\nüí° Note: Processed data is cached. To reprocess, set force_reprocess=True\")\n",
    "print(\"üöÄ Batch processing complete - much faster than image-by-image processing!\")\n",
    "\n",
    "# Final compatibility check (simplified to avoid NumPy 2.x issues)\n",
    "print(\"\\nüß™ Testing NumPy-PyTorch compatibility...\")\n",
    "try:\n",
    "    # Simple test that should work with NumPy 2.x\n",
    "    # Use explicit imports to avoid scoping issues\n",
    "    import numpy\n",
    "    import torch as pytorch\n",
    "    \n",
    "    test_data = [1, 2, 3]\n",
    "    test_array = numpy.array(test_data, dtype=numpy.float32)\n",
    "    test_tensor = pytorch.tensor(test_data, dtype=pytorch.float32)\n",
    "    \n",
    "    # Test basic operations\n",
    "    array_sum = test_array.sum()\n",
    "    tensor_sum = test_tensor.sum()\n",
    "    \n",
    "    # Test tensor conversion (this often fails with NumPy 2.x issues)\n",
    "    converted_tensor = pytorch.from_numpy(test_array.copy())\n",
    "    \n",
    "    print(\"‚úÖ NumPy-PyTorch integration working!\")\n",
    "    print(\"‚úÖ Ready to proceed with CIFAR-100 loading!\")\n",
    "    \n",
    "    # Note about NumPy 2.x warnings\n",
    "    if numpy.__version__.startswith('2.'):\n",
    "        print(\"‚ÑπÔ∏è Note: You may see NumPy 2.x compatibility warnings, but they don't affect functionality.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è NumPy-PyTorch compatibility issue: {e}\")\n",
    "    print(\"üí° This is likely a NumPy 2.x scoping issue, but shouldn't affect CIFAR-100 loading\")\n",
    "    print(\"‚úÖ You can still proceed to Step 4 - the data loading will work fine!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c64ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify CIFAR-100 data structure\n",
    "print(\"üîç Verifying CIFAR-100 data structure...\")\n",
    "\n",
    "# Check a few samples\n",
    "sample = train_dataset[0]\n",
    "image, target = sample\n",
    "print(f\"‚úÖ Sample structure: (image_tensor, integer_label)\")\n",
    "print(f\"   Image shape: {image.shape}\")\n",
    "print(f\"   Label: {target} ({cifar100_fine_labels[target]})\")\n",
    "print(f\"   Label type: {type(target)}\")\n",
    "\n",
    "print(f\"\\nüìä Dataset summary:\")\n",
    "print(f\"   Training samples: {len(train_dataset)}\")\n",
    "print(f\"   Test samples: {len(test_dataset)}\")\n",
    "print(f\"   Classes: 100 (fine labels 0-99)\")\n",
    "print(\"‚úÖ Ready for multi-stream processing!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73e3a1b",
   "metadata": {
    "id": "e73e3a1b"
   },
   "source": [
    "## 6. Visualize Sample Images: RGB and Brightness Side by Side\n",
    "\n",
    "Display sample images showing the original RGB and extracted brightness channels to understand the multi-stream transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68afa3b1",
   "metadata": {
    "id": "68afa3b1"
   },
   "outputs": [],
   "source": [
    "def visualize_rgb_brightness_samples(rgb_data, brightness_data, labels, num_samples=5):\n",
    "    \"\"\"\n",
    "    Visualize RGB and brightness images side by side.\n",
    "\n",
    "    Args:\n",
    "        rgb_data: RGB image data [N, 3, H, W]\n",
    "        brightness_data: Brightness image data [N, 1, H, W]\n",
    "        labels: Image labels\n",
    "        num_samples: Number of samples to visualize\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(num_samples, 2, figsize=(8, 2.5 * num_samples))\n",
    "    fig.suptitle('RGB vs Brightness Channel Comparison', fontsize=16, fontweight='bold')\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        # Get RGB image (convert from CHW to HWC for matplotlib)\n",
    "        rgb_img = np.transpose(rgb_data[i], (1, 2, 0))\n",
    "\n",
    "        # Get brightness image (squeeze channel dimension)\n",
    "        brightness_img = brightness_data[i, 0]  # Remove channel dimension\n",
    "\n",
    "        # Get class name\n",
    "        class_name = cifar100_fine_labels[labels[i]]\n",
    "\n",
    "        # Plot RGB image\n",
    "        axes[i, 0].imshow(rgb_img)\n",
    "        axes[i, 0].set_title(f'RGB - {class_name}', fontweight='bold')\n",
    "        axes[i, 0].axis('off')\n",
    "\n",
    "        # Plot brightness image\n",
    "        axes[i, 1].imshow(brightness_img, cmap='gray')\n",
    "        axes[i, 1].set_title(f'Brightness - {class_name}', fontweight='bold')\n",
    "        axes[i, 1].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize sample images\n",
    "print(\"üñºÔ∏è Sample RGB vs Brightness Images:\")\n",
    "visualize_rgb_brightness_samples(train_rgb, train_brightness, train_labels, num_samples=5)\n",
    "\n",
    "# Show data statistics\n",
    "def show_data_statistics(rgb_data, brightness_data, labels):\n",
    "    \"\"\"Show basic statistics about the data.\"\"\"\n",
    "    print(f\"\\nüìä Data Statistics:\")\n",
    "    print(f\"   RGB data range: [{rgb_data.min():.3f}, {rgb_data.max():.3f}]\")\n",
    "    print(f\"   Brightness data range: [{brightness_data.min():.3f}, {brightness_data.max():.3f}]\")\n",
    "    print(f\"   Number of unique classes: {len(np.unique(labels))}\")\n",
    "\n",
    "    # Class distribution\n",
    "    unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "    print(f\"   Samples per class: {counts.min()} - {counts.max()}\")\n",
    "    print(f\"   Average samples per class: {counts.mean():.1f}\")\n",
    "\n",
    "show_data_statistics(train_rgb, train_brightness, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a90244",
   "metadata": {
    "id": "c4a90244"
   },
   "source": [
    "## 7. Additional Data Visualizations\n",
    "\n",
    "Explore the data with helpful visualizations including class distribution and pixel intensity analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b394c3b3",
   "metadata": {
    "id": "b394c3b3"
   },
   "outputs": [],
   "source": [
    "# Class distribution visualization\n",
    "def plot_class_distribution(labels, title=\"Class Distribution\"):\n",
    "    \"\"\"Plot the distribution of classes in the dataset.\"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "\n",
    "    plt.bar(unique_labels, counts, alpha=0.7, color='skyblue', edgecolor='navy')\n",
    "    plt.title(title, fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Class ID')\n",
    "    plt.ylabel('Number of Samples')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Pixel intensity histograms\n",
    "def plot_intensity_histograms(rgb_data, brightness_data):\n",
    "    \"\"\"Plot histograms of pixel intensities for RGB and brightness channels.\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "    fig.suptitle('Pixel Intensity Distributions', fontsize=16, fontweight='bold')\n",
    "\n",
    "    # RGB histograms\n",
    "    colors = ['red', 'green', 'blue']\n",
    "    for i, color in enumerate(colors):\n",
    "        axes[0, 0].hist(rgb_data[:, i].flatten(), bins=50, alpha=0.6,\n",
    "                       color=color, label=f'{color.upper()} channel')\n",
    "    axes[0, 0].set_title('RGB Channel Intensities')\n",
    "    axes[0, 0].set_xlabel('Pixel Value')\n",
    "    axes[0, 0].set_ylabel('Frequency')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Brightness histogram\n",
    "    axes[0, 1].hist(brightness_data.flatten(), bins=50, alpha=0.7,\n",
    "                   color='gray', edgecolor='black')\n",
    "    axes[0, 1].set_title('Brightness Channel Intensities')\n",
    "    axes[0, 1].set_xlabel('Pixel Value')\n",
    "    axes[0, 1].set_ylabel('Frequency')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    # Mean pixel values per channel\n",
    "    rgb_means = np.mean(rgb_data, axis=(0, 2, 3))\n",
    "    brightness_mean = np.mean(brightness_data)\n",
    "\n",
    "    channel_names = ['Red', 'Green', 'Blue', 'Brightness']\n",
    "    channel_means = [rgb_means[0], rgb_means[1], rgb_means[2], brightness_mean]\n",
    "\n",
    "    axes[1, 0].bar(channel_names, channel_means,\n",
    "                  color=['red', 'green', 'blue', 'gray'], alpha=0.7)\n",
    "    axes[1, 0].set_title('Mean Pixel Values by Channel')\n",
    "    axes[1, 0].set_ylabel('Mean Pixel Value')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Sample grid\n",
    "    axes[1, 1].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Sample grid of images\n",
    "def plot_sample_grid(rgb_data, labels, grid_size=(4, 8)):\n",
    "    \"\"\"Plot a grid of sample images.\"\"\"\n",
    "    fig, axes = plt.subplots(grid_size[0], grid_size[1], figsize=(16, 8))\n",
    "    fig.suptitle('Sample Images from CIFAR-100 Dataset', fontsize=16, fontweight='bold')\n",
    "\n",
    "    for i in range(grid_size[0]):\n",
    "        for j in range(grid_size[1]):\n",
    "            idx = i * grid_size[1] + j\n",
    "            if idx < len(rgb_data):\n",
    "                img = np.transpose(rgb_data[idx], (1, 2, 0))\n",
    "                class_name = cifar100_fine_labels[labels[idx]]\n",
    "\n",
    "                axes[i, j].imshow(img)\n",
    "                axes[i, j].set_title(class_name, fontsize=8)\n",
    "                axes[i, j].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Generate visualizations\n",
    "print(\"üìä Generating additional visualizations...\")\n",
    "\n",
    "# Class distribution\n",
    "plot_class_distribution(train_labels, \"Training Set Class Distribution\")\n",
    "\n",
    "# Intensity histograms\n",
    "plot_intensity_histograms(train_rgb[:1000], train_brightness[:1000])  # Sample for speed\n",
    "\n",
    "# Sample grid\n",
    "plot_sample_grid(train_rgb, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86919e29",
   "metadata": {
    "id": "86919e29"
   },
   "source": [
    "## 8. Create Multi-Stream Neural Network Models\n",
    "\n",
    "Create and configure multi-stream models using factory functions and the built-in compile API for clean, maintainable model setup.\n",
    "\n",
    "**Key Features:**\n",
    "- **Updated Factory Functions**: Support for different input sizes (`color_input_size`, `brightness_input_size`)\n",
    "- **Built-in `.compile()` Method**: Keras-like model configuration with optimizer, loss, and metrics\n",
    "- **Automatic Parameter Counting**: Easy model comparison and analysis\n",
    "- **Device-Aware Initialization**: Automatic GPU detection and optimization\n",
    "- **Forward Pass Testing**: Proper API usage validation with both research and classification modes\n",
    "\n",
    "**Available Factory Functions:**\n",
    "- **Dense Models**: `base_multi_channel_small`, `base_multi_channel_medium`, `base_multi_channel_large`\n",
    "  - Now support: `color_input_size=3072, brightness_input_size=1024` for CIFAR-100\n",
    "  - Backward compatible: `input_size=N` for same-size streams\n",
    "- **CNN Models**: `multi_channel_resnet18`, `multi_channel_resnet34`, `multi_channel_resnet50`\n",
    "  - Support different channel counts: `color_input_channels=3, brightness_input_channels=1`\n",
    "\n",
    "**API Usage Examples:**\n",
    "```python\n",
    "# Dense model with different input sizes\n",
    "model = base_multi_channel_medium(\n",
    "    color_input_size=3072,      # RGB: 3*32*32\n",
    "    brightness_input_size=1024, # Brightness: 1*32*32  \n",
    "    num_classes=100\n",
    ")\n",
    "\n",
    "# CNN model with different channel counts\n",
    "model = multi_channel_resnet18(\n",
    "    color_input_channels=3,     # RGB channels\n",
    "    brightness_input_channels=1, # Brightness channels\n",
    "    num_classes=100\n",
    ")\n",
    "\n",
    "# Compile and use\n",
    "model.compile(optimizer='adam', learning_rate=0.001)\n",
    "model.fit(rgb_data, brightness_data, labels)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e83a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üè≠ Creating Multi-Stream Neural Network Models using Factory Functions...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müè≠ Creating Multi-Stream Neural Network Models using Factory Functions...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Check GPU availability and set device\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müñ•Ô∏è Using device: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "# Model Configuration and Creation using Factory Functions\n",
    "print(\"üè≠ Creating Multi-Stream Neural Network Models using Factory Functions...\")\n",
    "\n",
    "# Check GPU availability and set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üñ•Ô∏è Using device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "# Model configuration based on CIFAR-100 data\n",
    "print(f\"\\nüìä Model Configuration:\")\n",
    "num_classes = 100  # CIFAR-100 has 100 classes\n",
    "input_channels_rgb = 3  # RGB channels\n",
    "input_channels_brightness = 1  # Single brightness channel\n",
    "image_size = 32  # CIFAR-100 image size\n",
    "\n",
    "# Calculate input sizes for dense model (flattened features)\n",
    "rgb_input_size = input_channels_rgb * image_size * image_size  # 3 * 32 * 32 = 3072\n",
    "brightness_input_size = input_channels_brightness * image_size * image_size  # 1 * 32 * 32 = 1024\n",
    "\n",
    "print(f\"   Number of classes: {num_classes}\")\n",
    "print(f\"   RGB channels: {input_channels_rgb} (input size: {rgb_input_size})\")\n",
    "print(f\"   Brightness channels: {input_channels_brightness} (input size: {brightness_input_size})\")\n",
    "print(f\"   Image size: {image_size}x{image_size}\")\n",
    "\n",
    "# Import model classes and factory functions\n",
    "try:\n",
    "    from src.models.basic_multi_channel.base_multi_channel_network import (\n",
    "        BaseMultiChannelNetwork, \n",
    "        base_multi_channel_small, \n",
    "        base_multi_channel_medium, \n",
    "        base_multi_channel_large\n",
    "    )\n",
    "    from src.models.basic_multi_channel.multi_channel_resnet_network import (\n",
    "        MultiChannelResNetNetwork,\n",
    "        multi_channel_resnet18,\n",
    "        multi_channel_resnet34,\n",
    "        multi_channel_resnet50\n",
    "    )\n",
    "    print(\"‚úÖ Model classes and factory functions imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Failed to import model classes: {e}\")\n",
    "    print(\"üí° Make sure the project structure is correct and modules are available\")\n",
    "    raise\n",
    "\n",
    "# Create Dense Network using factory function\n",
    "print(\"\\nüî¨ Creating Dense Network using factory function...\")\n",
    "try:\n",
    "    # Use the factory function with separate input sizes for multi-stream\n",
    "    dense_model = base_multi_channel_medium(\n",
    "        color_input_size=rgb_input_size,\n",
    "        brightness_input_size=brightness_input_size,\n",
    "        num_classes=num_classes,\n",
    "        use_shared_classifier=True,  # Use shared classifier for efficiency\n",
    "        activation='relu',\n",
    "        device='auto'\n",
    "    )\n",
    "    \n",
    "    # Compile the model using the built-in API\n",
    "    print(\"‚öôÔ∏è Compiling Dense Network...\")\n",
    "    dense_model.compile(\n",
    "        optimizer='adam',\n",
    "        learning_rate=0.001,\n",
    "        loss='cross_entropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Count parameters\n",
    "    dense_params = sum(p.numel() for p in dense_model.parameters())\n",
    "    dense_trainable = sum(p.numel() for p in dense_model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"‚úÖ Dense Network created and compiled using factory function\")\n",
    "    print(f\"   Architecture: Medium (512->256->128)\")\n",
    "    print(f\"   Total parameters: {dense_params:,}\")\n",
    "    print(f\"   Trainable parameters: {dense_trainable:,}\")\n",
    "    print(f\"   RGB input size: {rgb_input_size} (flattened)\")\n",
    "    print(f\"   Brightness input size: {brightness_input_size} (flattened)\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to create Dense Network: {e}\")\n",
    "    print(f\"üí° Error details: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    dense_model = None\n",
    "\n",
    "# Create CNN Network using factory function\n",
    "print(\"\\nüî¨ Creating CNN Network using factory function...\")\n",
    "try:\n",
    "    # Use the factory function for ResNet-18 style CNN\n",
    "    cnn_model = multi_channel_resnet18(\n",
    "        num_classes=num_classes,\n",
    "        color_input_channels=input_channels_rgb,\n",
    "        brightness_input_channels=input_channels_brightness,\n",
    "        activation='relu',\n",
    "        device='auto'\n",
    "    )\n",
    "    \n",
    "    # Compile the model using the built-in API\n",
    "    print(\"‚öôÔ∏è Compiling CNN Network...\")\n",
    "    cnn_model.compile(\n",
    "        optimizer='adam',\n",
    "        learning_rate=0.001,\n",
    "        loss='cross_entropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Count parameters\n",
    "    cnn_params = sum(p.numel() for p in cnn_model.parameters())\n",
    "    cnn_trainable = sum(p.numel() for p in cnn_model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"‚úÖ CNN Network created and compiled using factory function\")\n",
    "    print(f\"   Architecture: ResNet-18 style (2,2,2,2 blocks)\")\n",
    "    print(f\"   Total parameters: {cnn_params:,}\")\n",
    "    print(f\"   Trainable parameters: {cnn_trainable:,}\")\n",
    "    print(f\"   Input shape: RGB {(input_channels_rgb, image_size, image_size)}, Brightness {(input_channels_brightness, image_size, image_size)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to create CNN Network: {e}\")\n",
    "    print(f\"üí° Error details: {str(e)}\")\n",
    "    cnn_model = None\n",
    "\n",
    "# Model comparison\n",
    "if dense_model is not None and cnn_model is not None:\n",
    "    print(f\"\\nüìà Model Comparison:\")\n",
    "    print(f\"   Dense Model: {dense_params:,} parameters\")\n",
    "    print(f\"   CNN Model: {cnn_params:,} parameters\")\n",
    "    print(f\"   CNN is {cnn_params/dense_params:.1f}x larger than Dense\")\n",
    "elif dense_model is not None:\n",
    "    print(f\"\\nüìà Available Models:\")\n",
    "    print(f\"   Dense Model: {dense_params:,} parameters\")\n",
    "elif cnn_model is not None:\n",
    "    print(f\"\\nüìà Available Models:\")\n",
    "    print(f\"   CNN Model: {cnn_params:,} parameters\")\n",
    "\n",
    "# Test model forward pass with sample data\n",
    "print(\"\\nüß™ Testing model forward pass with proper APIs...\")\n",
    "\n",
    "try:\n",
    "    # Create sample batch data\n",
    "    batch_size = 4\n",
    "    sample_rgb = torch.randn(batch_size, input_channels_rgb, image_size, image_size).to(device)\n",
    "    sample_brightness = torch.randn(batch_size, input_channels_brightness, image_size, image_size).to(device)\n",
    "    \n",
    "    print(f\"   Sample RGB shape: {sample_rgb.shape}\")\n",
    "    print(f\"   Sample brightness shape: {sample_brightness.shape}\")\n",
    "    \n",
    "    # Test Dense Model\n",
    "    if dense_model is not None:\n",
    "        # Flatten inputs for dense model with correct sizes\n",
    "        rgb_flat = sample_rgb.view(batch_size, rgb_input_size)  # Should be (4, 3072)\n",
    "        brightness_flat = sample_brightness.view(batch_size, brightness_input_size)  # Should be (4, 1024)\n",
    "        \n",
    "        print(f\"   Dense RGB flat shape: {rgb_flat.shape}\")\n",
    "        print(f\"   Dense brightness flat shape: {brightness_flat.shape}\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Test standard classification API\n",
    "            dense_output = dense_model(rgb_flat, brightness_flat)\n",
    "            print(f\"‚úÖ Dense model (forward_combined) output: {dense_output.shape}\")\n",
    "            \n",
    "            # Test research API - check model's fusion type first\n",
    "            if dense_model.use_shared_classifier:\n",
    "                # Shared classifier returns single output from forward()\n",
    "                dense_forward_output = dense_model.forward(rgb_flat, brightness_flat)\n",
    "                print(f\"‚úÖ Dense model (forward, shared) output: {dense_forward_output.shape}\")\n",
    "            else:\n",
    "                # Separate classifiers return tuple from forward()\n",
    "                color_logits, brightness_logits = dense_model.forward(rgb_flat, brightness_flat)\n",
    "                print(f\"‚úÖ Dense model (forward, separate) outputs: {color_logits.shape}, {brightness_logits.shape}\")\n",
    "    \n",
    "    # Test CNN Model\n",
    "    if cnn_model is not None:\n",
    "        with torch.no_grad():\n",
    "            # Test standard classification API\n",
    "            cnn_output = cnn_model(sample_rgb, sample_brightness)\n",
    "            print(f\"‚úÖ CNN model (forward_combined) output: {cnn_output.shape}\")\n",
    "            \n",
    "            # Test research API  \n",
    "            color_logits, brightness_logits = cnn_model.forward(sample_rgb, sample_brightness)\n",
    "            print(f\"‚úÖ CNN model (forward) outputs: {color_logits.shape}, {brightness_logits.shape}\")\n",
    "    \n",
    "    print(\"‚úÖ All model tests passed!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Model forward pass test failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "# Store available models for training\n",
    "available_models = {}\n",
    "if dense_model is not None:\n",
    "    available_models['Dense Network'] = dense_model\n",
    "if cnn_model is not None:\n",
    "    available_models['CNN Network'] = cnn_model\n",
    "\n",
    "if available_models:\n",
    "    print(f\"\\nüéØ {len(available_models)} model(s) ready for training:\")\n",
    "    for model_name in available_models.keys():\n",
    "        print(f\"   ‚úÖ {model_name}\")\n",
    "else:\n",
    "    print(\"\\n‚ùå No models available for training!\")\n",
    "    print(\"üí° Check the error messages above and fix the model creation issues\")\n",
    "\n",
    "print(\"\\nüéØ Model creation complete! Models are compiled and ready for training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb159c3",
   "metadata": {},
   "source": [
    "## 9. Prepare Data for Training\n",
    "\n",
    "Convert processed data to PyTorch tensors and create data loaders for efficient training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b481bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preparation for Training\n",
    "print(\"üì¶ Preparing data for training...\")\n",
    "\n",
    "# Check if we have processed data\n",
    "if 'train_rgb' not in locals() or 'train_brightness' not in locals():\n",
    "    print(\"‚ùå No processed training data found!\")\n",
    "    print(\"üí° Please run the data processing cells first (Step 5)\")\n",
    "    raise ValueError(\"Training data not available\")\n",
    "\n",
    "print(f\"‚úÖ Found processed data:\")\n",
    "print(f\"   Training RGB: {train_rgb.shape}\")\n",
    "print(f\"   Training Brightness: {train_brightness.shape}\")\n",
    "print(f\"   Training Labels: {train_labels.shape}\")\n",
    "print(f\"   Test RGB: {test_rgb.shape}\")\n",
    "print(f\"   Test Brightness: {test_brightness.shape}\")\n",
    "print(f\"   Test Labels: {test_labels.shape}\")\n",
    "\n",
    "# Convert numpy arrays to PyTorch tensors\n",
    "print(\"\\nüîÑ Converting to PyTorch tensors...\")\n",
    "\n",
    "# Training data\n",
    "train_rgb_tensor = torch.FloatTensor(train_rgb)\n",
    "train_brightness_tensor = torch.FloatTensor(train_brightness)\n",
    "train_labels_tensor = torch.LongTensor(train_labels)\n",
    "\n",
    "# Test data\n",
    "test_rgb_tensor = torch.FloatTensor(test_rgb)\n",
    "test_brightness_tensor = torch.FloatTensor(test_brightness)\n",
    "test_labels_tensor = torch.LongTensor(test_labels)\n",
    "\n",
    "print(f\"‚úÖ Tensors created:\")\n",
    "print(f\"   Training RGB tensor: {train_rgb_tensor.shape}, dtype: {train_rgb_tensor.dtype}\")\n",
    "print(f\"   Training brightness tensor: {train_brightness_tensor.shape}, dtype: {train_brightness_tensor.dtype}\")\n",
    "print(f\"   Training labels tensor: {train_labels_tensor.shape}, dtype: {train_labels_tensor.dtype}\")\n",
    "\n",
    "# Normalize data to [0, 1] range if needed\n",
    "if train_rgb_tensor.max() > 1.0:\n",
    "    print(\"\\nüìä Normalizing data to [0, 1] range...\")\n",
    "    train_rgb_tensor = train_rgb_tensor / 255.0\n",
    "    train_brightness_tensor = train_brightness_tensor / 255.0\n",
    "    test_rgb_tensor = test_rgb_tensor / 255.0\n",
    "    test_brightness_tensor = test_brightness_tensor / 255.0\n",
    "    print(f\"‚úÖ Data normalized: RGB range [{train_rgb_tensor.min():.3f}, {train_rgb_tensor.max():.3f}]\")\n",
    "\n",
    "# Create datasets\n",
    "print(\"\\nüóÇÔ∏è Creating PyTorch datasets...\")\n",
    "\n",
    "class MultiStreamDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Custom dataset for multi-stream data (RGB + Brightness)\"\"\"\n",
    "    \n",
    "    def __init__(self, rgb_data, brightness_data, labels):\n",
    "        self.rgb_data = rgb_data\n",
    "        self.brightness_data = brightness_data\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'rgb': self.rgb_data[idx],\n",
    "            'brightness': self.brightness_data[idx],\n",
    "            'label': self.labels[idx]\n",
    "        }\n",
    "\n",
    "# Create dataset instances\n",
    "train_dataset_multi = MultiStreamDataset(train_rgb_tensor, train_brightness_tensor, train_labels_tensor)\n",
    "test_dataset_multi = MultiStreamDataset(test_rgb_tensor, test_brightness_tensor, test_labels_tensor)\n",
    "\n",
    "print(f\"‚úÖ Datasets created:\")\n",
    "print(f\"   Training dataset: {len(train_dataset_multi)} samples\")\n",
    "print(f\"   Test dataset: {len(test_dataset_multi)} samples\")\n",
    "\n",
    "# Create data loaders\n",
    "print(\"\\nüöÄ Creating data loaders...\")\n",
    "\n",
    "batch_size = 32  # Adjust based on GPU memory\n",
    "num_workers = 2  # Adjust based on system\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset_multi,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=torch.cuda.is_available()\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset_multi,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=torch.cuda.is_available()\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Data loaders created:\")\n",
    "print(f\"   Training batches: {len(train_loader)}\")\n",
    "print(f\"   Test batches: {len(test_loader)}\")\n",
    "print(f\"   Batch size: {batch_size}\")\n",
    "\n",
    "# Test data loader\n",
    "print(\"\\nüß™ Testing data loader...\")\n",
    "try:\n",
    "    sample_batch = next(iter(train_loader))\n",
    "    print(f\"‚úÖ Sample batch loaded:\")\n",
    "    print(f\"   RGB batch shape: {sample_batch['rgb'].shape}\")\n",
    "    print(f\"   Brightness batch shape: {sample_batch['brightness'].shape}\")\n",
    "    print(f\"   Labels batch shape: {sample_batch['label'].shape}\")\n",
    "    print(f\"   Labels range: {sample_batch['label'].min().item()} - {sample_batch['label'].max().item()}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Data loader test failed: {e}\")\n",
    "\n",
    "print(\"\\nüìä Data statistics:\")\n",
    "print(f\"   Classes in training set: {len(torch.unique(train_labels_tensor))}\")\n",
    "print(f\"   Classes in test set: {len(torch.unique(test_labels_tensor))}\")\n",
    "print(f\"   RGB data range: [{train_rgb_tensor.min():.3f}, {train_rgb_tensor.max():.3f}]\")\n",
    "print(f\"   Brightness data range: [{train_brightness_tensor.min():.3f}, {train_brightness_tensor.max():.3f}]\")\n",
    "\n",
    "print(\"\\n‚úÖ Data preparation complete! Ready for training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6958995",
   "metadata": {},
   "source": [
    "## 10. Train Multi-Stream Models\n",
    "\n",
    "Train both the Dense and CNN models on the CIFAR-100 multi-stream data using the models' built-in `.fit()` API.\n",
    "\n",
    "**Key Features:**\n",
    "- Uses the models' built-in Keras-like `.fit()` method for clean, maintainable training\n",
    "- Automatic optimization: batch size, workers, mixed precision based on device\n",
    "- Built-in progress tracking and validation\n",
    "- Proper input shape handling for Dense vs CNN models\n",
    "- Consistent API across all model types\n",
    "\n",
    "**API Usage:**\n",
    "- `model.fit()` - Keras-like training API with automatic optimizations\n",
    "- `model()` - Primary method for training, inference, and evaluation\n",
    "- `model.forward()` - Research output (tuple of individual stream logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b72a1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Configuration and Implementation\n",
    "print(\"üöÄ Setting up training configuration...\")\n",
    "\n",
    "# Training hyperparameters\n",
    "num_epochs = 10  # Reduce for demo, increase for full training\n",
    "learning_rate = 0.001\n",
    "weight_decay = 1e-4\n",
    "\n",
    "print(f\"‚úÖ Training Configuration:\")\n",
    "print(f\"   Epochs: {num_epochs}\")\n",
    "print(f\"   Learning rate: {learning_rate}\")\n",
    "print(f\"   Weight decay: {weight_decay}\")\n",
    "print(f\"   Device: {device}\")\n",
    "\n",
    "# Prepare data for model's .fit() method\n",
    "# The models expect numpy arrays, so convert tensors back to numpy\n",
    "train_rgb_np = train_rgb_tensor.cpu().numpy()\n",
    "train_brightness_np = train_brightness_tensor.cpu().numpy()\n",
    "train_labels_np = train_labels_tensor.cpu().numpy()\n",
    "\n",
    "test_rgb_np = test_rgb_tensor.cpu().numpy()\n",
    "test_brightness_np = test_brightness_tensor.cpu().numpy()\n",
    "test_labels_np = test_labels_tensor.cpu().numpy()\n",
    "\n",
    "print(f\"\\nüìä Data ready for training:\")\n",
    "print(f\"   Training samples: {len(train_rgb_np)}\")\n",
    "print(f\"   Test samples: {len(test_rgb_np)}\")\n",
    "print(f\"   RGB input shape: {train_rgb_np.shape}\")\n",
    "print(f\"   Brightness input shape: {train_brightness_np.shape}\")\n",
    "\n",
    "# Check if models are available\n",
    "models_to_train = []\n",
    "\n",
    "if 'available_models' in locals() and available_models:\n",
    "    models_to_train = list(available_models.items())\n",
    "\n",
    "if not models_to_train:\n",
    "    print(\"‚ùå No models available for training!\")\n",
    "    print(\"üí° Please run the model creation cells first (Step 8)\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ Found {len(models_to_train)} models to train:\")\n",
    "    for name, _ in models_to_train:\n",
    "        print(f\"   - {name}\")\n",
    "\n",
    "print(\"\\nüéØ Ready to start training using model's built-in .fit() API!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db7cefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute Training for All Models Using Built-in API\n",
    "print(\"üöÄ Starting model training using the models' built-in .fit() API...\")\n",
    "\n",
    "import time\n",
    "\n",
    "# Store results for comparison\n",
    "training_results = {}\n",
    "\n",
    "# Train each model using their built-in .fit() method\n",
    "for model_name, model in models_to_train:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üèãÔ∏è Training {model_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Prepare input data based on model type\n",
    "        if 'Dense' in model_name:\n",
    "            # Dense models expect flattened input\n",
    "            rgb_input = train_rgb_np.reshape(train_rgb_np.shape[0], -1)\n",
    "            brightness_input = train_brightness_np.reshape(train_brightness_np.shape[0], -1)\n",
    "            val_rgb_input = test_rgb_np.reshape(test_rgb_np.shape[0], -1)\n",
    "            val_brightness_input = test_brightness_np.reshape(test_brightness_np.shape[0], -1)\n",
    "        else:\n",
    "            # CNN models expect image-like input\n",
    "            rgb_input = train_rgb_np\n",
    "            brightness_input = train_brightness_np\n",
    "            val_rgb_input = test_rgb_np\n",
    "            val_brightness_input = test_brightness_np\n",
    "        \n",
    "        print(f\"üìä Input shapes for {model_name}:\")\n",
    "        print(f\"   RGB: {rgb_input.shape}\")\n",
    "        print(f\"   Brightness: {brightness_input.shape}\")\n",
    "        \n",
    "        # Train using the model's built-in .fit() method\n",
    "        print(f\"\\nüî• Training {model_name} using .fit() API...\")\n",
    "        model.fit(\n",
    "            train_color_data=rgb_input,\n",
    "            train_brightness_data=brightness_input,\n",
    "            train_labels=train_labels_np,\n",
    "            val_color_data=val_rgb_input,\n",
    "            val_brightness_data=val_brightness_input,\n",
    "            val_labels=test_labels_np,\n",
    "            epochs=num_epochs,\n",
    "            learning_rate=learning_rate,\n",
    "            weight_decay=weight_decay,\n",
    "            verbose=1  # Show progress bars\n",
    "        )\n",
    "        \n",
    "        training_time = time.time() - start_time\n",
    "        \n",
    "        # Evaluate final accuracy using the model's built-in evaluation\n",
    "        print(f\"\\nüìà Evaluating {model_name}...\")\n",
    "        model.eval()\n",
    "        \n",
    "        # Get predictions on test set\n",
    "        with torch.no_grad():\n",
    "            if 'Dense' in model_name:\n",
    "                test_outputs = model(\n",
    "                    torch.tensor(val_rgb_input, dtype=torch.float32).to(device),\n",
    "                    torch.tensor(val_brightness_input, dtype=torch.float32).to(device)\n",
    "                )\n",
    "            else:\n",
    "                test_outputs = model(\n",
    "                    torch.tensor(test_rgb_np, dtype=torch.float32).to(device),\n",
    "                    torch.tensor(test_brightness_np, dtype=torch.float32).to(device)\n",
    "                )\n",
    "            \n",
    "            _, predicted = torch.max(test_outputs, 1)\n",
    "            test_labels_tensor_device = torch.tensor(test_labels_np, dtype=torch.long).to(device)\n",
    "            final_test_acc = (predicted == test_labels_tensor_device).float().mean().item() * 100\n",
    "        \n",
    "        # Store results\n",
    "        training_results[model_name] = {\n",
    "            'model': model,\n",
    "            'final_test_acc': final_test_acc,\n",
    "            'training_time': training_time,\n",
    "        }\n",
    "        \n",
    "        print(f\"‚úÖ {model_name} training complete!\")\n",
    "        print(f\"   Final test accuracy: {final_test_acc:.2f}%\")\n",
    "        print(f\"   Training time: {training_time:.1f}s ({training_time/60:.1f} min)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Training failed for {model_name}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        continue\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"üéâ All Training Complete!\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Display final results\n",
    "if training_results:\n",
    "    print(\"\\nüìä Final Results Summary:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for model_name, result in training_results.items():\n",
    "        print(f\"{model_name}:\")\n",
    "        print(f\"  Final Test Accuracy: {result['final_test_acc']:.2f}%\")\n",
    "        print(f\"  Training Time: {result['training_time']:.1f}s ({result['training_time']/60:.1f} min)\")\n",
    "        print()\n",
    "    \n",
    "    # Find best model\n",
    "    best_model_name = max(training_results.keys(), key=lambda k: training_results[k]['final_test_acc'])\n",
    "    best_acc = training_results[best_model_name]['final_test_acc']\n",
    "    \n",
    "    print(f\"üèÜ Best Model: {best_model_name} ({best_acc:.2f}% accuracy)\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No models were successfully trained!\")\n",
    "\n",
    "print(\"\\n‚úÖ Training phase complete using built-in model API!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ced1db",
   "metadata": {},
   "source": [
    "## 11. Training Results Visualization\n",
    "\n",
    "Visualize the training progress and compare model performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa553b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Training Results\n",
    "print(\"üìä Visualizing training results...\")\n",
    "\n",
    "def plot_model_comparison(training_results):\n",
    "    \"\"\"Create comparison charts for final model performance.\"\"\"\n",
    "    if not training_results:\n",
    "        print(\"‚ùå No training results to compare!\")\n",
    "        return\n",
    "    \n",
    "    model_names = list(training_results.keys())\n",
    "    test_accuracies = [result['final_test_acc'] for result in training_results.values()]\n",
    "    training_times = [result['training_time'] / 60 for result in training_results.values()]  # Convert to minutes\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    fig.suptitle('Model Performance Comparison', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Test Accuracy Comparison\n",
    "    bars1 = ax1.bar(model_names, test_accuracies, color=['skyblue', 'lightcoral', 'lightgreen', 'gold'][:len(model_names)])\n",
    "    ax1.set_title('Final Test Accuracy', fontweight='bold')\n",
    "    ax1.set_ylabel('Accuracy (%)')\n",
    "    ax1.set_ylim(0, max(test_accuracies) * 1.1 if test_accuracies else 1)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, acc in zip(bars1, test_accuracies):\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height + max(test_accuracies) * 0.01,\n",
    "                f'{acc:.2f}%', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Training Time Comparison\n",
    "    bars2 = ax2.bar(model_names, training_times, color=['skyblue', 'lightcoral', 'lightgreen', 'gold'][:len(model_names)])\n",
    "    ax2.set_title('Training Time', fontweight='bold')\n",
    "    ax2.set_ylabel('Time (minutes)')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, time_val in zip(bars2, training_times):\n",
    "        height = bar.get_height()\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., height + max(training_times) * 0.01,\n",
    "                f'{time_val:.1f}m', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_efficiency_analysis(training_results):\n",
    "    \"\"\"Create efficiency analysis chart.\"\"\"\n",
    "    if not training_results:\n",
    "        print(\"‚ùå No training results to analyze!\")\n",
    "        return\n",
    "    \n",
    "    model_names = list(training_results.keys())\n",
    "    test_accuracies = [result['final_test_acc'] for result in training_results.values()]\n",
    "    training_times = [result['training_time'] / 60 for result in training_results.values()]  # Convert to minutes\n",
    "    \n",
    "    # Calculate efficiency scores (accuracy per minute)\n",
    "    efficiency_scores = [acc / time if time > 0 else 0 for acc, time in zip(test_accuracies, training_times)]\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "    fig.suptitle('Model Efficiency Analysis (Accuracy per Minute)', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    bars = ax.bar(model_names, efficiency_scores, color=['skyblue', 'lightcoral', 'lightgreen', 'gold'][:len(model_names)])\n",
    "    ax.set_title('Efficiency Score (Accuracy % per Minute)', fontweight='bold')\n",
    "    ax.set_ylabel('Efficiency Score')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, score in zip(bars, efficiency_scores):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height + max(efficiency_scores) * 0.01,\n",
    "                f'{score:.2f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Generate visualizations if we have training results\n",
    "if 'training_results' in locals() and training_results:\n",
    "    print(\"üìä Generating model comparison charts...\")\n",
    "    plot_model_comparison(training_results)\n",
    "    \n",
    "    print(\"\\nüéØ Generating efficiency analysis...\")\n",
    "    plot_efficiency_analysis(training_results)\n",
    "    \n",
    "    # Print detailed comparison\n",
    "    print(\"\\nüìã Detailed Model Comparison:\")\n",
    "    print(\"-\" * 70)\n",
    "    print(f\"{'Model Name':<20} {'Test Acc (%)':<12} {'Time (min)':<12} {'Parameters':<15}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for model_name, result in training_results.items():\n",
    "        model = result['model']\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        time_min = result['training_time'] / 60\n",
    "        \n",
    "        print(f\"{model_name:<20} {result['final_test_acc']:<12.2f} {time_min:<12.1f} {total_params:<15,}\")\n",
    "    \n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    # Efficiency analysis\n",
    "    print(\"\\nüéØ Efficiency Analysis:\")\n",
    "    best_acc_model = max(training_results.keys(), key=lambda k: training_results[k]['final_test_acc'])\n",
    "    fastest_model = min(training_results.keys(), key=lambda k: training_results[k]['training_time'])\n",
    "    \n",
    "    print(f\"   üèÜ Best Accuracy: {best_acc_model} ({training_results[best_acc_model]['final_test_acc']:.2f}%)\")\n",
    "    print(f\"   ‚ö° Fastest Training: {fastest_model} ({training_results[fastest_model]['training_time']/60:.1f} min)\")\n",
    "    \n",
    "    # Calculate efficiency score (accuracy per minute)\n",
    "    efficiency_scores = {}\n",
    "    for model_name, result in training_results.items():\n",
    "        efficiency = result['final_test_acc'] / (result['training_time'] / 60)\n",
    "        efficiency_scores[model_name] = efficiency\n",
    "    \n",
    "    most_efficient = max(efficiency_scores.keys(), key=lambda k: efficiency_scores[k])\n",
    "    print(f\"   üéØ Most Efficient: {most_efficient} ({efficiency_scores[most_efficient]:.2f} acc%/min)\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No training results available for visualization!\")\n",
    "    print(\"üí° Make sure to run the training cells first (Step 10)\")\n",
    "\n",
    "print(\"\\n‚úÖ Training results visualization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bc16fe",
   "metadata": {},
   "source": [
    "## 12. Model Evaluation and Analysis\n",
    "\n",
    "Perform detailed evaluation including confusion matrix, per-class accuracy, and error analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ef03ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive Model Evaluation\n",
    "print(\"üîç Performing comprehensive model evaluation...\")\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import seaborn as sns\n",
    "\n",
    "def evaluate_model_detailed(model, model_name, test_loader, device, class_names):\n",
    "    \"\"\"\n",
    "    Perform detailed evaluation of a trained model.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model\n",
    "        model_name: Name of the model for display\n",
    "        test_loader: Test data loader\n",
    "        device: Device to run evaluation on\n",
    "        class_names: List of class names for CIFAR-100\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with evaluation metrics and predictions\n",
    "    \"\"\"\n",
    "    print(f\"\\nüî¨ Evaluating {model_name}...\")\n",
    "    \n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    all_probabilities = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        test_pbar = tqdm(test_loader, desc=f\"Evaluating {model_name}\")\n",
    "        \n",
    "        for batch in test_pbar:\n",
    "            rgb_data = batch['rgb'].to(device)\n",
    "            brightness_data = batch['brightness'].to(device)\n",
    "            targets = batch['label'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            if 'Dense' in model_name:\n",
    "                rgb_flat = rgb_data.view(rgb_data.size(0), -1)\n",
    "                brightness_flat = brightness_data.view(brightness_data.size(0), -1)\n",
    "                outputs = model(rgb_flat, brightness_flat)\n",
    "            else:\n",
    "                outputs = model(rgb_data, brightness_data)\n",
    "            \n",
    "            # Get predictions and probabilities\n",
    "            probabilities = torch.softmax(outputs, dim=1)\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "            \n",
    "            # Store results\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "            all_probabilities.extend(probabilities.cpu().numpy())\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    predictions = np.array(all_predictions)\n",
    "    targets = np.array(all_targets)\n",
    "    probabilities = np.array(all_probabilities)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(targets, predictions)\n",
    "    precision = precision_score(targets, predictions, average='weighted', zero_division=0)\n",
    "    recall = recall_score(targets, predictions, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(targets, predictions, average='weighted', zero_division=0)\n",
    "    \n",
    "    print(f\"‚úÖ {model_name} Evaluation Complete:\")\n",
    "    print(f\"   Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "    print(f\"   Precision: {precision:.4f}\")\n",
    "    print(f\"   Recall: {recall:.4f}\")\n",
    "    print(f\"   F1 Score: {f1:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'model_name': model_name,\n",
    "        'predictions': predictions,\n",
    "        'targets': targets,\n",
    "        'probabilities': probabilities,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1\n",
    "    }\n",
    "\n",
    "def plot_confusion_matrix(evaluation_result, class_names, figsize=(12, 10)):\n",
    "    \"\"\"Plot confusion matrix for model evaluation.\"\"\"\n",
    "    predictions = evaluation_result['predictions']\n",
    "    targets = evaluation_result['targets']\n",
    "    model_name = evaluation_result['model_name']\n",
    "    \n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(targets, predictions)\n",
    "    \n",
    "    # Normalize confusion matrix\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.heatmap(cm_normalized, annot=False, cmap='Blues', fmt='.2f',\n",
    "                xticklabels=False, yticklabels=False)\n",
    "    plt.title(f'Confusion Matrix - {model_name}\\nAccuracy: {evaluation_result[\"accuracy\"]:.3f}', \n",
    "              fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Predicted Class')\n",
    "    plt.ylabel('True Class')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def analyze_per_class_performance(evaluation_result, class_names, top_k=10):\n",
    "    \"\"\"Analyze per-class performance and show best/worst performing classes.\"\"\"\n",
    "    predictions = evaluation_result['predictions']\n",
    "    targets = evaluation_result['targets']\n",
    "    model_name = evaluation_result['model_name']\n",
    "    \n",
    "    # Calculate per-class accuracy\n",
    "    per_class_acc = []\n",
    "    per_class_counts = []\n",
    "    \n",
    "    for class_id in range(len(class_names)):\n",
    "        class_mask = targets == class_id\n",
    "        if class_mask.sum() > 0:\n",
    "            class_predictions = predictions[class_mask]\n",
    "            class_targets = targets[class_mask]\n",
    "            class_accuracy = (class_predictions == class_targets).mean()\n",
    "            per_class_acc.append(class_accuracy)\n",
    "            per_class_counts.append(class_mask.sum())\n",
    "        else:\n",
    "            per_class_acc.append(0.0)\n",
    "            per_class_counts.append(0)\n",
    "    \n",
    "    per_class_acc = np.array(per_class_acc)\n",
    "    per_class_counts = np.array(per_class_counts)\n",
    "    \n",
    "    # Sort by accuracy\n",
    "    sorted_indices = np.argsort(per_class_acc)\n",
    "    \n",
    "    print(f\"\\nüìä Per-Class Performance Analysis - {model_name}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Best performing classes\n",
    "    print(f\"\\nüèÜ Top {top_k} Best Performing Classes:\")\n",
    "    print(\"-\" * 50)\n",
    "    for i in range(-1, -top_k-1, -1):\n",
    "        idx = sorted_indices[i]\n",
    "        class_name = class_names[idx]\n",
    "        accuracy = per_class_acc[idx] * 100\n",
    "        count = per_class_counts[idx]\n",
    "        print(f\"{-i:2d}. {class_name:<20} {accuracy:6.2f}% ({count:3d} samples)\")\n",
    "    \n",
    "    # Worst performing classes\n",
    "    print(f\"\\nüí• Top {top_k} Worst Performing Classes:\")\n",
    "    print(\"-\" * 50)\n",
    "    for i in range(top_k):\n",
    "        idx = sorted_indices[i]\n",
    "        class_name = class_names[idx]\n",
    "        accuracy = per_class_acc[idx] * 100\n",
    "        count = per_class_counts[idx]\n",
    "        print(f\"{i+1:2d}. {class_name:<20} {accuracy:6.2f}% ({count:3d} samples)\")\n",
    "    \n",
    "    # Overall statistics\n",
    "    print(f\"\\nüìà Overall Statistics:\")\n",
    "    print(f\"   Mean per-class accuracy: {per_class_acc.mean()*100:.2f}%\")\n",
    "    print(f\"   Std per-class accuracy: {per_class_acc.std()*100:.2f}%\")\n",
    "    print(f\"   Best class accuracy: {per_class_acc.max()*100:.2f}%\")\n",
    "    print(f\"   Worst class accuracy: {per_class_acc.min()*100:.2f}%\")\n",
    "    \n",
    "    return per_class_acc, per_class_counts\n",
    "\n",
    "def plot_class_performance_distribution(evaluation_results, class_names):\n",
    "    \"\"\"Plot distribution of per-class accuracies for all models.\"\"\"\n",
    "    if not evaluation_results:\n",
    "        print(\"‚ùå No evaluation results to plot!\")\n",
    "        return\n",
    "    \n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    for i, (model_name, eval_result) in enumerate(evaluation_results.items()):\n",
    "        per_class_acc, _ = analyze_per_class_performance(eval_result, class_names, top_k=5)\n",
    "        \n",
    "        # Plot histogram\n",
    "        plt.subplot(2, len(evaluation_results), i + 1)\n",
    "        plt.hist(per_class_acc * 100, bins=20, alpha=0.7, edgecolor='black')\n",
    "        plt.title(f'{model_name}\\nPer-Class Accuracy Distribution')\n",
    "        plt.xlabel('Accuracy (%)')\n",
    "        plt.ylabel('Number of Classes')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot box plot\n",
    "        plt.subplot(2, len(evaluation_results), len(evaluation_results) + i + 1)\n",
    "        plt.boxplot(per_class_acc * 100, vert=True)\n",
    "        plt.title(f'{model_name}\\nAccuracy Box Plot')\n",
    "        plt.ylabel('Accuracy (%)')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Perform evaluation if we have trained models\n",
    "evaluation_results = {}\n",
    "\n",
    "if 'training_results' in locals() and training_results:\n",
    "    print(\"üîç Starting comprehensive evaluation...\")\n",
    "    \n",
    "    for model_name, training_result in training_results.items():\n",
    "        model = training_result['model']\n",
    "        \n",
    "        try:\n",
    "            eval_result = evaluate_model_detailed(\n",
    "                model=model,\n",
    "                model_name=model_name,\n",
    "                test_loader=test_loader,\n",
    "                device=device,\n",
    "                class_names=cifar100_fine_labels\n",
    "            )\n",
    "            evaluation_results[model_name] = eval_result\n",
    "            \n",
    "            # Plot confusion matrix for each model\n",
    "            print(f\"\\nüìä Generating confusion matrix for {model_name}...\")\n",
    "            plot_confusion_matrix(eval_result, cifar100_fine_labels, figsize=(10, 8))\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Evaluation failed for {model_name}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Generate comparison plots\n",
    "    if evaluation_results:\n",
    "        print(\"\\nüìà Generating per-class performance analysis...\")\n",
    "        plot_class_performance_distribution(evaluation_results, cifar100_fine_labels)\n",
    "        \n",
    "        # Compare models side by side\n",
    "        print(\"\\nüîÑ Model Performance Comparison:\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"{'Model':<20} {'Accuracy':<10} {'Precision':<10} {'Recall':<10} {'F1-Score':<10}\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        for model_name, eval_result in evaluation_results.items():\n",
    "            print(f\"{model_name:<20} {eval_result['accuracy']:<10.4f} {eval_result['precision']:<10.4f} \"\n",
    "                  f\"{eval_result['recall']:<10.4f} {eval_result['f1_score']:<10.4f}\")\n",
    "        \n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ùå No models were successfully evaluated!\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå No trained models available for evaluation!\")\n",
    "    print(\"üí° Make sure to run the training cells first (Step 10)\")\n",
    "\n",
    "print(\"\\n‚úÖ Model evaluation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881346fc",
   "metadata": {},
   "source": [
    "## 13. Model Saving and Inference Demo\n",
    "\n",
    "Save trained models and demonstrate inference on new samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d0f0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Saving and Inference Demo\n",
    "print(\"üíæ Setting up model saving and inference...\")\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def save_model(model, model_name, training_result, save_dir=\"models\"):\n",
    "    \"\"\"\n",
    "    Save a trained model with its metadata.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained PyTorch model\n",
    "        model_name: Name of the model\n",
    "        training_result: Training results dictionary\n",
    "        save_dir: Directory to save models\n",
    "    \"\"\"\n",
    "    # Create save directory\n",
    "    save_path = Path(save_dir)\n",
    "    save_path.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Prepare model info\n",
    "    model_info = {\n",
    "        'model_name': model_name,\n",
    "        'final_test_accuracy': training_result['final_test_acc'],\n",
    "        'training_time': training_result['training_time'],\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'model_class': model.__class__.__name__,\n",
    "        'num_parameters': sum(p.numel() for p in model.parameters()),\n",
    "        'training_history': training_result['history']\n",
    "    }\n",
    "    \n",
    "    # Save model\n",
    "    model_file = save_path / f\"{model_name.replace(' ', '_').lower()}_cifar100.pth\"\n",
    "    torch.save(model_info, model_file)\n",
    "    \n",
    "    print(f\"‚úÖ {model_name} saved to: {model_file}\")\n",
    "    return model_file\n",
    "\n",
    "def load_model(model_file, model_class, device):\n",
    "    \"\"\"\n",
    "    Load a saved model.\n",
    "    \n",
    "    Args:\n",
    "        model_file: Path to saved model file\n",
    "        model_class: Model class to instantiate\n",
    "        device: Device to load model on\n",
    "    \n",
    "    Returns:\n",
    "        Loaded model and metadata\n",
    "    \"\"\"\n",
    "    checkpoint = torch.load(model_file, map_location=device)\n",
    "    \n",
    "    # Print model info\n",
    "    print(f\"üìã Model Info:\")\n",
    "    print(f\"   Name: {checkpoint['model_name']}\")\n",
    "    print(f\"   Class: {checkpoint['model_class']}\")\n",
    "    print(f\"   Test Accuracy: {checkpoint['final_test_accuracy']:.2f}%\")\n",
    "    print(f\"   Parameters: {checkpoint['num_parameters']:,}\")\n",
    "    print(f\"   Training Time: {checkpoint['training_time']/60:.1f} minutes\")\n",
    "    \n",
    "    return checkpoint\n",
    "\n",
    "def demonstrate_inference(model, model_name, test_loader, device, class_names, num_samples=8):\n",
    "    \"\"\"\n",
    "    Demonstrate model inference on random test samples.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model\n",
    "        model_name: Name of the model\n",
    "        test_loader: Test data loader\n",
    "        device: Device to run inference on\n",
    "        class_names: List of class names\n",
    "        num_samples: Number of samples to demonstrate\n",
    "    \"\"\"\n",
    "    print(f\"\\nüéØ Demonstrating {model_name} inference...\")\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # Get a batch of test data\n",
    "    test_batch = next(iter(test_loader))\n",
    "    rgb_data = test_batch['rgb'][:num_samples].to(device)\n",
    "    brightness_data = test_batch['brightness'][:num_samples].to(device)\n",
    "    true_labels = test_batch['label'][:num_samples]\n",
    "    \n",
    "    # Make predictions\n",
    "    with torch.no_grad():\n",
    "        if 'Dense' in model_name:\n",
    "            rgb_flat = rgb_data.view(rgb_data.size(0), -1)\n",
    "            brightness_flat = brightness_data.view(brightness_data.size(0), -1)\n",
    "            outputs = model(rgb_flat, brightness_flat)\n",
    "        else:\n",
    "            outputs = model(rgb_data, brightness_data)\n",
    "        \n",
    "        probabilities = torch.softmax(outputs, dim=1)\n",
    "        _, predicted_labels = torch.max(outputs, 1)\n",
    "    \n",
    "    # Visualize results\n",
    "    fig, axes = plt.subplots(2, num_samples//2, figsize=(16, 8))\n",
    "    fig.suptitle(f'{model_name} - Inference Demo', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        # Get RGB image for display\n",
    "        rgb_img = rgb_data[i].cpu().numpy().transpose(1, 2, 0)\n",
    "        \n",
    "        # Get predictions\n",
    "        true_class = class_names[true_labels[i].item()]\n",
    "        pred_class = class_names[predicted_labels[i].item()]\n",
    "        confidence = probabilities[i][predicted_labels[i]].item() * 100\n",
    "        \n",
    "        # Determine color (green for correct, red for incorrect)\n",
    "        color = 'green' if true_labels[i] == predicted_labels[i] else 'red'\n",
    "        \n",
    "        # Plot\n",
    "        axes[i].imshow(rgb_img)\n",
    "        axes[i].set_title(f'True: {true_class}\\nPred: {pred_class}\\nConf: {confidence:.1f}%', \n",
    "                         color=color, fontweight='bold', fontsize=10)\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate accuracy for this batch\n",
    "    batch_accuracy = (predicted_labels.cpu() == true_labels).float().mean().item() * 100\n",
    "    print(f\"   Batch accuracy: {batch_accuracy:.1f}%\")\n",
    "    \n",
    "    return predicted_labels.cpu().numpy(), probabilities.cpu().numpy()\n",
    "\n",
    "# Save all trained models\n",
    "saved_models = {}\n",
    "\n",
    "if 'training_results' in locals() and training_results:\n",
    "    print(\"üíæ Saving trained models...\")\n",
    "    \n",
    "    for model_name, training_result in training_results.items():\n",
    "        try:\n",
    "            model_file = save_model(\n",
    "                model=training_result['model'],\n",
    "                model_name=model_name,\n",
    "                training_result=training_result\n",
    "            )\n",
    "            saved_models[model_name] = model_file\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to save {model_name}: {e}\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Saved {len(saved_models)} models to 'models/' directory\")\n",
    "    \n",
    "    # Demonstrate inference for each model\n",
    "    print(\"\\nüéØ Running inference demonstrations...\")\n",
    "    \n",
    "    for model_name, training_result in training_results.items():\n",
    "        try:\n",
    "            model = training_result['model']\n",
    "            predictions, probabilities = demonstrate_inference(\n",
    "                model=model,\n",
    "                model_name=model_name,\n",
    "                test_loader=test_loader,\n",
    "                device=device,\n",
    "                class_names=cifar100_fine_labels,\n",
    "                num_samples=8\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Inference demo failed for {model_name}: {e}\")\n",
    "            continue\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No trained models available for saving!\")\n",
    "    print(\"üí° Make sure to run the training cells first (Step 10)\")\n",
    "\n",
    "# Example of how to load a saved model (for future use)\n",
    "print(\"\\nüìñ Example: Loading a saved model (for future use)\")\n",
    "print(\"```python\")\n",
    "print(\"# To load a model in the future:\")\n",
    "print(\"checkpoint = torch.load('models/dense_network_cifar100.pth')\")\n",
    "print(\"model = BaseMultiChannelNetwork(...)  # Initialize with same parameters\")\n",
    "print(\"model.load_state_dict(checkpoint['model_state_dict'])\")\n",
    "print(\"model.eval()\")\n",
    "print(\"```\")\n",
    "\n",
    "print(\"\\n‚úÖ Model saving and inference demo complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cdb981",
   "metadata": {},
   "source": [
    "## 14. Conclusion and Summary\n",
    "\n",
    "Summary of results, key findings, and next steps for the multi-stream neural network project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8dce6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéâ Multi-Stream Neural Networks: Project Summary\n",
    "print(\"üìã Generating project summary...\")\n",
    "\n",
    "def generate_project_summary():\n",
    "    \"\"\"Generate a comprehensive summary of the project results.\"\"\"\n",
    "    \n",
    "    print(\"üéØ MULTI-STREAM NEURAL NETWORKS ON CIFAR-100\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(\"\\nüìä PROJECT OVERVIEW:\")\n",
    "    print(\"   ‚Ä¢ Dataset: CIFAR-100 (100 classes, 32x32 images)\")\n",
    "    print(\"   ‚Ä¢ Architecture: Multi-stream (RGB + Brightness channels)\")\n",
    "    print(\"   ‚Ä¢ Models: Dense Network vs CNN (ResNet-style)\")\n",
    "    print(\"   ‚Ä¢ Training: Multi-channel data with batch processing\")\n",
    "    print(\"   ‚Ä¢ Evaluation: Comprehensive analysis with visualizations\")\n",
    "    \n",
    "    if 'training_results' in locals() and training_results:\n",
    "        print(\"\\nüèÜ TRAINING RESULTS:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        best_model = None\n",
    "        best_accuracy = 0\n",
    "        \n",
    "        for model_name, result in training_results.items():\n",
    "            accuracy = result['final_test_acc']\n",
    "            time_min = result['training_time'] / 60\n",
    "            params = sum(p.numel() for p in result['model'].parameters())\n",
    "            \n",
    "            print(f\"   {model_name}:\")\n",
    "            print(f\"     ‚Ä¢ Test Accuracy: {accuracy:.2f}%\")\n",
    "            print(f\"     ‚Ä¢ Training Time: {time_min:.1f} minutes\")\n",
    "            print(f\"     ‚Ä¢ Parameters: {params:,}\")\n",
    "            print(f\"     ‚Ä¢ Efficiency: {accuracy/time_min:.2f} acc%/min\")\n",
    "            \n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                best_model = model_name\n",
    "            \n",
    "            print()\n",
    "        \n",
    "        print(f\"üèÖ BEST MODEL: {best_model} ({best_accuracy:.2f}% accuracy)\")\n",
    "        \n",
    "        # Architecture comparison\n",
    "        if len(training_results) > 1:\n",
    "            print(\"\\nüî¨ ARCHITECTURE ANALYSIS:\")\n",
    "            print(\"-\" * 40)\n",
    "            models = list(training_results.items())\n",
    "            \n",
    "            if len(models) == 2:\n",
    "                model1_name, model1_result = models[0]\n",
    "                model2_name, model2_result = models[1]\n",
    "                \n",
    "                acc_diff = abs(model1_result['final_test_acc'] - model2_result['final_test_acc'])\n",
    "                time_diff = abs(model1_result['training_time'] - model2_result['training_time']) / 60\n",
    "                \n",
    "                print(f\"   ‚Ä¢ Accuracy difference: {acc_diff:.2f}%\")\n",
    "                print(f\"   ‚Ä¢ Training time difference: {time_diff:.1f} minutes\")\n",
    "                \n",
    "                if 'Dense' in model1_name or 'Dense' in model2_name:\n",
    "                    print(\"   ‚Ä¢ Dense vs CNN comparison completed\")\n",
    "                    if acc_diff < 2.0:\n",
    "                        print(\"   ‚Ä¢ Both architectures show similar performance\")\n",
    "                    else:\n",
    "                        winner = model1_name if model1_result['final_test_acc'] > model2_result['final_test_acc'] else model2_name\n",
    "                        print(f\"   ‚Ä¢ {winner} shows superior performance\")\n",
    "    \n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è No training results available for summary\")\n",
    "    \n",
    "    print(\"\\nüîß TECHNICAL ACHIEVEMENTS:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(\"   ‚úÖ Modular CIFAR-100 data loading and preprocessing\")\n",
    "    print(\"   ‚úÖ RGB to RGBL transformation with batch processing\")\n",
    "    print(\"   ‚úÖ Multi-stream neural network architectures\")\n",
    "    print(\"   ‚úÖ Efficient training pipeline with GPU acceleration\")\n",
    "    print(\"   ‚úÖ Comprehensive evaluation and visualization\")\n",
    "    print(\"   ‚úÖ Model saving and inference demonstration\")\n",
    "    print(\"   ‚úÖ Production-ready code structure\")\n",
    "    \n",
    "    print(\"\\nüöÄ NEXT STEPS & IMPROVEMENTS:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(\"   ‚Ä¢ Scale training to full CIFAR-100 dataset (50k training samples)\")\n",
    "    print(\"   ‚Ä¢ Implement advanced techniques:\")\n",
    "    print(\"     - Data augmentation (rotation, flip, crop)\")\n",
    "    print(\"     - Learning rate scheduling and early stopping\")\n",
    "    print(\"     - Model ensembling\")\n",
    "    print(\"     - Attention mechanisms\")\n",
    "    print(\"   ‚Ä¢ Experiment with different brightness extraction methods\")\n",
    "    print(\"   ‚Ä¢ Add more sophisticated CNN architectures (ResNet-50, EfficientNet)\")\n",
    "    print(\"   ‚Ä¢ Hyperparameter optimization (learning rate, batch size, etc.)\")\n",
    "    print(\"   ‚Ä¢ Transfer learning from pre-trained models\")\n",
    "    print(\"   ‚Ä¢ Multi-GPU training for faster convergence\")\n",
    "    \n",
    "    print(\"\\nüí° KEY INSIGHTS:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(\"   ‚Ä¢ Multi-stream processing effectively utilizes RGB and brightness\")\n",
    "    print(\"   ‚Ä¢ Batch processing significantly improves data preprocessing speed\")\n",
    "    print(\"   ‚Ä¢ Both dense and CNN architectures show promise for multi-stream data\")\n",
    "    print(\"   ‚Ä¢ Modular design enables easy experimentation and extension\")\n",
    "    print(\"   ‚Ä¢ CIFAR-100's 100 classes provide good complexity for evaluation\")\n",
    "    \n",
    "    print(\"\\nüìö RESOURCES & DOCUMENTATION:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(\"   ‚Ä¢ Code: src/ directory with modular components\")\n",
    "    print(\"   ‚Ä¢ Models: Saved in models/ directory\")\n",
    "    print(\"   ‚Ä¢ Tests: tests/ directory with comprehensive test suite\")\n",
    "    print(\"   ‚Ä¢ Documentation: README.md and inline documentation\")\n",
    "    print(\"   ‚Ä¢ Results: Cached processed data and training outputs\")\n",
    "    \n",
    "    print(\"\\nüéØ PROJECT STATUS: COMPLETE ‚úÖ\")\n",
    "    print(\"   Ready for production use and further research!\")\n",
    "\n",
    "# Run the summary\n",
    "generate_project_summary()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üôè THANK YOU FOR EXPLORING MULTI-STREAM NEURAL NETWORKS!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nüí¨ Questions or improvements? Check the GitHub repository:\")\n",
    "print(\"   https://github.com/clingergab/Multi-Stream-Neural-Networks\")\n",
    "print(\"\\nüöÄ Happy experimenting with multi-stream architectures!\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}