[
  {
    "combination_id": 0,
    "parameters": {
      "learning_rate": 0.001,
      "batch_size": 64,
      "optimizer": "sgd",
      "weight_decay": 1e-05,
      "scheduler": "cosine",
      "transform": true
    },
    "metrics": {
      "best_val_loss": Infinity,
      "best_val_accuracy": 0.0,
      "final_val_loss": Infinity,
      "final_val_accuracy": 0.0,
      "epochs_completed": 0,
      "converged": false,
      "early_stopped": false,
      "early_stopping_info": {},
      "error": "Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"/Users/gclinger/Documents/projects/Multi-Stream-Neural-Networks/src/data_utils/dual_channel_dataset.py\", line 94, in __getitem__\n    rgb = self.transform(rgb)\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torchvision/transforms/transforms.py\", line 95, in __call__\n    img = t(img)\n          ^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torchvision/transforms/transforms.py\", line 137, in __call__\n    return F.to_tensor(pic)\n           ^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torchvision/transforms/functional.py\", line 142, in to_tensor\n    raise TypeError(f\"pic should be PIL Image or ndarray. Got {type(pic)}\")\nTypeError: pic should be PIL Image or ndarray. Got <class 'torch.Tensor'>\n"
    },
    "start_time": "2025-07-09T21:51:38.102034",
    "end_time": "2025-07-09T21:51:41.224813",
    "duration_minutes": 0.05204631666666667
  },
  {
    "combination_id": 1,
    "parameters": {
      "learning_rate": 0.001,
      "batch_size": 64,
      "optimizer": "sgd",
      "weight_decay": 1e-05,
      "scheduler": "cosine",
      "transform": false
    },
    "metrics": {
      "best_val_loss": Infinity,
      "best_val_accuracy": 0.0,
      "final_val_loss": Infinity,
      "final_val_accuracy": 0.0,
      "epochs_completed": 0,
      "converged": false,
      "early_stopped": false,
      "early_stopping_info": {},
      "error": "Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"/Users/gclinger/Documents/projects/Multi-Stream-Neural-Networks/src/data_utils/dual_channel_dataset.py\", line 94, in __getitem__\n    rgb = self.transform(rgb)\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torchvision/transforms/transforms.py\", line 95, in __call__\n    img = t(img)\n          ^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torchvision/transforms/transforms.py\", line 137, in __call__\n    return F.to_tensor(pic)\n           ^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torchvision/transforms/functional.py\", line 142, in to_tensor\n    raise TypeError(f\"pic should be PIL Image or ndarray. Got {type(pic)}\")\nTypeError: pic should be PIL Image or ndarray. Got <class 'torch.Tensor'>\n"
    },
    "start_time": "2025-07-09T21:51:41.225245",
    "end_time": "2025-07-09T21:51:44.332219",
    "duration_minutes": 0.0517829
  },
  {
    "combination_id": 2,
    "parameters": {
      "learning_rate": 0.001,
      "batch_size": 64,
      "optimizer": "sgd",
      "weight_decay": 1e-05,
      "scheduler": "oneCycle",
      "transform": true
    },
    "metrics": {
      "best_val_loss": Infinity,
      "best_val_accuracy": 0.0,
      "final_val_loss": Infinity,
      "final_val_accuracy": 0.0,
      "epochs_completed": 0,
      "converged": false,
      "early_stopped": false,
      "early_stopping_info": {},
      "error": "Unsupported scheduler type: oneCycle"
    },
    "start_time": "2025-07-09T21:51:44.332836",
    "end_time": "2025-07-09T21:51:54.726206",
    "duration_minutes": 0.17322283333333335
  },
  {
    "combination_id": 3,
    "parameters": {
      "learning_rate": 0.001,
      "batch_size": 64,
      "optimizer": "sgd",
      "weight_decay": 1e-05,
      "scheduler": "oneCycle",
      "transform": false
    },
    "metrics": {
      "best_val_loss": Infinity,
      "best_val_accuracy": 0.0,
      "final_val_loss": Infinity,
      "final_val_accuracy": 0.0,
      "epochs_completed": 0,
      "converged": false,
      "early_stopped": false,
      "early_stopping_info": {},
      "error": "Unsupported scheduler type: oneCycle"
    },
    "start_time": "2025-07-09T21:51:54.726455",
    "end_time": "2025-07-09T21:51:54.946485",
    "duration_minutes": 0.0036671666666666667
  },
  {
    "combination_id": 4,
    "parameters": {
      "learning_rate": 0.001,
      "batch_size": 64,
      "optimizer": "sgd",
      "weight_decay": 0.0001,
      "scheduler": "cosine",
      "transform": true
    },
    "metrics": {
      "best_val_loss": Infinity,
      "best_val_accuracy": 0.0,
      "final_val_loss": Infinity,
      "final_val_accuracy": 0.0,
      "epochs_completed": 0,
      "converged": false,
      "early_stopped": false,
      "early_stopping_info": {},
      "error": "Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"/Users/gclinger/Documents/projects/Multi-Stream-Neural-Networks/src/data_utils/dual_channel_dataset.py\", line 94, in __getitem__\n    rgb = self.transform(rgb)\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torchvision/transforms/transforms.py\", line 95, in __call__\n    img = t(img)\n          ^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torchvision/transforms/transforms.py\", line 137, in __call__\n    return F.to_tensor(pic)\n           ^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torchvision/transforms/functional.py\", line 142, in to_tensor\n    raise TypeError(f\"pic should be PIL Image or ndarray. Got {type(pic)}\")\nTypeError: pic should be PIL Image or ndarray. Got <class 'torch.Tensor'>\n"
    },
    "start_time": "2025-07-09T21:51:54.946816",
    "end_time": "2025-07-09T21:51:57.692844",
    "duration_minutes": 0.045767133333333335
  }
]