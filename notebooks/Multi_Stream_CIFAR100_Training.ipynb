{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f0aae91",
   "metadata": {
    "id": "4f0aae91"
   },
   "source": [
    "# Multi-Stream Neural Networks\n",
    "\n",
    "\n",
    "### Model Architectures\n",
    "1. **BaseMultiChannelNetwork**: Dense/fully-connected multi-stream processing\n",
    "2. **MultiChannelResNetNetwork**: CNN with residual connections for spatial features\n",
    "\n",
    "### API Design Philosophy\n",
    "- **`model(color, brightness)`** → Single tensor for training/inference\n",
    "- **`model.analyze_pathways(color, brightness)`** → Tuple for research analysis\n",
    "- **Keras-like training**: `.fit()`, `.evaluate()`, `.predict()` methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7051aee4",
   "metadata": {
    "id": "7051aee4"
   },
   "source": [
    "## Environment Setup & Requirements\n",
    "\n",
    "### Prerequisites\n",
    "- **Python 3.8+**\n",
    "- **PyTorch 1.12+** with CUDA support (recommended)\n",
    "- **Google Colab** (this notebook) or local Jupyter environment\n",
    "\n",
    "### Project Structure\n",
    "Our codebase is fully modularized:\n",
    "```\n",
    "Multi-Stream-Neural-Networks/\n",
    "├── src/\n",
    "│   ├── models/basic_multi_channel/     # Core model implementations\n",
    "│   │   ├── base_multi_channel_network.py    # Dense model\n",
    "│   │   └── multi_channel_resnet_network.py  # CNN model\n",
    "│   ├── utils/cifar100_loader.py        # CIFAR-100 data utilities\n",
    "│   ├── transforms/rgb_to_rgbl.py       # RGB→Brightness transform\n",
    "│   └── utils/device_utils.py           # GPU optimization utilities\n",
    "├── configs/                            # Model configuration files\n",
    "└── data/                               # Dataset location\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e80196",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "Mount Google Drive and navigate to the project directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823d044d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Mount Google Drive\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[1;32m      3\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57873d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Navigate to the project directory\n",
    "import os\n",
    "\n",
    "# Navigate to Drive and project directory\n",
    "project_path = '/content/drive/MyDrive/Multi-Stream-Neural-Networks'\n",
    "if os.path.exists(project_path):\n",
    "    os.chdir(project_path)\n",
    "    print(f\"✅ Found project at: {project_path}\")\n",
    "else:\n",
    "    print(f\"❌ Project not found at: {project_path}\")\n",
    "    print(\"💡 Please clone the repository first:\")\n",
    "    print(\"   !git clone https://github.com/yourusername/Multi-Stream-Neural-Networks.git\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1955bd1d",
   "metadata": {},
   "source": [
    "## 2. Update Repository\n",
    "\n",
    "Pull the latest changes from the repository to ensure we have the most recent codebase.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e805e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update repository with latest changes\n",
    "print(\"🔄 Pulling latest changes from repository...\")\n",
    "\n",
    "# Make sure we're in the right directory\n",
    "print(f\"📁 Current directory: {os.getcwd()}\")\n",
    "\n",
    "# Pull latest changes\n",
    "!git pull origin main\n",
    "\n",
    "print(\"\\n✅ Repository update complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1aa368",
   "metadata": {},
   "source": [
    "## 3. Install Dependencies\n",
    "\n",
    "Install required packages and dependencies for the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xo3JWyNVEGMZ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xo3JWyNVEGMZ",
    "outputId": "34f5af2f-a23c-4e75-848f-94bbf88fffb4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Installing required dependencies...\n",
      "Installing packages...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ torch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ torchvision\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ numpy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ seaborn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ tqdm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ scikit-learn\n",
      "✅ Pillow\n",
      "\n",
      "📚 Importing libraries...\n",
      "✅ All libraries imported successfully!\n",
      "\n",
      "🔧 PyTorch Setup:\n",
      "   PyTorch version: 2.7.0\n",
      "   CUDA available: False\n",
      "   Using CPU (consider GPU for faster training)\n",
      "\n",
      "🎯 Dependencies and imports complete!\n",
      "✅ Pillow\n",
      "\n",
      "📚 Importing libraries...\n",
      "✅ All libraries imported successfully!\n",
      "\n",
      "🔧 PyTorch Setup:\n",
      "   PyTorch version: 2.7.0\n",
      "   CUDA available: False\n",
      "   Using CPU (consider GPU for faster training)\n",
      "\n",
      "🎯 Dependencies and imports complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install Dependencies\n",
    "print(\"📦 Installing required dependencies...\")\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "import importlib\n",
    "\n",
    "def install_package(package):\n",
    "    \"\"\"Install a package if not already installed.\"\"\"\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package, \"--quiet\"])\n",
    "        return True\n",
    "    except subprocess.CalledProcessError:\n",
    "        return False\n",
    "\n",
    "# Required packages\n",
    "packages = [\n",
    "    \"torch\",\n",
    "    \"torchvision\", \n",
    "    \"numpy\",\n",
    "    \"matplotlib\",\n",
    "    \"seaborn\",\n",
    "    \"tqdm\",\n",
    "    \"scikit-learn\",\n",
    "    \"Pillow\"\n",
    "]\n",
    "\n",
    "print(\"Installing packages...\")\n",
    "for package in packages:\n",
    "    if install_package(package):\n",
    "        print(f\"✅ {package}\")\n",
    "    else:\n",
    "        print(f\"❌ Failed to install {package}\")\n",
    "\n",
    "# Install project requirements\n",
    "print(\"\\nInstalling project requirements...\")\n",
    "try:\n",
    "    !pip install -r requirements.txt\n",
    "    print(\"✅ Project requirements installed\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Error installing requirements: {e}\")\n",
    "    print(\"   Continuing with available packages\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ecd3cd",
   "metadata": {},
   "source": [
    "## 4. Import Libraries\n",
    "\n",
    "Import all necessary libraries and utilities for the project.\n",
    "\n",
    "### Important Technical Notes:\n",
    "- **Learning Rate Differences**: The MultiChannelResNetNetwork model requires a lower learning rate (0.0003) than BaseMultiChannelNetwork (0.001) for optimal training.\n",
    "- **Gradient Clipping**: PyTorch's gradient clipping function `torch.nn.utils.clip_grad_norm_` requires the parameter name `max_norm` (not `max_value`), which we ensure is used consistently throughout the codebase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9be5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "print(\"📚 Importing libraries and setting up the environment...\")\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "# Core PyTorch Libraries\n",
    "#------------------------------------------------------------------------------\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "\n",
    "# Monkey patch torch.nn.utils.clip_grad_norm_ to handle both 'max_norm' and 'max_value' parameters\n",
    "# This ensures backward compatibility with any code that might use either parameter name\n",
    "original_clip_grad_norm_ = torch.nn.utils.clip_grad_norm_\n",
    "\n",
    "def patched_clip_grad_norm_(parameters, max_norm=None, norm_type=2.0, error_if_nonfinite=False, **kwargs):\n",
    "    \"\"\"\n",
    "    Patched version of clip_grad_norm_ that handles both max_norm and max_value parameters.\n",
    "    This ensures compatibility with any code that might use the wrong parameter name.\n",
    "    \n",
    "    Args:\n",
    "        parameters: Model parameters to clip gradients for\n",
    "        max_norm: Maximum norm value (can be provided positionally)\n",
    "        norm_type: Type of norm to use\n",
    "        error_if_nonfinite: Whether to error on non-finite values\n",
    "        **kwargs: Additional keyword arguments that might include 'max_value'\n",
    "    \"\"\"\n",
    "    # If max_value is in kwargs, use it if max_norm is not already set\n",
    "    if max_norm is None and 'max_value' in kwargs:\n",
    "        max_norm = kwargs['max_value']\n",
    "        print(\"Warning: clip_grad_norm_ called with max_value instead of max_norm. Using max_value as max_norm.\")\n",
    "    \n",
    "    # Ensure max_norm is not None\n",
    "    if max_norm is None:\n",
    "        raise TypeError(\"patched_clip_grad_norm_() missing required argument 'max_norm'\")\n",
    "    \n",
    "    # Call the original function with the correct parameter name\n",
    "    return original_clip_grad_norm_(parameters, max_norm, norm_type, error_if_nonfinite)\n",
    "\n",
    "# Apply the monkey patch\n",
    "torch.nn.utils.clip_grad_norm_ = patched_clip_grad_norm_\n",
    "print(\"✅ Applied compatibility patch for torch.nn.utils.clip_grad_norm_\")\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "# Data Handling Libraries\n",
    "#------------------------------------------------------------------------------\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import subprocess\n",
    "import importlib\n",
    "import traceback\n",
    "import inspect  # For inspecting source code\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "# Visualization Libraries\n",
    "#------------------------------------------------------------------------------\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "# Progress Tracking and Machine Learning Libraries\n",
    "#------------------------------------------------------------------------------\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "# Google Colab Integration (if needed)\n",
    "#------------------------------------------------------------------------------\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    IN_COLAB = True\n",
    "    print(\"✅ Google Colab detected\")\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"ℹ️ Not running in Google Colab\")\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "# Project-Specific Imports\n",
    "#------------------------------------------------------------------------------\n",
    "# Add project root to path for imports\n",
    "project_root = Path('.').resolve()\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.append(str(project_root))\n",
    "\n",
    "# Import project modules (with error handling)\n",
    "try:\n",
    "    # Data processing\n",
    "    from src.utils.cifar100_loader import get_cifar100_datasets, create_validation_split\n",
    "    from src.transforms.rgb_to_rgbl import RGBtoRGBL\n",
    "    from src.transforms.dataset_utils import process_dataset_to_streams, create_dataloader_with_streams\n",
    "    \n",
    "    # Data augmentation\n",
    "    from src.transforms.augmentation import (\n",
    "        CIFAR100Augmentation, \n",
    "        AugmentedMultiStreamDataset,\n",
    "        MixUp, \n",
    "        create_augmented_dataloaders,\n",
    "        create_test_dataloader\n",
    "    )\n",
    "    \n",
    "    # Model builders\n",
    "    try:\n",
    "        from src.models.builders import create_model, list_available_models\n",
    "        MODEL_FACTORY_AVAILABLE = True\n",
    "    except ImportError:\n",
    "        MODEL_FACTORY_AVAILABLE = False\n",
    "        from src.models.basic_multi_channel.base_multi_channel_network import BaseMultiChannelNetwork, base_multi_channel_large\n",
    "        from src.models.basic_multi_channel.multi_channel_resnet_network import MultiChannelResNetNetwork, multi_channel_resnet50\n",
    "    \n",
    "    print(\"✅ Project modules imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"⚠️ Error importing project modules: {e}\")\n",
    "    print(\"   Some functionality may be limited\")\n",
    "\n",
    "# Set environment variables for CUDA\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'  # Better error diagnostics for CUDA\n",
    "\n",
    "print(\"✅ All libraries imported successfully!\")\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "# Environment Information\n",
    "#------------------------------------------------------------------------------\n",
    "# Check PyTorch setup\n",
    "print(f\"\\n🔧 PyTorch Setup:\")\n",
    "print(f\"   PyTorch version: {torch.__version__}\")\n",
    "print(f\"   CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   CUDA version: {torch.version.cuda}\")\n",
    "else:\n",
    "    print(\"   Using CPU (consider GPU for faster training)\")\n",
    "\n",
    "# Configure CUDA for better performance\n",
    "torch.backends.cudnn.benchmark = True  # Speed up training for fixed input sizes\n",
    "torch.backends.cudnn.deterministic = False  # Non-deterministic for speed\n",
    "\n",
    "# Verify the clip_grad_norm_ patching\n",
    "clip_grad_code = inspect.getsource(torch.nn.utils.clip_grad_norm_)\n",
    "print(\"\\n🔧 Verifying gradient clipping fix:\")\n",
    "print(f\"   Using patched clip_grad_norm_ function that handles both parameter formats\")\n",
    "\n",
    "print(\"\\n🎯 Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd732e2",
   "metadata": {},
   "source": [
    "## 5. Load Data\n",
    "\n",
    "Load the CIFAR-100 dataset using our optimized data loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2895f8fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 Setting up CIFAR-100 dataset loading...\n",
      "✅ CIFAR-100 loader utilities imported successfully\n",
      "📁 Loading CIFAR-100 datasets with train/validation/test split...\n",
      "❌ Error loading CIFAR-100 data: get_cifar100_datasets() got an unexpected keyword argument 'root'\n",
      "\n",
      "💡 Troubleshooting:\n",
      "   1. Check internet connection for CIFAR-100 download\n",
      "   2. Verify data directory permissions\n",
      "   3. Try clearing cache: rm -rf data/cifar-100\n",
      "   4. Check if src/utils/cifar100_loader.py exists\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "get_cifar100_datasets() got an unexpected keyword argument 'root'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 16\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m📁 Loading CIFAR-100 datasets with train/validation/test split...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# Load datasets using our optimized loader\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m     train_dataset, val_dataset, test_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mget_cifar100_datasets\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./data\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdownload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43mval_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 10% validation split from training data\u001b[39;49;00m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ CIFAR-100 datasets loaded successfully!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   📊 Training samples: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_dataset)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: get_cifar100_datasets() got an unexpected keyword argument 'root'"
     ]
    }
   ],
   "source": [
    "# 📊 CIFAR-100 Data Loading and Verification\n",
    "print(\"📁 Setting up CIFAR-100 dataset loading...\")\n",
    "\n",
    "# Load CIFAR-100 Dataset\n",
    "print(\"📁 Loading CIFAR-100 datasets with train/validation/test split...\")\n",
    "\n",
    "# Since we already imported the CIFAR-100 loader utilities in the main import cell,\n",
    "# we can check if they were imported successfully\n",
    "if 'get_cifar100_datasets' in globals():\n",
    "    print(\"✅ CIFAR-100 loader utilities imported successfully\")\n",
    "else:\n",
    "    print(\"❌ Failed to import CIFAR-100 utilities. Make sure src/utils/cifar100_loader.py exists\")\n",
    "    raise ImportError(\"Missing CIFAR-100 loader utilities\")\n",
    "\n",
    "# Load datasets using our optimized loader (returns train, test, class_names)\n",
    "train_dataset, test_dataset, class_names = get_cifar100_datasets(\n",
    "    data_dir='./data/cifar-100'\n",
    ")\n",
    "\n",
    "# Create validation split from training data\n",
    "train_dataset, val_dataset = create_validation_split(\n",
    "    train_dataset, \n",
    "    val_split=0.1\n",
    ")\n",
    "\n",
    "print(\"✅ CIFAR-100 datasets loaded successfully!\")\n",
    "print(f\"   📊 Training samples: {len(train_dataset):,}\")\n",
    "print(f\"   📊 Validation samples: {len(val_dataset):,}\")\n",
    "print(f\"   📊 Test samples: {len(test_dataset):,}\")\n",
    "print(f\"   🏷️ Number of classes: {len(class_names)}\")\n",
    "\n",
    "# Store class names for later use\n",
    "CIFAR100_FINE_LABELS = class_names\n",
    "\n",
    "print(\"\\n🎯 Data loading complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a99955e",
   "metadata": {},
   "source": [
    "## 6. Process Data\n",
    "\n",
    "Convert RGB images to RGB + Brightness (L) channels for multi-stream processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a78fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process Data: RGB to RGB+L (Brightness) Conversion\n",
    "print(\"🔄 Converting RGB images to RGB + Brightness streams...\")\n",
    "\n",
    "# Since we already imported the dataset utilities in the main import cell,\n",
    "# we can check if they were imported successfully\n",
    "if 'process_dataset_to_streams' in globals():\n",
    "    print(\"✅ Dataset processing utilities imported successfully\")\n",
    "else:\n",
    "    print(\"❌ Failed to import dataset utilities. Make sure src/transforms/dataset_utils.py exists\")\n",
    "    raise ImportError(\"Missing dataset utilities\")\n",
    "\n",
    "# Process all datasets\n",
    "print(\"Processing training dataset...\")\n",
    "train_rgb, train_brightness, train_labels_tensor = process_dataset_to_streams(\n",
    "    train_dataset, desc=\"Training data\"\n",
    ")\n",
    "\n",
    "print(\"Processing validation dataset...\")\n",
    "val_rgb, val_brightness, val_labels_tensor = process_dataset_to_streams(\n",
    "    val_dataset, desc=\"Validation data\"\n",
    ")\n",
    "\n",
    "print(\"Processing test dataset...\")\n",
    "test_rgb, test_brightness, test_labels_tensor = process_dataset_to_streams(\n",
    "    test_dataset, desc=\"Test data\"\n",
    ")\n",
    "\n",
    "print(\"\\n✅ Multi-stream conversion complete!\")\n",
    "print(f\"   🎨 RGB stream shape: {train_rgb.shape}\")\n",
    "print(f\"   💡 Brightness stream shape: {train_brightness.shape}\")\n",
    "print(f\"   📊 RGB range: [{train_rgb.min():.3f}, {train_rgb.max():.3f}]\")\n",
    "print(f\"   📊 Brightness range: [{train_brightness.min():.3f}, {train_brightness.max():.3f}]\")\n",
    "\n",
    "# Memory usage estimation\n",
    "rgb_memory = (train_rgb.nbytes + val_rgb.nbytes + test_rgb.nbytes) / 1e6\n",
    "brightness_memory = (train_brightness.nbytes + val_brightness.nbytes + test_brightness.nbytes) / 1e6\n",
    "total_memory = rgb_memory + brightness_memory\n",
    "\n",
    "print(f\"\\n📈 Processing Summary:\")\n",
    "print(f\"   📊 Total samples processed: {len(train_labels_tensor) + len(val_labels_tensor) + len(test_labels_tensor):,}\")\n",
    "print(f\"   🎨 RGB streams memory: {rgb_memory:.1f} MB\")\n",
    "print(f\"   💡 Brightness streams memory: {brightness_memory:.1f} MB\")\n",
    "print(f\"   💾 Total memory usage: {total_memory:.1f} MB\")\n",
    "\n",
    "print(\"\\n🎯 Data processing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e2429b",
   "metadata": {},
   "source": [
    "## 7. Data Verification\n",
    "\n",
    "Verify the processed data structure and consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb2f71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Verification\n",
    "print(\"🔍 Verifying processed data structure and consistency...\")\n",
    "\n",
    "def verify_data_integrity(rgb_data, brightness_data, labels, split_name):\n",
    "    # Check shapes and types\n",
    "    assert rgb_data.shape[0] == brightness_data.shape[0] == labels.shape[0], f\"Inconsistent sample counts in {split_name}!\"\n",
    "    assert rgb_data.shape[1:] == (3, 32, 32), f\"Unexpected RGB shape in {split_name}!\"\n",
    "    assert brightness_data.shape[1:] == (1, 32, 32), f\"Unexpected brightness shape in {split_name}!\"\n",
    "    assert 0 <= labels.min() and labels.max() < 100, f\"Invalid label range in {split_name}!\"\n",
    "    return rgb_data.shape[0]\n",
    "\n",
    "train_samples = verify_data_integrity(train_rgb, train_brightness, train_labels_tensor, \"Training\")\n",
    "val_samples = verify_data_integrity(val_rgb, val_brightness, val_labels_tensor, \"Validation\")\n",
    "test_samples = verify_data_integrity(test_rgb, test_brightness, test_labels_tensor, \"Test\")\n",
    "\n",
    "total_samples = train_samples + val_samples + test_samples\n",
    "all_labels = torch.cat([train_labels_tensor, val_labels_tensor, test_labels_tensor])\n",
    "unique_labels = torch.unique(all_labels)\n",
    "\n",
    "print(f\"\\n📈 Data Summary:\")\n",
    "print(f\"   Training: {train_samples:,}\")\n",
    "print(f\"   Validation: {val_samples:,}\")\n",
    "print(f\"   Test: {test_samples:,}\")\n",
    "print(f\"   Total: {total_samples:,}\")\n",
    "print(f\"   Unique classes: {len(unique_labels)}/100\")\n",
    "print(\"\\n✅ Data verification checks passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba0574d",
   "metadata": {},
   "source": [
    "## 8. Data Visualization\n",
    "\n",
    "Visualize sample images from both RGB and brightness streams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b744428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Visualization\n",
    "print(\"👁️ Visualizing sample images from both RGB and brightness streams...\")\n",
    "\n",
    "# Set up visualization\n",
    "plt.style.use('default')\n",
    "fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n",
    "fig.suptitle('Multi-Stream CIFAR-100 Samples: RGB vs Brightness', fontsize=14)\n",
    "\n",
    "# Select random samples\n",
    "np.random.seed(42)  # For reproducible results\n",
    "sample_indices = np.random.choice(len(train_rgb), 4, replace=False)\n",
    "\n",
    "for i, idx in enumerate(sample_indices):\n",
    "    # Get data\n",
    "    rgb_img = train_rgb[idx]\n",
    "    brightness_img = train_brightness[idx]\n",
    "    label = train_labels_tensor[idx].item()\n",
    "    class_name = CIFAR100_FINE_LABELS[label]\n",
    "    \n",
    "    # Convert tensors to NumPy arrays FOR VISUALIZATION ONLY\n",
    "    # Note: This conversion is necessary only for matplotlib visualization\n",
    "    # Model training uses tensors directly\n",
    "    rgb_np = rgb_img.permute(1, 2, 0).cpu().numpy()\n",
    "    rgb_np = np.clip(rgb_np, 0, 1)  # Ensure valid range\n",
    "    \n",
    "    # Brightness image\n",
    "    brightness_np = brightness_img.squeeze().cpu().numpy()\n",
    "    \n",
    "    # Plot RGB\n",
    "    axes[0, i].imshow(rgb_np)\n",
    "    axes[0, i].set_title(f'RGB: {class_name}', fontsize=10)\n",
    "    axes[0, i].axis('off')\n",
    "    \n",
    "    # Plot Brightness\n",
    "    axes[1, i].imshow(brightness_np, cmap='gray')\n",
    "    axes[1, i].set_title(f'Brightness: {class_name}', fontsize=10)\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"✅ Data visualization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5042e5c6",
   "metadata": {},
   "source": [
    "## 9. Data Analysis\n",
    "\n",
    "Perform basic data analysis on class distribution and stream characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee66628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Analysis\n",
    "print(\"📊 Performing basic data analysis...\")\n",
    "\n",
    "# Set up matplotlib for better visualizations\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Class Distribution Analysis\n",
    "print(\"\\n🏷️ Analyzing class distribution...\")\n",
    "\n",
    "# Training distribution - convert to NumPy only for visualization\n",
    "# Note: Only converting to NumPy here because bincount requires it\n",
    "train_counts = np.bincount(train_labels_tensor.cpu().numpy(), minlength=100)\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.bar(range(100), train_counts, alpha=0.7, color='skyblue')\n",
    "plt.title('Training Set Class Distribution', fontweight='bold')\n",
    "plt.xlabel('Class ID')\n",
    "plt.ylabel('Sample Count')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Stream Statistics Analysis\n",
    "print(\"\\n🎨 RGB vs Brightness stream characteristics:\")\n",
    "\n",
    "# Sample a subset for analysis\n",
    "sample_size = min(1000, len(train_rgb))\n",
    "indices = np.random.choice(len(train_rgb), sample_size, replace=False)\n",
    "rgb_sample = train_rgb[indices]\n",
    "brightness_sample = train_brightness[indices]\n",
    "\n",
    "# Calculate statistics - using PyTorch's built-in statistical functions\n",
    "# No need to convert to NumPy for these calculations\n",
    "rgb_stats = {\n",
    "    'mean': rgb_sample.mean().item(),\n",
    "    'std': rgb_sample.std().item(),\n",
    "    'min': rgb_sample.min().item(),\n",
    "    'max': rgb_sample.max().item()\n",
    "}\n",
    "\n",
    "brightness_stats = {\n",
    "    'mean': brightness_sample.mean().item(),\n",
    "    'std': brightness_sample.std().item(), \n",
    "    'min': brightness_sample.min().item(),\n",
    "    'max': brightness_sample.max().item()\n",
    "}\n",
    "\n",
    "print(f\"   🎨 RGB statistics:\")\n",
    "print(f\"      Mean: {rgb_stats['mean']:.3f}, Std: {rgb_stats['std']:.3f}\")\n",
    "print(f\"      Min: {rgb_stats['min']:.3f}, Max: {rgb_stats['max']:.3f}\")\n",
    "print(f\"   💡 Brightness statistics:\")\n",
    "print(f\"      Mean: {brightness_stats['mean']:.3f}, Std: {brightness_stats['std']:.3f}\")\n",
    "print(f\"      Min: {brightness_stats['min']:.3f}, Max: {brightness_stats['max']:.3f}\")\n",
    "\n",
    "print(\"\\n✅ Data analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803e7962",
   "metadata": {},
   "source": [
    "## 10. Data Augmentation\n",
    "\n",
    "Set up data augmentation for multi-stream training using the project's CIFAR-100 augmentation module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca38f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Augmentation\n",
    "print(\"🔄 Setting up data augmentation using project's implementation...\")\n",
    "\n",
    "# Create augmentation with custom settings for CIFAR-100\n",
    "augmentation_config = {\n",
    "    'horizontal_flip_prob': 0.5,  # 50% chance of flipping horizontally\n",
    "    'rotation_degrees': 10.0,     # Rotate up to ±10 degrees\n",
    "    'translate_range': 0.1,       # Translate up to 10% of image size\n",
    "    'scale_range': (0.9, 1.1),    # Scale between 90-110%\n",
    "    'color_jitter_strength': 0.3, # Moderate color jittering\n",
    "    'gaussian_noise_std': 0.01,   # Small amount of noise\n",
    "    'cutout_prob': 0.3,           # 30% chance of applying cutout\n",
    "    'cutout_size': 8,             # 8x8 pixel cutout\n",
    "    'enabled': True               # Enable augmentation\n",
    "}\n",
    "\n",
    "# Setup MixUp augmentation\n",
    "mixup_alpha = 0.2  # Alpha parameter for Beta distribution\n",
    "\n",
    "# Create augmented datasets and data loaders in one step\n",
    "print(\"\\n📊 Creating augmented DataLoaders...\")\n",
    "train_loader, val_loader = create_augmented_dataloaders(\n",
    "    train_rgb, train_brightness, train_labels_tensor,  # Training data\n",
    "    val_rgb, val_brightness, val_labels_tensor,        # Validation data\n",
    "    batch_size=512,                                    # Increased batch size for A100 GPU\n",
    "    dataset=\"cifar100\",                                # Dataset type\n",
    "    augmentation_config=augmentation_config,           # Augmentation settings\n",
    "    mixup_alpha=mixup_alpha,                           # MixUp parameter\n",
    "    num_workers=2,                                     # Parallel workers\n",
    "    pin_memory=torch.cuda.is_available()               # Pin memory if GPU available\n",
    ")\n",
    "\n",
    "# Create test dataloader separately (no augmentation)\n",
    "test_loader = create_test_dataloader(\n",
    "    test_rgb, test_brightness, test_labels_tensor,\n",
    "    batch_size=512,                                    # Increased batch size for A100 GPU\n",
    "    num_workers=2,\n",
    "    pin_memory=torch.cuda.is_available()\n",
    ")\n",
    "\n",
    "print(\"\\n✅ Data augmentation setup complete\")\n",
    "print(\"   Using project's CIFAR-100 specific augmentations:\")\n",
    "print(f\"   - Horizontal flips: {augmentation_config['horizontal_flip_prob']}\")\n",
    "print(f\"   - Rotation: ±{augmentation_config['rotation_degrees']}°\")\n",
    "print(f\"   - Translation: ±{augmentation_config['translate_range'] * 100}%\")\n",
    "print(f\"   - Color jitter strength: {augmentation_config['color_jitter_strength']}\")\n",
    "print(f\"   - Gaussian noise (std): {augmentation_config['gaussian_noise_std']}\")\n",
    "print(f\"   - Cutout: {augmentation_config['cutout_prob']} probability, {augmentation_config['cutout_size']}px\")\n",
    "print(f\"   - MixUp alpha: {mixup_alpha}\")\n",
    "print(f\"\\n   Training batches: {len(train_loader)}\")\n",
    "print(f\"   Validation batches: {len(val_loader)}\")\n",
    "print(f\"   Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83696378",
   "metadata": {},
   "source": [
    "## 11. Create and train Baseline ResNet50 Model\n",
    "\n",
    "Create a standard ResNet50 model for comparison with multi-stream models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6996da86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Baseline ResNet50 Model\n",
    "print(\"🏗️ Creating baseline ResNet50 model for comparison...\")\n",
    "\n",
    "# Check GPU availability and set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"🖥️ Using device: {device}\")\n",
    "\n",
    "# Create ResNet50 model modified for CIFAR-100\n",
    "class CifarResNet50(nn.Module):\n",
    "    def __init__(self, num_classes=100, pretrained=False):\n",
    "        super(CifarResNet50, self).__init__()\n",
    "        \n",
    "        # Load ResNet50 with or without pretrained weights\n",
    "        if pretrained:\n",
    "            print(\"   Using pretrained weights (ImageNet)\")\n",
    "            self.model = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "        else:\n",
    "            print(\"   Using randomly initialized weights for fair comparison\")\n",
    "            self.model = resnet50(weights=None)\n",
    "        \n",
    "        # Modify first conv layer to work with 32x32 CIFAR images instead of 224x224 ImageNet\n",
    "        self.model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        # Remove maxpool to preserve spatial dimensions for small images\n",
    "        self.model.maxpool = nn.Identity()\n",
    "        \n",
    "        # Replace final fully connected layer for CIFAR-100\n",
    "        self.model.fc = nn.Linear(self.model.fc.in_features, num_classes)\n",
    "        \n",
    "        # Move to device\n",
    "        self.to(device)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Create the model - explicitly using pretrained=False for fair comparison with multi-stream models\n",
    "print(\"\\n⚠️ IMPORTANT: Using NON-PRETRAINED weights for fair comparison with multi-stream models\")\n",
    "baseline_model = CifarResNet50(num_classes=100, pretrained=False)\n",
    "baseline_model = baseline_model.to(device)\n",
    "\n",
    "# Setup optimizer and loss with weight decay for regularization\n",
    "baseline_optimizer = optim.Adam(baseline_model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "baseline_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Count parameters\n",
    "baseline_params = sum(p.numel() for p in baseline_model.parameters())\n",
    "baseline_trainable = sum(p.numel() for p in baseline_model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\n✅ Baseline ResNet50 created successfully\")\n",
    "print(f\"   Architecture: Modified ResNet50 for CIFAR-100\")\n",
    "print(f\"   Total parameters: {baseline_params:,}\")\n",
    "print(f\"   Trainable parameters: {baseline_trainable:,}\")\n",
    "print(f\"   Input shape: (3, 32, 32)\")\n",
    "print(f\"   Device: {device}\")\n",
    "print(f\"   Initialization: Random (not pretrained)\")\n",
    "print(f\"   Optimizer: Adam with weight_decay=0.0001\")\n",
    "\n",
    "# Quick validation on dummy data\n",
    "print(\"\\n🏋️‍♂️ Validating model with dummy data...\")\n",
    "dummy_data = torch.randn(8, 3, 32, 32, device=device)\n",
    "dummy_labels = torch.randint(0, 100, (8,), device=device)\n",
    "\n",
    "baseline_model.train()\n",
    "outputs = baseline_model(dummy_data)\n",
    "loss = baseline_criterion(outputs, dummy_labels)\n",
    "print(f\"   ✅ Forward pass successful, loss: {loss.item():.4f}\")\n",
    "\n",
    "# Reset gradients\n",
    "baseline_optimizer.zero_grad()\n",
    "loss.backward()\n",
    "baseline_optimizer.step()\n",
    "print(f\"   ✅ Backward pass successful\")\n",
    "\n",
    "print(\"\\n✅ Baseline model ready for training!\")\n",
    "\n",
    "# Immediate training on dummy data for validation\n",
    "print(\"🏋️‍♂️ Starting immediate training on dummy data...\")\n",
    "\n",
    "# Create dummy CIFAR-100 data\n",
    "dummy_data = torch.randn(64, 3, 32, 32, device=device)  # 64 samples, 3 channels, 32x32 images\n",
    "dummy_labels = torch.randint(0, 100, (64,), device=device)  # 64 labels for 100 classes\n",
    "\n",
    "# Training loop (1 epoch)\n",
    "baseline_model.train()\n",
    "for epoch in range(1):  # Just 1 epoch for quick testing\n",
    "    # Forward pass\n",
    "    outputs = baseline_model(dummy_data)\n",
    "    loss = baseline_criterion(outputs, dummy_labels)\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    baseline_optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    baseline_optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/1], Loss: {loss.item():.4f}\")\n",
    "\n",
    "print(\"✅ Immediate training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40785a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Baseline ResNet50 Model\n",
    "print(\"🏋️‍♀️ Training baseline ResNet50 model...\")\n",
    "\n",
    "# Define training function with early stopping and learning rate scheduling\n",
    "def train_model(model, train_loader, val_loader, optimizer, criterion, num_epochs=100, patience=10, model_name=\"Model\"):\n",
    "    \"\"\"\n",
    "    Train a model and return training history.\n",
    "    \n",
    "    Args:\n",
    "        model: The model to train\n",
    "        train_loader: DataLoader for training data\n",
    "        val_loader: DataLoader for validation data\n",
    "        optimizer: Optimizer to use\n",
    "        criterion: Loss function\n",
    "        num_epochs: Number of epochs to train\n",
    "        patience: Early stopping patience\n",
    "        model_name: Name for logging\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with training history\n",
    "    \"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': []\n",
    "    }\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    best_model_state = None\n",
    "    no_improvement_count = 0\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, \n",
    "        mode='max',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        train_bar = tqdm(train_loader, desc=f\"{model_name} Training\")\n",
    "        \n",
    "        for batch_idx, data in enumerate(train_bar):\n",
    "            # Handle the baseline model which only needs RGB data - support both tuple and list types\n",
    "            if isinstance(data, (tuple, list)) and len(data) == 3:\n",
    "                rgb, _, targets = data  # Unpack RGB, ignore brightness\n",
    "            else:\n",
    "                # Fallback for unexpected data format\n",
    "                rgb, targets = data\n",
    "                \n",
    "            # Move to device\n",
    "            rgb, targets = rgb.to(device), targets.to(device)\n",
    "            \n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass - baseline model only takes RGB\n",
    "            outputs = model(rgb)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            # Add L2 regularization term (additional to weight decay in optimizer)\n",
    "            l2_lambda = 0.0001\n",
    "            l2_reg = 0.0\n",
    "            for param in model.parameters():\n",
    "                l2_reg += torch.norm(param, 2)\n",
    "            loss += l2_lambda * l2_reg\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Track statistics\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            train_total += targets.size(0)\n",
    "            train_correct += predicted.eq(targets).sum().item()\n",
    "            \n",
    "            # Update progress bar\n",
    "            train_bar.set_postfix({\n",
    "                'loss': train_loss/(batch_idx+1), \n",
    "                'acc': 100.*train_correct/train_total\n",
    "            })\n",
    "        \n",
    "        train_acc = 100. * train_correct / train_total\n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            val_bar = tqdm(val_loader, desc=f\"{model_name} Validation\")\n",
    "            \n",
    "            for batch_idx, data in enumerate(val_bar):\n",
    "                # Handle data format - support both tuple and list types\n",
    "                if isinstance(data, (tuple, list)) and len(data) == 3:\n",
    "                    rgb, _, targets = data  # Unpack RGB, ignore brightness\n",
    "                else:\n",
    "                    rgb, targets = data\n",
    "                \n",
    "                # Move to device\n",
    "                rgb, targets = rgb.to(device), targets.to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(rgb)\n",
    "                loss = criterion(outputs, targets)\n",
    "                \n",
    "                # Track statistics\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                val_total += targets.size(0)\n",
    "                val_correct += predicted.eq(targets).sum().item()\n",
    "                \n",
    "                # Update progress bar\n",
    "                val_bar.set_postfix({\n",
    "                    'loss': val_loss/(batch_idx+1), \n",
    "                    'acc': 100.*val_correct/val_total\n",
    "                })\n",
    "        \n",
    "        val_acc = 100. * val_correct / val_total\n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        \n",
    "        # Save history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        # Print epoch summary\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} Summary:\")\n",
    "        print(f\"   Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "        print(f\"   Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "        print(f\"   Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "        \n",
    "        # Update learning rate scheduler\n",
    "        scheduler.step(val_acc)\n",
    "        \n",
    "        # Track best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            no_improvement_count = 0\n",
    "            print(f\"   ✅ New best validation accuracy: {best_val_acc:.2f}%\")\n",
    "        else:\n",
    "            no_improvement_count += 1\n",
    "            print(f\"   ⚠️ No improvement for {no_improvement_count} epochs\")\n",
    "        \n",
    "        # Early stopping check\n",
    "        if no_improvement_count >= patience:\n",
    "            print(f\"   🛑 Early stopping triggered after {epoch+1} epochs\")\n",
    "            break\n",
    "    \n",
    "    # Restore best model\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    print(f\"\\n✅ {model_name} training complete!\")\n",
    "    print(f\"   Best validation accuracy: {best_val_acc:.2f}%\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "# Training configuration\n",
    "num_epochs = 100  # Maximum epochs\n",
    "patience = 10     # Early stopping patience\n",
    "batch_size = 512  # Increased batch size for A100 GPU\n",
    "\n",
    "print(f\"\\n🔧 Training Configuration:\")\n",
    "print(f\"   Epochs: {num_epochs} (with early stopping, patience={patience})\")\n",
    "print(f\"   Batch size: {batch_size} (optimized for A100 GPU)\")\n",
    "print(f\"   Optimizer: Adam with weight decay=0.0001\")\n",
    "print(f\"   Learning rate: 0.001 with ReduceLROnPlateau scheduling\")\n",
    "print(f\"   Regularization: L2 weight decay\")\n",
    "print(f\"   Early stopping: Patience {patience} epochs\")\n",
    "print(f\"   Loss: CrossEntropyLoss\")\n",
    "print(f\"   GPU acceleration: {torch.cuda.is_available()}\")\n",
    "print(f\"   GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}\")\n",
    "\n",
    "# Train baseline model\n",
    "baseline_history = train_model(\n",
    "    model=baseline_model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optimizer=baseline_optimizer,\n",
    "    criterion=baseline_criterion,\n",
    "    num_epochs=num_epochs,\n",
    "    patience=patience,\n",
    "    model_name=\"Baseline ResNet50\"\n",
    ")\n",
    "\n",
    "# Visualize training curves\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(baseline_history['train_acc'], label='Train')\n",
    "plt.plot(baseline_history['val_acc'], label='Validation')\n",
    "plt.title('Baseline ResNet50 Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(baseline_history['train_loss'], label='Train')\n",
    "plt.plot(baseline_history['val_loss'], label='Validation')\n",
    "plt.title('Baseline ResNet50 Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✅ Baseline model training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb3b0b7",
   "metadata": {},
   "source": [
    "## 12. Create Multi-stream models\n",
    "\n",
    "Now that we've trained our baseline ResNet50 model as a reference, let's create and train our multi-stream neural network models.\n",
    "\n",
    "The multi-stream models combine RGB and brightness (luminance) information through parallel processing pathways:\n",
    "\n",
    "1. **BaseMultiChannelNetwork**: A dense/fully-connected model with separate pathways for RGB and brightness\n",
    "2. **MultiChannelResNetNetwork**: A CNN-based model using ResNet architecture with separate pathways\n",
    "\n",
    "Both models use the same unified augmentation pipeline, regularization techniques, and training strategy as the baseline for fair comparison.\n",
    "\n",
    "### Advantages of Multi-Stream Processing\n",
    "- Separation of color and brightness information allows each pathway to specialize\n",
    "- The network can learn which stream is more informative for specific classes\n",
    "- Potential for improved robustness to variations in lighting conditions\n",
    "\n",
    "### Learning Rate Requirements\n",
    "Each architecture has different learning rate requirements for optimal training:\n",
    "\n",
    "- **BaseMultiChannelNetwork**: Works well with standard learning rates (~0.001)\n",
    "- **MultiChannelResNetNetwork**: Requires a lower learning rate (~0.0003) for stability\n",
    "  - The deeper ResNet architecture is more sensitive to learning rate\n",
    "  - Using a lower learning rate helps prevent oscillation during training\n",
    "  - This leads to better convergence and improved final accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfd0efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Multi-Stream Models\n",
    "print(\"🏗️ Creating Multi-Stream Neural Network Models...\")\n",
    "\n",
    "# Check GPU availability and set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"🖥️ Using device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"   Using CPU (CUDA not available)\")\n",
    "\n",
    "# Model configuration based on CIFAR-100 data\n",
    "print(f\"\\n📊 Model Configuration:\")\n",
    "print(f\"   Image size: 32x32 pixels\")\n",
    "print(f\"   RGB channels: 3\")\n",
    "print(f\"   Brightness channels: 1\") \n",
    "print(f\"   Number of classes: 100 (CIFAR-100)\")\n",
    "print(f\"   Device: {device}\")\n",
    "\n",
    "# Use the model factory imported in the main import cell\n",
    "if MODEL_FACTORY_AVAILABLE:\n",
    "    print(\"\\n📋 Available model types:\")\n",
    "    available_model_types = list_available_models()\n",
    "    for model_type in available_model_types:\n",
    "        print(f\"   ✅ {model_type}\")\n",
    "    \n",
    "    factory_available = True\n",
    "else:\n",
    "    print(f\"❌ Model factory not available\")\n",
    "    print(\"💡 Using direct model imports...\")\n",
    "    factory_available = False\n",
    "\n",
    "# Model dimensions for CIFAR-100\n",
    "input_channels_rgb = 3\n",
    "input_channels_brightness = 1  \n",
    "image_size = 32\n",
    "num_classes = 100\n",
    "\n",
    "# For dense models: flatten the image to 1D\n",
    "rgb_input_size = input_channels_rgb * image_size * image_size  # 3 * 32 * 32 = 3072\n",
    "brightness_input_size = input_channels_brightness * image_size * image_size  # 1 * 32 * 32 = 1024\n",
    "\n",
    "print(f\"\\n🔧 Model Input Configuration:\")\n",
    "print(f\"   RGB input size (dense): {rgb_input_size}\")\n",
    "print(f\"   Brightness input size (dense): {brightness_input_size}\")\n",
    "print(f\"   RGB input channels (CNN): {input_channels_rgb}\")\n",
    "print(f\"   Brightness input channels (CNN): {input_channels_brightness}\")\n",
    "\n",
    "# Create base_multi_channel_large (Dense/FC model)\n",
    "print(f\"\\n🏭 Creating base_multi_channel_large (Dense Model)...\")\n",
    "try:\n",
    "    if factory_available:\n",
    "        base_multi_channel_large_model = create_model(\n",
    "            'base_multi_channel_large',\n",
    "            color_input_size=rgb_input_size,\n",
    "            brightness_input_size=brightness_input_size,\n",
    "            num_classes=num_classes,\n",
    "            use_shared_classifier=True,\n",
    "            device=device  # Use detected device (CUDA if available)\n",
    "        )\n",
    "    else:\n",
    "        base_multi_channel_large_model = base_multi_channel_large(\n",
    "            color_input_size=rgb_input_size,\n",
    "            brightness_input_size=brightness_input_size,\n",
    "            num_classes=num_classes,\n",
    "            use_shared_classifier=True,\n",
    "            device=device  # Use detected device (CUDA if available)\n",
    "        )\n",
    "    \n",
    "    # Count parameters\n",
    "    large_dense_params = sum(p.numel() for p in base_multi_channel_large_model.parameters())\n",
    "    large_dense_trainable = sum(p.numel() for p in base_multi_channel_large_model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"✅ base_multi_channel_large created successfully\")\n",
    "    print(f\"   Architecture: Large Dense/FC Network\")\n",
    "    print(f\"   Total parameters: {large_dense_params:,}\")\n",
    "    print(f\"   Trainable parameters: {large_dense_trainable:,}\")\n",
    "    print(f\"   Input size: RGB {rgb_input_size}, Brightness {brightness_input_size}\")\n",
    "    print(f\"   Fusion strategy: Shared classifier\")\n",
    "    print(f\"   Device: {base_multi_channel_large_model.device}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Failed to create base_multi_channel_large: {e}\")\n",
    "    print(f\"💡 Error details: {str(e)}\")\n",
    "    traceback.print_exc()\n",
    "    base_multi_channel_large_model = None\n",
    "\n",
    "# Create multi_channel_resnet50 (CNN model)\n",
    "print(f\"\\n🏭 Creating multi_channel_resnet50 (CNN Model)...\")\n",
    "try:\n",
    "    if factory_available:\n",
    "        multi_channel_resnet50_model = create_model(\n",
    "            'multi_channel_resnet50',\n",
    "            color_input_channels=input_channels_rgb,\n",
    "            brightness_input_channels=input_channels_brightness,\n",
    "            num_classes=num_classes,\n",
    "            use_shared_classifier=True,\n",
    "            activation='relu',\n",
    "            device=device  # Use detected device (CUDA if available)\n",
    "        )\n",
    "    else:\n",
    "        multi_channel_resnet50_model = multi_channel_resnet50(\n",
    "            color_input_channels=input_channels_rgb,\n",
    "            brightness_input_channels=input_channels_brightness,\n",
    "            num_classes=num_classes,\n",
    "            use_shared_classifier=True,\n",
    "            activation='relu',\n",
    "            device=device  # Use detected device (CUDA if available)\n",
    "        )\n",
    "    \n",
    "    # Count parameters\n",
    "    resnet50_params = sum(p.numel() for p in multi_channel_resnet50_model.parameters())\n",
    "    resnet50_trainable = sum(p.numel() for p in multi_channel_resnet50_model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"✅ multi_channel_resnet50 created successfully\")\n",
    "    print(f\"   Architecture: ResNet-50 style CNN (3,4,6,3 blocks)\")\n",
    "    print(f\"   Total parameters: {resnet50_params:,}\")\n",
    "    print(f\"   Trainable parameters: {resnet50_trainable:,}\")\n",
    "    print(f\"   Input shape: RGB {(input_channels_rgb, image_size, image_size)}, Brightness {(input_channels_brightness, image_size, image_size)}\")\n",
    "    print(f\"   Fusion strategy: Shared classifier\")\n",
    "    print(f\"   Device: {multi_channel_resnet50_model.device}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Failed to create multi_channel_resnet50: {e}\")\n",
    "    print(f\"💡 Error details: {str(e)}\")\n",
    "    traceback.print_exc()\n",
    "    multi_channel_resnet50_model = None\n",
    "\n",
    "# Model comparison\n",
    "if base_multi_channel_large_model is not None and multi_channel_resnet50_model is not None:\n",
    "    print(f\"\\n📈 Model Comparison:\")\n",
    "    print(f\"   base_multi_channel_large: {large_dense_params:,} parameters\")\n",
    "    print(f\"   multi_channel_resnet50: {resnet50_params:,} parameters\")\n",
    "    print(f\"   ResNet-50 is {resnet50_params/large_dense_params:.1f}x larger than Large Dense\")\n",
    "    \n",
    "# Define learning rates for each model\n",
    "base_model_lr = 0.001  # Standard learning rate for BaseMultiChannelNetwork\n",
    "resnet_model_lr = 0.0003  # Lower learning rate for MultiChannelResNetNetwork for better stability\n",
    "    \n",
    "# Compile the models with proper optimizers\n",
    "if base_multi_channel_large_model is not None:\n",
    "    print(\"\\n🔧 Compiling base_multi_channel_large model...\")\n",
    "    base_multi_channel_large_model.compile(\n",
    "        optimizer='adamw',\n",
    "        learning_rate=base_model_lr,\n",
    "        weight_decay=0.0001,\n",
    "        loss='cross_entropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "if multi_channel_resnet50_model is not None:\n",
    "    print(\"\\n🔧 Compiling multi_channel_resnet50 model...\")\n",
    "    print(f\"   Using reduced learning rate ({resnet_model_lr}) for better stability\")\n",
    "    multi_channel_resnet50_model.compile(\n",
    "        optimizer='adamw',\n",
    "        learning_rate=resnet_model_lr,  # Use lower learning rate for ResNet model\n",
    "        weight_decay=0.0001,\n",
    "        loss='cross_entropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "print(\"\\n✅ Multi-stream models created and compiled successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89296cbe",
   "metadata": {},
   "source": [
    "## 13. Train Multi-Stream Models\n",
    "\n",
    "In this section, we'll train our multi-stream neural network models using our unified training approach:\n",
    "\n",
    "**DataLoader-Based Training**\n",
    "- Using the unified `fit()` method to pass DataLoaders directly\n",
    "- This enables on-the-fly augmentation during training\n",
    "- The model receives freshly augmented data in each epoch\n",
    "- More memory efficient as it doesn't require pre-extracting all augmented data\n",
    "- Keeps data on CPU and only moves batches to GPU during training\n",
    "\n",
    "Both models (BaseMultiChannelNetwork and MultiChannelResNetNetwork) support this unified training method, allowing for a consistent approach across all architectures.\n",
    "\n",
    "### Benefits of DataLoader-Based Training\n",
    "- Truly on-the-fly augmentation, creating different augmentations each epoch\n",
    "- More memory efficient (doesn't store all augmented samples)\n",
    "- Consistent with how the baseline ResNet model is trained\n",
    "- Better generalization through exposure to more augmented variations\n",
    "- Scales better to large datasets by keeping data on CPU until needed\n",
    "\n",
    "The training code includes error handling and appropriate batch size selection based on the device capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9d3cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train base_multi_channel_large model using unified fit() API with DataLoaders\n",
    "print(\"🏋️‍♀️ Training BaseMultiChannelNetwork model using DataLoaders for on-the-fly augmentation...\")\n",
    "\n",
    "# Training configuration for all models\n",
    "num_epochs = 100  # Full training run with early stopping\n",
    "early_stopping_patience = 10\n",
    "\n",
    "print(f\"\\n🔧 Training Configuration for BaseMultiChannelNetwork:\")\n",
    "print(f\"   Epochs: {num_epochs} (with early stopping, patience={early_stopping_patience})\")\n",
    "print(f\"   Batch size: Auto-selected based on device\")  # Let the model choose optimal batch size\n",
    "print(f\"   Optimizer: AdamW with weight decay=0.0001\")\n",
    "print(f\"   Learning rate: {base_model_lr} with scheduler\")\n",
    "print(f\"   Device: {base_multi_channel_large_model.device}\")\n",
    "print(f\"   Training Method: Unified fit() with DataLoader for on-the-fly augmentation\")\n",
    "\n",
    "# Clear any existing tqdm bars\n",
    "try:\n",
    "    tqdm._instances.clear()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Ensure gradient clipping is working correctly\n",
    "# Note: We've fixed parameter name issues in the implementation\n",
    "print(\"🔧 Ensuring gradient clipping is implemented correctly...\")\n",
    "\n",
    "# Train using unified fit() API which can take DataLoaders directly\n",
    "print(\"\\n🚀 Starting training with fit() API using DataLoader input...\")\n",
    "try:\n",
    "    base_multi_channel_large_history = base_multi_channel_large_model.fit(\n",
    "        train_loader=train_loader,  # Pass DataLoader directly\n",
    "        val_loader=val_loader,      # Pass DataLoader directly\n",
    "        epochs=num_epochs,\n",
    "        early_stopping_patience=early_stopping_patience,\n",
    "        scheduler_type='cosine',\n",
    "        min_lr=1e-6,\n",
    "        verbose=1,\n",
    "        learning_rate=base_model_lr,  # Use defined learning rate\n",
    "        weight_decay=0.0001\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Encountered an error: {e}\")\n",
    "    print(\"🔍 Error details:\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    \n",
    "    print(\"\\n🔄 Adjusting training parameters and trying again...\")\n",
    "    \n",
    "    # Try with reduced batch size and explicit GPU parameters\n",
    "    base_multi_channel_large_history = base_multi_channel_large_model.fit(\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        epochs=num_epochs,\n",
    "        early_stopping_patience=early_stopping_patience,\n",
    "        scheduler_type='cosine',\n",
    "        batch_size=64,  # Specify a smaller batch size\n",
    "        min_lr=1e-6,\n",
    "        verbose=1,\n",
    "        learning_rate=base_model_lr,  # Use defined learning rate\n",
    "        weight_decay=0.0001,\n",
    "        num_workers=2,     # Explicit worker count\n",
    "        pin_memory=True    # Pin memory for faster GPU transfer\n",
    "    )\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(base_multi_channel_large_history['accuracy'], label='Train')\n",
    "plt.plot(base_multi_channel_large_history['val_accuracy'], label='Validation')\n",
    "plt.title('BaseMultiChannelNetwork Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(base_multi_channel_large_history['loss'], label='Train')\n",
    "plt.plot(base_multi_channel_large_history['val_loss'], label='Validation')\n",
    "plt.title('BaseMultiChannelNetwork Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✅ BaseMultiChannelNetwork training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8be8d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train multi_channel_resnet50 model using unified fit() API with DataLoaders\n",
    "print(\"🏋️‍♀️ Training MultiChannelResNetNetwork model using DataLoaders for on-the-fly augmentation...\")\n",
    "\n",
    "# Use a reduced learning rate for MultiChannelResNetNetwork for better stability\n",
    "# Note: Deep CNN architectures like ResNet often benefit from lower learning rates \n",
    "# compared to simpler models. The reduced learning rate (0.0003) helps prevent \n",
    "# oscillation during training and improves convergence, especially with the \n",
    "# complex interaction between the two input streams.\n",
    "multi_channel_resnet_lr = 0.0003  # Lower learning rate for better convergence\n",
    "\n",
    "print(f\"\\n🔧 Training Configuration for MultiChannelResNetNetwork:\")\n",
    "print(f\"   Epochs: {num_epochs} (with early stopping, patience={early_stopping_patience})\")\n",
    "print(f\"   Batch size: Auto-selected based on device\")  # Let the model choose optimal batch size\n",
    "print(f\"   Optimizer: AdamW with weight_decay=0.0001\")\n",
    "print(f\"   Learning rate: {multi_channel_resnet_lr} (reduced for better stability)\")\n",
    "print(f\"   Device: {multi_channel_resnet50_model.device}\")\n",
    "print(f\"   Training Method: Unified fit() with DataLoader for on-the-fly augmentation\")\n",
    "\n",
    "# Make sure CUDA_LAUNCH_BLOCKING is still set\n",
    "if 'CUDA_LAUNCH_BLOCKING' not in os.environ:\n",
    "    os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "# Clear any existing tqdm bars\n",
    "try:\n",
    "    tqdm._instances.clear()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Fix for gradient clipping parameter name issue\n",
    "# The code has been updated to use max_norm instead of max_value\n",
    "try:\n",
    "    # Check if the updated code is available\n",
    "    import inspect\n",
    "    clip_grad_code = inspect.getsource(torch.nn.utils.clip_grad_norm_)\n",
    "    print(\"✅ Using correct gradient clipping implementation\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Could not verify gradient clipping implementation: {e}\")\n",
    "    \n",
    "# Train using unified fit() API which can take DataLoaders directly\n",
    "print(\"\\n🚀 Starting training with fit() API using DataLoader input...\")\n",
    "try:\n",
    "    multi_channel_resnet50_history = multi_channel_resnet50_model.fit(\n",
    "        train_loader=train_loader,  # Pass DataLoader directly\n",
    "        val_loader=val_loader,      # Pass DataLoader directly\n",
    "        epochs=num_epochs,\n",
    "        early_stopping_patience=early_stopping_patience,\n",
    "        scheduler_type='cosine',\n",
    "        min_lr=1e-6,\n",
    "        verbose=1,\n",
    "        learning_rate=multi_channel_resnet_lr  # Use reduced learning rate\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Encountered an error: {e}\")\n",
    "    print(\"🔍 Error details:\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    \n",
    "    print(\"\\n🔄 Adjusting training parameters and trying again...\")\n",
    "    \n",
    "    # Try with reduced batch size and specific device handling\n",
    "    multi_channel_resnet50_history = multi_channel_resnet50_model.fit(\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        epochs=num_epochs,\n",
    "        early_stopping_patience=early_stopping_patience,\n",
    "        scheduler_type='cosine',\n",
    "        batch_size=256,  # Specify batch size explicitly \n",
    "        min_lr=1e-6,\n",
    "        verbose=1,\n",
    "        learning_rate=multi_channel_resnet_lr,  # Use reduced learning rate\n",
    "        weight_decay=0.0001,\n",
    "        num_workers=2,     # Explicit worker count\n",
    "        pin_memory=True    # Pin memory for faster GPU transfer\n",
    "    )\n",
    "    \n",
    "# Plot training history\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(multi_channel_resnet50_history['accuracy'], label='Train')\n",
    "plt.plot(multi_channel_resnet50_history['val_accuracy'], label='Validation')\n",
    "plt.title('MultiChannelResNetNetwork Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(multi_channel_resnet50_history['loss'], label='Train')\n",
    "plt.plot(multi_channel_resnet50_history['val_loss'], label='Validation')\n",
    "plt.title('MultiChannelResNetNetwork Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"✅ CNN model training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7344fe",
   "metadata": {},
   "source": [
    "### Note on Gradient Clipping Implementation\n",
    "\n",
    "The `MultiChannelResNetNetwork` uses gradient clipping for stable training, which helps prevent exploding gradients. We fixed two specific issues related to gradient clipping:\n",
    "\n",
    "1. **Parameter Name Fix**:\n",
    "```python\n",
    "# Incorrect:\n",
    "torch.nn.utils.clip_grad_norm_(self.parameters(), max_value=float('inf'))\n",
    "\n",
    "# Correct:\n",
    "torch.nn.utils.clip_grad_norm_(self.parameters(), float('inf'))  # Using positional arguments\n",
    "# OR\n",
    "torch.nn.utils.clip_grad_norm_(self.parameters(), max_norm=float('inf'))  # Using keyword arguments\n",
    "```\n",
    "\n",
    "2. **Parameter Order Fix**:\n",
    "In PyTorch, the signature for `clip_grad_norm_` is:\n",
    "```python\n",
    "def clip_grad_norm_(parameters, max_norm, norm_type=2.0, error_if_nonfinite=False)\n",
    "```\n",
    "Where `max_norm` is a required positional parameter. Our code now properly handles this.\n",
    "\n",
    "3. **Compatibility Fix**:\n",
    "We've implemented a compatibility wrapper that ensures both parameter name formats work properly, which is especially important when loading models trained with different versions of the code.\n",
    "\n",
    "This ensures that:\n",
    "1. Gradient measurements work correctly during debugging\n",
    "2. Gradient clipping applies properly during training\n",
    "3. The model can converge without numerical instability\n",
    "\n",
    "For MultiChannelResNetNetwork, we've also reduced the learning rate to 0.0003 which, combined with gradient clipping, provides optimal training stability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b017e5fc",
   "metadata": {},
   "source": [
    "## 14. Evaluate Models\n",
    "\n",
    "Evaluate the trained models on the test set and compare their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a8186a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Models\n",
    "print(\"📊 Evaluating models on the test set...\")\n",
    "\n",
    "# Evaluate baseline model (which doesn't have our API)\n",
    "def evaluate_baseline_model(model, test_loader, criterion, model_name=\"Model\"):\n",
    "    \"\"\"\n",
    "    Evaluate the baseline ResNet model on the test set.\n",
    "    \n",
    "    Args:\n",
    "        model: The model to evaluate\n",
    "        test_loader: DataLoader for test data\n",
    "        criterion: Loss function\n",
    "        model_name: Name for logging\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with evaluation metrics\n",
    "    \"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "    model.eval()\n",
    "    \n",
    "    test_loss = 0.0\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        test_bar = tqdm(test_loader, desc=f\"{model_name} Testing\")\n",
    "        \n",
    "        for batch_idx, data in enumerate(test_bar):\n",
    "            # Handle different data formats - support both tuple and list types\n",
    "            if isinstance(data, (list, tuple)) and len(data) == 3:\n",
    "                rgb, _, targets = data  # Baseline only uses RGB\n",
    "            else:\n",
    "                # Handle unexpected data format\n",
    "                if not isinstance(data, (list, tuple)):\n",
    "                    raise ValueError(f\"Expected data to be list or tuple, got {type(data)}\")\n",
    "                elif len(data) != 3:\n",
    "                    raise ValueError(f\"Expected data to have 3 elements, got {len(data)}\")\n",
    "                else:\n",
    "                    rgb, targets = data\n",
    "                \n",
    "            # Move to device\n",
    "            rgb = rgb.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            # Baseline model only takes RGB\n",
    "            outputs = model(rgb)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            # Track statistics\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            test_total += targets.size(0)\n",
    "            test_correct += predicted.eq(targets).sum().item()\n",
    "            \n",
    "            # Store predictions and targets for detailed metrics\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "            \n",
    "            # Update progress bar\n",
    "            test_bar.set_postfix({\n",
    "                'loss': test_loss/(batch_idx+1), \n",
    "                'acc': 100.*test_correct/test_total\n",
    "            })\n",
    "    \n",
    "    test_acc = 100. * test_correct / test_total\n",
    "    test_loss = test_loss / len(test_loader)\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\n📈 {model_name} Test Results:\")\n",
    "    print(f\"   Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"   Test Accuracy: {test_acc:.2f}%\")\n",
    "    \n",
    "    # Generate classification report - using sklearn.metrics imported in the main import cell\n",
    "    report = classification_report(\n",
    "        all_targets, \n",
    "        all_predictions, \n",
    "        target_names=[CIFAR100_FINE_LABELS[i] for i in range(100)],\n",
    "        output_dict=True\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'test_loss': test_loss,\n",
    "        'test_acc': test_acc,\n",
    "        'classification_report': report,\n",
    "        'predictions': all_predictions,\n",
    "        'targets': all_targets\n",
    "    }\n",
    "\n",
    "# Evaluate baseline model (doesn't have our API)\n",
    "baseline_results = evaluate_baseline_model(\n",
    "    model=baseline_model,\n",
    "    test_loader=test_loader,\n",
    "    criterion=baseline_criterion,\n",
    "    model_name=\"Baseline ResNet50\"\n",
    ")\n",
    "\n",
    "# Evaluate multi-stream models using the unified API - directly with DataLoader\n",
    "print(\"\\n📊 Evaluating multi-stream models using unified API with DataLoader...\")\n",
    "\n",
    "# For BaseMultiChannelNetwork\n",
    "print(\"   Evaluating BaseMultiChannelNetwork (dense model)...\")\n",
    "base_multi_channel_large_results = base_multi_channel_large_model.evaluate(\n",
    "    test_loader=test_loader,  # Use DataLoader directly\n",
    "    batch_size=None  # Let the model decide optimal batch size\n",
    ")\n",
    "\n",
    "# Convert results for consistency with baseline format\n",
    "base_multi_channel_large_results['test_acc'] = base_multi_channel_large_results['accuracy'] * 100\n",
    "\n",
    "# For MultiChannelResNetNetwork\n",
    "print(\"   Evaluating MultiChannelResNetNetwork (CNN model)...\")\n",
    "multi_channel_resnet50_results = multi_channel_resnet50_model.evaluate(\n",
    "    test_loader=test_loader,  # Use DataLoader directly\n",
    "    batch_size=None  # Let the model decide optimal batch size\n",
    ")\n",
    "\n",
    "# Convert results for consistency with baseline format\n",
    "multi_channel_resnet50_results['test_acc'] = multi_channel_resnet50_results['accuracy'] * 100\n",
    "\n",
    "# Compare models\n",
    "print(\"\\n🔍 Model Comparison on Test Set:\")\n",
    "print(f\"   Baseline ResNet50: {baseline_results['test_acc']:.2f}%\")\n",
    "print(f\"   BaseMultiChannelNetwork: {base_multi_channel_large_results['test_acc']:.2f}%\")\n",
    "print(f\"   MultiChannelResNetNetwork: {multi_channel_resnet50_results['test_acc']:.2f}%\")\n",
    "\n",
    "# Calculate improvement over baseline\n",
    "base_improvement = base_multi_channel_large_results['test_acc'] - baseline_results['test_acc']\n",
    "resnet_improvement = multi_channel_resnet50_results['test_acc'] - baseline_results['test_acc']\n",
    "\n",
    "print(f\"\\n📊 Improvement over Baseline:\")\n",
    "print(f\"   BaseMultiChannelNetwork: {base_improvement:.2f}% points\")\n",
    "print(f\"   MultiChannelResNetNetwork: {resnet_improvement:.2f}% points\")\n",
    "\n",
    "# Visualize training curves comparison for all models\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Convert baseline history keys to match multi-stream model history keys\n",
    "baseline_history_converted = {\n",
    "    'train_accuracy': [acc/100 for acc in baseline_history['train_acc']],\n",
    "    'val_accuracy': [acc/100 for acc in baseline_history['val_acc']],\n",
    "    'train_loss': baseline_history['train_loss'],\n",
    "    'val_loss': baseline_history['val_loss']\n",
    "}\n",
    "\n",
    "# Accuracy curves\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(baseline_history_converted['train_accuracy'], label='Baseline Train')\n",
    "plt.plot(baseline_history_converted['val_accuracy'], label='Baseline Val')\n",
    "plt.plot(base_multi_channel_large_history['train_accuracy'], label='Dense Multi-Stream Train')\n",
    "plt.plot(base_multi_channel_large_history['val_accuracy'], label='Dense Multi-Stream Val')\n",
    "plt.plot(multi_channel_resnet50_history['train_accuracy'], label='ResNet Multi-Stream Train')\n",
    "plt.plot(multi_channel_resnet50_history['val_accuracy'], label='ResNet Multi-Stream Val')\n",
    "plt.title('Model Accuracy Comparison')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Loss curves\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(baseline_history_converted['train_loss'], label='Baseline Train')\n",
    "plt.plot(baseline_history_converted['val_loss'], label='Baseline Val')\n",
    "plt.plot(base_multi_channel_large_history['train_loss'], label='Dense Multi-Stream Train')\n",
    "plt.plot(base_multi_channel_large_history['val_loss'], label='Dense Multi-Stream Val')\n",
    "plt.plot(multi_channel_resnet50_history['train_loss'], label='ResNet Multi-Stream Train')\n",
    "plt.plot(multi_channel_resnet50_history['val_loss'], label='ResNet Multi-Stream Val')\n",
    "plt.title('Model Loss Comparison')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Additional pathway analysis for multi-stream models\n",
    "print(\"\\n🔬 Multi-Stream Pathway Analysis:\")\n",
    "\n",
    "# Analyze pathway importance for multi-stream models\n",
    "base_pathway_importance = base_multi_channel_large_model.analyze_pathway_weights()\n",
    "resnet_pathway_importance = multi_channel_resnet50_model.analyze_pathway_weights()\n",
    "\n",
    "print(\"BaseMultiChannelNetwork Pathway Importance:\")\n",
    "for key, value in base_pathway_importance.items():\n",
    "    print(f\"   {key}: {value:.4f}\")\n",
    "    \n",
    "print(\"\\nMultiChannelResNetNetwork Pathway Importance:\")\n",
    "for key, value in resnet_pathway_importance.items():\n",
    "    print(f\"   {key}: {value:.4f}\")\n",
    "\n",
    "print(\"\\n✅ Model evaluation complete!\")\n",
    "\n",
    "# Summarize overfitting analysis\n",
    "print(\"\\n🔍 Overfitting Analysis:\")\n",
    "for model_name, history in [\n",
    "    (\"Baseline ResNet50\", baseline_history_converted),\n",
    "    (\"BaseMultiChannelNetwork\", base_multi_channel_large_history),\n",
    "    (\"MultiChannelResNetNetwork\", multi_channel_resnet50_history)\n",
    "]:\n",
    "    # Convert to numpy arrays for calculations, handling different key formats\n",
    "    if 'train_acc' in history:\n",
    "        train_acc = np.array(history['train_acc'])\n",
    "        val_acc = np.array(history['val_acc'])\n",
    "    else:\n",
    "        train_acc = np.array(history['train_accuracy']) * 100  # Convert to percentage\n",
    "        val_acc = np.array(history['val_accuracy']) * 100  # Convert to percentage\n",
    "    \n",
    "    gap = train_acc - val_acc\n",
    "    last_gap = gap[-1]\n",
    "    max_gap = np.max(gap)\n",
    "    avg_last_5 = np.mean(gap[-5:]) if len(gap) >= 5 else np.mean(gap)\n",
    "    \n",
    "    print(f\"\\n   {model_name}:\")\n",
    "    print(f\"      Final train-val gap: {last_gap:.2f}%\")\n",
    "    print(f\"      Maximum gap during training: {max_gap:.2f}%\")\n",
    "    print(f\"      Average gap in last 5 epochs: {avg_last_5:.2f}%\")\n",
    "    \n",
    "    # Evaluate overfitting level\n",
    "    if avg_last_5 < 3:\n",
    "        print(f\"      ✅ No significant overfitting (gap < 3%)\")\n",
    "    elif avg_last_5 < 7:\n",
    "        print(f\"      ⚠️ Mild overfitting (3% ≤ gap < 7%)\")\n",
    "    elif avg_last_5 < 15:\n",
    "        print(f\"      🔴 Moderate overfitting (7% ≤ gap < 15%)\")\n",
    "    else:\n",
    "        print(f\"      ❌ Severe overfitting (gap ≥ 15%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483e3a42",
   "metadata": {},
   "source": [
    "## 15. Pathway Analysis\n",
    "\n",
    "Analyze the contribution of each pathway (RGB and brightness) to the model's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702b32d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pathway Analysis\n",
    "print(\"🔍 Analyzing pathway contributions for multi-stream models...\")\n",
    "\n",
    "def analyze_pathways(model, test_loader, num_samples=100, model_name=\"Model\"):\n",
    "    \"\"\"\n",
    "    Analyze contributions of RGB and brightness pathways.\n",
    "    \n",
    "    Args:\n",
    "        model: The multi-stream model to analyze\n",
    "        test_loader: DataLoader for test data\n",
    "        num_samples: Number of samples to analyze\n",
    "        model_name: Name for logging\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with pathway analysis results\n",
    "    \"\"\"\n",
    "    device = next(model.parameters()).device if hasattr(model, 'parameters') else model.device\n",
    "    model.eval()\n",
    "    \n",
    "    all_rgb = []\n",
    "    all_brightness = []\n",
    "    all_targets = []\n",
    "    all_combined_outputs = []\n",
    "    all_rgb_outputs = []\n",
    "    all_brightness_outputs = []\n",
    "    \n",
    "    sample_count = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            # Break when we have enough samples\n",
    "            if sample_count >= num_samples:\n",
    "                break\n",
    "            \n",
    "            # Handle different data formats - support both tuple and list types\n",
    "            if isinstance(data, (list, tuple)) and len(data) == 3:\n",
    "                rgb, brightness, targets = data\n",
    "            else:\n",
    "                # Handle unexpected data format\n",
    "                if not isinstance(data, (list, tuple)):\n",
    "                    raise ValueError(f\"Expected data to be list or tuple, got {type(data)}\")\n",
    "                elif len(data) != 3:\n",
    "                    raise ValueError(f\"Expected data to have 3 elements, got {len(data)}\")\n",
    "                else:\n",
    "                    raise ValueError(\"Unexpected data format from test_loader\")\n",
    "            \n",
    "            # Collect only the samples we need\n",
    "            remaining = num_samples - sample_count\n",
    "            if remaining < len(rgb):\n",
    "                rgb = rgb[:remaining]\n",
    "                brightness = brightness[:remaining]\n",
    "                targets = targets[:remaining]\n",
    "            \n",
    "            # Move to device\n",
    "            rgb, brightness, targets = rgb.to(device), brightness.to(device), targets.to(device)\n",
    "            \n",
    "            # Get outputs from combined model and analyze pathways\n",
    "            combined_outputs = model(rgb, brightness)\n",
    "            rgb_outputs, brightness_outputs = model.analyze_pathways(rgb, brightness)\n",
    "            \n",
    "            all_rgb.append(rgb.cpu())\n",
    "            all_brightness.append(brightness.cpu())\n",
    "            all_targets.append(targets.cpu())\n",
    "            all_combined_outputs.append(combined_outputs.cpu())\n",
    "            all_rgb_outputs.append(rgb_outputs.cpu())\n",
    "            all_brightness_outputs.append(brightness_outputs.cpu())\n",
    "            \n",
    "            sample_count += len(rgb)\n",
    "    \n",
    "    # Concatenate all data\n",
    "    all_rgb = torch.cat(all_rgb)\n",
    "    all_brightness = torch.cat(all_brightness)\n",
    "    all_targets = torch.cat(all_targets)\n",
    "    all_combined_outputs = torch.cat(all_combined_outputs)\n",
    "    all_rgb_outputs = torch.cat(all_rgb_outputs)\n",
    "    all_brightness_outputs = torch.cat(all_brightness_outputs)\n",
    "    \n",
    "    # Calculate accuracy for each pathway\n",
    "    _, combined_preds = all_combined_outputs.max(1)\n",
    "    _, rgb_preds = all_rgb_outputs.max(1)\n",
    "    _, brightness_preds = all_brightness_outputs.max(1)\n",
    "    \n",
    "    combined_acc = 100. * (combined_preds == all_targets).sum().item() / len(all_targets)\n",
    "    rgb_acc = 100. * (rgb_preds == all_targets).sum().item() / len(all_targets)\n",
    "    brightness_acc = 100. * (brightness_preds == all_targets).sum().item() / len(all_targets)\n",
    "    \n",
    "    print(f\"\\n📊 {model_name} Pathway Analysis:\")\n",
    "    print(f\"   Combined accuracy: {combined_acc:.2f}%\")\n",
    "    print(f\"   RGB pathway accuracy: {rgb_acc:.2f}%\")\n",
    "    print(f\"   Brightness pathway accuracy: {brightness_acc:.2f}%\")\n",
    "    \n",
    "    # Calculate pathway agreement\n",
    "    rgb_brightness_agreement = 100. * (rgb_preds == brightness_preds).sum().item() / len(all_targets)\n",
    "    combined_rgb_agreement = 100. * (combined_preds == rgb_preds).sum().item() / len(all_targets)\n",
    "    combined_brightness_agreement = 100. * (combined_preds == brightness_preds).sum().item() / len(all_targets)\n",
    "    \n",
    "    print(f\"\\n🤝 Pathway Agreement:\")\n",
    "    print(f\"   RGB-Brightness agreement: {rgb_brightness_agreement:.2f}%\")\n",
    "    print(f\"   Combined-RGB agreement: {combined_rgb_agreement:.2f}%\")\n",
    "    print(f\"   Combined-Brightness agreement: {combined_brightness_agreement:.2f}%\")\n",
    "    \n",
    "    return {\n",
    "        'combined_acc': combined_acc,\n",
    "        'rgb_acc': rgb_acc,\n",
    "        'brightness_acc': brightness_acc,\n",
    "        'rgb_brightness_agreement': rgb_brightness_agreement,\n",
    "        'combined_rgb_agreement': combined_rgb_agreement,\n",
    "        'combined_brightness_agreement': combined_brightness_agreement\n",
    "    }\n",
    "\n",
    "# Analyze BaseMultiChannelNetwork\n",
    "base_multi_channel_large_pathway_analysis = analyze_pathways(\n",
    "    model=base_multi_channel_large_model,\n",
    "    test_loader=test_loader,\n",
    "    num_samples=500,  # Increased sample size for A100 GPU\n",
    "    model_name=\"BaseMultiChannelNetwork\"\n",
    ")\n",
    "\n",
    "# Analyze MultiChannelResNetNetwork\n",
    "multi_channel_resnet50_pathway_analysis = analyze_pathways(\n",
    "    model=multi_channel_resnet50_model,\n",
    "    test_loader=test_loader,\n",
    "    num_samples=500,  # Increased sample size for A100 GPU\n",
    "    model_name=\"MultiChannelResNetNetwork\"\n",
    ")\n",
    "\n",
    "# Visualize Model Predictions and Pathway Analysis\n",
    "print(\"🔍 Visualizing model predictions and pathway analysis...\")\n",
    "\n",
    "# Get a batch of test data\n",
    "test_batch = next(iter(test_loader))\n",
    "test_rgb, test_brightness, test_labels = test_batch\n",
    "test_rgb = test_rgb.to(device)\n",
    "test_brightness = test_brightness.to(device)\n",
    "test_labels = test_labels.to(device)\n",
    "\n",
    "# Get predictions from all models\n",
    "with torch.no_grad():\n",
    "    # Baseline model only takes RGB\n",
    "    baseline_outputs = baseline_model(test_rgb)\n",
    "    baseline_probs = torch.softmax(baseline_outputs, dim=1)\n",
    "    baseline_preds = torch.argmax(baseline_probs, dim=1)\n",
    "    \n",
    "    # Multi-stream models take both inputs directly\n",
    "    base_multi_channel_outputs = base_multi_channel_large_model(test_rgb, test_brightness)\n",
    "    base_color_outputs, base_brightness_outputs = base_multi_channel_large_model.analyze_pathways(\n",
    "        test_rgb, test_brightness\n",
    "    )\n",
    "    \n",
    "    multi_channel_resnet_outputs = multi_channel_resnet50_model(test_rgb, test_brightness)\n",
    "    resnet_color_outputs, resnet_brightness_outputs = multi_channel_resnet50_model.analyze_pathways(\n",
    "        test_rgb, test_brightness\n",
    "    )\n",
    "    \n",
    "    # Calculate probabilities and predictions\n",
    "    base_multi_channel_probs = torch.softmax(base_multi_channel_outputs, dim=1)\n",
    "    base_multi_channel_preds = torch.argmax(base_multi_channel_probs, dim=1)\n",
    "    \n",
    "    multi_channel_resnet_probs = torch.softmax(multi_channel_resnet_outputs, dim=1)\n",
    "    multi_channel_resnet_preds = torch.argmax(multi_channel_resnet_probs, dim=1)\n",
    "    \n",
    "    base_color_probs = torch.softmax(base_color_outputs, dim=1)\n",
    "    base_brightness_probs = torch.softmax(base_brightness_outputs, dim=1)\n",
    "    base_color_preds = torch.argmax(base_color_probs, dim=1)\n",
    "    base_brightness_preds = torch.argmax(base_brightness_probs, dim=1)\n",
    "    \n",
    "    resnet_color_probs = torch.softmax(resnet_color_outputs, dim=1)\n",
    "    resnet_brightness_probs = torch.softmax(resnet_brightness_outputs, dim=1)\n",
    "    resnet_color_preds = torch.argmax(resnet_color_probs, dim=1)\n",
    "    resnet_brightness_preds = torch.argmax(resnet_brightness_probs, dim=1)\n",
    "\n",
    "# Move to CPU for visualization\n",
    "test_rgb = test_rgb.cpu()\n",
    "test_brightness = test_brightness.cpu()\n",
    "test_labels = test_labels.cpu()\n",
    "baseline_preds = baseline_preds.cpu()\n",
    "base_multi_channel_preds = base_multi_channel_preds.cpu()\n",
    "multi_channel_resnet_preds = multi_channel_resnet_preds.cpu()\n",
    "base_color_preds = base_color_preds.cpu()\n",
    "base_brightness_preds = base_brightness_preds.cpu()\n",
    "resnet_color_preds = resnet_color_preds.cpu()\n",
    "resnet_brightness_preds = resnet_brightness_preds.cpu()\n",
    "\n",
    "# Select a subset of samples for visualization\n",
    "n_samples = min(8, test_rgb.size(0))\n",
    "sample_indices = np.random.choice(test_rgb.size(0), n_samples, replace=False)\n",
    "\n",
    "# Plot original images, model predictions, and pathway contributions\n",
    "fig, axes = plt.subplots(n_samples, 6, figsize=(18, 3*n_samples))\n",
    "if n_samples == 1:\n",
    "    axes = axes.reshape(1, -1)\n",
    "\n",
    "for i, idx in enumerate(sample_indices):\n",
    "    # Get RGB image\n",
    "    rgb_img = test_rgb[idx].permute(1, 2, 0).numpy()\n",
    "    rgb_img = np.clip(rgb_img, 0, 1)\n",
    "    \n",
    "    # Get brightness image\n",
    "    brightness_img = test_brightness[idx, 0].numpy()\n",
    "    \n",
    "    # True label and predictions\n",
    "    true_label = test_labels[idx].item()\n",
    "    baseline_pred = baseline_preds[idx].item()\n",
    "    multi_channel_pred = multi_channel_resnet_preds[idx].item()\n",
    "    color_pathway_pred = resnet_color_preds[idx].item()\n",
    "    brightness_pathway_pred = resnet_brightness_preds[idx].item()\n",
    "    \n",
    "    # Plot original RGB image\n",
    "    axes[i, 0].imshow(rgb_img)\n",
    "    axes[i, 0].set_title(f\"True: {CIFAR100_FINE_LABELS[true_label]}\")\n",
    "    axes[i, 0].axis('off')\n",
    "    \n",
    "    # Plot brightness image\n",
    "    axes[i, 1].imshow(brightness_img, cmap='gray')\n",
    "    axes[i, 1].set_title(\"Brightness Channel\")\n",
    "    axes[i, 1].axis('off')\n",
    "    \n",
    "    # Plot baseline model prediction\n",
    "    axes[i, 2].imshow(rgb_img)\n",
    "    correct = true_label == baseline_pred\n",
    "    title_color = 'green' if correct else 'red'\n",
    "    axes[i, 2].set_title(f\"Baseline: {CIFAR100_FINE_LABELS[baseline_pred]}\", color=title_color)\n",
    "    axes[i, 2].axis('off')\n",
    "    \n",
    "    # Plot multi-stream model prediction\n",
    "    axes[i, 3].imshow(rgb_img)\n",
    "    correct = true_label == multi_channel_pred\n",
    "    title_color = 'green' if correct else 'red'\n",
    "    axes[i, 3].set_title(f\"Multi-Stream: {CIFAR100_FINE_LABELS[multi_channel_pred]}\", color=title_color)\n",
    "    axes[i, 3].axis('off')\n",
    "    \n",
    "    # Plot color pathway prediction\n",
    "    axes[i, 4].imshow(rgb_img)\n",
    "    correct = true_label == color_pathway_pred\n",
    "    title_color = 'green' if correct else 'red'\n",
    "    axes[i, 4].set_title(f\"Color Path: {CIFAR100_FINE_LABELS[color_pathway_pred]}\", color=title_color)\n",
    "    axes[i, 4].axis('off')\n",
    "    \n",
    "    # Plot brightness pathway prediction\n",
    "    axes[i, 5].imshow(brightness_img, cmap='gray')\n",
    "    correct = true_label == brightness_pathway_pred\n",
    "    title_color = 'green' if correct else 'red'\n",
    "    axes[i, 5].set_title(f\"Brightness Path: {CIFAR100_FINE_LABELS[brightness_pathway_pred]}\", color=title_color)\n",
    "    axes[i, 5].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualize pathway feature importance\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Get pathway importance\n",
    "base_importance = base_multi_channel_large_model.get_pathway_importance()\n",
    "resnet_importance = multi_channel_resnet50_model.get_pathway_importance()\n",
    "\n",
    "# Plot importance values\n",
    "labels = ['Color Pathway', 'Brightness Pathway']\n",
    "base_values = [base_importance['color_pathway'], base_importance['brightness_pathway']]\n",
    "resnet_values = [resnet_importance['color_pathway'], resnet_importance['brightness_pathway']]\n",
    "\n",
    "x = np.arange(len(labels))\n",
    "width = 0.35\n",
    "\n",
    "plt.bar(x - width/2, base_values, width, label='BaseMultiChannelNetwork')\n",
    "plt.bar(x + width/2, resnet_values, width, label='MultiChannelResNetNetwork')\n",
    "\n",
    "plt.ylabel('Relative Importance')\n",
    "plt.title('Pathway Importance Analysis')\n",
    "plt.xticks(x, labels)\n",
    "plt.ylim(0, 1)\n",
    "plt.legend()\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✅ Visualization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55adcd79",
   "metadata": {},
   "source": [
    "## 16. Save Models\n",
    "\n",
    "Save the trained models for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89734e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Models\n",
    "print(\"💾 Saving trained models...\")\n",
    "\n",
    "# Create models directory if it doesn't exist\n",
    "models_dir = './models'\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "# Save baseline model (using PyTorch standard method)\n",
    "baseline_path = os.path.join(models_dir, 'baseline_resnet50.pth')\n",
    "torch.save(baseline_model.state_dict(), baseline_path)\n",
    "print(f\"✅ Baseline ResNet50 saved to {baseline_path}\")\n",
    "\n",
    "# Save multi-stream models using their built-in save_model method\n",
    "base_multi_channel_path = os.path.join(models_dir, 'base_multi_channel_large.pth')\n",
    "base_multi_channel_large_model.save_model(base_multi_channel_path)\n",
    "print(f\"✅ BaseMultiChannelNetwork saved to {base_multi_channel_path}\")\n",
    "\n",
    "multi_channel_resnet_path = os.path.join(models_dir, 'multi_channel_resnet50.pth')\n",
    "multi_channel_resnet50_model.save_model(multi_channel_resnet_path)\n",
    "print(f\"✅ MultiChannelResNetNetwork saved to {multi_channel_resnet_path}\")\n",
    "\n",
    "# Save training histories for future reference\n",
    "history_dir = os.path.join(models_dir, 'history')\n",
    "os.makedirs(history_dir, exist_ok=True)\n",
    "\n",
    "# Convert torch tensors to lists for JSON serialization\n",
    "def prepare_for_json(history):\n",
    "    json_history = {}\n",
    "    for key, value in history.items():\n",
    "        if isinstance(value, list):\n",
    "            # Check if list contains tensors\n",
    "            if value and isinstance(value[0], torch.Tensor):\n",
    "                json_history[key] = [float(v.item()) for v in value]\n",
    "            else:\n",
    "                # Convert all values to float for consistency\n",
    "                json_history[key] = [float(v) if not isinstance(v, (list, tuple)) else v for v in value]\n",
    "        else:\n",
    "            json_history[key] = value\n",
    "    return json_history\n",
    "\n",
    "# Baseline history conversion\n",
    "json_baseline_history = {}\n",
    "for key, value in baseline_history.items():\n",
    "    json_baseline_history[key] = [float(v) for v in value]\n",
    "    \n",
    "# Save histories\n",
    "with open(os.path.join(history_dir, 'baseline_history.json'), 'w') as f:\n",
    "    json.dump(json_baseline_history, f)\n",
    "\n",
    "with open(os.path.join(history_dir, 'base_multi_channel_history.json'), 'w') as f:\n",
    "    json.dump(prepare_for_json(base_multi_channel_large_history), f)\n",
    "    \n",
    "with open(os.path.join(history_dir, 'multi_channel_resnet_history.json'), 'w') as f:\n",
    "    json.dump(prepare_for_json(multi_channel_resnet50_history), f)\n",
    "\n",
    "print(f\"✅ Training histories saved to {history_dir}\")\n",
    "\n",
    "# Save model metadata for easier reloading\n",
    "metadata = {\n",
    "    'baseline': {\n",
    "        'model_type': 'CifarResNet50',\n",
    "        'num_classes': 100,\n",
    "        'parameters': baseline_params,\n",
    "        'path': baseline_path\n",
    "    },\n",
    "    'base_multi_channel_large': {\n",
    "        'model_type': 'BaseMultiChannelNetwork',\n",
    "        'color_input_size': rgb_input_size,\n",
    "        'brightness_input_size': brightness_input_size,\n",
    "        'num_classes': 100,\n",
    "        'parameters': large_dense_params,\n",
    "        'path': base_multi_channel_path\n",
    "    },\n",
    "    'multi_channel_resnet50': {\n",
    "        'model_type': 'MultiChannelResNetNetwork',\n",
    "        'color_input_channels': input_channels_rgb,\n",
    "        'brightness_input_channels': input_channels_brightness,\n",
    "        'num_classes': 100,\n",
    "        'parameters': resnet50_params,\n",
    "        'path': multi_channel_resnet_path\n",
    "    },\n",
    "    'dataset': 'CIFAR-100',\n",
    "    'training_date': time.strftime('%Y-%m-%d')\n",
    "}\n",
    "\n",
    "with open(os.path.join(models_dir, 'model_metadata.json'), 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(f\"✅ Model metadata saved to {os.path.join(models_dir, 'model_metadata.json')}\")\n",
    "print(\"\\n💾 All models and training data saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fdbb77",
   "metadata": {},
   "source": [
    "## 17. Summary\n",
    "\n",
    "Summarize the results and findings from our multi-stream neural network experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334c5338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "print(\"📋 Multi-Stream Neural Networks CIFAR-100 Training Summary\")\n",
    "\n",
    "# Training summary\n",
    "print(\"\\n🏋️‍♀️ Training Results:\")\n",
    "# Handle different key formats\n",
    "if 'val_acc' in baseline_history:\n",
    "    baseline_final_val = baseline_history['val_acc'][-1]\n",
    "else:\n",
    "    baseline_final_val = baseline_history['val_accuracy'][-1] * 100\n",
    "    \n",
    "if 'val_acc' in base_multi_channel_large_history:\n",
    "    base_final_val = base_multi_channel_large_history['val_acc'][-1]\n",
    "else:\n",
    "    base_final_val = base_multi_channel_large_history['val_accuracy'][-1] * 100\n",
    "    \n",
    "if 'val_acc' in multi_channel_resnet50_history:\n",
    "    resnet_final_val = multi_channel_resnet50_history['val_acc'][-1]\n",
    "else:\n",
    "    resnet_final_val = multi_channel_resnet50_history['val_accuracy'][-1] * 100\n",
    "\n",
    "print(f\"   Baseline ResNet50 final validation accuracy: {baseline_final_val:.2f}%\")\n",
    "print(f\"   BaseMultiChannelNetwork final validation accuracy: {base_final_val:.2f}%\")\n",
    "print(f\"   MultiChannelResNetNetwork final validation accuracy: {resnet_final_val:.2f}%\")\n",
    "\n",
    "# Testing summary\n",
    "print(\"\\n🧪 Testing Results:\")\n",
    "print(f\"   Baseline ResNet50 test accuracy: {baseline_results['test_acc']:.2f}%\")\n",
    "print(f\"   BaseMultiChannelNetwork test accuracy: {base_multi_channel_large_results['test_acc']:.2f}%\")\n",
    "print(f\"   MultiChannelResNetNetwork test accuracy: {multi_channel_resnet50_results['test_acc']:.2f}%\")\n",
    "\n",
    "# Pathway analysis summary\n",
    "print(\"\\n🔍 Pathway Analysis Summary:\")\n",
    "print(\"   BaseMultiChannelNetwork:\")\n",
    "print(f\"      Combined accuracy: {base_multi_channel_large_pathway_analysis['combined_acc']:.2f}%\")\n",
    "print(f\"      RGB pathway: {base_multi_channel_large_pathway_analysis['rgb_acc']:.2f}%, Brightness pathway: {base_multi_channel_large_pathway_analysis['brightness_acc']:.2f}%\")\n",
    "\n",
    "print(\"   MultiChannelResNetNetwork:\")\n",
    "print(f\"      Combined accuracy: {multi_channel_resnet50_pathway_analysis['combined_acc']:.2f}%\")\n",
    "print(f\"      RGB pathway: {multi_channel_resnet50_pathway_analysis['rgb_acc']:.2f}%, Brightness pathway: {multi_channel_resnet50_pathway_analysis['brightness_acc']:.2f}%\")\n",
    "\n",
    "# Learning rate summary\n",
    "print(\"\\n📊 Learning Rate Strategy:\")\n",
    "print(f\"   BaseMultiChannelNetwork: {base_model_lr} (standard learning rate)\")\n",
    "print(f\"   MultiChannelResNetNetwork: {resnet_model_lr} (reduced learning rate)\")\n",
    "print(\"   - The MultiChannelResNetNetwork benefits from a lower learning rate due to its deeper architecture\")\n",
    "print(\"   - This helps prevent oscillation during training and improves convergence\")\n",
    "print(\"   - Recommended range for MultiChannelResNetNetwork: 0.0001 to 0.0005\")\n",
    "\n",
    "# Create a summary table\n",
    "summary_data = {\n",
    "    'Model': ['Baseline ResNet50', 'BaseMultiChannelNetwork', 'MultiChannelResNetNetwork'],\n",
    "    'Test Acc (%)': [\n",
    "        f\"{baseline_results['test_acc']:.2f}\",\n",
    "        f\"{base_multi_channel_large_results['test_acc']:.2f}\",\n",
    "        f\"{multi_channel_resnet50_results['test_acc']:.2f}\"\n",
    "    ],\n",
    "    'RGB Pathway (%)': [\n",
    "        'N/A',\n",
    "        f\"{base_multi_channel_large_pathway_analysis['rgb_acc']:.2f}\",\n",
    "        f\"{multi_channel_resnet50_pathway_analysis['rgb_acc']:.2f}\"\n",
    "    ],\n",
    "    'Brightness Pathway (%)': [\n",
    "        'N/A',\n",
    "        f\"{base_multi_channel_large_pathway_analysis['brightness_acc']:.2f}\",\n",
    "        f\"{multi_channel_resnet50_pathway_analysis['brightness_acc']:.2f}\"\n",
    "    ],\n",
    "    'Parameters': [\n",
    "        f\"{sum(p.numel() for p in baseline_model.parameters()):,}\",\n",
    "        f\"{large_dense_params:,}\",\n",
    "        f\"{resnet50_params:,}\"\n",
    "    ],\n",
    "    'Learning Rate': [\n",
    "        \"0.001\",\n",
    "        f\"{base_model_lr}\",\n",
    "        f\"{resnet_model_lr}\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Use pandas to create a nice table\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "display(summary_df)\n",
    "\n",
    "print(\"\\n📝 Key Findings:\")\n",
    "print(\"   1. Multi-stream models can leverage both RGB and brightness information\")\n",
    "print(\"   2. The RGB pathway typically contributes more to accuracy than brightness\")\n",
    "print(\"   3. The combined model performs better than individual pathways\")\n",
    "print(\"   4. MultiChannelResNetNetwork architecture is more powerful but requires more parameters\")\n",
    "print(\"   5. The unified fit() method works efficiently with both direct data and DataLoaders\")\n",
    "print(\"   6. MultiChannelResNetNetwork requires a lower learning rate (0.0003) for optimal performance\")\n",
    "\n",
    "print(\"\\n🔑 API Best Practices:\")\n",
    "print(\"   1. Use the unified fit() method with DataLoaders for memory-efficient training:\")\n",
    "print(\"      model.fit(train_loader=train_loader, val_loader=val_loader, ...)\")\n",
    "print(\"   2. For direct data input, the same method works seamlessly:\")\n",
    "print(\"      model.fit(train_color_data=X_rgb, train_brightness_data=X_brightness, ...)\")\n",
    "print(\"   3. Use DataLoaders for large datasets to keep data on CPU until needed\")\n",
    "print(\"   4. Leverage on-the-fly augmentation through DataLoaders for better generalization\")\n",
    "print(\"   5. Set an appropriate learning rate for each model architecture:\")\n",
    "print(\"      - BaseMultiChannelNetwork: 0.001 (standard)\")\n",
    "print(\"      - MultiChannelResNetNetwork: 0.0003 (reduced)\")\n",
    "\n",
    "print(\"\\n🎯 Next Steps:\")\n",
    "print(\"   1. Try different fusion strategies\")\n",
    "print(\"   2. Experiment with balancing pathway contributions\")\n",
    "print(\"   3. Apply to more complex datasets\")\n",
    "print(\"   4. Optimize model architectures based on pathway analysis\")\n",
    "\n",
    "print(\"\\n✨ Thank you for exploring Multi-Stream Neural Networks! ✨\")\n",
    "\n",
    "# Visualize Model Predictions and Pathway Analysis\n",
    "print(\"🔍 Visualizing model predictions and pathway analysis...\")\n",
    "\n",
    "# Get a batch of test data\n",
    "test_batch = next(iter(test_loader))\n",
    "# Handle different data formats - support both tuple and list types\n",
    "if isinstance(test_batch, (list, tuple)) and len(test_batch) == 3:\n",
    "    test_rgb, test_brightness, test_labels = test_batch\n",
    "else:\n",
    "    # Handle unexpected data format\n",
    "    if not isinstance(test_batch, (list, tuple)):\n",
    "        raise ValueError(f\"Expected test_batch to be list or tuple, got {type(test_batch)}\")\n",
    "    elif len(test_batch) != 3:\n",
    "        raise ValueError(f\"Expected test_batch to have 3 elements, got {len(test_batch)}\")\n",
    "    else:\n",
    "        raise ValueError(\"Unexpected data format from test_loader\")\n",
    "\n",
    "test_rgb = test_rgb.to(device)\n",
    "test_brightness = test_brightness.to(device)\n",
    "test_labels = test_labels.to(device)\n",
    "\n",
    "# Get predictions from all models\n",
    "with torch.no_grad():\n",
    "    # Baseline model only takes RGB\n",
    "    baseline_outputs = baseline_model(test_rgb)\n",
    "    baseline_probs = torch.softmax(baseline_outputs, dim=1)\n",
    "    baseline_preds = torch.argmax(baseline_probs, dim=1)\n",
    "    \n",
    "    # Multi-stream models take both inputs - unified API handles reshaping internally if needed\n",
    "    base_multi_channel_outputs = base_multi_channel_large_model(test_rgb, test_brightness)\n",
    "    base_color_outputs, base_brightness_outputs = base_multi_channel_large_model.analyze_pathways(test_rgb, test_brightness)\n",
    "    \n",
    "    multi_channel_resnet_outputs = multi_channel_resnet50_model(test_rgb, test_brightness)\n",
    "    resnet_color_outputs, resnet_brightness_outputs = multi_channel_resnet50_model.analyze_pathways(test_rgb, test_brightness)\n",
    "    \n",
    "    # Calculate probabilities and predictions\n",
    "    base_multi_channel_probs = torch.softmax(base_multi_channel_outputs, dim=1)\n",
    "    base_multi_channel_preds = torch.argmax(base_multi_channel_probs, dim=1)\n",
    "    \n",
    "    multi_channel_resnet_probs = torch.softmax(multi_channel_resnet_outputs, dim=1)\n",
    "    multi_channel_resnet_preds = torch.argmax(multi_channel_resnet_probs, dim=1)\n",
    "    \n",
    "    base_color_probs = torch.softmax(base_color_outputs, dim=1)\n",
    "    base_brightness_probs = torch.softmax(base_brightness_outputs, dim=1)\n",
    "    base_color_preds = torch.argmax(base_color_probs, dim=1)\n",
    "    base_brightness_preds = torch.argmax(base_brightness_probs, dim=1)\n",
    "    \n",
    "    resnet_color_probs = torch.softmax(resnet_color_outputs, dim=1)\n",
    "    resnet_brightness_probs = torch.softmax(resnet_brightness_outputs, dim=1)\n",
    "    resnet_color_preds = torch.argmax(resnet_color_probs, dim=1)\n",
    "    resnet_brightness_preds = torch.argmax(resnet_brightness_probs, dim=1)\n",
    "\n",
    "# Move to CPU for visualization\n",
    "test_rgb = test_rgb.cpu()\n",
    "test_brightness = test_brightness.cpu()\n",
    "test_labels = test_labels.cpu()\n",
    "baseline_preds = baseline_preds.cpu()\n",
    "base_multi_channel_preds = base_multi_channel_preds.cpu()\n",
    "multi_channel_resnet_preds = multi_channel_resnet_preds.cpu()\n",
    "base_color_preds = base_color_preds.cpu()\n",
    "base_brightness_preds = base_brightness_preds.cpu()\n",
    "resnet_color_preds = resnet_color_preds.cpu()\n",
    "resnet_brightness_preds = resnet_brightness_preds.cpu()\n",
    "\n",
    "# Select a subset of samples for visualization\n",
    "n_samples = min(8, test_rgb.size(0))\n",
    "sample_indices = np.random.choice(test_rgb.size(0), n_samples, replace=False)\n",
    "\n",
    "# Plot original images, model predictions, and pathway contributions\n",
    "fig, axes = plt.subplots(n_samples, 6, figsize=(18, 3*n_samples))\n",
    "if n_samples == 1:\n",
    "    axes = axes.reshape(1, -1)\n",
    "\n",
    "for i, idx in enumerate(sample_indices):\n",
    "    # Get RGB image\n",
    "    rgb_img = test_rgb[idx].permute(1, 2, 0).numpy()\n",
    "    rgb_img = np.clip(rgb_img, 0, 1)\n",
    "    \n",
    "    # Get brightness image\n",
    "    brightness_img = test_brightness[idx, 0].numpy()\n",
    "    \n",
    "    # True label and predictions\n",
    "    true_label = test_labels[idx].item()\n",
    "    baseline_pred = baseline_preds[idx].item()\n",
    "    multi_channel_pred = multi_channel_resnet_preds[idx].item()\n",
    "    color_pathway_pred = resnet_color_preds[idx].item()\n",
    "    brightness_pathway_pred = resnet_brightness_preds[idx].item()\n",
    "    \n",
    "    # Plot original RGB image\n",
    "    axes[i, 0].imshow(rgb_img)\n",
    "    axes[i, 0].set_title(f\"True: {CIFAR100_FINE_LABELS[true_label]}\")\n",
    "    axes[i, 0].axis('off')\n",
    "    \n",
    "    # Plot brightness image\n",
    "    axes[i, 1].imshow(brightness_img, cmap='gray')\n",
    "    axes[i, 1].set_title(\"Brightness Channel\")\n",
    "    axes[i, 1].axis('off')\n",
    "    \n",
    "    # Plot baseline model prediction\n",
    "    axes[i, 2].imshow(rgb_img)\n",
    "    correct = true_label == baseline_pred\n",
    "    title_color = 'green' if correct else 'red'\n",
    "    axes[i, 2].set_title(f\"Baseline: {CIFAR100_FINE_LABELS[baseline_pred]}\", color=title_color)\n",
    "    axes[i, 2].axis('off')\n",
    "    \n",
    "    # Plot multi-stream model prediction\n",
    "    axes[i, 3].imshow(rgb_img)\n",
    "    correct = true_label == multi_channel_pred\n",
    "    title_color = 'green' if correct else 'red'\n",
    "    axes[i, 3].set_title(f\"Multi-Stream: {CIFAR100_FINE_LABELS[multi_channel_pred]}\", color=title_color)\n",
    "    axes[i, 3].axis('off')\n",
    "    \n",
    "    # Plot color pathway prediction\n",
    "    axes[i, 4].imshow(rgb_img)\n",
    "    correct = true_label == color_pathway_pred\n",
    "    title_color = 'green' if correct else 'red'\n",
    "    axes[i, 4].set_title(f\"Color Path: {CIFAR100_FINE_LABELS[color_pathway_pred]}\", color=title_color)\n",
    "    axes[i, 4].axis('off')\n",
    "    \n",
    "    # Plot brightness pathway prediction\n",
    "    axes[i, 5].imshow(brightness_img, cmap='gray')\n",
    "    correct = true_label == brightness_pathway_pred\n",
    "    title_color = 'green' if correct else 'red'\n",
    "    axes[i, 5].set_title(f\"Brightness Path: {CIFAR100_FINE_LABELS[brightness_pathway_pred]}\", color=title_color)\n",
    "    axes[i, 5].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualize pathway feature importance\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Get pathway importance\n",
    "base_importance = base_multi_channel_large_model.get_pathway_importance()\n",
    "resnet_importance = multi_channel_resnet50_model.get_pathway_importance()\n",
    "\n",
    "# Plot importance values\n",
    "labels = ['Color Pathway', 'Brightness Pathway']\n",
    "base_values = [base_importance['color_pathway'], base_importance['brightness_pathway']]\n",
    "resnet_values = [resnet_importance['color_pathway'], resnet_importance['brightness_pathway']]\n",
    "\n",
    "x = np.arange(len(labels))\n",
    "width = 0.35\n",
    "\n",
    "plt.bar(x - width/2, base_values, width, label='BaseMultiChannelNetwork')\n",
    "plt.bar(x + width/2, resnet_values, width, label='MultiChannelResNetNetwork')\n",
    "\n",
    "plt.ylabel('Relative Importance')\n",
    "plt.title('Pathway Importance Analysis')\n",
    "plt.xticks(x, labels)\n",
    "plt.ylim(0, 1)\n",
    "plt.legend()\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✅ Visualization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655f3b1e",
   "metadata": {},
   "source": [
    "## Summary and Conclusions\n",
    "\n",
    "In this notebook, we've built, trained, and evaluated multi-stream neural networks for CIFAR-100 classification:\n",
    "\n",
    "1. **Data Preparation**\n",
    "   - Processed CIFAR-100 data into RGB and brightness streams\n",
    "   - Applied unified data augmentation with the same transformations for all models\n",
    "   - Created robust data processing pipelines with batch processing and memory optimization\n",
    "\n",
    "2. **Baseline Model**\n",
    "   - Trained a non-pretrained ResNet50 baseline modified for CIFAR-100\n",
    "   - Used proper regularization (weight decay, early stopping)\n",
    "   - Applied learning rate scheduling for optimal convergence\n",
    "\n",
    "3. **Multi-Stream Models**\n",
    "   - Implemented two multi-stream architectures:\n",
    "     - BaseMultiChannelNetwork (dense/fully-connected)\n",
    "     - MultiChannelResNetNetwork (CNN with residual connections)\n",
    "   - Both models process RGB and brightness data in parallel pathways\n",
    "   - Used shared classifier for final prediction\n",
    "   - Leveraged the models' built-in APIs for efficient training and evaluation:\n",
    "     - `compile()`: Set up optimizer, loss function, and metrics\n",
    "     - `fit()`: Unified training method that works with both DataLoaders (recommended) and direct data arrays\n",
    "     - `evaluate()`: Evaluated models on test data\n",
    "     - `analyze_pathways()`: Analyzed pathway contributions and importance\n",
    "\n",
    "4. **Fair Comparison Framework**\n",
    "   - All models trained with:\n",
    "     - Same optimization settings (Adam/AdamW optimizer)\n",
    "     - Same regularization (weight decay, optional dropout)\n",
    "     - Same learning rate scheduling\n",
    "     - Same early stopping patience\n",
    "     - Same data augmentation pipeline\n",
    "\n",
    "5. **Results and Analysis**\n",
    "   - Compared performance on test set\n",
    "   - Identified classes where multi-stream processing provides advantages\n",
    "   - Analyzed model convergence and learning patterns\n",
    "   - Used pathway analysis to understand feature importance in each stream\n",
    "\n",
    "The results demonstrate that separating color and brightness information into distinct processing pathways can lead to improved classification performance compared to a standard RGB-only approach.\n",
    "\n",
    "### Key Advantage of the Enhanced Training Pipeline\n",
    "\n",
    "By implementing the unified `fit()` method in our multi-stream models, we achieved:\n",
    "- **Flexibility** - Works with both DataLoaders and direct data arrays\n",
    "- **True on-the-fly augmentation** - Each batch receives freshly augmented data\n",
    "- **Consistent training approach** across all models (baseline and multi-stream)\n",
    "- **Better generalization** through exposure to more diverse augmented samples\n",
    "- **Memory efficiency** for large datasets by keeping data on CPU until needed\n",
    "- **Simplified workflow** - direct DataLoader usage without manual data extraction\n",
    "\n",
    "### Proper API Usage\n",
    "\n",
    "For Multi-Stream Neural Networks, the recommended API pattern is:\n",
    "1. Create model with `model = create_model(...)` or direct constructor\n",
    "2. Compile model with `model.compile(...)`\n",
    "3. Train model with `model.fit(train_loader, val_loader, ...)` for on-the-fly augmentation with DataLoaders\n",
    "4. Or train with `model.fit(train_color_data, train_brightness_data, ...)` using NumPy arrays or tensors if preferred\n",
    "5. Evaluate with `model.evaluate(...)`\n",
    "6. Analyze pathways with `model.analyze_pathways(...)`\n",
    "\n",
    "This provides a clean, flexible interface that is consistent across all model architectures.\n",
    "\n",
    "### Next Steps\n",
    "- Perform additional ablation studies on different pathway configurations\n",
    "- Explore other fusion mechanisms beyond shared classifiers\n",
    "- Apply these models to more complex datasets and real-world applications\n",
    "- Analyze computational efficiency and potential optimizations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca28f6c",
   "metadata": {},
   "source": [
    "## Technical Notes and Bug Fixes\n",
    "\n",
    "During the implementation of this notebook, we addressed several technical issues to ensure optimal training:\n",
    "\n",
    "### 1. Gradient Clipping Fix\n",
    "\n",
    "We fixed a parameter name issue in the `clip_grad_norm_` function call:\n",
    "- Changed `max_value=float('inf')` to `max_norm=float('inf')` \n",
    "- This ensures correct gradient measurement and clipping\n",
    "\n",
    "### 2. Learning Rate Optimization\n",
    "\n",
    "We optimized learning rates for each architecture:\n",
    "- BaseMultiChannelNetwork: Standard learning rate (0.001)\n",
    "- MultiChannelResNetNetwork: Reduced learning rate (0.0003) for better stability\n",
    "\n",
    "### 3. Memory Management\n",
    "\n",
    "The notebook implements several memory optimization techniques:\n",
    "- On-the-fly data augmentation instead of precomputing all augmented samples\n",
    "- Careful tensor cleanup after each batch\n",
    "- Periodic cache clearing for GPU memory\n",
    "- Automatic batch size selection based on available GPU memory\n",
    "\n",
    "These improvements ensure that both models train efficiently, converge properly, and make the best use of available computational resources."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
