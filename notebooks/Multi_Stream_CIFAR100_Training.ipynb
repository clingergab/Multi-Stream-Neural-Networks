{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f0aae91",
   "metadata": {},
   "source": [
    "# Multi-Stream Neural Networks: CIFAR-100 Training\n",
    "\n",
    "This notebook demonstrates the full pipeline for training multi-stream neural networks on CIFAR-100 data:\n",
    "\n",
    "üöÄ **Features:**\n",
    "- Automatic GPU detection and optimization\n",
    "- RGB to RGBL preprocessing with visualizations\n",
    "- BaseMultiChannelNetwork (Dense) and MultiChannelResNetNetwork (CNN) models\n",
    "- Dynamic progress bars during training\n",
    "- Comprehensive evaluation and analysis\n",
    "\n",
    "**Hardware Requirements:**\n",
    "- Google Colab with GPU runtime (A100/V100 recommended)\n",
    "- Sufficient memory for CIFAR-100 dataset processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7051aee4",
   "metadata": {},
   "source": [
    "## 1. Clone Repository and Set Up Working Directory\n",
    "\n",
    "First, we'll clone the Multi-Stream Neural Networks repository to our Google Drive and set up the working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432574db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Navigate to Drive and clone repository\n",
    "import os\n",
    "os.chdir('/content/drive/MyDrive')\n",
    "\n",
    "# Clone the repository (replace with your actual repository URL)\n",
    "!git clone https://github.com/yourusername/Multi-Stream-Neural-Networks.git\n",
    "\n",
    "# Change to the project directory\n",
    "os.chdir('/content/drive/MyDrive/Multi-Stream-Neural-Networks')\n",
    "\n",
    "# Verify we're in the right directory\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "print(\"\\nDirectory contents:\")\n",
    "!ls -la"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac36a39",
   "metadata": {},
   "source": [
    "## 2. Install and Import Required Libraries\n",
    "\n",
    "Install any missing dependencies and import all necessary libraries for the multi-stream neural network training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c193f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install tqdm matplotlib seaborn scikit-learn\n",
    "\n",
    "# System and utility imports\n",
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add project to Python path\n",
    "sys.path.append('/content/drive/MyDrive/Multi-Stream-Neural-Networks')\n",
    "\n",
    "# Core libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Data and visualization\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "from typing import Tuple, Dict, List\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"üì¶ All libraries imported successfully!\")\n",
    "print(f\"üî• PyTorch version: {torch.__version__}\")\n",
    "print(f\"üöÄ CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4abf9ff",
   "metadata": {},
   "source": [
    "## 3. Load CIFAR-100 Dataset\n",
    "\n",
    "Load the CIFAR-100 dataset from the data folder. We assume the data folder structure matches the repository structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0884a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our data loading utilities\n",
    "from src.utils.colab_utils import load_cifar10  # We'll adapt this for CIFAR-100\n",
    "\n",
    "# Check if data folder exists\n",
    "data_path = \"data/cifar-100\"\n",
    "if os.path.exists(data_path):\n",
    "    print(f\"‚úÖ Data folder found at: {data_path}\")\n",
    "else:\n",
    "    print(f\"‚ùå Data folder not found. Creating data structure...\")\n",
    "    os.makedirs(data_path, exist_ok=True)\n",
    "    print(\"üìÅ Please manually upload CIFAR-100 data to the data folder\")\n",
    "\n",
    "# Define CIFAR-100 classes for reference\n",
    "cifar100_fine_labels = [\n",
    "    'apple', 'aquarium_fish', 'baby', 'bear', 'beaver', 'bed', 'bee', 'beetle',\n",
    "    'bicycle', 'bottle', 'bowl', 'boy', 'bridge', 'bus', 'butterfly', 'camel',\n",
    "    'can', 'castle', 'caterpillar', 'cattle', 'chair', 'chimpanzee', 'clock',\n",
    "    'cloud', 'cockroach', 'couch', 'crab', 'crocodile', 'cup', 'dinosaur',\n",
    "    'dolphin', 'elephant', 'flatfish', 'forest', 'fox', 'girl', 'hamster',\n",
    "    'house', 'kangaroo', 'keyboard', 'lamp', 'lawn_mower', 'leopard', 'lion',\n",
    "    'lizard', 'lobster', 'man', 'maple_tree', 'motorcycle', 'mountain', 'mouse',\n",
    "    'mushroom', 'oak_tree', 'orange', 'orchid', 'otter', 'palm_tree', 'pear',\n",
    "    'pickup_truck', 'pine_tree', 'plain', 'plate', 'poppy', 'porcupine',\n",
    "    'possum', 'rabbit', 'raccoon', 'ray', 'road', 'rocket', 'rose',\n",
    "    'sea', 'seal', 'shark', 'shrew', 'skunk', 'skyscraper', 'snail', 'snake',\n",
    "    'spider', 'squirrel', 'streetcar', 'sunflower', 'sweet_pepper', 'table',\n",
    "    'tank', 'telephone', 'television', 'tiger', 'tractor', 'train', 'trout',\n",
    "    'tulip', 'turtle', 'wardrobe', 'whale', 'willow_tree', 'wolf', 'woman',\n",
    "    'worm'\n",
    "]\n",
    "\n",
    "# Load CIFAR-100 data using torchvision (fallback if data folder is empty)\n",
    "def load_cifar100_data():\n",
    "    # Transform to convert PIL images to tensors\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    \n",
    "    try:\n",
    "        # Try loading from local data folder first\n",
    "        train_dataset = torchvision.datasets.CIFAR100(\n",
    "            root='./data', train=True, download=False, transform=transform\n",
    "        )\n",
    "        test_dataset = torchvision.datasets.CIFAR100(\n",
    "            root='./data', train=False, download=False, transform=transform\n",
    "        )\n",
    "        print(\"‚úÖ Loaded CIFAR-100 from local data folder\")\n",
    "    except:\n",
    "        # Download if not available locally\n",
    "        print(\"‚¨áÔ∏è Downloading CIFAR-100 dataset...\")\n",
    "        train_dataset = torchvision.datasets.CIFAR100(\n",
    "            root='./data', train=True, download=True, transform=transform\n",
    "        )\n",
    "        test_dataset = torchvision.datasets.CIFAR100(\n",
    "            root='./data', train=False, download=True, transform=transform\n",
    "        )\n",
    "        print(\"‚úÖ CIFAR-100 dataset downloaded and loaded\")\n",
    "    \n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "# Load the datasets\n",
    "train_dataset, test_dataset = load_cifar100_data()\n",
    "\n",
    "print(f\"üìä Dataset Info:\")\n",
    "print(f\"   Training samples: {len(train_dataset)}\")\n",
    "print(f\"   Test samples: {len(test_dataset)}\")\n",
    "print(f\"   Number of classes: 100\")\n",
    "print(f\"   Image size: 32x32x3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6ae3dd",
   "metadata": {},
   "source": [
    "## 4. Preprocess Data: RGB to RGBL Transformation\n",
    "\n",
    "Apply preprocessing to convert RGB images to both RGB and brightness (luminance) channels. This creates our multi-stream data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945c9ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our RGB to RGBL transformation\n",
    "from src.transforms.rgb_to_rgbl import RGBtoRGBL\n",
    "\n",
    "def convert_dataset_to_multi_stream(dataset, max_samples=None):\n",
    "    \"\"\"\n",
    "    Convert a CIFAR-100 dataset to multi-stream format (RGB + Brightness).\n",
    "    \n",
    "    Args:\n",
    "        dataset: CIFAR-100 dataset\n",
    "        max_samples: Maximum number of samples to process (for faster testing)\n",
    "    \n",
    "    Returns:\n",
    "        rgb_data: RGB channel data [N, 3, 32, 32]\n",
    "        brightness_data: Brightness channel data [N, 1, 32, 32]\n",
    "        labels: Class labels [N]\n",
    "    \"\"\"\n",
    "    print(f\"üîÑ Converting dataset to multi-stream format...\")\n",
    "    \n",
    "    # Initialize RGB to RGBL transform\n",
    "    rgb_to_rgbl = RGBtoRGBL()\n",
    "    \n",
    "    # Determine number of samples to process\n",
    "    num_samples = len(dataset) if max_samples is None else min(max_samples, len(dataset))\n",
    "    \n",
    "    # Initialize arrays\n",
    "    rgb_data = []\n",
    "    brightness_data = []\n",
    "    labels = []\n",
    "    \n",
    "    # Process samples with progress bar\n",
    "    for i in tqdm(range(num_samples), desc=\"Processing images\"):\n",
    "        image, label = dataset[i]\n",
    "        \n",
    "        # Convert to RGBL\n",
    "        rgbl_image = rgb_to_rgbl(image)\n",
    "        \n",
    "        # Split RGB and brightness channels\n",
    "        rgb_channels = rgbl_image[:3]  # First 3 channels (RGB)\n",
    "        brightness_channel = rgbl_image[3:4]  # Last channel (Brightness)\n",
    "        \n",
    "        rgb_data.append(rgb_channels)\n",
    "        brightness_data.append(brightness_channel)\n",
    "        labels.append(label)\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    rgb_data = torch.stack(rgb_data).numpy()\n",
    "    brightness_data = torch.stack(brightness_data).numpy()\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    print(f\"‚úÖ Conversion complete!\")\n",
    "    print(f\"   RGB data shape: {rgb_data.shape}\")\n",
    "    print(f\"   Brightness data shape: {brightness_data.shape}\")\n",
    "    print(f\"   Labels shape: {labels.shape}\")\n",
    "    \n",
    "    return rgb_data, brightness_data, labels\n",
    "\n",
    "# Convert training data (use subset for faster processing in demo)\n",
    "print(\"üöÄ Processing training data...\")\n",
    "train_rgb, train_brightness, train_labels = convert_dataset_to_multi_stream(\n",
    "    train_dataset, max_samples=5000  # Reduce for faster demo\n",
    ")\n",
    "\n",
    "# Convert test data (use subset for faster processing in demo)\n",
    "print(\"\\nüß™ Processing test data...\")\n",
    "test_rgb, test_brightness, test_labels = convert_dataset_to_multi_stream(\n",
    "    test_dataset, max_samples=1000  # Reduce for faster demo\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Final Dataset Shapes:\")\n",
    "print(f\"   Training RGB: {train_rgb.shape}\")\n",
    "print(f\"   Training Brightness: {train_brightness.shape}\")\n",
    "print(f\"   Training Labels: {train_labels.shape}\")\n",
    "print(f\"   Test RGB: {test_rgb.shape}\")\n",
    "print(f\"   Test Brightness: {test_brightness.shape}\")\n",
    "print(f\"   Test Labels: {test_labels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73e3a1b",
   "metadata": {},
   "source": [
    "## 5. Visualize Sample Images: RGB and Brightness Side by Side\n",
    "\n",
    "Display sample images showing the original RGB and extracted brightness channels to understand the multi-stream transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68afa3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_rgb_brightness_samples(rgb_data, brightness_data, labels, num_samples=5):\n",
    "    \"\"\"\n",
    "    Visualize RGB and brightness images side by side.\n",
    "    \n",
    "    Args:\n",
    "        rgb_data: RGB image data [N, 3, H, W]\n",
    "        brightness_data: Brightness image data [N, 1, H, W]\n",
    "        labels: Image labels\n",
    "        num_samples: Number of samples to visualize\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(num_samples, 2, figsize=(8, 2.5 * num_samples))\n",
    "    fig.suptitle('RGB vs Brightness Channel Comparison', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        # Get RGB image (convert from CHW to HWC for matplotlib)\n",
    "        rgb_img = np.transpose(rgb_data[i], (1, 2, 0))\n",
    "        \n",
    "        # Get brightness image (squeeze channel dimension)\n",
    "        brightness_img = brightness_data[i, 0]  # Remove channel dimension\n",
    "        \n",
    "        # Get class name\n",
    "        class_name = cifar100_fine_labels[labels[i]]\n",
    "        \n",
    "        # Plot RGB image\n",
    "        axes[i, 0].imshow(rgb_img)\n",
    "        axes[i, 0].set_title(f'RGB - {class_name}', fontweight='bold')\n",
    "        axes[i, 0].axis('off')\n",
    "        \n",
    "        # Plot brightness image\n",
    "        axes[i, 1].imshow(brightness_img, cmap='gray')\n",
    "        axes[i, 1].set_title(f'Brightness - {class_name}', fontweight='bold')\n",
    "        axes[i, 1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize sample images\n",
    "print(\"üñºÔ∏è Sample RGB vs Brightness Images:\")\n",
    "visualize_rgb_brightness_samples(train_rgb, train_brightness, train_labels, num_samples=5)\n",
    "\n",
    "# Show data statistics\n",
    "def show_data_statistics(rgb_data, brightness_data, labels):\n",
    "    \"\"\"Show basic statistics about the data.\"\"\"\n",
    "    print(f\"\\nüìä Data Statistics:\")\n",
    "    print(f\"   RGB data range: [{rgb_data.min():.3f}, {rgb_data.max():.3f}]\")\n",
    "    print(f\"   Brightness data range: [{brightness_data.min():.3f}, {brightness_data.max():.3f}]\")\n",
    "    print(f\"   Number of unique classes: {len(np.unique(labels))}\")\n",
    "    \n",
    "    # Class distribution\n",
    "    unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "    print(f\"   Samples per class: {counts.min()} - {counts.max()}\")\n",
    "    print(f\"   Average samples per class: {counts.mean():.1f}\")\n",
    "\n",
    "show_data_statistics(train_rgb, train_brightness, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a90244",
   "metadata": {},
   "source": [
    "## 6. Additional Data Visualizations\n",
    "\n",
    "Let's explore the data with helpful visualizations including class distribution and pixel intensity analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b394c3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class distribution visualization\n",
    "def plot_class_distribution(labels, title=\"Class Distribution\"):\n",
    "    \"\"\"Plot the distribution of classes in the dataset.\"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "    \n",
    "    plt.bar(unique_labels, counts, alpha=0.7, color='skyblue', edgecolor='navy')\n",
    "    plt.title(title, fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Class ID')\n",
    "    plt.ylabel('Number of Samples')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Pixel intensity histograms\n",
    "def plot_intensity_histograms(rgb_data, brightness_data):\n",
    "    \"\"\"Plot histograms of pixel intensities for RGB and brightness channels.\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "    fig.suptitle('Pixel Intensity Distributions', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # RGB histograms\n",
    "    colors = ['red', 'green', 'blue']\n",
    "    for i, color in enumerate(colors):\n",
    "        axes[0, 0].hist(rgb_data[:, i].flatten(), bins=50, alpha=0.6, \n",
    "                       color=color, label=f'{color.upper()} channel')\n",
    "    axes[0, 0].set_title('RGB Channel Intensities')\n",
    "    axes[0, 0].set_xlabel('Pixel Value')\n",
    "    axes[0, 0].set_ylabel('Frequency')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Brightness histogram\n",
    "    axes[0, 1].hist(brightness_data.flatten(), bins=50, alpha=0.7, \n",
    "                   color='gray', edgecolor='black')\n",
    "    axes[0, 1].set_title('Brightness Channel Intensities')\n",
    "    axes[0, 1].set_xlabel('Pixel Value')\n",
    "    axes[0, 1].set_ylabel('Frequency')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Mean pixel values per channel\n",
    "    rgb_means = np.mean(rgb_data, axis=(0, 2, 3))\n",
    "    brightness_mean = np.mean(brightness_data)\n",
    "    \n",
    "    channel_names = ['Red', 'Green', 'Blue', 'Brightness']\n",
    "    channel_means = [rgb_means[0], rgb_means[1], rgb_means[2], brightness_mean]\n",
    "    \n",
    "    axes[1, 0].bar(channel_names, channel_means, \n",
    "                  color=['red', 'green', 'blue', 'gray'], alpha=0.7)\n",
    "    axes[1, 0].set_title('Mean Pixel Values by Channel')\n",
    "    axes[1, 0].set_ylabel('Mean Pixel Value')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Sample grid\n",
    "    axes[1, 1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Sample grid of images\n",
    "def plot_sample_grid(rgb_data, labels, grid_size=(4, 8)):\n",
    "    \"\"\"Plot a grid of sample images.\"\"\"\n",
    "    fig, axes = plt.subplots(grid_size[0], grid_size[1], figsize=(16, 8))\n",
    "    fig.suptitle('Sample Images from CIFAR-100 Dataset', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    for i in range(grid_size[0]):\n",
    "        for j in range(grid_size[1]):\n",
    "            idx = i * grid_size[1] + j\n",
    "            if idx < len(rgb_data):\n",
    "                img = np.transpose(rgb_data[idx], (1, 2, 0))\n",
    "                class_name = cifar100_fine_labels[labels[idx]]\n",
    "                \n",
    "                axes[i, j].imshow(img)\n",
    "                axes[i, j].set_title(class_name, fontsize=8)\n",
    "                axes[i, j].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Generate visualizations\n",
    "print(\"üìä Generating additional visualizations...\")\n",
    "\n",
    "# Class distribution\n",
    "plot_class_distribution(train_labels, \"Training Set Class Distribution\")\n",
    "\n",
    "# Intensity histograms\n",
    "plot_intensity_histograms(train_rgb[:1000], train_brightness[:1000])  # Sample for speed\n",
    "\n",
    "# Sample grid\n",
    "plot_sample_grid(train_rgb, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86919e29",
   "metadata": {},
   "source": [
    "## 7. Create Multi-Stream Neural Network Models\n",
    "\n",
    "Now we'll create both the BaseMultiChannelNetwork (dense) and MultiChannelResNetNetwork (CNN) models for comparison."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
