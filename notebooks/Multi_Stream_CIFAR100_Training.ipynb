{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f0aae91",
   "metadata": {
    "id": "4f0aae91"
   },
   "source": [
    "# Multi-Stream Neural Networks: CIFAR100 Training\n",
    "\n",
    "This notebook demonstrates training Multi-Stream Neural Networks (MSNN) on the CIFAR100 dataset.\n",
    "\n",
    "The notebook follows this structure:\n",
    "1. Environment setup and repository update\n",
    "2. Installation of dependencies and importing libraries\n",
    "3. Data loading, processing, verification, visualization and analysis \n",
    "4. Model creation, training and evaluation\n",
    "5. Pathway analysis and model saving\n",
    "\n",
    "## Multi-Stream Neural Networks\n",
    "\n",
    "This notebook demonstrates the full pipeline for training multi-stream neural networks:\n",
    "\n",
    "### Key Features\n",
    "- **Unified API Design**: Consistent interface across all models\n",
    "- **Two Fusion Strategies**: Shared classifier (recommended) vs separate classifiers\n",
    "- **Multiple Architectures**: Dense networks and CNN (ResNet) models\n",
    "- **GPU Optimization**: Automatic device detection with mixed precision\n",
    "- **Research Tools**: Pathway analysis for multi-stream insights\n",
    "\n",
    "### Model Architectures\n",
    "1. **BaseMultiChannelNetwork**: Dense/fully-connected multi-stream processing\n",
    "2. **MultiChannelResNetNetwork**: CNN with residual connections for spatial features\n",
    "\n",
    "### API Design Philosophy\n",
    "- **`model(color, brightness)`** â†’ Single tensor for training/inference\n",
    "- **`model.analyze_pathways(color, brightness)`** â†’ Tuple for research analysis\n",
    "- **Keras-like training**: `.fit()`, `.evaluate()`, `.predict()` methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7051aee4",
   "metadata": {
    "id": "7051aee4"
   },
   "source": [
    "## Environment Setup & Requirements\n",
    "\n",
    "### Prerequisites\n",
    "- **Python 3.8+**\n",
    "- **PyTorch 1.12+** with CUDA support (recommended)\n",
    "- **Google Colab** (this notebook) or local Jupyter environment\n",
    "\n",
    "### Project Structure\n",
    "Our codebase is fully modularized:\n",
    "```\n",
    "Multi-Stream-Neural-Networks/\n",
    "â”œâ”€â”€ src/\n",
    "â”‚   â”œâ”€â”€ models/basic_multi_channel/     # Core model implementations\n",
    "â”‚   â”‚   â”œâ”€â”€ base_multi_channel_network.py    # Dense model\n",
    "â”‚   â”‚   â””â”€â”€ multi_channel_resnet_network.py  # CNN model\n",
    "â”‚   â”œâ”€â”€ utils/cifar100_loader.py        # CIFAR-100 data utilities\n",
    "â”‚   â”œâ”€â”€ transforms/rgb_to_rgbl.py       # RGBâ†’Brightness transform\n",
    "â”‚   â””â”€â”€ utils/device_utils.py           # GPU optimization utilities\n",
    "â”œâ”€â”€ configs/                            # Model configuration files\n",
    "â””â”€â”€ data/                               # Dataset location\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e80196",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "Mount Google Drive and navigate to the project directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823d044d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Mount Google Drive\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[1;32m      3\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57873d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Navigate to the project directory\n",
    "import os\n",
    "\n",
    "# Navigate to Drive and project directory\n",
    "project_path = '/content/drive/MyDrive/Multi-Stream-Neural-Networks'\n",
    "if os.path.exists(project_path):\n",
    "    os.chdir(project_path)\n",
    "    print(f\"âœ… Found project at: {project_path}\")\n",
    "else:\n",
    "    print(f\"âŒ Project not found at: {project_path}\")\n",
    "    print(\"ğŸ’¡ Please clone the repository first:\")\n",
    "    print(\"   !git clone https://github.com/yourusername/Multi-Stream-Neural-Networks.git\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1955bd1d",
   "metadata": {},
   "source": [
    "## 2. Update Repository\n",
    "\n",
    "Pull the latest changes from the repository to ensure we have the most recent codebase.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e805e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update repository with latest changes\n",
    "print(\"ğŸ”„ Pulling latest changes from repository...\")\n",
    "\n",
    "# Make sure we're in the right directory\n",
    "print(f\"ğŸ“ Current directory: {os.getcwd()}\")\n",
    "\n",
    "# Pull latest changes\n",
    "!git pull origin main\n",
    "\n",
    "print(\"\\nâœ… Repository update complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1aa368",
   "metadata": {},
   "source": [
    "## 3. Install Dependencies\n",
    "\n",
    "Install required packages and dependencies for the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xo3JWyNVEGMZ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xo3JWyNVEGMZ",
    "outputId": "34f5af2f-a23c-4e75-848f-94bbf88fffb4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¦ Installing required dependencies...\n",
      "Installing packages...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… torch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… torchvision\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… numpy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… seaborn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… tqdm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… scikit-learn\n",
      "âœ… Pillow\n",
      "\n",
      "ğŸ“š Importing libraries...\n",
      "âœ… All libraries imported successfully!\n",
      "\n",
      "ğŸ”§ PyTorch Setup:\n",
      "   PyTorch version: 2.7.0\n",
      "   CUDA available: False\n",
      "   Using CPU (consider GPU for faster training)\n",
      "\n",
      "ğŸ¯ Dependencies and imports complete!\n",
      "âœ… Pillow\n",
      "\n",
      "ğŸ“š Importing libraries...\n",
      "âœ… All libraries imported successfully!\n",
      "\n",
      "ğŸ”§ PyTorch Setup:\n",
      "   PyTorch version: 2.7.0\n",
      "   CUDA available: False\n",
      "   Using CPU (consider GPU for faster training)\n",
      "\n",
      "ğŸ¯ Dependencies and imports complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install Dependencies\n",
    "print(\"ğŸ“¦ Installing required dependencies...\")\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "import importlib\n",
    "\n",
    "def install_package(package):\n",
    "    \"\"\"Install a package if not already installed.\"\"\"\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package, \"--quiet\"])\n",
    "        return True\n",
    "    except subprocess.CalledProcessError:\n",
    "        return False\n",
    "\n",
    "# Required packages\n",
    "packages = [\n",
    "    \"torch\",\n",
    "    \"torchvision\", \n",
    "    \"numpy\",\n",
    "    \"matplotlib\",\n",
    "    \"seaborn\",\n",
    "    \"tqdm\",\n",
    "    \"scikit-learn\",\n",
    "    \"Pillow\"\n",
    "]\n",
    "\n",
    "print(\"Installing packages...\")\n",
    "for package in packages:\n",
    "    if install_package(package):\n",
    "        print(f\"âœ… {package}\")\n",
    "    else:\n",
    "        print(f\"âŒ Failed to install {package}\")\n",
    "\n",
    "# Install project requirements\n",
    "print(\"\\nInstalling project requirements...\")\n",
    "try:\n",
    "    !pip install -r requirements.txt\n",
    "    print(\"âœ… Project requirements installed\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Error installing requirements: {e}\")\n",
    "    print(\"   Continuing with available packages\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ecd3cd",
   "metadata": {},
   "source": [
    "## 4. Import Libraries\n",
    "\n",
    "Import all necessary libraries and utilities for the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9be5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "print(\"ğŸ“š Importing libraries and setting up the environment...\")\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "# Core PyTorch Libraries\n",
    "#------------------------------------------------------------------------------\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "# Data Handling Libraries\n",
    "#------------------------------------------------------------------------------\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import subprocess\n",
    "import importlib\n",
    "import traceback\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "# Visualization Libraries\n",
    "#------------------------------------------------------------------------------\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "# Progress Tracking and Machine Learning Libraries\n",
    "#------------------------------------------------------------------------------\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "# Google Colab Integration (if needed)\n",
    "#------------------------------------------------------------------------------\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    IN_COLAB = True\n",
    "    print(\"âœ… Google Colab detected\")\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"â„¹ï¸ Not running in Google Colab\")\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "# Project-Specific Imports\n",
    "#------------------------------------------------------------------------------\n",
    "# Add project root to path for imports\n",
    "project_root = Path('.').resolve()\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.append(str(project_root))\n",
    "\n",
    "# Import project modules (with error handling)\n",
    "try:\n",
    "    # Data processing\n",
    "    from src.utils.cifar100_loader import get_cifar100_datasets, create_validation_split\n",
    "    from src.transforms.rgb_to_rgbl import RGBtoRGBL\n",
    "    \n",
    "    # Data augmentation\n",
    "    from src.transforms.augmentation import (\n",
    "        CIFAR100Augmentation, \n",
    "        AugmentedMultiStreamDataset,\n",
    "        MixUp, \n",
    "        create_augmented_dataloaders,\n",
    "        create_test_dataloader\n",
    "    )\n",
    "    \n",
    "    # Model builders\n",
    "    try:\n",
    "        from src.models.builders import create_model, list_available_models\n",
    "        MODEL_FACTORY_AVAILABLE = True\n",
    "    except ImportError:\n",
    "        MODEL_FACTORY_AVAILABLE = False\n",
    "        from src.models.basic_multi_channel.base_multi_channel_network import BaseMultiChannelNetwork, base_multi_channel_large\n",
    "        from src.models.basic_multi_channel.multi_channel_resnet_network import MultiChannelResNetNetwork, multi_channel_resnet50\n",
    "    \n",
    "    print(\"âœ… Project modules imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"âš ï¸ Error importing project modules: {e}\")\n",
    "    print(\"   Some functionality may be limited\")\n",
    "\n",
    "# Set environment variables for CUDA\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'  # Better error diagnostics for CUDA\n",
    "\n",
    "print(\"âœ… All libraries imported successfully!\")\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "# Environment Information\n",
    "#------------------------------------------------------------------------------\n",
    "# Check PyTorch setup\n",
    "print(f\"\\nğŸ”§ PyTorch Setup:\")\n",
    "print(f\"   PyTorch version: {torch.__version__}\")\n",
    "print(f\"   CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   CUDA version: {torch.version.cuda}\")\n",
    "else:\n",
    "    print(\"   Using CPU (consider GPU for faster training)\")\n",
    "\n",
    "# Configure CUDA for better performance\n",
    "torch.backends.cudnn.benchmark = True  # Speed up training for fixed input sizes\n",
    "torch.backends.cudnn.deterministic = False  # Non-deterministic for speed\n",
    "\n",
    "print(\"\\nğŸ¯ Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd732e2",
   "metadata": {},
   "source": [
    "## 5. Load Data\n",
    "\n",
    "Load the CIFAR-100 dataset using our optimized data loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2895f8fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Setting up CIFAR-100 dataset loading...\n",
      "âœ… CIFAR-100 loader utilities imported successfully\n",
      "ğŸ“ Loading CIFAR-100 datasets with train/validation/test split...\n",
      "âŒ Error loading CIFAR-100 data: get_cifar100_datasets() got an unexpected keyword argument 'root'\n",
      "\n",
      "ğŸ’¡ Troubleshooting:\n",
      "   1. Check internet connection for CIFAR-100 download\n",
      "   2. Verify data directory permissions\n",
      "   3. Try clearing cache: rm -rf data/cifar-100\n",
      "   4. Check if src/utils/cifar100_loader.py exists\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "get_cifar100_datasets() got an unexpected keyword argument 'root'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 16\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mğŸ“ Loading CIFAR-100 datasets with train/validation/test split...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# Load datasets using our optimized loader\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m     train_dataset, val_dataset, test_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mget_cifar100_datasets\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./data\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdownload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43mval_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 10% validation split from training data\u001b[39;49;00m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâœ… CIFAR-100 datasets loaded successfully!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   ğŸ“Š Training samples: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_dataset)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: get_cifar100_datasets() got an unexpected keyword argument 'root'"
     ]
    }
   ],
   "source": [
    "# ğŸ“Š CIFAR-100 Data Loading and Verification\n",
    "print(\"ğŸ“ Setting up CIFAR-100 dataset loading...\")\n",
    "\n",
    "# Load CIFAR-100 Dataset\n",
    "print(\"ğŸ“ Loading CIFAR-100 datasets with train/validation/test split...\")\n",
    "\n",
    "# Since we already imported the CIFAR-100 loader utilities in the main import cell,\n",
    "# we can check if they were imported successfully\n",
    "if 'get_cifar100_datasets' in globals():\n",
    "    print(\"âœ… CIFAR-100 loader utilities imported successfully\")\n",
    "else:\n",
    "    print(\"âŒ Failed to import CIFAR-100 utilities. Make sure src/utils/cifar100_loader.py exists\")\n",
    "    raise ImportError(\"Missing CIFAR-100 loader utilities\")\n",
    "\n",
    "# Load datasets using our optimized loader (returns train, test, class_names)\n",
    "train_dataset, test_dataset, class_names = get_cifar100_datasets(\n",
    "    data_dir='./data/cifar-100'\n",
    ")\n",
    "\n",
    "# Create validation split from training data\n",
    "train_dataset, val_dataset = create_validation_split(\n",
    "    train_dataset, \n",
    "    val_split=0.1\n",
    ")\n",
    "\n",
    "print(\"âœ… CIFAR-100 datasets loaded successfully!\")\n",
    "print(f\"   ğŸ“Š Training samples: {len(train_dataset):,}\")\n",
    "print(f\"   ğŸ“Š Validation samples: {len(val_dataset):,}\")\n",
    "print(f\"   ğŸ“Š Test samples: {len(test_dataset):,}\")\n",
    "print(f\"   ğŸ·ï¸ Number of classes: {len(class_names)}\")\n",
    "\n",
    "# Store class names for later use\n",
    "CIFAR100_FINE_LABELS = class_names\n",
    "\n",
    "print(\"\\nğŸ¯ Data loading complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a99955e",
   "metadata": {},
   "source": [
    "## 6. Process Data\n",
    "\n",
    "Convert RGB images to RGB + Brightness (L) channels for multi-stream processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a78fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process Data: RGB to RGB+L (Brightness) Conversion\n",
    "print(\"ğŸ”„ Converting RGB images to RGB + Brightness streams...\")\n",
    "\n",
    "# Since we already imported the RGB to RGB+L transform in the main import cell,\n",
    "# we can check if it was imported successfully\n",
    "if 'RGBtoRGBL' in globals():\n",
    "    print(\"âœ… RGB to RGB+L transform imported successfully\")\n",
    "else:\n",
    "    print(\"âŒ Failed to import RGB to RGB+L transform. Make sure src/transforms/rgb_to_rgbl.py exists\")\n",
    "    raise ImportError(\"Missing RGB to RGB+L transform\")\n",
    "\n",
    "# Initialize the transform\n",
    "rgb_to_rgbl = RGBtoRGBL()\n",
    "\n",
    "# Function to process a dataset batch-wise for memory efficiency\n",
    "def process_dataset_to_streams(dataset, batch_size=1000, desc=\"Processing\"):\n",
    "    \"\"\"\n",
    "    Convert RGB dataset to RGB + Brightness streams efficiently.\n",
    "    \n",
    "    Args:\n",
    "        dataset: Dataset with RGB images (PyTorch dataset format)\n",
    "        batch_size: Size of batches for memory-efficient processing\n",
    "        desc: Description for progress bar\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (rgb_stream, brightness_stream, labels_tensor)\n",
    "    \"\"\"\n",
    "    rgb_tensors = []\n",
    "    brightness_tensors = []\n",
    "    labels = []\n",
    "    \n",
    "    # Process in batches to manage memory\n",
    "    for i in tqdm(range(0, len(dataset), batch_size), desc=desc):\n",
    "        batch_end = min(i + batch_size, len(dataset))\n",
    "        batch_data = []\n",
    "        batch_labels = []\n",
    "        \n",
    "        # Collect batch data\n",
    "        for j in range(i, batch_end):\n",
    "            data, label = dataset[j]\n",
    "            batch_data.append(data)\n",
    "            batch_labels.append(label)\n",
    "        \n",
    "        # Convert to tensor batch\n",
    "        batch_tensor = torch.stack(batch_data)\n",
    "        \n",
    "        # Apply RGB to RGB+L transform\n",
    "        rgb_batch, brightness_batch = rgb_to_rgbl(batch_tensor)\n",
    "        \n",
    "        rgb_tensors.append(rgb_batch)\n",
    "        brightness_tensors.append(brightness_batch)\n",
    "        labels.extend(batch_labels)\n",
    "    \n",
    "    # Concatenate all batches\n",
    "    rgb_stream = torch.cat(rgb_tensors, dim=0)\n",
    "    brightness_stream = torch.cat(brightness_tensors, dim=0)\n",
    "    labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
    "    \n",
    "    return rgb_stream, brightness_stream, labels_tensor\n",
    "\n",
    "# Process all datasets\n",
    "print(\"Processing training dataset...\")\n",
    "train_rgb, train_brightness, train_labels_tensor = process_dataset_to_streams(\n",
    "    train_dataset, desc=\"Training data\"\n",
    ")\n",
    "\n",
    "print(\"Processing validation dataset...\")\n",
    "val_rgb, val_brightness, val_labels_tensor = process_dataset_to_streams(\n",
    "    val_dataset, desc=\"Validation data\"\n",
    ")\n",
    "\n",
    "print(\"Processing test dataset...\")\n",
    "test_rgb, test_brightness, test_labels_tensor = process_dataset_to_streams(\n",
    "    test_dataset, desc=\"Test data\"\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… Multi-stream conversion complete!\")\n",
    "print(f\"   ğŸ¨ RGB stream shape: {train_rgb.shape}\")\n",
    "print(f\"   ğŸ’¡ Brightness stream shape: {train_brightness.shape}\")\n",
    "print(f\"   ğŸ“Š RGB range: [{train_rgb.min():.3f}, {train_rgb.max():.3f}]\")\n",
    "print(f\"   ğŸ“Š Brightness range: [{train_brightness.min():.3f}, {train_brightness.max():.3f}]\")\n",
    "\n",
    "# Memory usage estimation\n",
    "rgb_memory = (train_rgb.nbytes + val_rgb.nbytes + test_rgb.nbytes) / 1e6\n",
    "brightness_memory = (train_brightness.nbytes + val_brightness.nbytes + test_brightness.nbytes) / 1e6\n",
    "total_memory = rgb_memory + brightness_memory\n",
    "\n",
    "print(f\"\\nğŸ“ˆ Processing Summary:\")\n",
    "print(f\"   ğŸ“Š Total samples processed: {len(train_labels_tensor) + len(val_labels_tensor) + len(test_labels_tensor):,}\")\n",
    "print(f\"   ğŸ¨ RGB streams memory: {rgb_memory:.1f} MB\")\n",
    "print(f\"   ğŸ’¡ Brightness streams memory: {brightness_memory:.1f} MB\")\n",
    "print(f\"   ğŸ’¾ Total memory usage: {total_memory:.1f} MB\")\n",
    "\n",
    "print(\"\\nğŸ¯ Data processing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e2429b",
   "metadata": {},
   "source": [
    "## 7. Data Verification\n",
    "\n",
    "Verify the processed data structure and consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb2f71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Verification\n",
    "print(\"ğŸ” Verifying processed data structure and consistency...\")\n",
    "\n",
    "def verify_data_integrity(rgb_data, brightness_data, labels, split_name):\n",
    "    # Check shapes and types\n",
    "    assert rgb_data.shape[0] == brightness_data.shape[0] == labels.shape[0], f\"Inconsistent sample counts in {split_name}!\"\n",
    "    assert rgb_data.shape[1:] == (3, 32, 32), f\"Unexpected RGB shape in {split_name}!\"\n",
    "    assert brightness_data.shape[1:] == (1, 32, 32), f\"Unexpected brightness shape in {split_name}!\"\n",
    "    assert 0 <= labels.min() and labels.max() < 100, f\"Invalid label range in {split_name}!\"\n",
    "    return rgb_data.shape[0]\n",
    "\n",
    "train_samples = verify_data_integrity(train_rgb, train_brightness, train_labels_tensor, \"Training\")\n",
    "val_samples = verify_data_integrity(val_rgb, val_brightness, val_labels_tensor, \"Validation\")\n",
    "test_samples = verify_data_integrity(test_rgb, test_brightness, test_labels_tensor, \"Test\")\n",
    "\n",
    "total_samples = train_samples + val_samples + test_samples\n",
    "all_labels = torch.cat([train_labels_tensor, val_labels_tensor, test_labels_tensor])\n",
    "unique_labels = torch.unique(all_labels)\n",
    "\n",
    "print(f\"\\nğŸ“ˆ Data Summary:\")\n",
    "print(f\"   Training: {train_samples:,}\")\n",
    "print(f\"   Validation: {val_samples:,}\")\n",
    "print(f\"   Test: {test_samples:,}\")\n",
    "print(f\"   Total: {total_samples:,}\")\n",
    "print(f\"   Unique classes: {len(unique_labels)}/100\")\n",
    "print(\"\\nâœ… Data verification checks passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba0574d",
   "metadata": {},
   "source": [
    "## 8. Data Visualization\n",
    "\n",
    "Visualize sample images from both RGB and brightness streams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b744428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Visualization\n",
    "print(\"ğŸ‘ï¸ Visualizing sample images from both RGB and brightness streams...\")\n",
    "\n",
    "# Set up visualization\n",
    "plt.style.use('default')\n",
    "fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n",
    "fig.suptitle('Multi-Stream CIFAR-100 Samples: RGB vs Brightness', fontsize=14)\n",
    "\n",
    "# Select random samples\n",
    "np.random.seed(42)  # For reproducible results\n",
    "sample_indices = np.random.choice(len(train_rgb), 4, replace=False)\n",
    "\n",
    "for i, idx in enumerate(sample_indices):\n",
    "    # Get data\n",
    "    rgb_img = train_rgb[idx]\n",
    "    brightness_img = train_brightness[idx]\n",
    "    label = train_labels_tensor[idx].item()\n",
    "    class_name = CIFAR100_FINE_LABELS[label]\n",
    "    \n",
    "    # RGB image (convert from tensor to numpy)\n",
    "    rgb_np = rgb_img.permute(1, 2, 0).numpy()\n",
    "    rgb_np = np.clip(rgb_np, 0, 1)  # Ensure valid range\n",
    "    \n",
    "    # Brightness image\n",
    "    brightness_np = brightness_img.squeeze().numpy()\n",
    "    \n",
    "    # Plot RGB\n",
    "    axes[0, i].imshow(rgb_np)\n",
    "    axes[0, i].set_title(f'RGB: {class_name}', fontsize=10)\n",
    "    axes[0, i].axis('off')\n",
    "    \n",
    "    # Plot Brightness\n",
    "    axes[1, i].imshow(brightness_np, cmap='gray')\n",
    "    axes[1, i].set_title(f'Brightness: {class_name}', fontsize=10)\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Data visualization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5042e5c6",
   "metadata": {},
   "source": [
    "## 9. Data Analysis\n",
    "\n",
    "Perform basic data analysis on class distribution and stream characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee66628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Analysis\n",
    "print(\"ğŸ“Š Performing basic data analysis...\")\n",
    "\n",
    "# Set up matplotlib for better visualizations\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Class Distribution Analysis\n",
    "print(\"\\nğŸ·ï¸ Analyzing class distribution...\")\n",
    "\n",
    "# Training distribution\n",
    "train_counts = np.bincount(train_labels_tensor.numpy(), minlength=100)\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.bar(range(100), train_counts, alpha=0.7, color='skyblue')\n",
    "plt.title('Training Set Class Distribution', fontweight='bold')\n",
    "plt.xlabel('Class ID')\n",
    "plt.ylabel('Sample Count')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Stream Statistics Analysis\n",
    "print(\"\\nğŸ¨ RGB vs Brightness stream characteristics:\")\n",
    "\n",
    "# Sample a subset for analysis\n",
    "sample_size = min(1000, len(train_rgb))\n",
    "indices = np.random.choice(len(train_rgb), sample_size, replace=False)\n",
    "rgb_sample = train_rgb[indices]\n",
    "brightness_sample = train_brightness[indices]\n",
    "\n",
    "# Calculate statistics\n",
    "rgb_stats = {\n",
    "    'mean': rgb_sample.mean().item(),\n",
    "    'std': rgb_sample.std().item(),\n",
    "    'min': rgb_sample.min().item(),\n",
    "    'max': rgb_sample.max().item()\n",
    "}\n",
    "\n",
    "brightness_stats = {\n",
    "    'mean': brightness_sample.mean().item(),\n",
    "    'std': brightness_sample.std().item(), \n",
    "    'min': brightness_sample.min().item(),\n",
    "    'max': brightness_sample.max().item()\n",
    "}\n",
    "\n",
    "print(f\"   ğŸ¨ RGB statistics:\")\n",
    "print(f\"      Mean: {rgb_stats['mean']:.3f}, Std: {rgb_stats['std']:.3f}\")\n",
    "print(f\"      Min: {rgb_stats['min']:.3f}, Max: {rgb_stats['max']:.3f}\")\n",
    "print(f\"   ğŸ’¡ Brightness statistics:\")\n",
    "print(f\"      Mean: {brightness_stats['mean']:.3f}, Std: {brightness_stats['std']:.3f}\")\n",
    "print(f\"      Min: {brightness_stats['min']:.3f}, Max: {brightness_stats['max']:.3f}\")\n",
    "\n",
    "print(\"\\nâœ… Data analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803e7962",
   "metadata": {},
   "source": [
    "## 10. Data Augmentation\n",
    "\n",
    "Set up data augmentation for multi-stream training using the project's CIFAR-100 augmentation module.\n",
    "\n",
    "Data augmentation helps improve model generalization by creating variations of the training data. In this notebook, we'll use on-the-fly augmentation through PyTorch DataLoaders, which:\n",
    "\n",
    "1. Applies transformations randomly during training\n",
    "2. Creates unique augmentations in each epoch\n",
    "3. Preserves the original data while generating variations\n",
    "\n",
    "We'll create augmented DataLoaders that:\n",
    "- Apply transformations to the original data during batch loading\n",
    "- Handle both RGB and brightness streams consistently\n",
    "- Can be used directly with our models through the `fit_dataloader()` method\n",
    "\n",
    "This approach is consistent with the standard PyTorch best practices for data augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca38f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Augmentation\n",
    "print(\"ğŸ”„ Setting up data augmentation using project's implementation...\")\n",
    "\n",
    "# Create augmentation with custom settings for CIFAR-100\n",
    "augmentation_config = {\n",
    "    'horizontal_flip_prob': 0.5,  # 50% chance of flipping horizontally\n",
    "    'rotation_degrees': 10.0,     # Rotate up to Â±10 degrees\n",
    "    'translate_range': 0.1,       # Translate up to 10% of image size\n",
    "    'scale_range': (0.9, 1.1),    # Scale between 90-110%\n",
    "    'color_jitter_strength': 0.3, # Moderate color jittering\n",
    "    'gaussian_noise_std': 0.01,   # Small amount of noise\n",
    "    'cutout_prob': 0.3,           # 30% chance of applying cutout\n",
    "    'cutout_size': 8,             # 8x8 pixel cutout\n",
    "    'enabled': True               # Enable augmentation\n",
    "}\n",
    "\n",
    "# Setup MixUp augmentation\n",
    "mixup_alpha = 0.2  # Alpha parameter for Beta distribution\n",
    "\n",
    "# Create augmented datasets and data loaders in one step\n",
    "print(\"\\nğŸ“Š Creating augmented DataLoaders...\")\n",
    "train_loader, val_loader = create_augmented_dataloaders(\n",
    "    train_rgb, train_brightness, train_labels_tensor,  # Training data\n",
    "    val_rgb, val_brightness, val_labels_tensor,        # Validation data\n",
    "    batch_size=512,                                    # Increased batch size for A100 GPU\n",
    "    dataset=\"cifar100\",                                # Dataset type\n",
    "    augmentation_config=augmentation_config,           # Augmentation settings\n",
    "    mixup_alpha=mixup_alpha,                           # MixUp parameter\n",
    "    num_workers=2,                                     # Parallel workers\n",
    "    pin_memory=torch.cuda.is_available()               # Pin memory if GPU available\n",
    ")\n",
    "\n",
    "# Create test dataloader separately (no augmentation)\n",
    "test_loader = create_test_dataloader(\n",
    "    test_rgb, test_brightness, test_labels_tensor,\n",
    "    batch_size=512,                                    # Increased batch size for A100 GPU\n",
    "    num_workers=2,\n",
    "    pin_memory=torch.cuda.is_available()\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… Data augmentation setup complete\")\n",
    "print(\"   Using project's CIFAR-100 specific augmentations:\")\n",
    "print(f\"   - Horizontal flips: {augmentation_config['horizontal_flip_prob']}\")\n",
    "print(f\"   - Rotation: Â±{augmentation_config['rotation_degrees']}Â°\")\n",
    "print(f\"   - Translation: Â±{augmentation_config['translate_range'] * 100}%\")\n",
    "print(f\"   - Color jitter strength: {augmentation_config['color_jitter_strength']}\")\n",
    "print(f\"   - Gaussian noise (std): {augmentation_config['gaussian_noise_std']}\")\n",
    "print(f\"   - Cutout: {augmentation_config['cutout_prob']} probability, {augmentation_config['cutout_size']}px\")\n",
    "print(f\"   - MixUp alpha: {mixup_alpha}\")\n",
    "print(f\"\\n   Training batches: {len(train_loader)}\")\n",
    "print(f\"   Validation batches: {len(val_loader)}\")\n",
    "print(f\"   Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c639b3",
   "metadata": {},
   "source": [
    "## 11. Prepare Data for Training\n",
    "\n",
    "Create DataLoaders with the processed data for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05b1d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Data for Training\n",
    "print(\"ğŸ”„ Preparing data for model training...\")\n",
    "\n",
    "# The data loaders were already created in the previous cell:\n",
    "#  - train_loader: Training data with augmentation\n",
    "#  - val_loader: Validation data without augmentation\n",
    "#  - test_loader: Test data without augmentation\n",
    "\n",
    "# Confirm data loader settings\n",
    "print(f\"\\nğŸ“¦ DataLoader configuration:\")\n",
    "print(f\"   Batch size: {next(iter(train_loader))[0].shape[0]}\")\n",
    "print(f\"   Number of workers: 2\")\n",
    "print(f\"   Pin memory: {torch.cuda.is_available()}\")\n",
    "print(f\"   Training with augmentation: Yes\")\n",
    "print(f\"   Training with MixUp: {'Yes' if mixup_alpha is not None else 'No'}\")\n",
    "\n",
    "# Now let's prepare the data in both formats needed for different model types:\n",
    "# 1. Original format (N, C, H, W) for CNN models\n",
    "# 2. Reshaped format (N, C*H*W) for dense models\n",
    "\n",
    "print(\"\\nğŸ”„ Preparing data in both formats for different model architectures...\")\n",
    "\n",
    "# For CNN models: keep the original tensor format (N, C, H, W)\n",
    "# We already have these as train_rgb, train_brightness, etc.\n",
    "train_color_data = train_rgb.cpu().numpy()  # Original shape (N, C, H, W)\n",
    "train_brightness_data = train_brightness.cpu().numpy()  # Original shape (N, C, H, W)\n",
    "val_color_data = val_rgb.cpu().numpy()  # Original shape (N, C, H, W)\n",
    "val_brightness_data = val_brightness.cpu().numpy()  # Original shape (N, C, H, W)\n",
    "test_color_data = test_rgb.cpu().numpy()  # Original shape (N, C, H, W)\n",
    "test_brightness_data = test_brightness.cpu().numpy()  # Original shape (N, C, H, W)\n",
    "\n",
    "# Convert labels to numpy arrays\n",
    "train_labels = train_labels_tensor.cpu().numpy()\n",
    "val_labels = val_labels_tensor.cpu().numpy()\n",
    "test_labels = test_labels_tensor.cpu().numpy()\n",
    "\n",
    "print(f\"CNN data shapes:\")\n",
    "print(f\"   RGB train: {train_color_data.shape}, Brightness train: {train_brightness_data.shape}\")\n",
    "print(f\"   RGB val: {val_color_data.shape}, Brightness val: {val_brightness_data.shape}\")\n",
    "print(f\"   RGB test: {test_color_data.shape}, Brightness test: {test_brightness_data.shape}\")\n",
    "\n",
    "# Prepare flattened data for dense models (flattening the spatial dimensions)\n",
    "# Reshape from (N, C, H, W) to (N, C*H*W)\n",
    "train_color_data_reshaped = train_color_data.reshape(train_color_data.shape[0], -1)\n",
    "train_brightness_data_reshaped = train_brightness_data.reshape(train_brightness_data.shape[0], -1)\n",
    "val_color_data_reshaped = val_color_data.reshape(val_color_data.shape[0], -1)\n",
    "val_brightness_data_reshaped = val_brightness_data.reshape(val_brightness_data.shape[0], -1)\n",
    "test_color_data_reshaped = test_color_data.reshape(test_color_data.shape[0], -1)\n",
    "test_brightness_data_reshaped = test_brightness_data.reshape(test_brightness_data.shape[0], -1)\n",
    "\n",
    "print(f\"\\nDense data shapes (flattened):\")\n",
    "print(f\"   RGB train: {train_color_data_reshaped.shape}, Brightness train: {train_brightness_data_reshaped.shape}\")\n",
    "print(f\"   RGB val: {val_color_data_reshaped.shape}, Brightness val: {val_brightness_data_reshaped.shape}\")\n",
    "print(f\"   RGB test: {test_color_data_reshaped.shape}, Brightness test: {test_brightness_data_reshaped.shape}\")\n",
    "\n",
    "print(\"\\nâœ… Data preparation complete for both CNN and dense models!\")\n",
    "\n",
    "# For dense models: also create these variables for compatibility with existing code\n",
    "train_color_data_reshaped = train_color_data.reshape(train_color_data.shape[0], -1)\n",
    "train_brightness_data_reshaped = train_brightness_data.reshape(train_brightness_data.shape[0], -1)\n",
    "val_color_data_reshaped = val_color_data.reshape(val_color_data.shape[0], -1)\n",
    "val_brightness_data_reshaped = val_brightness_data.reshape(val_brightness_data.shape[0], -1)\n",
    "test_color_data_reshaped = test_color_data.reshape(test_color_data.shape[0], -1)\n",
    "test_brightness_data_reshaped = test_brightness_data.reshape(test_brightness_data.shape[0], -1)\n",
    "\n",
    "# Calculate expected dimensions\n",
    "rgb_input_size = 3 * 32 * 32  # 3 channels x 32 x 32 for CIFAR-100\n",
    "brightness_input_size = 1 * 32 * 32  # 1 channel x 32 x 32 for CIFAR-100\n",
    "\n",
    "print(f\"âœ… Dense format ready - RGB: {train_color_data_reshaped.shape}, Brightness: {train_brightness_data_reshaped.shape}\")\n",
    "\n",
    "# Verify dimensions\n",
    "if train_color_data_reshaped.shape[1] != rgb_input_size:\n",
    "    print(f\"âš ï¸ Warning: Reshaped RGB size {train_color_data_reshaped.shape[1]} doesn't match expected size {rgb_input_size}\")\n",
    "if train_brightness_data_reshaped.shape[1] != brightness_input_size:\n",
    "    print(f\"âš ï¸ Warning: Reshaped brightness size {train_brightness_data_reshaped.shape[1]} doesn't match expected size {brightness_input_size}\")\n",
    "\n",
    "# Make sure CUDA_LAUNCH_BLOCKING is set for more detailed error messages\n",
    "if 'CUDA_LAUNCH_BLOCKING' not in os.environ:\n",
    "    os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "# Configure CUDA for better performance\n",
    "torch.backends.cudnn.benchmark = True  # Speed up training for fixed input sizes\n",
    "torch.backends.cudnn.deterministic = False  # Non-deterministic for speed\n",
    "\n",
    "# Sample batch for verification\n",
    "sample_color, sample_brightness, sample_labels = next(iter(train_loader))\n",
    "print(f\"\\nğŸ“Š Sample batch shapes from data loader:\")\n",
    "print(f\"   Color batch: {sample_color.shape}\")\n",
    "print(f\"   Brightness batch: {sample_brightness.shape}\")\n",
    "print(f\"   Labels batch: {sample_labels.shape}\")\n",
    "\n",
    "print(\"\\nğŸ¯ All data formats are ready for model training!\")\n",
    "\n",
    "# Initialize some training parameters that will be used by multiple models\n",
    "if 'num_epochs' not in globals():\n",
    "    num_epochs = 20\n",
    "if 'early_stopping_patience' not in globals():\n",
    "    early_stopping_patience = 5\n",
    "\n",
    "print(f\"\\nğŸ”§ Training Configuration:\")\n",
    "print(f\"   Default epochs: {num_epochs}\")\n",
    "print(f\"   Default early stopping patience: {early_stopping_patience}\")\n",
    "print(f\"   Default batch size: 512 (optimized for A100 GPU)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83696378",
   "metadata": {},
   "source": [
    "## 12. Create and train Baseline ResNet50 Model\n",
    "\n",
    "Create a standard ResNet50 model for comparison with multi-stream models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6996da86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Baseline ResNet50 Model\n",
    "print(\"ğŸ—ï¸ Creating baseline ResNet50 model for comparison...\")\n",
    "\n",
    "# Check GPU availability and set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"ğŸ–¥ï¸ Using device: {device}\")\n",
    "\n",
    "# Create ResNet50 model modified for CIFAR-100\n",
    "class CifarResNet50(nn.Module):\n",
    "    def __init__(self, num_classes=100, pretrained=False):\n",
    "        super(CifarResNet50, self).__init__()\n",
    "        \n",
    "        # Load ResNet50 with or without pretrained weights\n",
    "        if pretrained:\n",
    "            print(\"   Using pretrained weights (ImageNet)\")\n",
    "            self.model = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "        else:\n",
    "            print(\"   Using randomly initialized weights for fair comparison\")\n",
    "            self.model = resnet50(weights=None)\n",
    "        \n",
    "        # Modify first conv layer to work with 32x32 CIFAR images instead of 224x224 ImageNet\n",
    "        self.model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        # Remove maxpool to preserve spatial dimensions for small images\n",
    "        self.model.maxpool = nn.Identity()\n",
    "        \n",
    "        # Replace final fully connected layer for CIFAR-100\n",
    "        self.model.fc = nn.Linear(self.model.fc.in_features, num_classes)\n",
    "        \n",
    "        # Move to device\n",
    "        self.to(device)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Create the model - explicitly using pretrained=False for fair comparison with multi-stream models\n",
    "print(\"\\nâš ï¸ IMPORTANT: Using NON-PRETRAINED weights for fair comparison with multi-stream models\")\n",
    "baseline_model = CifarResNet50(num_classes=100, pretrained=False)\n",
    "baseline_model = baseline_model.to(device)\n",
    "\n",
    "# Setup optimizer and loss with weight decay for regularization\n",
    "baseline_optimizer = optim.Adam(baseline_model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "baseline_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Count parameters\n",
    "baseline_params = sum(p.numel() for p in baseline_model.parameters())\n",
    "baseline_trainable = sum(p.numel() for p in baseline_model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nâœ… Baseline ResNet50 created successfully\")\n",
    "print(f\"   Architecture: Modified ResNet50 for CIFAR-100\")\n",
    "print(f\"   Total parameters: {baseline_params:,}\")\n",
    "print(f\"   Trainable parameters: {baseline_trainable:,}\")\n",
    "print(f\"   Input shape: (3, 32, 32)\")\n",
    "print(f\"   Device: {device}\")\n",
    "print(f\"   Initialization: Random (not pretrained)\")\n",
    "print(f\"   Optimizer: Adam with weight_decay=0.0001\")\n",
    "\n",
    "# Quick validation on dummy data\n",
    "print(\"\\nğŸ‹ï¸â€â™‚ï¸ Validating model with dummy data...\")\n",
    "dummy_data = torch.randn(8, 3, 32, 32, device=device)\n",
    "dummy_labels = torch.randint(0, 100, (8,), device=device)\n",
    "\n",
    "baseline_model.train()\n",
    "outputs = baseline_model(dummy_data)\n",
    "loss = baseline_criterion(outputs, dummy_labels)\n",
    "print(f\"   âœ… Forward pass successful, loss: {loss.item():.4f}\")\n",
    "\n",
    "# Reset gradients\n",
    "baseline_optimizer.zero_grad()\n",
    "loss.backward()\n",
    "baseline_optimizer.step()\n",
    "print(f\"   âœ… Backward pass successful\")\n",
    "\n",
    "print(\"\\nâœ… Baseline model ready for training!\")\n",
    "\n",
    "# Immediate training on dummy data for validation\n",
    "print(\"ğŸ‹ï¸â€â™‚ï¸ Starting immediate training on dummy data...\")\n",
    "\n",
    "# Create dummy CIFAR-100 data\n",
    "dummy_data = torch.randn(64, 3, 32, 32, device=device)  # 64 samples, 3 channels, 32x32 images\n",
    "dummy_labels = torch.randint(0, 100, (64,), device=device)  # 64 labels for 100 classes\n",
    "\n",
    "# Training loop (1 epoch)\n",
    "baseline_model.train()\n",
    "for epoch in range(1):  # Just 1 epoch for quick testing\n",
    "    # Forward pass\n",
    "    outputs = baseline_model(dummy_data)\n",
    "    loss = baseline_criterion(outputs, dummy_labels)\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    baseline_optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    baseline_optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/1], Loss: {loss.item():.4f}\")\n",
    "\n",
    "print(\"âœ… Immediate training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40785a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Baseline ResNet50 Model\n",
    "print(\"ğŸ‹ï¸â€â™€ï¸ Training baseline ResNet50 model...\")\n",
    "\n",
    "# Define training function with early stopping and learning rate scheduling\n",
    "def train_model(model, train_loader, val_loader, optimizer, criterion, num_epochs=100, patience=10, model_name=\"Model\"):\n",
    "    \"\"\"\n",
    "    Train a model and return training history.\n",
    "    \n",
    "    Args:\n",
    "        model: The model to train\n",
    "        train_loader: DataLoader for training data\n",
    "        val_loader: DataLoader for validation data\n",
    "        optimizer: Optimizer to use\n",
    "        criterion: Loss function\n",
    "        num_epochs: Number of epochs to train\n",
    "        patience: Early stopping patience\n",
    "        model_name: Name for logging\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with training history\n",
    "    \"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': []\n",
    "    }\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    best_model_state = None\n",
    "    no_improvement_count = 0\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, \n",
    "        mode='max',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        train_bar = tqdm(train_loader, desc=f\"{model_name} Training\")\n",
    "        \n",
    "        for batch_idx, data in enumerate(train_bar):\n",
    "            # Handle the baseline model which only needs RGB data - support both tuple and list types\n",
    "            if isinstance(data, (tuple, list)) and len(data) == 3:\n",
    "                rgb, _, targets = data  # Unpack RGB, ignore brightness\n",
    "            else:\n",
    "                # Fallback for unexpected data format\n",
    "                rgb, targets = data\n",
    "                \n",
    "            # Move to device\n",
    "            rgb, targets = rgb.to(device), targets.to(device)\n",
    "            \n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass - baseline model only takes RGB\n",
    "            outputs = model(rgb)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            # Add L2 regularization term (additional to weight decay in optimizer)\n",
    "            l2_lambda = 0.0001\n",
    "            l2_reg = 0.0\n",
    "            for param in model.parameters():\n",
    "                l2_reg += torch.norm(param, 2)\n",
    "            loss += l2_lambda * l2_reg\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Track statistics\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            train_total += targets.size(0)\n",
    "            train_correct += predicted.eq(targets).sum().item()\n",
    "            \n",
    "            # Update progress bar\n",
    "            train_bar.set_postfix({\n",
    "                'loss': train_loss/(batch_idx+1), \n",
    "                'acc': 100.*train_correct/train_total\n",
    "            })\n",
    "        \n",
    "        train_acc = 100. * train_correct / train_total\n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            val_bar = tqdm(val_loader, desc=f\"{model_name} Validation\")\n",
    "            \n",
    "            for batch_idx, data in enumerate(val_bar):\n",
    "                # Handle data format - support both tuple and list types\n",
    "                if isinstance(data, (tuple, list)) and len(data) == 3:\n",
    "                    rgb, _, targets = data  # Unpack RGB, ignore brightness\n",
    "                else:\n",
    "                    rgb, targets = data\n",
    "                \n",
    "                # Move to device\n",
    "                rgb, targets = rgb.to(device), targets.to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(rgb)\n",
    "                loss = criterion(outputs, targets)\n",
    "                \n",
    "                # Track statistics\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                val_total += targets.size(0)\n",
    "                val_correct += predicted.eq(targets).sum().item()\n",
    "                \n",
    "                # Update progress bar\n",
    "                val_bar.set_postfix({\n",
    "                    'loss': val_loss/(batch_idx+1), \n",
    "                    'acc': 100.*val_correct/val_total\n",
    "                })\n",
    "        \n",
    "        val_acc = 100. * val_correct / val_total\n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        \n",
    "        # Save history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        # Print epoch summary\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} Summary:\")\n",
    "        print(f\"   Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "        print(f\"   Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "        print(f\"   Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "        \n",
    "        # Update learning rate scheduler\n",
    "        scheduler.step(val_acc)\n",
    "        \n",
    "        # Track best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            no_improvement_count = 0\n",
    "            print(f\"   âœ… New best validation accuracy: {best_val_acc:.2f}%\")\n",
    "        else:\n",
    "            no_improvement_count += 1\n",
    "            print(f\"   âš ï¸ No improvement for {no_improvement_count} epochs\")\n",
    "        \n",
    "        # Early stopping check\n",
    "        if no_improvement_count >= patience:\n",
    "            print(f\"   ğŸ›‘ Early stopping triggered after {epoch+1} epochs\")\n",
    "            break\n",
    "    \n",
    "    # Restore best model\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    print(f\"\\nâœ… {model_name} training complete!\")\n",
    "    print(f\"   Best validation accuracy: {best_val_acc:.2f}%\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "# Training configuration\n",
    "num_epochs = 100  # Maximum epochs\n",
    "patience = 10     # Early stopping patience\n",
    "batch_size = 512  # Increased batch size for A100 GPU\n",
    "\n",
    "print(f\"\\nğŸ”§ Training Configuration:\")\n",
    "print(f\"   Epochs: {num_epochs} (with early stopping, patience={patience})\")\n",
    "print(f\"   Batch size: {batch_size} (optimized for A100 GPU)\")\n",
    "print(f\"   Optimizer: Adam with weight decay=0.0001\")\n",
    "print(f\"   Learning rate: 0.001 with ReduceLROnPlateau scheduling\")\n",
    "print(f\"   Regularization: L2 weight decay\")\n",
    "print(f\"   Early stopping: Patience {patience} epochs\")\n",
    "print(f\"   Loss: CrossEntropyLoss\")\n",
    "print(f\"   GPU acceleration: {torch.cuda.is_available()}\")\n",
    "print(f\"   GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}\")\n",
    "\n",
    "# Train baseline model\n",
    "baseline_history = train_model(\n",
    "    model=baseline_model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optimizer=baseline_optimizer,\n",
    "    criterion=baseline_criterion,\n",
    "    num_epochs=num_epochs,\n",
    "    patience=patience,\n",
    "    model_name=\"Baseline ResNet50\"\n",
    ")\n",
    "\n",
    "# Visualize training curves\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(baseline_history['train_acc'], label='Train')\n",
    "plt.plot(baseline_history['val_acc'], label='Validation')\n",
    "plt.title('Baseline ResNet50 Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(baseline_history['train_loss'], label='Train')\n",
    "plt.plot(baseline_history['val_loss'], label='Validation')\n",
    "plt.title('Baseline ResNet50 Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… Baseline model training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb3b0b7",
   "metadata": {},
   "source": [
    "## 13. Create Multi-stream models\n",
    "\n",
    "Now that we've trained our baseline ResNet50 model as a reference, let's create and train our multi-stream neural network models.\n",
    "\n",
    "The multi-stream models combine RGB and brightness (luminance) information through parallel processing pathways:\n",
    "\n",
    "1. **BaseMultiChannelNetwork**: A dense/fully-connected model with separate pathways for RGB and brightness\n",
    "2. **MultiChannelResNetNetwork**: A CNN-based model using ResNet architecture with separate pathways\n",
    "\n",
    "Both models use the same unified augmentation pipeline, regularization techniques, and training strategy as the baseline for fair comparison.\n",
    "\n",
    "### Advantages of Multi-Stream Processing\n",
    "- Separation of color and brightness information allows each pathway to specialize\n",
    "- The network can learn which stream is more informative for specific classes\n",
    "- Potential for improved robustness to variations in lighting conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfd0efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Multi-Stream Models\n",
    "print(\"ğŸ—ï¸ Creating Multi-Stream Neural Network Models...\")\n",
    "\n",
    "# Check GPU availability and set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"ğŸ–¥ï¸ Using device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"   Using CPU (CUDA not available)\")\n",
    "\n",
    "# Model configuration based on CIFAR-100 data\n",
    "print(f\"\\nğŸ“Š Model Configuration:\")\n",
    "print(f\"   Image size: 32x32 pixels\")\n",
    "print(f\"   RGB channels: 3\")\n",
    "print(f\"   Brightness channels: 1\") \n",
    "print(f\"   Number of classes: 100 (CIFAR-100)\")\n",
    "print(f\"   Device: {device}\")\n",
    "\n",
    "# Use the model factory imported in the main import cell\n",
    "if MODEL_FACTORY_AVAILABLE:\n",
    "    print(\"\\nğŸ“‹ Available model types:\")\n",
    "    available_model_types = list_available_models()\n",
    "    for model_type in available_model_types:\n",
    "        print(f\"   âœ… {model_type}\")\n",
    "    \n",
    "    factory_available = True\n",
    "else:\n",
    "    print(f\"âŒ Model factory not available\")\n",
    "    print(\"ğŸ’¡ Using direct model imports...\")\n",
    "    factory_available = False\n",
    "\n",
    "# Model dimensions for CIFAR-100\n",
    "input_channels_rgb = 3\n",
    "input_channels_brightness = 1  \n",
    "image_size = 32\n",
    "num_classes = 100\n",
    "\n",
    "# For dense models: flatten the image to 1D\n",
    "rgb_input_size = input_channels_rgb * image_size * image_size  # 3 * 32 * 32 = 3072\n",
    "brightness_input_size = input_channels_brightness * image_size * image_size  # 1 * 32 * 32 = 1024\n",
    "\n",
    "print(f\"\\nğŸ”§ Model Input Configuration:\")\n",
    "print(f\"   RGB input size (dense): {rgb_input_size}\")\n",
    "print(f\"   Brightness input size (dense): {brightness_input_size}\")\n",
    "print(f\"   RGB input channels (CNN): {input_channels_rgb}\")\n",
    "print(f\"   Brightness input channels (CNN): {input_channels_brightness}\")\n",
    "\n",
    "# Create base_multi_channel_large (Dense/FC model)\n",
    "print(f\"\\nğŸ­ Creating base_multi_channel_large (Dense Model)...\")\n",
    "try:\n",
    "    if factory_available:\n",
    "        base_multi_channel_large_model = create_model(\n",
    "            'base_multi_channel_large',\n",
    "            color_input_size=rgb_input_size,\n",
    "            brightness_input_size=brightness_input_size,\n",
    "            num_classes=num_classes,\n",
    "            use_shared_classifier=True,\n",
    "            device=device  # Use detected device (CUDA if available)\n",
    "        )\n",
    "    else:\n",
    "        base_multi_channel_large_model = base_multi_channel_large(\n",
    "            color_input_size=rgb_input_size,\n",
    "            brightness_input_size=brightness_input_size,\n",
    "            num_classes=num_classes,\n",
    "            use_shared_classifier=True,\n",
    "            device=device  # Use detected device (CUDA if available)\n",
    "        )\n",
    "    \n",
    "    # Count parameters\n",
    "    large_dense_params = sum(p.numel() for p in base_multi_channel_large_model.parameters())\n",
    "    large_dense_trainable = sum(p.numel() for p in base_multi_channel_large_model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"âœ… base_multi_channel_large created successfully\")\n",
    "    print(f\"   Architecture: Large Dense/FC Network\")\n",
    "    print(f\"   Total parameters: {large_dense_params:,}\")\n",
    "    print(f\"   Trainable parameters: {large_dense_trainable:,}\")\n",
    "    print(f\"   Input size: RGB {rgb_input_size}, Brightness {brightness_input_size}\")\n",
    "    print(f\"   Fusion strategy: Shared classifier\")\n",
    "    print(f\"   Device: {base_multi_channel_large_model.device}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Failed to create base_multi_channel_large: {e}\")\n",
    "    print(f\"ğŸ’¡ Error details: {str(e)}\")\n",
    "    traceback.print_exc()\n",
    "    base_multi_channel_large_model = None\n",
    "\n",
    "# Create multi_channel_resnet50 (CNN model)\n",
    "print(f\"\\nğŸ­ Creating multi_channel_resnet50 (CNN Model)...\")\n",
    "try:\n",
    "    if factory_available:\n",
    "        multi_channel_resnet50_model = create_model(\n",
    "            'multi_channel_resnet50',\n",
    "            color_input_channels=input_channels_rgb,\n",
    "            brightness_input_channels=input_channels_brightness,\n",
    "            num_classes=num_classes,\n",
    "            use_shared_classifier=True,\n",
    "            activation='relu',\n",
    "            device=device  # Use detected device (CUDA if available)\n",
    "        )\n",
    "    else:\n",
    "        multi_channel_resnet50_model = multi_channel_resnet50(\n",
    "            color_input_channels=input_channels_rgb,\n",
    "            brightness_input_channels=input_channels_brightness,\n",
    "            num_classes=num_classes,\n",
    "            use_shared_classifier=True,\n",
    "            activation='relu',\n",
    "            device=device  # Use detected device (CUDA if available)\n",
    "        )\n",
    "    \n",
    "    # Count parameters\n",
    "    resnet50_params = sum(p.numel() for p in multi_channel_resnet50_model.parameters())\n",
    "    resnet50_trainable = sum(p.numel() for p in multi_channel_resnet50_model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"âœ… multi_channel_resnet50 created successfully\")\n",
    "    print(f\"   Architecture: ResNet-50 style CNN (3,4,6,3 blocks)\")\n",
    "    print(f\"   Total parameters: {resnet50_params:,}\")\n",
    "    print(f\"   Trainable parameters: {resnet50_trainable:,}\")\n",
    "    print(f\"   Input shape: RGB {(input_channels_rgb, image_size, image_size)}, Brightness {(input_channels_brightness, image_size, image_size)}\")\n",
    "    print(f\"   Fusion strategy: Shared classifier\")\n",
    "    print(f\"   Device: {multi_channel_resnet50_model.device}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Failed to create multi_channel_resnet50: {e}\")\n",
    "    print(f\"ğŸ’¡ Error details: {str(e)}\")\n",
    "    traceback.print_exc()\n",
    "    multi_channel_resnet50_model = None\n",
    "\n",
    "# Model comparison\n",
    "if base_multi_channel_large_model is not None and multi_channel_resnet50_model is not None:\n",
    "    print(f\"\\nğŸ“ˆ Model Comparison:\")\n",
    "    print(f\"   base_multi_channel_large: {large_dense_params:,} parameters\")\n",
    "    print(f\"   multi_channel_resnet50: {resnet50_params:,} parameters\")\n",
    "    print(f\"   ResNet-50 is {resnet50_params/large_dense_params:.1f}x larger than Large Dense\")\n",
    "    \n",
    "# Compile the models with proper optimizers\n",
    "if base_multi_channel_large_model is not None:\n",
    "    print(\"\\nğŸ”§ Compiling base_multi_channel_large model...\")\n",
    "    base_multi_channel_large_model.compile(\n",
    "        optimizer='adamw',\n",
    "        learning_rate=0.001,\n",
    "        weight_decay=0.0001,\n",
    "        loss='cross_entropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "if multi_channel_resnet50_model is not None:\n",
    "    print(\"\\nğŸ”§ Compiling multi_channel_resnet50 model...\")\n",
    "    multi_channel_resnet50_model.compile(\n",
    "        optimizer='adamw',\n",
    "        learning_rate=0.001,\n",
    "        weight_decay=0.0001,\n",
    "        loss='cross_entropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "print(\"\\nâœ… Multi-stream models created and compiled successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89296cbe",
   "metadata": {},
   "source": [
    "## 14. Train Multi-Stream Models\n",
    "\n",
    "In this section, we'll train our multi-stream neural network models using two approaches:\n",
    "\n",
    "1. **Direct DataLoader Training (Primary Method)**\n",
    "   - Using the `fit_dataloader()` method to pass DataLoaders directly\n",
    "   - This enables on-the-fly augmentation during training\n",
    "   - The model receives freshly augmented data in each epoch\n",
    "   - More memory efficient as it doesn't require pre-extracting all augmented data\n",
    "\n",
    "2. **Pre-extracted Data Training (Fallback Method)**\n",
    "   - Using the `fit()` method with pre-extracted data arrays\n",
    "   - Falls back to this approach if direct DataLoader training fails\n",
    "   - Still uses augmented data, but extracted once before training\n",
    "\n",
    "Both models (BaseMultiChannelNetwork and MultiChannelResNetNetwork) support these training methods, allowing for a consistent approach across all architectures.\n",
    "\n",
    "### Benefits of Direct DataLoader Training\n",
    "- Truly on-the-fly augmentation, creating different augmentations each epoch\n",
    "- More memory efficient (doesn't store all augmented samples)\n",
    "- Consistent with how the baseline ResNet model is trained\n",
    "- Better generalization through exposure to more augmented variations\n",
    "\n",
    "The training code includes error handling to fall back to the original method if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9d3cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train base_multi_channel_large model using built-in fit_dataloader() API\n",
    "print(\"ğŸ‹ï¸â€â™€ï¸ Training BaseMultiChannelNetwork model using DataLoaders for on-the-fly augmentation...\")\n",
    "\n",
    "# Training configuration for all models\n",
    "num_epochs = 100  # Full training run with early stopping\n",
    "early_stopping_patience = 10\n",
    "\n",
    "print(f\"\\nğŸ”§ Training Configuration for BaseMultiChannelNetwork:\")\n",
    "print(f\"   Epochs: {num_epochs} (with early stopping, patience={early_stopping_patience})\")\n",
    "print(f\"   Batch size: 64\")  # Using 64 as batch size to prevent CUDA errors\n",
    "print(f\"   Optimizer: AdamW with weight decay=0.0001\")\n",
    "print(f\"   Learning rate: 0.001 with scheduler\")\n",
    "print(f\"   Device: {base_multi_channel_large_model.device}\")\n",
    "print(f\"   Training Method: fit_dataloader with on-the-fly augmentation\")\n",
    "\n",
    "# Clear any existing tqdm bars\n",
    "try:\n",
    "    tqdm._instances.clear()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Train using fit_dataloader() API which takes DataLoaders directly\n",
    "print(\"\\nğŸš€ Starting training with fit_dataloader() API for on-the-fly augmentation...\")\n",
    "try:\n",
    "    base_multi_channel_large_history = base_multi_channel_large_model.fit_dataloader(\n",
    "        train_loader=train_loader,  # Pass DataLoader directly\n",
    "        val_loader=val_loader,      # Pass DataLoader directly\n",
    "        epochs=num_epochs,\n",
    "        early_stopping_patience=early_stopping_patience,\n",
    "        scheduler_type='cosine',\n",
    "        min_lr=1e-6,\n",
    "        verbose=1,\n",
    "        learning_rate=0.001,\n",
    "        weight_decay=0.0001\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Encountered an error: {e}\")\n",
    "    print(\"ğŸ” Error details:\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    \n",
    "    print(\"\\nğŸ”„ Trying alternative approach...\")\n",
    "    # If the fit_dataloader method isn't working correctly, fall back to the original approach\n",
    "    \n",
    "    # Extract data from data loaders for fit() method\n",
    "    # Get train data\n",
    "    train_color_tensors = []\n",
    "    train_brightness_tensors = []\n",
    "    train_label_tensors = []\n",
    "\n",
    "    print(\"\\nğŸ”„ Extracting train data from DataLoaders...\")\n",
    "    for data in train_loader:\n",
    "        # Handle different data formats - support both tuple and list types\n",
    "        if isinstance(data, (list, tuple)) and len(data) == 3:\n",
    "            rgb, brightness, labels = data\n",
    "        else:\n",
    "            # Handle unexpected data format\n",
    "            if not isinstance(data, (list, tuple)):\n",
    "                raise ValueError(f\"Expected data to be list or tuple, got {type(data)}\")\n",
    "            elif len(data) != 3:\n",
    "                raise ValueError(f\"Expected data to have 3 elements, got {len(data)}\")\n",
    "            else:\n",
    "                raise ValueError(\"Unexpected data format from train_loader\")\n",
    "            \n",
    "        train_color_tensors.append(rgb)\n",
    "        train_brightness_tensors.append(brightness)\n",
    "        train_label_tensors.append(labels)\n",
    "\n",
    "    train_color_data = torch.cat(train_color_tensors, dim=0).cpu().numpy()\n",
    "    train_brightness_data = torch.cat(train_brightness_tensors, dim=0).cpu().numpy()\n",
    "    train_labels = torch.cat(train_label_tensors, dim=0).cpu().numpy()\n",
    "\n",
    "    # Get validation data\n",
    "    val_color_tensors = []\n",
    "    val_brightness_tensors = []\n",
    "    val_label_tensors = []\n",
    "\n",
    "    print(\"ğŸ”„ Extracting validation data from DataLoaders...\")\n",
    "    for data in val_loader:\n",
    "        # Handle different data formats - support both tuple and list types\n",
    "        if isinstance(data, (list, tuple)) and len(data) == 3:\n",
    "            rgb, brightness, labels = data\n",
    "        else:\n",
    "            # Handle unexpected data format\n",
    "            if not isinstance(data, (list, tuple)):\n",
    "                raise ValueError(f\"Expected data to be list or tuple, got {type(data)}\")\n",
    "            elif len(data) != 3:\n",
    "                raise ValueError(f\"Expected data to have 3 elements, got {len(data)}\")\n",
    "            else:\n",
    "                raise ValueError(\"Unexpected data format from val_loader\")\n",
    "            \n",
    "        val_color_tensors.append(rgb)\n",
    "        val_brightness_tensors.append(brightness)\n",
    "        val_label_tensors.append(labels)\n",
    "\n",
    "    val_color_data = torch.cat(val_color_tensors, dim=0).cpu().numpy()\n",
    "    val_brightness_data = torch.cat(val_brightness_tensors, dim=0).cpu().numpy()\n",
    "    val_labels = torch.cat(val_label_tensors, dim=0).cpu().numpy()\n",
    "\n",
    "    print(f\"âœ… Extracted data shapes:\")\n",
    "    print(f\"   Train color: {train_color_data.shape}, Train brightness: {train_brightness_data.shape}, Train labels: {train_labels.shape}\")\n",
    "    print(f\"   Val color: {val_color_data.shape}, Val brightness: {val_brightness_data.shape}, Val labels: {val_labels.shape}\")\n",
    "\n",
    "    # IMPORTANT: Reshape the data correctly for the model - this fixes the dimension mismatch\n",
    "    # The dense model expects flattened inputs (N, C*H*W) rather than images (N, C, H, W)\n",
    "    print(\"\\nğŸ”„ Reshaping data for dense model...\")\n",
    "\n",
    "    # Get the expected input sizes from the model definition\n",
    "    rgb_input_size = base_multi_channel_large_model.color_input_size  # Should be 3072 for CIFAR-100 (3*32*32)\n",
    "    brightness_input_size = base_multi_channel_large_model.brightness_input_size  # Should be 1024 for CIFAR-100 (1*32*32)\n",
    "\n",
    "    # Verify the expected shapes\n",
    "    print(f\"   Expected RGB input size: {rgb_input_size}\")\n",
    "    print(f\"   Expected brightness input size: {brightness_input_size}\")\n",
    "\n",
    "    # Reshape RGB data from (N, C, H, W) to (N, C*H*W)\n",
    "    train_color_data_reshaped = train_color_data.reshape(train_color_data.shape[0], -1)\n",
    "    val_color_data_reshaped = val_color_data.reshape(val_color_data.shape[0], -1)\n",
    "\n",
    "    # Reshape brightness data from (N, C, H, W) to (N, C*H*W)\n",
    "    train_brightness_data_reshaped = train_brightness_data.reshape(train_brightness_data.shape[0], -1)\n",
    "    val_brightness_data_reshaped = val_brightness_data.reshape(val_brightness_data.shape[0], -1)\n",
    "\n",
    "    # Verify the reshaped dimensions match what the model expects\n",
    "    print(f\"   Reshaped train color: {train_color_data_reshaped.shape} (should be [N, {rgb_input_size}])\")\n",
    "    print(f\"   Reshaped train brightness: {train_brightness_data_reshaped.shape} (should be [N, {brightness_input_size}])\")\n",
    "    print(f\"   Reshaped val color: {val_color_data_reshaped.shape} (should be [N, {rgb_input_size}])\")\n",
    "    print(f\"   Reshaped val brightness: {val_brightness_data_reshaped.shape} (should be [N, {brightness_input_size}])\")\n",
    "\n",
    "    # Double-check that shapes match exactly what's expected\n",
    "    if train_color_data_reshaped.shape[1] != rgb_input_size:\n",
    "        print(f\"âš ï¸ Warning: Reshaped RGB size {train_color_data_reshaped.shape[1]} doesn't match expected size {rgb_input_size}\")\n",
    "    if train_brightness_data_reshaped.shape[1] != brightness_input_size:\n",
    "        print(f\"âš ï¸ Warning: Reshaped brightness size {train_brightness_data_reshaped.shape[1]} doesn't match expected size {brightness_input_size}\")\n",
    "\n",
    "    # Clear any existing tqdm bars\n",
    "    try:\n",
    "        tqdm._instances.clear()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Train using fit() API which takes numpy arrays directly\n",
    "    print(\"\\nğŸš€ Starting training with fit() API as fallback...\")\n",
    "    base_multi_channel_large_history = base_multi_channel_large_model.fit(\n",
    "        train_color_data=train_color_data_reshaped,  # Use reshaped data\n",
    "        train_brightness_data=train_brightness_data_reshaped,  # Use reshaped data\n",
    "        train_labels=train_labels,\n",
    "        val_color_data=val_color_data_reshaped,  # Use reshaped data\n",
    "        val_brightness_data=val_brightness_data_reshaped,  # Use reshaped data\n",
    "        val_labels=val_labels,\n",
    "        batch_size=64,  # Using 64 as batch size to prevent CUDA errors\n",
    "        epochs=num_epochs,\n",
    "        early_stopping_patience=early_stopping_patience,\n",
    "        verbose=1,\n",
    "        num_workers=0,  # Reduce worker processes to avoid CUDA initialization issues\n",
    "        pin_memory=False  # Try disabling pin_memory which can cause issues with CUDA\n",
    "    )\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(base_multi_channel_large_history['accuracy'], label='Train')\n",
    "plt.plot(base_multi_channel_large_history['val_accuracy'], label='Validation')\n",
    "plt.title('BaseMultiChannelNetwork Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(base_multi_channel_large_history['loss'], label='Train')\n",
    "plt.plot(base_multi_channel_large_history['val_loss'], label='Validation')\n",
    "plt.title('BaseMultiChannelNetwork Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… BaseMultiChannelNetwork training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8be8d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train multi_channel_resnet50 model using built-in fit_dataloader() API\n",
    "print(\"ğŸ‹ï¸â€â™€ï¸ Training MultiChannelResNetNetwork model using DataLoaders for on-the-fly augmentation...\")\n",
    "\n",
    "print(f\"\\nğŸ”§ Training Configuration for MultiChannelResNetNetwork:\")\n",
    "print(f\"   Epochs: {num_epochs} (with early stopping, patience={early_stopping_patience})\")\n",
    "print(f\"   Batch size: 512\")  # Increased batch size for A100 GPU\n",
    "print(f\"   Optimizer: AdamW with weight decay=0.0001\")\n",
    "print(f\"   Learning rate: 0.001 with scheduler\")\n",
    "print(f\"   Device: {multi_channel_resnet50_model.device}\")\n",
    "print(f\"   Training Method: fit_dataloader with on-the-fly augmentation\")\n",
    "\n",
    "# Make sure CUDA_LAUNCH_BLOCKING is still set\n",
    "if 'CUDA_LAUNCH_BLOCKING' not in os.environ:\n",
    "    os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "# Clear any existing tqdm bars\n",
    "try:\n",
    "    tqdm._instances.clear()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Train using fit_dataloader() API which takes DataLoaders directly\n",
    "print(\"\\nğŸš€ Starting training with fit_dataloader() API for on-the-fly augmentation...\")\n",
    "try:\n",
    "    multi_channel_resnet50_history = multi_channel_resnet50_model.fit_dataloader(\n",
    "        train_loader=train_loader,  # Pass DataLoader directly\n",
    "        val_loader=val_loader,      # Pass DataLoader directly\n",
    "        epochs=num_epochs,\n",
    "        early_stopping_patience=early_stopping_patience,\n",
    "        scheduler_type='cosine',\n",
    "        min_lr=1e-6,\n",
    "        verbose=1,\n",
    "        learning_rate=0.001,\n",
    "        weight_decay=0.0001\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Encountered an error: {e}\")\n",
    "    print(\"ğŸ” Error details:\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    \n",
    "    print(\"\\nğŸ” Data consistency check...\")\n",
    "    print(f\"   We have two sets of data in the notebook:\")\n",
    "    print(f\"   1. Original processed data: RGB: {train_rgb.shape}, Brightness: {train_brightness.shape}, Labels: {train_labels_tensor.shape}\")\n",
    "    print(f\"   2. Augmented data from DataLoaders: RGB: {train_color_data.shape}, Brightness: {train_brightness_data.shape}, Labels: {train_labels.shape}\")\n",
    "    \n",
    "    # Falling back to the original approach using extracted arrays\n",
    "    print(\"\\nğŸ”„ Trying alternative approach...\")\n",
    "    # If the fit_dataloader method isn't working correctly, fall back to the original approach\n",
    "    \n",
    "    # IMPORTANT: Use the augmented data from DataLoaders for training\n",
    "    # This data includes augmentations (flips, rotations, color jitter, etc.)\n",
    "    cnn_train_color = torch.tensor(train_color_data)  # Augmented RGB data\n",
    "    cnn_train_brightness = torch.tensor(train_brightness_data)  # Augmented brightness data\n",
    "    cnn_train_labels = train_labels  # Augmented labels\n",
    "    cnn_val_color = torch.tensor(val_color_data)  # Validation RGB data\n",
    "    cnn_val_brightness = torch.tensor(val_brightness_data)  # Validation brightness data\n",
    "    cnn_val_labels = val_labels  # Validation labels\n",
    "\n",
    "    print(\"\\nğŸ” Using augmented data from DataLoaders for better training:\")\n",
    "    print(f\"   Training - RGB: {cnn_train_color.shape}, Brightness: {cnn_train_brightness.shape}, Labels: {cnn_train_labels.shape}\")\n",
    "    print(f\"   Validation - RGB: {cnn_val_color.shape}, Brightness: {cnn_val_brightness.shape}, Labels: {cnn_val_labels.shape}\")\n",
    "    print(f\"   âœ… Using augmented data with {cnn_train_color.shape[0]} samples (vs. {train_rgb.shape[0]} in original data)\")\n",
    "\n",
    "    # Train using fit() API which takes numpy arrays directly\n",
    "    print(\"\\nğŸš€ Starting training with fit() API as fallback...\")\n",
    "    try:\n",
    "        multi_channel_resnet50_history = multi_channel_resnet50_model.fit(\n",
    "            train_color_data=cnn_train_color.numpy(),\n",
    "            train_brightness_data=cnn_train_brightness.numpy(),\n",
    "            train_labels=cnn_train_labels,\n",
    "            val_color_data=cnn_val_color.numpy(),\n",
    "            val_brightness_data=cnn_val_brightness.numpy(),\n",
    "            val_labels=cnn_val_labels,\n",
    "            batch_size=512,  # Increased batch size for A100 GPU\n",
    "            epochs=num_epochs,\n",
    "            early_stopping_patience=early_stopping_patience,\n",
    "            verbose=1,\n",
    "            num_workers=0,  # Reduce worker processes to avoid CUDA initialization issues\n",
    "            pin_memory=False  # Try disabling pin_memory which can cause issues with CUDA\n",
    "        )\n",
    "    except RuntimeError as e:\n",
    "        print(f\"âš ï¸ Encountered a CUDA error: {e}\")\n",
    "        print(\"ğŸ” Attempting with alternative configuration...\")\n",
    "        \n",
    "        # Try with data already on the GPU\n",
    "        device = multi_channel_resnet50_model.device\n",
    "        \n",
    "        # Convert data to tensors on the GPU (using CNN format)\n",
    "        train_color_tensor = cnn_train_color.to(device)\n",
    "        train_brightness_tensor = cnn_train_brightness.to(device)\n",
    "        val_color_tensor = cnn_val_color.to(device)\n",
    "        val_brightness_tensor = cnn_val_brightness.to(device)\n",
    "        train_labels_tensor = torch.tensor(cnn_train_labels, device=device, dtype=torch.long)\n",
    "        val_labels_tensor = torch.tensor(cnn_val_labels, device=device, dtype=torch.long)\n",
    "        \n",
    "        # Clear any existing tqdm bars again\n",
    "        try:\n",
    "            for inst in list(getattr(tqdm, '_instances', [])):\n",
    "                try:\n",
    "                    inst.close()\n",
    "                except Exception:\n",
    "                    pass\n",
    "        except Exception:\n",
    "            pass\n",
    "            \n",
    "        # Try with direct tensor input instead of numpy arrays\n",
    "        multi_channel_resnet50_history = multi_channel_resnet50_model.fit(\n",
    "            train_color_data=train_color_tensor,\n",
    "            train_brightness_data=train_brightness_tensor,\n",
    "            train_labels=train_labels_tensor,\n",
    "            val_color_data=val_color_tensor,\n",
    "            val_brightness_data=val_brightness_tensor,\n",
    "            val_labels=val_labels_tensor,\n",
    "            batch_size=512,  # Increased batch size for A100 GPU\n",
    "            epochs=num_epochs,\n",
    "            early_stopping_patience=early_stopping_patience,\n",
    "            verbose=1,\n",
    "            num_workers=0,\n",
    "            pin_memory=False\n",
    "        )\n",
    "    \n",
    "# Plot training history\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(multi_channel_resnet50_history['accuracy'], label='Train')\n",
    "plt.plot(multi_channel_resnet50_history['val_accuracy'], label='Validation')\n",
    "plt.title('MultiChannelResNetNetwork Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(multi_channel_resnet50_history['loss'], label='Train')\n",
    "plt.plot(multi_channel_resnet50_history['val_loss'], label='Validation')\n",
    "plt.title('MultiChannelResNetNetwork Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… CNN model training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b017e5fc",
   "metadata": {},
   "source": [
    "## 15. Evaluate Models\n",
    "\n",
    "Evaluate the trained models on the test set and compare their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a8186a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Models\n",
    "print(\"ğŸ“Š Evaluating models on the test set...\")\n",
    "\n",
    "# Evaluate baseline model (which doesn't have our API)\n",
    "def evaluate_baseline_model(model, test_loader, criterion, model_name=\"Model\"):\n",
    "    \"\"\"\n",
    "    Evaluate the baseline ResNet model on the test set.\n",
    "    \n",
    "    Args:\n",
    "        model: The model to evaluate\n",
    "        test_loader: DataLoader for test data\n",
    "        criterion: Loss function\n",
    "        model_name: Name for logging\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with evaluation metrics\n",
    "    \"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "    model.eval()\n",
    "    \n",
    "    test_loss = 0.0\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        test_bar = tqdm(test_loader, desc=f\"{model_name} Testing\")\n",
    "        \n",
    "        for batch_idx, data in enumerate(test_bar):\n",
    "            # Handle different data formats - support both tuple and list types\n",
    "            if isinstance(data, (list, tuple)) and len(data) == 3:\n",
    "                rgb, _, targets = data  # Baseline only uses RGB\n",
    "            else:\n",
    "                # Handle unexpected data format\n",
    "                if not isinstance(data, (list, tuple)):\n",
    "                    raise ValueError(f\"Expected data to be list or tuple, got {type(data)}\")\n",
    "                elif len(data) != 3:\n",
    "                    raise ValueError(f\"Expected data to have 3 elements, got {len(data)}\")\n",
    "                else:\n",
    "                    rgb, targets = data\n",
    "                \n",
    "            # Move to device\n",
    "            rgb = rgb.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            # Baseline model only takes RGB\n",
    "            outputs = model(rgb)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            # Track statistics\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            test_total += targets.size(0)\n",
    "            test_correct += predicted.eq(targets).sum().item()\n",
    "            \n",
    "            # Store predictions and targets for detailed metrics\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "            \n",
    "            # Update progress bar\n",
    "            test_bar.set_postfix({\n",
    "                'loss': test_loss/(batch_idx+1), \n",
    "                'acc': 100.*test_correct/test_total\n",
    "            })\n",
    "    \n",
    "    test_acc = 100. * test_correct / test_total\n",
    "    test_loss = test_loss / len(test_loader)\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\nğŸ“ˆ {model_name} Test Results:\")\n",
    "    print(f\"   Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"   Test Accuracy: {test_acc:.2f}%\")\n",
    "    \n",
    "    # Generate classification report - using sklearn.metrics imported in the main import cell\n",
    "    report = classification_report(\n",
    "        all_targets, \n",
    "        all_predictions, \n",
    "        target_names=[CIFAR100_FINE_LABELS[i] for i in range(100)],\n",
    "        output_dict=True\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'test_loss': test_loss,\n",
    "        'test_acc': test_acc,\n",
    "        'classification_report': report,\n",
    "        'predictions': all_predictions,\n",
    "        'targets': all_targets\n",
    "    }\n",
    "\n",
    "# Evaluate baseline model (doesn't have our API)\n",
    "baseline_results = evaluate_baseline_model(\n",
    "    model=baseline_model,\n",
    "    test_loader=test_loader,\n",
    "    criterion=baseline_criterion,\n",
    "    model_name=\"Baseline ResNet50\"\n",
    ")\n",
    "\n",
    "# Extract test data for evaluation using our API\n",
    "test_color_tensors = []\n",
    "test_brightness_tensors = []\n",
    "test_label_tensors = []\n",
    "\n",
    "# Safely extract data from test_loader, handling both tuple and list formats\n",
    "for data in test_loader:\n",
    "    # Handle different data formats - support both tuple and list types\n",
    "    if isinstance(data, (list, tuple)) and len(data) == 3:\n",
    "        rgb, brightness, labels = data\n",
    "    else:\n",
    "        # Handle unexpected data format\n",
    "        if not isinstance(data, (list, tuple)):\n",
    "            raise ValueError(f\"Expected data to be list or tuple, got {type(data)}\")\n",
    "        elif len(data) != 3:\n",
    "            raise ValueError(f\"Expected data to have 3 elements, got {len(data)}\")\n",
    "        else:\n",
    "            raise ValueError(\"Unexpected data format from test_loader\")\n",
    "    \n",
    "    test_color_tensors.append(rgb)\n",
    "    test_brightness_tensors.append(brightness)\n",
    "    test_label_tensors.append(labels)\n",
    "\n",
    "test_color_data = torch.cat(test_color_tensors, dim=0).cpu().numpy()\n",
    "test_brightness_data = torch.cat(test_brightness_tensors, dim=0).cpu().numpy()\n",
    "test_labels = torch.cat(test_label_tensors, dim=0).cpu().numpy()\n",
    "\n",
    "# Reshape the data for dense models\n",
    "print(\"\\nğŸ”„ Preparing test data for evaluation...\")\n",
    "test_color_data_reshaped = test_color_data.reshape(test_color_data.shape[0], -1)\n",
    "test_brightness_data_reshaped = test_brightness_data.reshape(test_brightness_data.shape[0], -1)\n",
    "\n",
    "print(f\"   Original RGB test data shape: {test_color_data.shape}\")\n",
    "print(f\"   Original brightness test data shape: {test_brightness_data.shape}\")\n",
    "print(f\"   Reshaped RGB test data shape: {test_color_data_reshaped.shape}\")\n",
    "print(f\"   Reshaped brightness test data shape: {test_brightness_data_reshaped.shape}\")\n",
    "\n",
    "# Evaluate BaseMultiChannelNetwork (use reshaped data for dense model)\n",
    "print(\"\\nğŸ“Š Evaluating multi-stream models using built-in API...\")\n",
    "print(\"   Evaluating BaseMultiChannelNetwork (dense model)...\")\n",
    "\n",
    "# Verify the input shapes match the model expectations\n",
    "base_model_rgb_size = base_multi_channel_large_model.color_input_size\n",
    "base_model_brightness_size = base_multi_channel_large_model.brightness_input_size\n",
    "\n",
    "if test_color_data_reshaped.shape[1] != base_model_rgb_size:\n",
    "    print(f\"âš ï¸ Warning: Test RGB shape {test_color_data_reshaped.shape[1]} doesn't match model expected size {base_model_rgb_size}\")\n",
    "if test_brightness_data_reshaped.shape[1] != base_model_brightness_size:\n",
    "    print(f\"âš ï¸ Warning: Test brightness shape {test_brightness_data_reshaped.shape[1]} doesn't match model expected size {base_model_brightness_size}\")\n",
    "\n",
    "# Evaluate BaseMultiChannelNetwork using reshaped data\n",
    "base_multi_channel_large_results = base_multi_channel_large_model.evaluate(\n",
    "    test_color_data=test_color_data_reshaped,  # Use reshaped (flattened) data for dense model\n",
    "    test_brightness_data=test_brightness_data_reshaped,  # Use reshaped data\n",
    "    test_labels=test_labels,\n",
    "    batch_size=512  # Increased batch size for A100 GPU\n",
    ")\n",
    "\n",
    "# Convert results for consistency with baseline format\n",
    "base_multi_channel_large_results['test_acc'] = base_multi_channel_large_results['accuracy'] * 100\n",
    "\n",
    "# Check if the ResNet model is CNN-based\n",
    "is_resnet_cnn = hasattr(multi_channel_resnet50_model, 'is_cnn') and multi_channel_resnet50_model.is_cnn\n",
    "if not hasattr(multi_channel_resnet50_model, 'is_cnn'):\n",
    "    is_resnet_cnn = 'resnet' in multi_channel_resnet50_model.__class__.__name__.lower()\n",
    "\n",
    "# Choose the appropriate data format for ResNet model\n",
    "print(f\"   Evaluating MultiChannelResNetNetwork ({'CNN' if is_resnet_cnn else 'dense'} model)...\")\n",
    "\n",
    "if is_resnet_cnn:\n",
    "    # Use original shape for CNN models\n",
    "    resnet_test_color = test_color_data\n",
    "    resnet_test_brightness = test_brightness_data\n",
    "    print(f\"   Using original image format (N,C,H,W) for CNN model\")\n",
    "else:\n",
    "    # Use reshaped data for dense models\n",
    "    resnet_test_color = test_color_data_reshaped\n",
    "    resnet_test_brightness = test_brightness_data_reshaped\n",
    "    print(f\"   Using reshaped format (N,C*H*W) for dense model\")\n",
    "\n",
    "# Evaluate MultiChannelResNetNetwork with appropriate data format\n",
    "multi_channel_resnet50_results = multi_channel_resnet50_model.evaluate(\n",
    "    test_color_data=resnet_test_color,\n",
    "    test_brightness_data=resnet_test_brightness,\n",
    "    test_labels=test_labels,\n",
    "    batch_size=512  # Increased batch size for A100 GPU\n",
    ")\n",
    "\n",
    "# Convert results for consistency with baseline format\n",
    "multi_channel_resnet50_results['test_acc'] = multi_channel_resnet50_results['accuracy'] * 100\n",
    "\n",
    "# Compare models\n",
    "print(\"\\nğŸ” Model Comparison on Test Set:\")\n",
    "print(f\"   Baseline ResNet50: {baseline_results['test_acc']:.2f}%\")\n",
    "print(f\"   BaseMultiChannelNetwork: {base_multi_channel_large_results['test_acc']:.2f}%\")\n",
    "print(f\"   MultiChannelResNetNetwork: {multi_channel_resnet50_results['test_acc']:.2f}%\")\n",
    "\n",
    "# Calculate improvement over baseline\n",
    "base_improvement = base_multi_channel_large_results['test_acc'] - baseline_results['test_acc']\n",
    "resnet_improvement = multi_channel_resnet50_results['test_acc'] - baseline_results['test_acc']\n",
    "\n",
    "print(f\"\\nğŸ“Š Improvement over Baseline:\")\n",
    "print(f\"   BaseMultiChannelNetwork: {base_improvement:.2f}% points\")\n",
    "print(f\"   MultiChannelResNetNetwork: {resnet_improvement:.2f}% points\")\n",
    "\n",
    "# Visualize training curves comparison for all models\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Convert baseline history keys to match multi-stream model history keys\n",
    "baseline_history_converted = {\n",
    "    'train_accuracy': [acc/100 for acc in baseline_history['train_acc']],\n",
    "    'val_accuracy': [acc/100 for acc in baseline_history['val_acc']],\n",
    "    'train_loss': baseline_history['train_loss'],\n",
    "    'val_loss': baseline_history['val_loss']\n",
    "}\n",
    "\n",
    "# Accuracy curves\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(baseline_history_converted['train_accuracy'], label='Baseline Train')\n",
    "plt.plot(baseline_history_converted['val_accuracy'], label='Baseline Val')\n",
    "plt.plot(base_multi_channel_large_history['train_accuracy'], label='Dense Multi-Stream Train')\n",
    "plt.plot(base_multi_channel_large_history['val_accuracy'], label='Dense Multi-Stream Val')\n",
    "plt.plot(multi_channel_resnet50_history['train_accuracy'], label='ResNet Multi-Stream Train')\n",
    "plt.plot(multi_channel_resnet50_history['val_accuracy'], label='ResNet Multi-Stream Val')\n",
    "plt.title('Model Accuracy Comparison')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Loss curves\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(baseline_history_converted['train_loss'], label='Baseline Train')\n",
    "plt.plot(baseline_history_converted['val_loss'], label='Baseline Val')\n",
    "plt.plot(base_multi_channel_large_history['train_loss'], label='Dense Multi-Stream Train')\n",
    "plt.plot(base_multi_channel_large_history['val_loss'], label='Dense Multi-Stream Val')\n",
    "plt.plot(multi_channel_resnet50_history['train_loss'], label='ResNet Multi-Stream Train')\n",
    "plt.plot(multi_channel_resnet50_history['val_loss'], label='ResNet Multi-Stream Val')\n",
    "plt.title('Model Loss Comparison')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Additional pathway analysis for multi-stream models\n",
    "print(\"\\nğŸ”¬ Multi-Stream Pathway Analysis:\")\n",
    "\n",
    "# Analyze pathway importance for multi-stream models\n",
    "base_pathway_importance = base_multi_channel_large_model.analyze_pathway_weights()\n",
    "resnet_pathway_importance = multi_channel_resnet50_model.analyze_pathway_weights()\n",
    "\n",
    "print(\"BaseMultiChannelNetwork Pathway Importance:\")\n",
    "for key, value in base_pathway_importance.items():\n",
    "    print(f\"   {key}: {value:.4f}\")\n",
    "    \n",
    "print(\"\\nMultiChannelResNetNetwork Pathway Importance:\")\n",
    "for key, value in resnet_pathway_importance.items():\n",
    "    print(f\"   {key}: {value:.4f}\")\n",
    "\n",
    "print(\"\\nâœ… Model evaluation complete!\")\n",
    "\n",
    "# Summarize overfitting analysis\n",
    "print(\"\\nğŸ” Overfitting Analysis:\")\n",
    "for model_name, history in [\n",
    "    (\"Baseline ResNet50\", baseline_history_converted),\n",
    "    (\"BaseMultiChannelNetwork\", base_multi_channel_large_history),\n",
    "    (\"MultiChannelResNetNetwork\", multi_channel_resnet50_history)\n",
    "]:\n",
    "    # Convert to numpy arrays for calculations, handling different key formats\n",
    "    if 'train_acc' in history:\n",
    "        train_acc = np.array(history['train_acc'])\n",
    "        val_acc = np.array(history['val_acc'])\n",
    "    else:\n",
    "        train_acc = np.array(history['train_accuracy']) * 100  # Convert to percentage\n",
    "        val_acc = np.array(history['val_accuracy']) * 100  # Convert to percentage\n",
    "    \n",
    "    gap = train_acc - val_acc\n",
    "    last_gap = gap[-1]\n",
    "    max_gap = np.max(gap)\n",
    "    avg_last_5 = np.mean(gap[-5:]) if len(gap) >= 5 else np.mean(gap)\n",
    "    \n",
    "    print(f\"\\n   {model_name}:\")\n",
    "    print(f\"      Final train-val gap: {last_gap:.2f}%\")\n",
    "    print(f\"      Maximum gap during training: {max_gap:.2f}%\")\n",
    "    print(f\"      Average gap in last 5 epochs: {avg_last_5:.2f}%\")\n",
    "    \n",
    "    # Evaluate overfitting level\n",
    "    if avg_last_5 < 3:\n",
    "        print(f\"      âœ… No significant overfitting (gap < 3%)\")\n",
    "    elif avg_last_5 < 7:\n",
    "        print(f\"      âš ï¸ Mild overfitting (3% â‰¤ gap < 7%)\")\n",
    "    elif avg_last_5 < 15:\n",
    "        print(f\"      ğŸ”´ Moderate overfitting (7% â‰¤ gap < 15%)\")\n",
    "    else:\n",
    "        print(f\"      âŒ Severe overfitting (gap â‰¥ 15%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483e3a42",
   "metadata": {},
   "source": [
    "## 16. Pathway Analysis\n",
    "\n",
    "Analyze the contribution of each pathway (RGB and brightness) to the model's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702b32d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pathway Analysis\n",
    "print(\"ğŸ” Analyzing pathway contributions for multi-stream models...\")\n",
    "\n",
    "def analyze_pathways(model, test_loader, num_samples=100, model_name=\"Model\"):\n",
    "    \"\"\"\n",
    "    Analyze contributions of RGB and brightness pathways.\n",
    "    \n",
    "    Args:\n",
    "        model: The multi-stream model to analyze\n",
    "        test_loader: DataLoader for test data\n",
    "        num_samples: Number of samples to analyze\n",
    "        model_name: Name for logging\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with pathway analysis results\n",
    "    \"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "    model.eval()\n",
    "    \n",
    "    # Check if model is CNN-based or dense\n",
    "    is_cnn_model = hasattr(model, 'is_cnn') and model.is_cnn\n",
    "    if not hasattr(model, 'is_cnn'):\n",
    "        is_cnn_model = 'resnet' in model.__class__.__name__.lower()\n",
    "    \n",
    "    print(f\"   Model type: {'CNN' if is_cnn_model else 'Dense'}\")\n",
    "    \n",
    "    all_rgb = []\n",
    "    all_brightness = []\n",
    "    all_targets = []\n",
    "    all_combined_outputs = []\n",
    "    all_rgb_outputs = []\n",
    "    all_brightness_outputs = []\n",
    "    \n",
    "    sample_count = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            # Break when we have enough samples\n",
    "            if sample_count >= num_samples:\n",
    "                break\n",
    "            \n",
    "            # Handle different data formats - support both tuple and list types\n",
    "            if isinstance(data, (list, tuple)) and len(data) == 3:\n",
    "                rgb, brightness, targets = data\n",
    "            else:\n",
    "                # Handle unexpected data format\n",
    "                if not isinstance(data, (list, tuple)):\n",
    "                    raise ValueError(f\"Expected data to be list or tuple, got {type(data)}\")\n",
    "                elif len(data) != 3:\n",
    "                    raise ValueError(f\"Expected data to have 3 elements, got {len(data)}\")\n",
    "                else:\n",
    "                    raise ValueError(\"Unexpected data format from test_loader\")\n",
    "            \n",
    "            # Collect only the samples we need\n",
    "            remaining = num_samples - sample_count\n",
    "            if remaining < len(rgb):\n",
    "                rgb = rgb[:remaining]\n",
    "                brightness = brightness[:remaining]\n",
    "                targets = targets[:remaining]\n",
    "            \n",
    "            # Move to device\n",
    "            rgb, brightness, targets = rgb.to(device), brightness.to(device), targets.to(device)\n",
    "            \n",
    "            # For dense models, we need to reshape the data\n",
    "            if not is_cnn_model:\n",
    "                # Reshape (N, C, H, W) to (N, C*H*W)\n",
    "                rgb_reshaped = rgb.reshape(rgb.size(0), -1)\n",
    "                brightness_reshaped = brightness.reshape(brightness.size(0), -1)\n",
    "                \n",
    "                # Get outputs from combined and individual pathways\n",
    "                combined_outputs = model(rgb_reshaped, brightness_reshaped)\n",
    "                \n",
    "                # Use the analyze_pathways method to get individual pathway outputs\n",
    "                rgb_outputs, brightness_outputs = model.analyze_pathways(rgb_reshaped, brightness_reshaped)\n",
    "            else:\n",
    "                # CNN models use the original shape\n",
    "                combined_outputs = model(rgb, brightness)\n",
    "                rgb_outputs, brightness_outputs = model.analyze_pathways(rgb, brightness)\n",
    "            \n",
    "            all_rgb.append(rgb.cpu())\n",
    "            all_brightness.append(brightness.cpu())\n",
    "            all_targets.append(targets.cpu())\n",
    "            all_combined_outputs.append(combined_outputs.cpu())\n",
    "            all_rgb_outputs.append(rgb_outputs.cpu())\n",
    "            all_brightness_outputs.append(brightness_outputs.cpu())\n",
    "            \n",
    "            sample_count += len(rgb)\n",
    "    \n",
    "    # Concatenate all data\n",
    "    all_rgb = torch.cat(all_rgb)\n",
    "    all_brightness = torch.cat(all_brightness)\n",
    "    all_targets = torch.cat(all_targets)\n",
    "    all_combined_outputs = torch.cat(all_combined_outputs)\n",
    "    all_rgb_outputs = torch.cat(all_rgb_outputs)\n",
    "    all_brightness_outputs = torch.cat(all_brightness_outputs)\n",
    "    \n",
    "    # Calculate accuracy for each pathway\n",
    "    _, combined_preds = all_combined_outputs.max(1)\n",
    "    _, rgb_preds = all_rgb_outputs.max(1)\n",
    "    _, brightness_preds = all_brightness_outputs.max(1)\n",
    "    \n",
    "    combined_acc = 100. * (combined_preds == all_targets).sum().item() / len(all_targets)\n",
    "    rgb_acc = 100. * (rgb_preds == all_targets).sum().item() / len(all_targets)\n",
    "    brightness_acc = 100. * (brightness_preds == all_targets).sum().item() / len(all_targets)\n",
    "    \n",
    "    print(f\"\\nğŸ“Š {model_name} Pathway Analysis:\")\n",
    "    print(f\"   Combined accuracy: {combined_acc:.2f}%\")\n",
    "    print(f\"   RGB pathway accuracy: {rgb_acc:.2f}%\")\n",
    "    print(f\"   Brightness pathway accuracy: {brightness_acc:.2f}%\")\n",
    "    \n",
    "    # Calculate pathway agreement\n",
    "    rgb_brightness_agreement = 100. * (rgb_preds == brightness_preds).sum().item() / len(all_targets)\n",
    "    combined_rgb_agreement = 100. * (combined_preds == rgb_preds).sum().item() / len(all_targets)\n",
    "    combined_brightness_agreement = 100. * (combined_preds == brightness_preds).sum().item() / len(all_targets)\n",
    "    \n",
    "    print(f\"\\nğŸ¤ Pathway Agreement:\")\n",
    "    print(f\"   RGB-Brightness agreement: {rgb_brightness_agreement:.2f}%\")\n",
    "    print(f\"   Combined-RGB agreement: {combined_rgb_agreement:.2f}%\")\n",
    "    print(f\"   Combined-Brightness agreement: {combined_brightness_agreement:.2f}%\")\n",
    "    \n",
    "    return {\n",
    "        'combined_acc': combined_acc,\n",
    "        'rgb_acc': rgb_acc,\n",
    "        'brightness_acc': brightness_acc,\n",
    "        'rgb_brightness_agreement': rgb_brightness_agreement,\n",
    "        'combined_rgb_agreement': combined_rgb_agreement,\n",
    "        'combined_brightness_agreement': combined_brightness_agreement\n",
    "    }\n",
    "\n",
    "# Analyze BaseMultiChannelNetwork\n",
    "base_multi_channel_large_pathway_analysis = analyze_pathways(\n",
    "    model=base_multi_channel_large_model,\n",
    "    test_loader=test_loader,\n",
    "    num_samples=500,  # Increased sample size for A100 GPU\n",
    "    model_name=\"BaseMultiChannelNetwork\"\n",
    ")\n",
    "\n",
    "# Analyze MultiChannelResNetNetwork\n",
    "multi_channel_resnet50_pathway_analysis = analyze_pathways(\n",
    "    model=multi_channel_resnet50_model,\n",
    "    test_loader=test_loader,\n",
    "    num_samples=500,  # Increased sample size for A100 GPU\n",
    "    model_name=\"MultiChannelResNetNetwork\"\n",
    ")\n",
    "\n",
    "# Visualize Model Predictions and Pathway Analysis\n",
    "print(\"ğŸ” Visualizing model predictions and pathway analysis...\")\n",
    "\n",
    "# Get a batch of test data\n",
    "test_batch = next(iter(test_loader))\n",
    "test_rgb, test_brightness, test_labels = test_batch\n",
    "test_rgb = test_rgb.to(device)\n",
    "test_brightness = test_brightness.to(device)\n",
    "test_labels = test_labels.to(device)\n",
    "\n",
    "# Determine model types\n",
    "base_is_cnn = hasattr(base_multi_channel_large_model, 'is_cnn') and base_multi_channel_large_model.is_cnn\n",
    "if not hasattr(base_multi_channel_large_model, 'is_cnn'):\n",
    "    base_is_cnn = 'resnet' in base_multi_channel_large_model.__class__.__name__.lower()\n",
    "\n",
    "resnet_is_cnn = hasattr(multi_channel_resnet50_model, 'is_cnn') and multi_channel_resnet50_model.is_cnn\n",
    "if not hasattr(multi_channel_resnet50_model, 'is_cnn'):\n",
    "    resnet_is_cnn = 'resnet' in multi_channel_resnet50_model.__class__.__name__.lower()\n",
    "\n",
    "# Get predictions from all models\n",
    "with torch.no_grad():\n",
    "    # Baseline model only takes RGB\n",
    "    baseline_outputs = baseline_model(test_rgb)\n",
    "    baseline_probs = torch.softmax(baseline_outputs, dim=1)\n",
    "    baseline_preds = torch.argmax(baseline_probs, dim=1)\n",
    "    \n",
    "    # Multi-stream models need proper reshaping for dense models\n",
    "    # For base_multi_channel_large_model (usually dense)\n",
    "    if not base_is_cnn:\n",
    "        # Reshape for dense model\n",
    "        test_rgb_reshaped = test_rgb.reshape(test_rgb.size(0), -1)\n",
    "        test_brightness_reshaped = test_brightness.reshape(test_brightness.size(0), -1)\n",
    "        \n",
    "        # Forward pass with reshaped data\n",
    "        base_multi_channel_outputs = base_multi_channel_large_model(test_rgb_reshaped, test_brightness_reshaped)\n",
    "        base_color_outputs, base_brightness_outputs = base_multi_channel_large_model.analyze_pathways(\n",
    "            test_rgb_reshaped, test_brightness_reshaped\n",
    "        )\n",
    "    else:\n",
    "        # CNN model uses original shape\n",
    "        base_multi_channel_outputs = base_multi_channel_large_model(test_rgb, test_brightness)\n",
    "        base_color_outputs, base_brightness_outputs = base_multi_channel_large_model.analyze_pathways(\n",
    "            test_rgb, test_brightness\n",
    "        )\n",
    "    \n",
    "    # For multi_channel_resnet50_model (usually CNN)\n",
    "    if not resnet_is_cnn:\n",
    "        # Reshape for dense model\n",
    "        test_rgb_reshaped = test_rgb.reshape(test_rgb.size(0), -1)\n",
    "        test_brightness_reshaped = test_brightness.reshape(test_brightness.size(0), -1)\n",
    "        \n",
    "        # Forward pass with reshaped data\n",
    "        multi_channel_resnet_outputs = multi_channel_resnet50_model(test_rgb_reshaped, test_brightness_reshaped)\n",
    "        resnet_color_outputs, resnet_brightness_outputs = multi_channel_resnet50_model.analyze_pathways(\n",
    "            test_rgb_reshaped, test_brightness_reshaped\n",
    "        )\n",
    "    else:\n",
    "        # CNN model uses original shape\n",
    "        multi_channel_resnet_outputs = multi_channel_resnet50_model(test_rgb, test_brightness)\n",
    "        resnet_color_outputs, resnet_brightness_outputs = multi_channel_resnet50_model.analyze_pathways(\n",
    "            test_rgb, test_brightness\n",
    "        )\n",
    "    \n",
    "    # Calculate probabilities and predictions\n",
    "    base_multi_channel_probs = torch.softmax(base_multi_channel_outputs, dim=1)\n",
    "    base_multi_channel_preds = torch.argmax(base_multi_channel_probs, dim=1)\n",
    "    \n",
    "    multi_channel_resnet_probs = torch.softmax(multi_channel_resnet_outputs, dim=1)\n",
    "    multi_channel_resnet_preds = torch.argmax(multi_channel_resnet_probs, dim=1)\n",
    "    \n",
    "    base_color_probs = torch.softmax(base_color_outputs, dim=1)\n",
    "    base_brightness_probs = torch.softmax(base_brightness_outputs, dim=1)\n",
    "    base_color_preds = torch.argmax(base_color_probs, dim=1)\n",
    "    base_brightness_preds = torch.argmax(base_brightness_probs, dim=1)\n",
    "    \n",
    "    resnet_color_probs = torch.softmax(resnet_color_outputs, dim=1)\n",
    "    resnet_brightness_probs = torch.softmax(resnet_brightness_outputs, dim=1)\n",
    "    resnet_color_preds = torch.argmax(resnet_color_probs, dim=1)\n",
    "    resnet_brightness_preds = torch.argmax(resnet_brightness_probs, dim=1)\n",
    "\n",
    "# Move to CPU for visualization\n",
    "test_rgb = test_rgb.cpu()\n",
    "test_brightness = test_brightness.cpu()\n",
    "test_labels = test_labels.cpu()\n",
    "baseline_preds = baseline_preds.cpu()\n",
    "base_multi_channel_preds = base_multi_channel_preds.cpu()\n",
    "multi_channel_resnet_preds = multi_channel_resnet_preds.cpu()\n",
    "base_color_preds = base_color_preds.cpu()\n",
    "base_brightness_preds = base_brightness_preds.cpu()\n",
    "resnet_color_preds = resnet_color_preds.cpu()\n",
    "resnet_brightness_preds = resnet_brightness_preds.cpu()\n",
    "\n",
    "# Select a subset of samples for visualization\n",
    "n_samples = min(8, test_rgb.size(0))\n",
    "sample_indices = np.random.choice(test_rgb.size(0), n_samples, replace=False)\n",
    "\n",
    "# Plot original images, model predictions, and pathway contributions\n",
    "fig, axes = plt.subplots(n_samples, 6, figsize=(18, 3*n_samples))\n",
    "if n_samples == 1:\n",
    "    axes = axes.reshape(1, -1)\n",
    "\n",
    "for i, idx in enumerate(sample_indices):\n",
    "    # Get RGB image\n",
    "    rgb_img = test_rgb[idx].permute(1, 2, 0).numpy()\n",
    "    rgb_img = np.clip(rgb_img, 0, 1)\n",
    "    \n",
    "    # Get brightness image\n",
    "    brightness_img = test_brightness[idx, 0].numpy()\n",
    "    \n",
    "    # True label and predictions\n",
    "    true_label = test_labels[idx].item()\n",
    "    baseline_pred = baseline_preds[idx].item()\n",
    "    multi_channel_pred = multi_channel_resnet_preds[idx].item()\n",
    "    color_pathway_pred = resnet_color_preds[idx].item()\n",
    "    brightness_pathway_pred = resnet_brightness_preds[idx].item()\n",
    "    \n",
    "    # Plot original RGB image\n",
    "    axes[i, 0].imshow(rgb_img)\n",
    "    axes[i, 0].set_title(f\"True: {CIFAR100_FINE_LABELS[true_label]}\")\n",
    "    axes[i, 0].axis('off')\n",
    "    \n",
    "    # Plot brightness image\n",
    "    axes[i, 1].imshow(brightness_img, cmap='gray')\n",
    "    axes[i, 1].set_title(\"Brightness Channel\")\n",
    "    axes[i, 1].axis('off')\n",
    "    \n",
    "    # Plot baseline model prediction\n",
    "    axes[i, 2].imshow(rgb_img)\n",
    "    correct = true_label == baseline_pred\n",
    "    title_color = 'green' if correct else 'red'\n",
    "    axes[i, 2].set_title(f\"Baseline: {CIFAR100_FINE_LABELS[baseline_pred]}\", color=title_color)\n",
    "    axes[i, 2].axis('off')\n",
    "    \n",
    "    # Plot multi-stream model prediction\n",
    "    axes[i, 3].imshow(rgb_img)\n",
    "    correct = true_label == multi_channel_pred\n",
    "    title_color = 'green' if correct else 'red'\n",
    "    axes[i, 3].set_title(f\"Multi-Stream: {CIFAR100_FINE_LABELS[multi_channel_pred]}\", color=title_color)\n",
    "    axes[i, 3].axis('off')\n",
    "    \n",
    "    # Plot color pathway prediction\n",
    "    axes[i, 4].imshow(rgb_img)\n",
    "    correct = true_label == color_pathway_pred\n",
    "    title_color = 'green' if correct else 'red'\n",
    "    axes[i, 4].set_title(f\"Color Path: {CIFAR100_FINE_LABELS[color_pathway_pred]}\", color=title_color)\n",
    "    axes[i, 4].axis('off')\n",
    "    \n",
    "    # Plot brightness pathway prediction\n",
    "    axes[i, 5].imshow(brightness_img, cmap='gray')\n",
    "    correct = true_label == brightness_pathway_pred\n",
    "    title_color = 'green' if correct else 'red'\n",
    "    axes[i, 5].set_title(f\"Brightness Path: {CIFAR100_FINE_LABELS[brightness_pathway_pred]}\", color=title_color)\n",
    "    axes[i, 5].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualize pathway feature importance\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Get pathway importance\n",
    "base_importance = base_multi_channel_large_model.get_pathway_importance()\n",
    "resnet_importance = multi_channel_resnet50_model.get_pathway_importance()\n",
    "\n",
    "# Plot importance values\n",
    "labels = ['Color Pathway', 'Brightness Pathway']\n",
    "base_values = [base_importance['color_pathway'], base_importance['brightness_pathway']]\n",
    "resnet_values = [resnet_importance['color_pathway'], resnet_importance['brightness_pathway']]\n",
    "\n",
    "x = np.arange(len(labels))\n",
    "width = 0.35\n",
    "\n",
    "plt.bar(x - width/2, base_values, width, label='BaseMultiChannelNetwork')\n",
    "plt.bar(x + width/2, resnet_values, width, label='MultiChannelResNetNetwork')\n",
    "\n",
    "plt.ylabel('Relative Importance')\n",
    "plt.title('Pathway Importance Analysis')\n",
    "plt.xticks(x, labels)\n",
    "plt.ylim(0, 1)\n",
    "plt.legend()\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… Visualization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55adcd79",
   "metadata": {},
   "source": [
    "## 17. Save Models\n",
    "\n",
    "Save the trained models for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89734e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Models\n",
    "print(\"ğŸ’¾ Saving trained models...\")\n",
    "\n",
    "# Create models directory if it doesn't exist\n",
    "models_dir = './models'\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "# Save baseline model (using PyTorch standard method)\n",
    "baseline_path = os.path.join(models_dir, 'baseline_resnet50.pth')\n",
    "torch.save(baseline_model.state_dict(), baseline_path)\n",
    "print(f\"âœ… Baseline ResNet50 saved to {baseline_path}\")\n",
    "\n",
    "# Save multi-stream models using their built-in save_model method\n",
    "base_multi_channel_path = os.path.join(models_dir, 'base_multi_channel_large.pth')\n",
    "base_multi_channel_large_model.save_model(base_multi_channel_path)\n",
    "print(f\"âœ… BaseMultiChannelNetwork saved to {base_multi_channel_path}\")\n",
    "\n",
    "multi_channel_resnet_path = os.path.join(models_dir, 'multi_channel_resnet50.pth')\n",
    "multi_channel_resnet50_model.save_model(multi_channel_resnet_path)\n",
    "print(f\"âœ… MultiChannelResNetNetwork saved to {multi_channel_resnet_path}\")\n",
    "\n",
    "# Save training histories for future reference\n",
    "history_dir = os.path.join(models_dir, 'history')\n",
    "os.makedirs(history_dir, exist_ok=True)\n",
    "\n",
    "# Convert torch tensors to lists for JSON serialization\n",
    "def prepare_for_json(history):\n",
    "    json_history = {}\n",
    "    for key, value in history.items():\n",
    "        if isinstance(value, list):\n",
    "            # Check if list contains tensors\n",
    "            if value and isinstance(value[0], torch.Tensor):\n",
    "                json_history[key] = [float(v.item()) for v in value]\n",
    "            else:\n",
    "                # Convert all values to float for consistency\n",
    "                json_history[key] = [float(v) if not isinstance(v, (list, tuple)) else v for v in value]\n",
    "        else:\n",
    "            json_history[key] = value\n",
    "    return json_history\n",
    "\n",
    "# Baseline history conversion\n",
    "json_baseline_history = {}\n",
    "for key, value in baseline_history.items():\n",
    "    json_baseline_history[key] = [float(v) for v in value]\n",
    "    \n",
    "# Save histories\n",
    "with open(os.path.join(history_dir, 'baseline_history.json'), 'w') as f:\n",
    "    json.dump(json_baseline_history, f)\n",
    "\n",
    "with open(os.path.join(history_dir, 'base_multi_channel_history.json'), 'w') as f:\n",
    "    json.dump(prepare_for_json(base_multi_channel_large_history), f)\n",
    "    \n",
    "with open(os.path.join(history_dir, 'multi_channel_resnet_history.json'), 'w') as f:\n",
    "    json.dump(prepare_for_json(multi_channel_resnet50_history), f)\n",
    "\n",
    "print(f\"âœ… Training histories saved to {history_dir}\")\n",
    "\n",
    "# Save model metadata for easier reloading\n",
    "metadata = {\n",
    "    'baseline': {\n",
    "        'model_type': 'CifarResNet50',\n",
    "        'num_classes': 100,\n",
    "        'parameters': baseline_params,\n",
    "        'path': baseline_path\n",
    "    },\n",
    "    'base_multi_channel_large': {\n",
    "        'model_type': 'BaseMultiChannelNetwork',\n",
    "        'color_input_size': rgb_input_size,\n",
    "        'brightness_input_size': brightness_input_size,\n",
    "        'num_classes': 100,\n",
    "        'parameters': large_dense_params,\n",
    "        'path': base_multi_channel_path\n",
    "    },\n",
    "    'multi_channel_resnet50': {\n",
    "        'model_type': 'MultiChannelResNetNetwork',\n",
    "        'color_input_channels': input_channels_rgb,\n",
    "        'brightness_input_channels': input_channels_brightness,\n",
    "        'num_classes': 100,\n",
    "        'parameters': resnet50_params,\n",
    "        'path': multi_channel_resnet_path\n",
    "    },\n",
    "    'dataset': 'CIFAR-100',\n",
    "    'training_date': time.strftime('%Y-%m-%d')\n",
    "}\n",
    "\n",
    "with open(os.path.join(models_dir, 'model_metadata.json'), 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(f\"âœ… Model metadata saved to {os.path.join(models_dir, 'model_metadata.json')}\")\n",
    "print(\"\\nğŸ’¾ All models and training data saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fdbb77",
   "metadata": {},
   "source": [
    "## 18. Summary\n",
    "\n",
    "Summarize the results and findings from our multi-stream neural network experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334c5338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "print(\"ğŸ“‹ Multi-Stream Neural Networks CIFAR-100 Training Summary\")\n",
    "\n",
    "# Training summary\n",
    "print(\"\\nğŸ‹ï¸â€â™€ï¸ Training Results:\")\n",
    "print(f\"   Baseline ResNet50 final validation accuracy: {baseline_history['val_acc'][-1]:.2f}%\")\n",
    "print(f\"   BaseMultiChannelNetwork final validation accuracy: {base_multi_channel_large_history['val_acc'][-1]:.2f}%\")\n",
    "print(f\"   MultiChannelResNetNetwork final validation accuracy: {multi_channel_resnet50_history['val_acc'][-1]:.2f}%\")\n",
    "\n",
    "# Testing summary\n",
    "print(\"\\nğŸ§ª Testing Results:\")\n",
    "print(f\"   Baseline ResNet50 test accuracy: {baseline_results['test_acc']:.2f}%\")\n",
    "print(f\"   BaseMultiChannelNetwork test accuracy: {base_multi_channel_large_results['test_acc']:.2f}%\")\n",
    "print(f\"   MultiChannelResNetNetwork test accuracy: {multi_channel_resnet50_results['test_acc']:.2f}%\")\n",
    "\n",
    "# Pathway analysis summary\n",
    "print(\"\\nğŸ” Pathway Analysis Summary:\")\n",
    "print(\"   BaseMultiChannelNetwork:\")\n",
    "print(f\"      Combined accuracy: {base_multi_channel_large_pathway_analysis['combined_acc']:.2f}%\")\n",
    "print(f\"      RGB pathway: {base_multi_channel_large_pathway_analysis['rgb_acc']:.2f}%, Brightness pathway: {base_multi_channel_large_pathway_analysis['brightness_acc']:.2f}%\")\n",
    "\n",
    "print(\"   MultiChannelResNetNetwork:\")\n",
    "print(f\"      Combined accuracy: {multi_channel_resnet50_pathway_analysis['combined_acc']:.2f}%\")\n",
    "print(f\"      RGB pathway: {multi_channel_resnet50_pathway_analysis['rgb_acc']:.2f}%, Brightness pathway: {multi_channel_resnet50_pathway_analysis['brightness_acc']:.2f}%\")\n",
    "\n",
    "# Create a summary table\n",
    "summary_data = {\n",
    "    'Model': ['Baseline ResNet50', 'BaseMultiChannelNetwork', 'MultiChannelResNetNetwork'],\n",
    "    'Test Acc (%)': [\n",
    "        f\"{baseline_results['test_acc']:.2f}\",\n",
    "        f\"{base_multi_channel_large_results['test_acc']:.2f}\",\n",
    "        f\"{multi_channel_resnet50_results['test_acc']:.2f}\"\n",
    "    ],\n",
    "    'RGB Pathway (%)': [\n",
    "        'N/A',\n",
    "        f\"{base_multi_channel_large_pathway_analysis['rgb_acc']:.2f}\",\n",
    "        f\"{multi_channel_resnet50_pathway_analysis['rgb_acc']:.2f}\"\n",
    "    ],\n",
    "    'Brightness Pathway (%)': [\n",
    "        'N/A',\n",
    "        f\"{base_multi_channel_large_pathway_analysis['brightness_acc']:.2f}\",\n",
    "        f\"{multi_channel_resnet50_pathway_analysis['brightness_acc']:.2f}\"\n",
    "    ],\n",
    "    'Parameters': [\n",
    "        f\"{sum(p.numel() for p in baseline_model.parameters()):,}\",\n",
    "        f\"{large_dense_params:,}\",\n",
    "        f\"{resnet50_params:,}\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Use pandas to create a nice table\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "display(summary_df)\n",
    "\n",
    "print(\"\\nğŸ“ Key Findings:\")\n",
    "print(\"   1. Multi-stream models can leverage both RGB and brightness information\")\n",
    "print(\"   2. The RGB pathway typically contributes more to accuracy than brightness\")\n",
    "print(\"   3. The combined model performs better than individual pathways\")\n",
    "print(\"   4. MultiChannelResNetNetwork architecture is more powerful but requires more parameters\")\n",
    "\n",
    "print(\"\\nğŸ¯ Next Steps:\")\n",
    "print(\"   1. Try different fusion strategies\")\n",
    "print(\"   2. Experiment with balancing pathway contributions\")\n",
    "print(\"   3. Apply to more complex datasets\")\n",
    "print(\"   4. Optimize model architectures based on pathway analysis\")\n",
    "\n",
    "print(\"\\nâœ¨ Thank you for exploring Multi-Stream Neural Networks! âœ¨\")\n",
    "\n",
    "# Visualize Model Predictions and Pathway Analysis\n",
    "print(\"ğŸ” Visualizing model predictions and pathway analysis...\")\n",
    "\n",
    "# Get a batch of test data\n",
    "test_batch = next(iter(test_loader))\n",
    "# Handle different data formats - support both tuple and list types\n",
    "if isinstance(test_batch, (list, tuple)) and len(test_batch) == 3:\n",
    "    test_rgb, test_brightness, test_labels = test_batch\n",
    "else:\n",
    "    # Handle unexpected data format\n",
    "    if not isinstance(test_batch, (list, tuple)):\n",
    "        raise ValueError(f\"Expected test_batch to be list or tuple, got {type(test_batch)}\")\n",
    "    elif len(test_batch) != 3:\n",
    "        raise ValueError(f\"Expected test_batch to have 3 elements, got {len(test_batch)}\")\n",
    "    else:\n",
    "        raise ValueError(\"Unexpected data format from test_loader\")\n",
    "\n",
    "test_rgb = test_rgb.to(device)\n",
    "test_brightness = test_brightness.to(device)\n",
    "test_labels = test_labels.to(device)\n",
    "\n",
    "# Get predictions from all models\n",
    "with torch.no_grad():\n",
    "    # Baseline model only takes RGB\n",
    "    baseline_outputs = baseline_model(test_rgb)\n",
    "    baseline_probs = torch.softmax(baseline_outputs, dim=1)\n",
    "    baseline_preds = torch.argmax(baseline_probs, dim=1)\n",
    "    \n",
    "    # Multi-stream models take both inputs\n",
    "    base_multi_channel_outputs = base_multi_channel_large_model(test_rgb, test_brightness)\n",
    "    base_multi_channel_probs = torch.softmax(base_multi_channel_outputs, dim=1)\n",
    "    base_multi_channel_preds = torch.argmax(base_multi_channel_probs, dim=1)\n",
    "    \n",
    "    multi_channel_resnet_outputs = multi_channel_resnet50_model(test_rgb, test_brightness)\n",
    "    multi_channel_resnet_probs = torch.softmax(multi_channel_resnet_outputs, dim=1)\n",
    "    multi_channel_resnet_preds = torch.argmax(multi_channel_resnet_probs, dim=1)\n",
    "    \n",
    "    # Get separate pathway predictions from multi-stream models using analyze_pathways\n",
    "    base_color_outputs, base_brightness_outputs = base_multi_channel_large_model.analyze_pathways(test_rgb, test_brightness)\n",
    "    base_color_probs = torch.softmax(base_color_outputs, dim=1)\n",
    "    base_brightness_probs = torch.softmax(base_brightness_outputs, dim=1)\n",
    "    base_color_preds = torch.argmax(base_color_probs, dim=1)\n",
    "    base_brightness_preds = torch.argmax(base_brightness_probs, dim=1)\n",
    "    \n",
    "    resnet_color_outputs, resnet_brightness_outputs = multi_channel_resnet50_model.analyze_pathways(test_rgb, test_brightness)\n",
    "    resnet_color_probs = torch.softmax(resnet_color_outputs, dim=1)\n",
    "    resnet_brightness_probs = torch.softmax(resnet_brightness_outputs, dim=1)\n",
    "    resnet_color_preds = torch.argmax(resnet_color_probs, dim=1)\n",
    "    resnet_brightness_preds = torch.argmax(resnet_brightness_probs, dim=1)\n",
    "\n",
    "# Move to CPU for visualization\n",
    "test_rgb = test_rgb.cpu()\n",
    "test_brightness = test_brightness.cpu()\n",
    "test_labels = test_labels.cpu()\n",
    "baseline_preds = baseline_preds.cpu()\n",
    "base_multi_channel_preds = base_multi_channel_preds.cpu()\n",
    "multi_channel_resnet_preds = multi_channel_resnet_preds.cpu()\n",
    "base_color_preds = base_color_preds.cpu()\n",
    "base_brightness_preds = base_brightness_preds.cpu()\n",
    "resnet_color_preds = resnet_color_preds.cpu()\n",
    "resnet_brightness_preds = resnet_brightness_preds.cpu()\n",
    "\n",
    "# Select a subset of samples for visualization\n",
    "n_samples = min(8, test_rgb.size(0))\n",
    "sample_indices = np.random.choice(test_rgb.size(0), n_samples, replace=False)\n",
    "\n",
    "# Plot original images, model predictions, and pathway contributions\n",
    "fig, axes = plt.subplots(n_samples, 6, figsize=(18, 3*n_samples))\n",
    "if n_samples == 1:\n",
    "    axes = axes.reshape(1, -1)\n",
    "\n",
    "for i, idx in enumerate(sample_indices):\n",
    "    # Get RGB image\n",
    "    rgb_img = test_rgb[idx].permute(1, 2, 0).numpy()\n",
    "    rgb_img = np.clip(rgb_img, 0, 1)\n",
    "    \n",
    "    # Get brightness image\n",
    "    brightness_img = test_brightness[idx, 0].numpy()\n",
    "    \n",
    "    # True label and predictions\n",
    "    true_label = test_labels[idx].item()\n",
    "    baseline_pred = baseline_preds[idx].item()\n",
    "    multi_channel_pred = multi_channel_resnet_preds[idx].item()\n",
    "    color_pathway_pred = resnet_color_preds[idx].item()\n",
    "    brightness_pathway_pred = resnet_brightness_preds[idx].item()\n",
    "    \n",
    "    # Plot original RGB image\n",
    "    axes[i, 0].imshow(rgb_img)\n",
    "    axes[i, 0].set_title(f\"True: {CIFAR100_FINE_LABELS[true_label]}\")\n",
    "    axes[i, 0].axis('off')\n",
    "    \n",
    "    # Plot brightness image\n",
    "    axes[i, 1].imshow(brightness_img, cmap='gray')\n",
    "    axes[i, 1].set_title(\"Brightness Channel\")\n",
    "    axes[i, 1].axis('off')\n",
    "    \n",
    "    # Plot baseline model prediction\n",
    "    axes[i, 2].imshow(rgb_img)\n",
    "    correct = true_label == baseline_pred\n",
    "    title_color = 'green' if correct else 'red'\n",
    "    axes[i, 2].set_title(f\"Baseline: {CIFAR100_FINE_LABELS[baseline_pred]}\", color=title_color)\n",
    "    axes[i, 2].axis('off')\n",
    "    \n",
    "    # Plot multi-stream model prediction\n",
    "    axes[i, 3].imshow(rgb_img)\n",
    "    correct = true_label == multi_channel_pred\n",
    "    title_color = 'green' if correct else 'red'\n",
    "    axes[i, 3].set_title(f\"Multi-Stream: {CIFAR100_FINE_LABELS[multi_channel_pred]}\", color=title_color)\n",
    "    axes[i, 3].axis('off')\n",
    "    \n",
    "    # Plot color pathway prediction\n",
    "    axes[i, 4].imshow(rgb_img)\n",
    "    correct = true_label == color_pathway_pred\n",
    "    title_color = 'green' if correct else 'red'\n",
    "    axes[i, 4].set_title(f\"Color Path: {CIFAR100_FINE_LABELS[color_pathway_pred]}\", color=title_color)\n",
    "    axes[i, 4].axis('off')\n",
    "    \n",
    "    # Plot brightness pathway prediction\n",
    "    axes[i, 5].imshow(brightness_img, cmap='gray')\n",
    "    correct = true_label == brightness_pathway_pred\n",
    "    title_color = 'green' if correct else 'red'\n",
    "    axes[i, 5].set_title(f\"Brightness Path: {CIFAR100_FINE_LABELS[brightness_pathway_pred]}\", color=title_color)\n",
    "    axes[i, 5].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualize pathway feature importance\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Get pathway importance\n",
    "base_importance = base_multi_channel_large_model.get_pathway_importance()\n",
    "resnet_importance = multi_channel_resnet50_model.get_pathway_importance()\n",
    "\n",
    "# Plot importance values\n",
    "labels = ['Color Pathway', 'Brightness Pathway']\n",
    "base_values = [base_importance['color_pathway'], base_importance['brightness_pathway']]\n",
    "resnet_values = [resnet_importance['color_pathway'], resnet_importance['brightness_pathway']]\n",
    "\n",
    "x = np.arange(len(labels))\n",
    "width = 0.35\n",
    "\n",
    "plt.bar(x - width/2, base_values, width, label='BaseMultiChannelNetwork')\n",
    "plt.bar(x + width/2, resnet_values, width, label='MultiChannelResNetNetwork')\n",
    "\n",
    "plt.ylabel('Relative Importance')\n",
    "plt.title('Pathway Importance Analysis')\n",
    "plt.xticks(x, labels)\n",
    "plt.ylim(0, 1)\n",
    "plt.legend()\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… Visualization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655f3b1e",
   "metadata": {},
   "source": [
    "## Summary and Conclusions\n",
    "\n",
    "In this notebook, we've built, trained, and evaluated multi-stream neural networks for CIFAR-100 classification:\n",
    "\n",
    "1. **Data Preparation**\n",
    "   - Processed CIFAR-100 data into RGB and brightness streams\n",
    "   - Applied unified data augmentation with the same transformations for all models\n",
    "   - Created robust data processing pipelines with batch processing and memory optimization\n",
    "\n",
    "2. **Baseline Model**\n",
    "   - Trained a non-pretrained ResNet50 baseline modified for CIFAR-100\n",
    "   - Used proper regularization (weight decay, early stopping)\n",
    "   - Applied learning rate scheduling for optimal convergence\n",
    "\n",
    "3. **Multi-Stream Models**\n",
    "   - Implemented two multi-stream architectures:\n",
    "     - BaseMultiChannelNetwork (dense/fully-connected)\n",
    "     - MultiChannelResNetNetwork (CNN with residual connections)\n",
    "   - Both models process RGB and brightness data in parallel pathways\n",
    "   - Used shared classifier for final prediction\n",
    "   - Leveraged the models' built-in APIs for efficient training and evaluation:\n",
    "     - `compile()`: Set up optimizer, loss function, and metrics\n",
    "     - `fit_dataloader()`: Trained models directly with DataLoaders for on-the-fly augmentation\n",
    "     - `fit()`: Alternative training method using NumPy arrays/tensors with built-in early stopping and scheduling\n",
    "     - `evaluate()`: Evaluated models on test data\n",
    "     - `analyze_pathways()`: Analyzed pathway contributions and importance\n",
    "\n",
    "4. **Fair Comparison Framework**\n",
    "   - All models trained with:\n",
    "     - Same optimization settings (Adam/AdamW optimizer)\n",
    "     - Same regularization (weight decay, optional dropout)\n",
    "     - Same learning rate scheduling\n",
    "     - Same early stopping patience\n",
    "     - Same data augmentation pipeline\n",
    "\n",
    "5. **Results and Analysis**\n",
    "   - Compared performance on test set\n",
    "   - Identified classes where multi-stream processing provides advantages\n",
    "   - Analyzed model convergence and learning patterns\n",
    "   - Used pathway analysis to understand feature importance in each stream\n",
    "\n",
    "The results demonstrate that separating color and brightness information into distinct processing pathways can lead to improved classification performance compared to a standard RGB-only approach.\n",
    "\n",
    "### Key Advantage of the Enhanced Training Pipeline\n",
    "\n",
    "By implementing the `fit_dataloader()` method in our multi-stream models, we achieved:\n",
    "- **True on-the-fly augmentation** - Each batch receives freshly augmented data\n",
    "- **Consistent training approach** across all models (baseline and multi-stream)\n",
    "- **Better generalization** through exposure to more diverse augmented samples\n",
    "- **Memory efficiency** by not storing pre-extracted augmented data\n",
    "- **Simplified workflow** - direct DataLoader usage without manual data extraction\n",
    "\n",
    "### Proper API Usage\n",
    "\n",
    "For Multi-Stream Neural Networks, the recommended API pattern is:\n",
    "1. Create model with `model = create_model(...)` or direct constructor\n",
    "2. Compile model with `model.compile(...)`\n",
    "3. Train model with `model.fit_dataloader(train_loader, val_loader, ...)` for on-the-fly augmentation\n",
    "4. Or train with `model.fit(...)` using NumPy arrays or tensors if preferred\n",
    "5. Evaluate with `model.evaluate(...)`\n",
    "6. Analyze pathways with `model.analyze_pathways(...)`\n",
    "\n",
    "This provides a clean, flexible interface that is consistent across all model architectures.\n",
    "\n",
    "### Next Steps\n",
    "- Perform additional ablation studies on different pathway configurations\n",
    "- Explore other fusion mechanisms beyond shared classifiers\n",
    "- Apply these models to more complex datasets and real-world applications\n",
    "- Analyze computational efficiency and potential optimizations"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
