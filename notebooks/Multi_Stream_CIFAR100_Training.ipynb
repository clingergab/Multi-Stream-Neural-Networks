{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f0aae91",
   "metadata": {
    "id": "4f0aae91"
   },
   "source": [
    "# Multi-Stream Neural Networks: CIFAR100 Training\n",
    "\n",
    "This notebook demonstrates training Multi-Stream Neural Networks (MSNN) on the CIFAR100 dataset.\n",
    "\n",
    "The notebook follows this structure:\n",
    "1. Environment setup and repository update\n",
    "2. Installation of dependencies and importing libraries\n",
    "3. Data loading, processing, verification, visualization and analysis \n",
    "4. Model creation, training and evaluation\n",
    "5. Pathway analysis and model saving\n",
    "\n",
    "## Multi-Stream Neural Networks\n",
    "\n",
    "This notebook demonstrates the full pipeline for training multi-stream neural networks:\n",
    "\n",
    "### Key Features\n",
    "- **Unified API Design**: Consistent interface across all models\n",
    "- **Two Fusion Strategies**: Shared classifier (recommended) vs separate classifiers\n",
    "- **Multiple Architectures**: Dense networks and CNN (ResNet) models\n",
    "- **GPU Optimization**: Automatic device detection with mixed precision\n",
    "- **Research Tools**: Pathway analysis for multi-stream insights\n",
    "\n",
    "### Model Architectures\n",
    "1. **BaseMultiChannelNetwork**: Dense/fully-connected multi-stream processing\n",
    "2. **MultiChannelResNetNetwork**: CNN with residual connections for spatial features\n",
    "\n",
    "### API Design Philosophy\n",
    "- **`model(color, brightness)`** → Single tensor for training/inference\n",
    "- **`model.analyze_pathways(color, brightness)`** → Tuple for research analysis\n",
    "- **Keras-like training**: `.fit()`, `.evaluate()`, `.predict()` methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7051aee4",
   "metadata": {
    "id": "7051aee4"
   },
   "source": [
    "## Environment Setup & Requirements\n",
    "\n",
    "### Prerequisites\n",
    "- **Python 3.8+**\n",
    "- **PyTorch 1.12+** with CUDA support (recommended)\n",
    "- **Google Colab** (this notebook) or local Jupyter environment\n",
    "\n",
    "### Project Structure\n",
    "Our codebase is fully modularized:\n",
    "```\n",
    "Multi-Stream-Neural-Networks/\n",
    "├── src/\n",
    "│   ├── models/basic_multi_channel/     # Core model implementations\n",
    "│   │   ├── base_multi_channel_network.py    # Dense model\n",
    "│   │   └── multi_channel_resnet_network.py  # CNN model\n",
    "│   ├── utils/cifar100_loader.py        # CIFAR-100 data utilities\n",
    "│   ├── transforms/rgb_to_rgbl.py       # RGB→Brightness transform\n",
    "│   └── utils/device_utils.py           # GPU optimization utilities\n",
    "├── configs/                            # Model configuration files\n",
    "└── data/                               # Dataset location\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e80196",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "Mount Google Drive and navigate to the project directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823d044d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Mount Google Drive\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[1;32m      3\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57873d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Navigate to the project directory\n",
    "import os\n",
    "\n",
    "# Navigate to Drive and project directory\n",
    "project_path = '/content/drive/MyDrive/Multi-Stream-Neural-Networks'\n",
    "if os.path.exists(project_path):\n",
    "    os.chdir(project_path)\n",
    "    print(f\"✅ Found project at: {project_path}\")\n",
    "else:\n",
    "    print(f\"❌ Project not found at: {project_path}\")\n",
    "    print(\"💡 Please clone the repository first:\")\n",
    "    print(\"   !git clone https://github.com/yourusername/Multi-Stream-Neural-Networks.git\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1955bd1d",
   "metadata": {},
   "source": [
    "## 2. Update Repository\n",
    "\n",
    "Pull the latest changes from the repository to ensure we have the most recent codebase.\n",
    "\n",
    "## 📊 Data Loading and Preprocessing\n",
    "\n",
    "We'll use our **optimized CIFAR-100 data loader** that handles:\n",
    "- ✅ **Automatic download** and caching\n",
    "- ✅ **Train/Validation/Test splits** with proper stratification  \n",
    "- ✅ **RGB → Brightness conversion** using luminance weights\n",
    "- ✅ **Tensor formatting** ready for PyTorch models\n",
    "- ✅ **Memory efficient** processing for large datasets\n",
    "\n",
    "### 🎨 Multi-Stream Data Strategy\n",
    "- **RGB Stream**: Full color information (3 channels)\n",
    "- **Brightness Stream**: Luminance-based brightness (1 channel)\n",
    "- **Combined Processing**: Fusion strategies for optimal performance\n",
    "\n",
    "The data loader ensures both streams are properly aligned and normalized for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e805e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update repository with latest changes\n",
    "print(\"🔄 Pulling latest changes from repository...\")\n",
    "\n",
    "# Make sure we're in the right directory\n",
    "print(f\"📁 Current directory: {os.getcwd()}\")\n",
    "\n",
    "# Pull latest changes\n",
    "!git pull origin main\n",
    "\n",
    "print(\"\\n✅ Repository update complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1aa368",
   "metadata": {},
   "source": [
    "## 3. Install Dependencies\n",
    "\n",
    "Install required packages and dependencies for the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xo3JWyNVEGMZ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xo3JWyNVEGMZ",
    "outputId": "34f5af2f-a23c-4e75-848f-94bbf88fffb4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Installing required dependencies...\n",
      "Installing packages...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ torch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ torchvision\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ numpy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ seaborn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ tqdm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ scikit-learn\n",
      "✅ Pillow\n",
      "\n",
      "📚 Importing libraries...\n",
      "✅ All libraries imported successfully!\n",
      "\n",
      "🔧 PyTorch Setup:\n",
      "   PyTorch version: 2.7.0\n",
      "   CUDA available: False\n",
      "   Using CPU (consider GPU for faster training)\n",
      "\n",
      "🎯 Dependencies and imports complete!\n",
      "✅ Pillow\n",
      "\n",
      "📚 Importing libraries...\n",
      "✅ All libraries imported successfully!\n",
      "\n",
      "🔧 PyTorch Setup:\n",
      "   PyTorch version: 2.7.0\n",
      "   CUDA available: False\n",
      "   Using CPU (consider GPU for faster training)\n",
      "\n",
      "🎯 Dependencies and imports complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install Dependencies\n",
    "print(\"📦 Installing required dependencies...\")\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_package(package):\n",
    "    \"\"\"Install a package if not already installed.\"\"\"\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package, \"--quiet\"])\n",
    "        return True\n",
    "    except subprocess.CalledProcessError:\n",
    "        return False\n",
    "\n",
    "# Required packages\n",
    "packages = [\n",
    "    \"torch\",\n",
    "    \"torchvision\", \n",
    "    \"numpy\",\n",
    "    \"matplotlib\",\n",
    "    \"seaborn\",\n",
    "    \"tqdm\",\n",
    "    \"scikit-learn\",\n",
    "    \"Pillow\"\n",
    "]\n",
    "\n",
    "print(\"Installing packages...\")\n",
    "for package in packages:\n",
    "    if install_package(package):\n",
    "        print(f\"✅ {package}\")\n",
    "    else:\n",
    "        print(f\"❌ Failed to install {package}\")\n",
    "\n",
    "# Install project requirements\n",
    "print(\"\\nInstalling project requirements...\")\n",
    "!pip install -r requirements.txt\n",
    "\n",
    "print(\"\\n✅ Dependencies installation complete!\")\n",
    "\n",
    "# Data Augmentation\n",
    "print(\"🔄 Setting up data augmentation using project's implementation...\")\n",
    "\n",
    "# Import the project's augmentation module\n",
    "try:\n",
    "    from src.transforms.augmentation import (\n",
    "    CIFAR100Augmentation,\n",
    "    AugmentedMultiStreamDataset,\n",
    "    MixUp,\n",
    "    create_augmented_dataloaders,\n",
    "    create_test_dataloader\n",
    ")\n",
    "    print(\"✅ Augmentation module imported successfully\")\n",
    "except ImportError:\n",
    "    print(\"❌ Failed to import augmentation module. Make sure src/transforms/augmentation.py exists\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62186b17",
   "metadata": {},
   "source": [
    "### Note on Data Augmentation\n",
    "\n",
    "The data augmentation module has been relocated from `src.utils.augmentation` to `src.transforms.augmentation` for better code organization. The augmentation module includes:\n",
    "\n",
    "- Dataset-specific augmentation classes (CIFAR100Augmentation, ImageNetAugmentation)\n",
    "- Multi-stream dataset handling (AugmentedMultiStreamDataset)\n",
    "- Advanced augmentation techniques (MixUp)\n",
    "- Helper functions for creating dataloaders\n",
    "\n",
    "This notebook has been updated to use the new import paths."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ecd3cd",
   "metadata": {},
   "source": [
    "## 4. Import Libraries\n",
    "\n",
    "Import all necessary libraries and utilities for the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9be5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "print(\"📚 Importing libraries and setting up the environment...\")\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "# Core PyTorch Libraries\n",
    "#------------------------------------------------------------------------------\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "# Data Handling Libraries\n",
    "#------------------------------------------------------------------------------\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "# Visualization Libraries\n",
    "#------------------------------------------------------------------------------\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "# Progress Tracking and Machine Learning Libraries\n",
    "#------------------------------------------------------------------------------\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "# Project Setup\n",
    "#------------------------------------------------------------------------------\n",
    "# Add project root to path for imports\n",
    "project_root = Path('.').resolve()\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.append(str(project_root))\n",
    "    \n",
    "print(\"✅ All libraries imported successfully!\")\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "# Environment Information\n",
    "#------------------------------------------------------------------------------\n",
    "# Check PyTorch setup\n",
    "print(f\"\\n🔧 PyTorch Setup:\")\n",
    "print(f\"   PyTorch version: {torch.__version__}\")\n",
    "print(f\"   CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   CUDA version: {torch.version.cuda}\")\n",
    "else:\n",
    "    print(\"   Using CPU (consider GPU for faster training)\")\n",
    "\n",
    "print(\"\\n🎯 Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd732e2",
   "metadata": {},
   "source": [
    "## 5. Load Data\n",
    "\n",
    "Load the CIFAR-100 dataset using our optimized data loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2895f8fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 Setting up CIFAR-100 dataset loading...\n",
      "✅ CIFAR-100 loader utilities imported successfully\n",
      "📁 Loading CIFAR-100 datasets with train/validation/test split...\n",
      "❌ Error loading CIFAR-100 data: get_cifar100_datasets() got an unexpected keyword argument 'root'\n",
      "\n",
      "💡 Troubleshooting:\n",
      "   1. Check internet connection for CIFAR-100 download\n",
      "   2. Verify data directory permissions\n",
      "   3. Try clearing cache: rm -rf data/cifar-100\n",
      "   4. Check if src/utils/cifar100_loader.py exists\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "get_cifar100_datasets() got an unexpected keyword argument 'root'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 16\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m📁 Loading CIFAR-100 datasets with train/validation/test split...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# Load datasets using our optimized loader\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m     train_dataset, val_dataset, test_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mget_cifar100_datasets\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./data\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdownload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43mval_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 10% validation split from training data\u001b[39;49;00m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ CIFAR-100 datasets loaded successfully!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   📊 Training samples: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_dataset)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: get_cifar100_datasets() got an unexpected keyword argument 'root'"
     ]
    }
   ],
   "source": [
    "# 📊 CIFAR-100 Data Loading and Verification\n",
    "print(\"📁 Setting up CIFAR-100 dataset loading...\")\n",
    "\n",
    "# Load CIFAR-100 Dataset\n",
    "print(\"📁 Loading CIFAR-100 datasets with train/validation/test split...\")\n",
    "\n",
    "try:\n",
    "    from src.utils.cifar100_loader import get_cifar100_datasets, create_validation_split\n",
    "    print(\"✅ CIFAR-100 loader utilities imported successfully\")\n",
    "except ImportError:\n",
    "    print(\"❌ Failed to import CIFAR-100 utilities. Make sure src/utils/cifar100_loader.py exists\")\n",
    "    raise\n",
    "\n",
    "# Load datasets using our optimized loader (returns train, test, class_names)\n",
    "train_dataset, test_dataset, class_names = get_cifar100_datasets(\n",
    "    data_dir='./data/cifar-100'\n",
    ")\n",
    "\n",
    "# Create validation split from training data\n",
    "train_dataset, val_dataset = create_validation_split(\n",
    "    train_dataset, \n",
    "    val_split=0.1\n",
    ")\n",
    "\n",
    "print(\"✅ CIFAR-100 datasets loaded successfully!\")\n",
    "print(f\"   📊 Training samples: {len(train_dataset):,}\")\n",
    "print(f\"   📊 Validation samples: {len(val_dataset):,}\")\n",
    "print(f\"   📊 Test samples: {len(test_dataset):,}\")\n",
    "print(f\"   🏷️ Number of classes: {len(class_names)}\")\n",
    "\n",
    "# Store class names for later use\n",
    "CIFAR100_FINE_LABELS = class_names\n",
    "\n",
    "print(\"\\n🎯 Data loading complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a99955e",
   "metadata": {},
   "source": [
    "## 6. Process Data\n",
    "\n",
    "Convert RGB images to RGB + Brightness (L) channels for multi-stream processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a78fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process Data: RGB to RGB+L (Brightness) Conversion\n",
    "print(\"🔄 Converting RGB images to RGB + Brightness streams...\")\n",
    "\n",
    "try:\n",
    "    from src.transforms.rgb_to_rgbl import RGBtoRGBL\n",
    "    print(\"✅ RGB to RGB+L transform imported successfully\")\n",
    "except ImportError:\n",
    "    print(\"❌ Failed to import RGB to RGB+L transform. Make sure src/transforms/rgb_to_rgbl.py exists\")\n",
    "    raise\n",
    "\n",
    "# Initialize the transform\n",
    "rgb_to_rgbl = RGBtoRGBL()\n",
    "\n",
    "# Function to process a dataset batch-wise for memory efficiency\n",
    "def process_dataset_to_streams(dataset, batch_size=1000, desc=\"Processing\"):\n",
    "    \"\"\"\n",
    "    Convert RGB dataset to RGB + Brightness streams efficiently.\n",
    "    \n",
    "    Args:\n",
    "        dataset: Dataset with RGB images (PyTorch dataset format)\n",
    "        batch_size: Size of batches for memory-efficient processing\n",
    "        desc: Description for progress bar\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (rgb_stream, brightness_stream, labels_tensor)\n",
    "    \"\"\"\n",
    "    rgb_tensors = []\n",
    "    brightness_tensors = []\n",
    "    labels = []\n",
    "    \n",
    "    # Process in batches to manage memory\n",
    "    for i in tqdm(range(0, len(dataset), batch_size), desc=desc):\n",
    "        batch_end = min(i + batch_size, len(dataset))\n",
    "        batch_data = []\n",
    "        batch_labels = []\n",
    "        \n",
    "        # Collect batch data\n",
    "        for j in range(i, batch_end):\n",
    "            data, label = dataset[j]\n",
    "            batch_data.append(data)\n",
    "            batch_labels.append(label)\n",
    "        \n",
    "        # Convert to tensor batch\n",
    "        batch_tensor = torch.stack(batch_data)\n",
    "        \n",
    "        # Apply RGB to RGB+L transform\n",
    "        rgb_batch, brightness_batch = rgb_to_rgbl(batch_tensor)\n",
    "        \n",
    "        rgb_tensors.append(rgb_batch)\n",
    "        brightness_tensors.append(brightness_batch)\n",
    "        labels.extend(batch_labels)\n",
    "    \n",
    "    # Concatenate all batches\n",
    "    rgb_stream = torch.cat(rgb_tensors, dim=0)\n",
    "    brightness_stream = torch.cat(brightness_tensors, dim=0)\n",
    "    labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
    "    \n",
    "    return rgb_stream, brightness_stream, labels_tensor\n",
    "\n",
    "# Process all datasets\n",
    "print(\"Processing training dataset...\")\n",
    "train_rgb, train_brightness, train_labels_tensor = process_dataset_to_streams(\n",
    "    train_dataset, desc=\"Training data\"\n",
    ")\n",
    "\n",
    "print(\"Processing validation dataset...\")\n",
    "val_rgb, val_brightness, val_labels_tensor = process_dataset_to_streams(\n",
    "    val_dataset, desc=\"Validation data\"\n",
    ")\n",
    "\n",
    "print(\"Processing test dataset...\")\n",
    "test_rgb, test_brightness, test_labels_tensor = process_dataset_to_streams(\n",
    "    test_dataset, desc=\"Test data\"\n",
    ")\n",
    "\n",
    "print(\"\\n✅ Multi-stream conversion complete!\")\n",
    "print(f\"   🎨 RGB stream shape: {train_rgb.shape}\")\n",
    "print(f\"   💡 Brightness stream shape: {train_brightness.shape}\")\n",
    "print(f\"   📊 RGB range: [{train_rgb.min():.3f}, {train_rgb.max():.3f}]\")\n",
    "print(f\"   📊 Brightness range: [{train_brightness.min():.3f}, {train_brightness.max():.3f}]\")\n",
    "\n",
    "# Memory usage estimation\n",
    "rgb_memory = (train_rgb.nbytes + val_rgb.nbytes + test_rgb.nbytes) / 1e6\n",
    "brightness_memory = (train_brightness.nbytes + val_brightness.nbytes + test_brightness.nbytes) / 1e6\n",
    "total_memory = rgb_memory + brightness_memory\n",
    "\n",
    "print(f\"\\n📈 Processing Summary:\")\n",
    "print(f\"   📊 Total samples processed: {len(train_labels_tensor) + len(val_labels_tensor) + len(test_labels_tensor):,}\")\n",
    "print(f\"   🎨 RGB streams memory: {rgb_memory:.1f} MB\")\n",
    "print(f\"   💡 Brightness streams memory: {brightness_memory:.1f} MB\")\n",
    "print(f\"   💾 Total memory usage: {total_memory:.1f} MB\")\n",
    "\n",
    "print(\"\\n🎯 Data processing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e2429b",
   "metadata": {},
   "source": [
    "## 7. Data Verification\n",
    "\n",
    "Verify the processed data structure and consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb2f71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Verification\n",
    "print(\"🔍 Verifying processed data structure and consistency...\")\n",
    "\n",
    "def verify_data_integrity(rgb_data, brightness_data, labels, split_name):\n",
    "    # Check shapes and types\n",
    "    assert rgb_data.shape[0] == brightness_data.shape[0] == labels.shape[0], f\"Inconsistent sample counts in {split_name}!\"\n",
    "    assert rgb_data.shape[1:] == (3, 32, 32), f\"Unexpected RGB shape in {split_name}!\"\n",
    "    assert brightness_data.shape[1:] == (1, 32, 32), f\"Unexpected brightness shape in {split_name}!\"\n",
    "    assert 0 <= labels.min() and labels.max() < 100, f\"Invalid label range in {split_name}!\"\n",
    "    return rgb_data.shape[0]\n",
    "\n",
    "train_samples = verify_data_integrity(train_rgb, train_brightness, train_labels_tensor, \"Training\")\n",
    "val_samples = verify_data_integrity(val_rgb, val_brightness, val_labels_tensor, \"Validation\")\n",
    "test_samples = verify_data_integrity(test_rgb, test_brightness, test_labels_tensor, \"Test\")\n",
    "\n",
    "total_samples = train_samples + val_samples + test_samples\n",
    "all_labels = torch.cat([train_labels_tensor, val_labels_tensor, test_labels_tensor])\n",
    "unique_labels = torch.unique(all_labels)\n",
    "\n",
    "print(f\"\\n📈 Data Summary:\")\n",
    "print(f\"   Training: {train_samples:,}\")\n",
    "print(f\"   Validation: {val_samples:,}\")\n",
    "print(f\"   Test: {test_samples:,}\")\n",
    "print(f\"   Total: {total_samples:,}\")\n",
    "print(f\"   Unique classes: {len(unique_labels)}/100\")\n",
    "print(\"\\n✅ Data verification checks passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba0574d",
   "metadata": {},
   "source": [
    "## 8. Data Visualization\n",
    "\n",
    "Visualize sample images from both RGB and brightness streams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b744428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Visualization\n",
    "print(\"👁️ Visualizing sample images from both RGB and brightness streams...\")\n",
    "\n",
    "# Set up visualization\n",
    "plt.style.use('default')\n",
    "fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n",
    "fig.suptitle('Multi-Stream CIFAR-100 Samples: RGB vs Brightness', fontsize=14)\n",
    "\n",
    "# Select random samples\n",
    "np.random.seed(42)  # For reproducible results\n",
    "sample_indices = np.random.choice(len(train_rgb), 4, replace=False)\n",
    "\n",
    "for i, idx in enumerate(sample_indices):\n",
    "    # Get data\n",
    "    rgb_img = train_rgb[idx]\n",
    "    brightness_img = train_brightness[idx]\n",
    "    label = train_labels_tensor[idx].item()\n",
    "    class_name = CIFAR100_FINE_LABELS[label]\n",
    "    \n",
    "    # RGB image (convert from tensor to numpy)\n",
    "    rgb_np = rgb_img.permute(1, 2, 0).numpy()\n",
    "    rgb_np = np.clip(rgb_np, 0, 1)  # Ensure valid range\n",
    "    \n",
    "    # Brightness image\n",
    "    brightness_np = brightness_img.squeeze().numpy()\n",
    "    \n",
    "    # Plot RGB\n",
    "    axes[0, i].imshow(rgb_np)\n",
    "    axes[0, i].set_title(f'RGB: {class_name}', fontsize=10)\n",
    "    axes[0, i].axis('off')\n",
    "    \n",
    "    # Plot Brightness\n",
    "    axes[1, i].imshow(brightness_np, cmap='gray')\n",
    "    axes[1, i].set_title(f'Brightness: {class_name}', fontsize=10)\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"✅ Data visualization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5042e5c6",
   "metadata": {},
   "source": [
    "## 9. Data Analysis\n",
    "\n",
    "Perform basic data analysis on class distribution and stream characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee66628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Analysis\n",
    "print(\"📊 Performing basic data analysis...\")\n",
    "\n",
    "# Set up matplotlib for better visualizations\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Class Distribution Analysis\n",
    "print(\"\\n🏷️ Analyzing class distribution...\")\n",
    "\n",
    "# Training distribution\n",
    "train_counts = np.bincount(train_labels_tensor.numpy(), minlength=100)\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.bar(range(100), train_counts, alpha=0.7, color='skyblue')\n",
    "plt.title('Training Set Class Distribution', fontweight='bold')\n",
    "plt.xlabel('Class ID')\n",
    "plt.ylabel('Sample Count')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Stream Statistics Analysis\n",
    "print(\"\\n🎨 RGB vs Brightness stream characteristics:\")\n",
    "\n",
    "# Sample a subset for analysis\n",
    "sample_size = min(1000, len(train_rgb))\n",
    "indices = np.random.choice(len(train_rgb), sample_size, replace=False)\n",
    "rgb_sample = train_rgb[indices]\n",
    "brightness_sample = train_brightness[indices]\n",
    "\n",
    "# Calculate statistics\n",
    "rgb_stats = {\n",
    "    'mean': rgb_sample.mean().item(),\n",
    "    'std': rgb_sample.std().item(),\n",
    "    'min': rgb_sample.min().item(),\n",
    "    'max': rgb_sample.max().item()\n",
    "}\n",
    "\n",
    "brightness_stats = {\n",
    "    'mean': brightness_sample.mean().item(),\n",
    "    'std': brightness_sample.std().item(), \n",
    "    'min': brightness_sample.min().item(),\n",
    "    'max': brightness_sample.max().item()\n",
    "}\n",
    "\n",
    "print(f\"   🎨 RGB statistics:\")\n",
    "print(f\"      Mean: {rgb_stats['mean']:.3f}, Std: {rgb_stats['std']:.3f}\")\n",
    "print(f\"      Min: {rgb_stats['min']:.3f}, Max: {rgb_stats['max']:.3f}\")\n",
    "print(f\"   💡 Brightness statistics:\")\n",
    "print(f\"      Mean: {brightness_stats['mean']:.3f}, Std: {brightness_stats['std']:.3f}\")\n",
    "print(f\"      Min: {brightness_stats['min']:.3f}, Max: {brightness_stats['max']:.3f}\")\n",
    "\n",
    "print(\"\\n✅ Data analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803e7962",
   "metadata": {},
   "source": [
    "## 10. Data Augmentation\n",
    "\n",
    "Set up data augmentation for multi-stream training using the project's CIFAR-100 augmentation module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca38f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Augmentation\n",
    "print(\"🔄 Setting up data augmentation using project's implementation...\")\n",
    "\n",
    "# First try standard import\n",
    "try:\n",
    "    from src.transforms.augmentation import (\n",
    "        CIFAR100Augmentation, \n",
    "        AugmentedMultiStreamDataset,\n",
    "        MixUp, \n",
    "        create_augmented_dataloaders,\n",
    "        create_test_dataloader\n",
    "    )\n",
    "    print(\"✅ Augmentation module imported successfully using standard import\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ Standard import failed: {e}\")\n",
    "    print(\"💡 Trying alternative import approaches...\")\n",
    "    \n",
    "    # Method 1: Import directly from file\n",
    "    try:\n",
    "        import os\n",
    "        import sys\n",
    "        import importlib.util\n",
    "        \n",
    "        # Get absolute path to the augmentation module\n",
    "        project_root = os.path.abspath('.')\n",
    "        augmentation_path = os.path.join(project_root, \"src\", \"transforms\", \"augmentation.py\")\n",
    "        \n",
    "        if os.path.exists(augmentation_path):\n",
    "            print(f\"Found augmentation.py at: {augmentation_path}\")\n",
    "            \n",
    "            # Import module from file path\n",
    "            spec = importlib.util.spec_from_file_location(\"augmentation\", augmentation_path)\n",
    "            augmentation = importlib.util.module_from_spec(spec)\n",
    "            spec.loader.exec_module(augmentation)\n",
    "            \n",
    "            # Get the required classes and functions\n",
    "            CIFAR100Augmentation = augmentation.CIFAR100Augmentation\n",
    "            AugmentedMultiStreamDataset = augmentation.AugmentedMultiStreamDataset\n",
    "            MixUp = augmentation.MixUp\n",
    "            create_augmented_dataloaders = augmentation.create_augmented_dataloaders\n",
    "            create_test_dataloader = augmentation.create_test_dataloader\n",
    "            \n",
    "            print(\"✅ Augmentation module imported successfully using direct file import\")\n",
    "        else:\n",
    "            print(f\"❌ Could not find augmentation.py at: {augmentation_path}\")\n",
    "            raise ImportError(\"Augmentation module not found\")\n",
    "    except Exception as e2:\n",
    "        print(f\"❌ Alternative import also failed: {e2}\")\n",
    "        print(\"💡 Please check your project structure and paths\")\n",
    "        \n",
    "        # As a last resort, define minimal versions of the required classes\n",
    "        print(\"⚠️ Using fallback minimal implementations...\")\n",
    "        \n",
    "        class CIFAR100Augmentation:\n",
    "            def __init__(self, **kwargs):\n",
    "                self.enabled = kwargs.get('enabled', True)\n",
    "                print(\"Created minimal CIFAR100Augmentation (fallback)\")\n",
    "                \n",
    "        class AugmentedMultiStreamDataset(torch.utils.data.Dataset):\n",
    "            def __init__(self, color_data, brightness_data, labels, **kwargs):\n",
    "                self.color_data = color_data\n",
    "                self.brightness_data = brightness_data\n",
    "                self.labels = labels\n",
    "                print(\"Created minimal AugmentedMultiStreamDataset (fallback)\")\n",
    "                \n",
    "            def __len__(self):\n",
    "                return len(self.labels)\n",
    "                \n",
    "            def __getitem__(self, idx):\n",
    "                return self.color_data[idx], self.brightness_data[idx], self.labels[idx]\n",
    "                \n",
    "        class MixUp:\n",
    "            def __init__(self, alpha=0.2):\n",
    "                self.alpha = alpha\n",
    "                print(\"Created minimal MixUp (fallback)\")\n",
    "                \n",
    "        def create_augmented_dataloaders(train_color, train_brightness, train_labels,\n",
    "                                        val_color, val_brightness, val_labels, **kwargs):\n",
    "            batch_size = kwargs.get('batch_size', 32)\n",
    "            # Create simple dataloaders without augmentation\n",
    "            train_dataset = AugmentedMultiStreamDataset(train_color, train_brightness, train_labels)\n",
    "            val_dataset = AugmentedMultiStreamDataset(val_color, val_brightness, val_labels)\n",
    "            \n",
    "            train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "            val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size)\n",
    "            \n",
    "            print(\"Created minimal dataloaders (fallback)\")\n",
    "            return train_loader, val_loader\n",
    "            \n",
    "        def create_test_dataloader(test_color, test_brightness, test_labels, **kwargs):\n",
    "            batch_size = kwargs.get('batch_size', 32)\n",
    "            test_dataset = AugmentedMultiStreamDataset(test_color, test_brightness, test_labels)\n",
    "            test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size)\n",
    "            print(\"Created minimal test dataloader (fallback)\")\n",
    "            return test_loader\n",
    "\n",
    "# Create augmentation with custom settings for CIFAR-100\n",
    "augmentation_config = {\n",
    "    'horizontal_flip_prob': 0.5,  # 50% chance of flipping horizontally\n",
    "    'rotation_degrees': 10.0,     # Rotate up to ±10 degrees\n",
    "    'translate_range': 0.1,       # Translate up to 10% of image size\n",
    "    'scale_range': (0.9, 1.1),    # Scale between 90-110%\n",
    "    'color_jitter_strength': 0.3, # Moderate color jittering\n",
    "    'gaussian_noise_std': 0.01,   # Small amount of noise\n",
    "    'cutout_prob': 0.3,           # 30% chance of applying cutout\n",
    "    'cutout_size': 8,             # 8x8 pixel cutout\n",
    "    'enabled': True               # Enable augmentation\n",
    "}\n",
    "\n",
    "# Setup MixUp augmentation\n",
    "mixup_alpha = 0.2  # Alpha parameter for Beta distribution\n",
    "\n",
    "# Create augmented datasets and data loaders in one step\n",
    "print(\"\\n📊 Creating augmented DataLoaders...\")\n",
    "train_loader, val_loader = create_augmented_dataloaders(\n",
    "    train_rgb, train_brightness, train_labels_tensor,  # Training data\n",
    "    val_rgb, val_brightness, val_labels_tensor,        # Validation data\n",
    "    batch_size=64,                                     # Batch size\n",
    "    dataset=\"cifar100\",                                # Dataset type\n",
    "    augmentation_config=augmentation_config,           # Augmentation settings\n",
    "    mixup_alpha=mixup_alpha,                           # MixUp parameter\n",
    "    num_workers=2,                                     # Parallel workers\n",
    "    pin_memory=torch.cuda.is_available()               # Pin memory if GPU available\n",
    ")\n",
    "\n",
    "# Create test dataloader separately (no augmentation)\n",
    "test_loader = create_test_dataloader(\n",
    "    test_rgb, test_brightness, test_labels_tensor,\n",
    "    batch_size=64,\n",
    "    num_workers=2,\n",
    "    pin_memory=torch.cuda.is_available()\n",
    ")\n",
    "\n",
    "print(\"\\n✅ Data augmentation setup complete\")\n",
    "print(\"   Using project's CIFAR-100 specific augmentations:\")\n",
    "print(f\"   - Horizontal flips: {augmentation_config['horizontal_flip_prob']}\")\n",
    "print(f\"   - Rotation: ±{augmentation_config['rotation_degrees']}°\")\n",
    "print(f\"   - Translation: ±{augmentation_config['translate_range'] * 100}%\")\n",
    "print(f\"   - Color jitter strength: {augmentation_config['color_jitter_strength']}\")\n",
    "print(f\"   - Gaussian noise (std): {augmentation_config['gaussian_noise_std']}\")\n",
    "print(f\"   - Cutout: {augmentation_config['cutout_prob']} probability, {augmentation_config['cutout_size']}px\")\n",
    "print(f\"   - MixUp alpha: {mixup_alpha}\")\n",
    "print(f\"\\n   Training batches: {len(train_loader)}\")\n",
    "print(f\"   Validation batches: {len(val_loader)}\")\n",
    "print(f\"   Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c639b3",
   "metadata": {},
   "source": [
    "## 11. Prepare Data for Training\n",
    "\n",
    "Create DataLoaders with the processed data for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05b1d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Data for Training\n",
    "print(\"🔄 Data preparation complete and ready for model training!\")\n",
    "\n",
    "# The data loaders were already created in the previous cell:\n",
    "#  - train_loader: Training data with augmentation\n",
    "#  - val_loader: Validation data without augmentation\n",
    "#  - test_loader: Test data without augmentation\n",
    "\n",
    "# Confirm data loader settings\n",
    "print(f\"\\n📦 DataLoader configuration:\")\n",
    "print(f\"   Batch size: {next(iter(train_loader))[0].shape[0]}\")\n",
    "print(f\"   Number of workers: 2\")\n",
    "print(f\"   Pin memory: {torch.cuda.is_available()}\")\n",
    "print(f\"   Training with augmentation: Yes\")\n",
    "print(f\"   Training with MixUp: {'Yes' if mixup_alpha is not None else 'No'}\")\n",
    "\n",
    "# Sample batch for verification\n",
    "sample_color, sample_brightness, sample_labels = next(iter(train_loader))\n",
    "print(f\"\\n📊 Sample batch shapes:\")\n",
    "print(f\"   Color batch: {sample_color.shape}\")\n",
    "print(f\"   Brightness batch: {sample_brightness.shape}\")\n",
    "print(f\"   Labels batch: {sample_labels.shape}\")\n",
    "\n",
    "print(\"\\n🎯 All data loaders are ready for model training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83696378",
   "metadata": {},
   "source": [
    "## 12. Create Baseline ResNet50 Model\n",
    "\n",
    "Create a standard ResNet50 model for comparison with multi-stream models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6996da86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Baseline ResNet50 Model\n",
    "print(\"🏗️ Creating baseline ResNet50 model for comparison...\")\n",
    "\n",
    "# Import ResNet from torchvision\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "\n",
    "# Check GPU availability and set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"🖥️ Using device: {device}\")\n",
    "\n",
    "# Create ResNet50 model modified for CIFAR-100\n",
    "class CifarResNet50(nn.Module):\n",
    "    def __init__(self, num_classes=100, pretrained=True):\n",
    "        super(CifarResNet50, self).__init__()\n",
    "        \n",
    "        # Load pretrained ResNet50\n",
    "        if pretrained:\n",
    "            self.model = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "        else:\n",
    "            self.model = resnet50(weights=None)\n",
    "        \n",
    "        # Modify first conv layer to work with 32x32 CIFAR images instead of 224x224 ImageNet\n",
    "        self.model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        # Remove maxpool to preserve spatial dimensions for small images\n",
    "        self.model.maxpool = nn.Identity()\n",
    "        \n",
    "        # Replace final fully connected layer for CIFAR-100\n",
    "        self.model.fc = nn.Linear(self.model.fc.in_features, num_classes)\n",
    "        \n",
    "        # Move to device\n",
    "        self.to(device)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Create the model\n",
    "baseline_model = CifarResNet50(num_classes=100, pretrained=True)\n",
    "baseline_model = baseline_model.to(device)\n",
    "\n",
    "# Setup optimizer and loss\n",
    "baseline_optimizer = optim.Adam(baseline_model.parameters(), lr=0.001)\n",
    "baseline_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Count parameters\n",
    "baseline_params = sum(p.numel() for p in baseline_model.parameters())\n",
    "baseline_trainable = sum(p.numel() for p in baseline_model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"✅ Baseline ResNet50 created successfully\")\n",
    "print(f\"   Architecture: Modified ResNet50 for CIFAR-100\")\n",
    "print(f\"   Total parameters: {baseline_params:,}\")\n",
    "print(f\"   Trainable parameters: {baseline_trainable:,}\")\n",
    "print(f\"   Input shape: (3, 32, 32)\")\n",
    "print(f\"   Device: {device}\")\n",
    "\n",
    "print(\"\\n✅ Baseline model created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb3b0b7",
   "metadata": {},
   "source": [
    "## 13. Create Multi-Stream Models\n",
    "\n",
    "Create multi-stream neural network models using our project APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfd0efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Multi-Stream Models\n",
    "print(\"🏗️ Creating Multi-Stream Neural Network Models...\")\n",
    "\n",
    "# Check GPU availability and set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"🖥️ Using device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"   Using CPU (CUDA not available)\")\n",
    "\n",
    "# Model configuration based on CIFAR-100 data\n",
    "print(f\"\\n📊 Model Configuration:\")\n",
    "print(f\"   Image size: 32x32 pixels\")\n",
    "print(f\"   RGB channels: 3\")\n",
    "print(f\"   Brightness channels: 1\") \n",
    "print(f\"   Number of classes: 100 (CIFAR-100)\")\n",
    "print(f\"   Device: {device}\")\n",
    "\n",
    "# Import model factory for clean model creation\n",
    "try:\n",
    "    from src.models.builders import create_model, list_available_models\n",
    "    \n",
    "    print(\"\\n📋 Available model types:\")\n",
    "    available_model_types = list_available_models()\n",
    "    for model_type in available_model_types:\n",
    "        print(f\"   ✅ {model_type}\")\n",
    "        \n",
    "except ImportError as e:\n",
    "    print(f\"❌ Failed to import model factory: {e}\")\n",
    "    print(\"💡 Falling back to direct model imports...\")\n",
    "    \n",
    "    try:\n",
    "        from src.models.basic_multi_channel.base_multi_channel_network import BaseMultiChannelNetwork as base_multi_channel_large\n",
    "        from src.models.basic_multi_channel.multi_channel_resnet_network import MultiChannelResNetNetwork as multi_channel_resnet50\n",
    "        print(\"✅ Direct imports successful\")\n",
    "    except ImportError as direct_e:\n",
    "        print(f\"❌ Direct imports also failed: {direct_e}\")\n",
    "        raise\n",
    "\n",
    "# Model dimensions for CIFAR-100\n",
    "input_channels_rgb = 3\n",
    "input_channels_brightness = 1  \n",
    "image_size = 32\n",
    "num_classes = 100\n",
    "\n",
    "# For dense models: flatten the image to 1D\n",
    "rgb_input_size = input_channels_rgb * image_size * image_size  # 3 * 32 * 32 = 3072\n",
    "brightness_input_size = input_channels_brightness * image_size * image_size  # 1 * 32 * 32 = 1024\n",
    "\n",
    "print(f\"\\n🔧 Model Input Configuration:\")\n",
    "print(f\"   RGB input size (dense): {rgb_input_size}\")\n",
    "print(f\"   Brightness input size (dense): {brightness_input_size}\")\n",
    "print(f\"   RGB input channels (CNN): {input_channels_rgb}\")\n",
    "print(f\"   Brightness input channels (CNN): {input_channels_brightness}\")\n",
    "\n",
    "# Create base_multi_channel_large (Dense/FC model)\n",
    "print(f\"\\n🏭 Creating base_multi_channel_large (Dense Model)...\")\n",
    "try:\n",
    "    if 'create_model' in locals():\n",
    "        base_multi_channel_large_model = create_model(\n",
    "            'base_multi_channel_large',\n",
    "            color_input_size=rgb_input_size,\n",
    "            brightness_input_size=brightness_input_size,\n",
    "            num_classes=num_classes,\n",
    "            use_shared_classifier=True,\n",
    "            device=device  # Use detected device (CUDA if available)\n",
    "        )\n",
    "    else:\n",
    "        base_multi_channel_large_model = base_multi_channel_large(\n",
    "            color_input_size=rgb_input_size,\n",
    "            brightness_input_size=brightness_input_size,\n",
    "            num_classes=num_classes,\n",
    "            use_shared_classifier=True,\n",
    "            device=device  # Use detected device (CUDA if available)\n",
    "        )\n",
    "    \n",
    "    # Count parameters\n",
    "    large_dense_params = sum(p.numel() for p in base_multi_channel_large_model.parameters())\n",
    "    large_dense_trainable = sum(p.numel() for p in base_multi_channel_large_model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"✅ base_multi_channel_large created successfully\")\n",
    "    print(f\"   Architecture: Large Dense/FC Network\")\n",
    "    print(f\"   Total parameters: {large_dense_params:,}\")\n",
    "    print(f\"   Trainable parameters: {large_dense_trainable:,}\")\n",
    "    print(f\"   Input size: RGB {rgb_input_size}, Brightness {brightness_input_size}\")\n",
    "    print(f\"   Fusion strategy: Shared classifier\")\n",
    "    print(f\"   Device: {base_multi_channel_large_model.device}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Failed to create base_multi_channel_large: {e}\")\n",
    "    print(f\"💡 Error details: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    base_multi_channel_large_model = None\n",
    "\n",
    "# Create multi_channel_resnet50 (CNN model)\n",
    "print(f\"\\n🏭 Creating multi_channel_resnet50 (CNN Model)...\")\n",
    "try:\n",
    "    if 'create_model' in locals():\n",
    "        multi_channel_resnet50_model = create_model(\n",
    "            'multi_channel_resnet50',\n",
    "            color_input_channels=input_channels_rgb,\n",
    "            brightness_input_channels=input_channels_brightness,\n",
    "            num_classes=num_classes,\n",
    "            use_shared_classifier=True,\n",
    "            activation='relu',\n",
    "            device=device  # Use detected device (CUDA if available)\n",
    "        )\n",
    "    else:\n",
    "        multi_channel_resnet50_model = multi_channel_resnet50(\n",
    "            color_input_channels=input_channels_rgb,\n",
    "            brightness_input_channels=input_channels_brightness,\n",
    "            num_classes=num_classes,\n",
    "            use_shared_classifier=True,\n",
    "            activation='relu',\n",
    "            device=device  # Use detected device (CUDA if available)\n",
    "        )\n",
    "    \n",
    "    # Count parameters\n",
    "    resnet50_params = sum(p.numel() for p in multi_channel_resnet50_model.parameters())\n",
    "    resnet50_trainable = sum(p.numel() for p in multi_channel_resnet50_model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"✅ multi_channel_resnet50 created successfully\")\n",
    "    print(f\"   Architecture: ResNet-50 style CNN (3,4,6,3 blocks)\")\n",
    "    print(f\"   Total parameters: {resnet50_params:,}\")\n",
    "    print(f\"   Trainable parameters: {resnet50_trainable:,}\")\n",
    "    print(f\"   Input shape: RGB {(input_channels_rgb, image_size, image_size)}, Brightness {(input_channels_brightness, image_size, image_size)}\")\n",
    "    print(f\"   Fusion strategy: Shared classifier\")\n",
    "    print(f\"   Device: {multi_channel_resnet50_model.device}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Failed to create multi_channel_resnet50: {e}\")\n",
    "    print(f\"💡 Error details: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    multi_channel_resnet50_model = None\n",
    "\n",
    "# Model comparison\n",
    "if base_multi_channel_large_model is not None and multi_channel_resnet50_model is not None:\n",
    "    print(f\"\\n📈 Model Comparison:\")\n",
    "    print(f\"   base_multi_channel_large: {large_dense_params:,} parameters\")\n",
    "    print(f\"   multi_channel_resnet50: {resnet50_params:,} parameters\")\n",
    "    print(f\"   ResNet-50 is {resnet50_params/large_dense_params:.1f}x larger than Large Dense\")\n",
    "    \n",
    "print(\"\\n✅ Multi-stream models created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c10dff",
   "metadata": {},
   "source": [
    "## 14. Train Models\n",
    "\n",
    "Train the baseline and multi-stream models on the CIFAR-100 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26b4ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Models\n",
    "print(\"🏋️‍♀️ Training models on CIFAR-100 dataset...\")\n",
    "\n",
    "# Utility function for model training with early stopping and learning rate decay\n",
    "def train_model(model, train_loader, val_loader, optimizer, criterion, num_epochs=100, patience=10, model_name=\"Model\"):\n",
    "    \"\"\"\n",
    "    Train a model and return training history.\n",
    "    \n",
    "    Args:\n",
    "        model: The model to train\n",
    "        train_loader: DataLoader for training data\n",
    "        val_loader: DataLoader for validation data\n",
    "        optimizer: Optimizer to use\n",
    "        criterion: Loss function\n",
    "        num_epochs: Number of epochs to train\n",
    "        patience: Early stopping patience\n",
    "        model_name: Name for logging\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with training history\n",
    "    \"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': []\n",
    "    }\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    best_model_state = None\n",
    "    no_improvement_count = 0\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, \n",
    "        mode='max',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        train_bar = tqdm(train_loader, desc=f\"{model_name} Training\")\n",
    "        \n",
    "        for batch_idx, (rgb, brightness, targets) in enumerate(train_bar):\n",
    "            # Move to device\n",
    "            rgb, brightness, targets = rgb.to(device), brightness.to(device), targets.to(device)\n",
    "            \n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass - use model's __call__ method with both streams\n",
    "            outputs = model(rgb, brightness)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            # Add L2 regularization term\n",
    "            l2_lambda = 0.0001\n",
    "            l2_reg = 0.0\n",
    "            for param in model.parameters():\n",
    "                l2_reg += torch.norm(param, 2)\n",
    "            loss += l2_lambda * l2_reg\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Track statistics\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            train_total += targets.size(0)\n",
    "            train_correct += predicted.eq(targets).sum().item()\n",
    "            \n",
    "            # Update progress bar\n",
    "            train_bar.set_postfix({\n",
    "                'loss': train_loss/(batch_idx+1), \n",
    "                'acc': 100.*train_correct/train_total,\n",
    "                'lr': optimizer.param_groups[0]['lr']\n",
    "            })\n",
    "        \n",
    "        train_acc = 100. * train_correct / train_total\n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            val_bar = tqdm(val_loader, desc=f\"{model_name} Validation\")\n",
    "            \n",
    "            for batch_idx, (rgb, brightness, targets) in enumerate(val_bar):\n",
    "                # Move to device\n",
    "                rgb, brightness, targets = rgb.to(device), brightness.to(device), targets.to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(rgb, brightness)\n",
    "                loss = criterion(outputs, targets)\n",
    "                \n",
    "                # Track statistics\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                val_total += targets.size(0)\n",
    "                val_correct += predicted.eq(targets).sum().item()\n",
    "                \n",
    "                # Update progress bar\n",
    "                val_bar.set_postfix({\n",
    "                    'loss': val_loss/(batch_idx+1), \n",
    "                    'acc': 100.*val_correct/val_total\n",
    "                })\n",
    "        \n",
    "        val_acc = 100. * val_correct / val_total\n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        \n",
    "        # Save history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        # Print epoch summary\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} Summary:\")\n",
    "        print(f\"   Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "        print(f\"   Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "        print(f\"   Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "        \n",
    "        # Update learning rate scheduler\n",
    "        scheduler.step(val_acc)\n",
    "        \n",
    "        # Track best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            no_improvement_count = 0\n",
    "            print(f\"   ✅ New best validation accuracy: {best_val_acc:.2f}%\")\n",
    "        else:\n",
    "            no_improvement_count += 1\n",
    "            print(f\"   ⚠️ No improvement for {no_improvement_count} epochs\")\n",
    "        \n",
    "        # Early stopping check\n",
    "        if no_improvement_count >= patience:\n",
    "            print(f\"   🛑 Early stopping triggered after {epoch+1} epochs\")\n",
    "            break\n",
    "    \n",
    "    # Restore best model\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    print(f\"\\n✅ {model_name} training complete!\")\n",
    "    print(f\"   Best validation accuracy: {best_val_acc:.2f}%\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "# Set up optimizers and criterion for multi-stream models with weight decay\n",
    "base_multi_channel_large_optimizer = optim.Adam(\n",
    "    base_multi_channel_large_model.parameters(), \n",
    "    lr=0.001,\n",
    "    weight_decay=0.0001  # L2 regularization\n",
    ")\n",
    "\n",
    "multi_channel_resnet50_optimizer = optim.Adam(\n",
    "    multi_channel_resnet50_model.parameters(), \n",
    "    lr=0.001,\n",
    "    weight_decay=0.0001  # L2 regularization\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training configuration\n",
    "num_epochs = 100  # Full training run\n",
    "patience = 10     # Early stopping patience\n",
    "batch_size = 64   # Batch size from previous cell\n",
    "\n",
    "print(f\"\\n🔧 Training Configuration:\")\n",
    "print(f\"   Epochs: {num_epochs} (with early stopping, patience={patience})\")\n",
    "print(f\"   Batch size: {batch_size}\")\n",
    "print(f\"   Optimizer: Adam with weight decay=0.0001\")\n",
    "print(f\"   Learning rate: 0.001 with ReduceLROnPlateau scheduling\")\n",
    "print(f\"   Regularization: L2 weight decay + dropout in models\")\n",
    "print(f\"   Early stopping: Patience {patience} epochs\")\n",
    "print(f\"   Loss: CrossEntropyLoss\")\n",
    "print(f\"   GPU acceleration: {torch.cuda.is_available()}\")\n",
    "\n",
    "# Train baseline model\n",
    "print(\"\\n🏋️‍♀️ Training baseline ResNet50 model...\")\n",
    "baseline_history = train_model(\n",
    "    model=baseline_model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optimizer=baseline_optimizer,\n",
    "    criterion=baseline_criterion,\n",
    "    num_epochs=num_epochs,\n",
    "    patience=patience,\n",
    "    model_name=\"Baseline ResNet50\"\n",
    ")\n",
    "\n",
    "# Train base_multi_channel_large model\n",
    "print(\"\\n🏋️‍♀️ Training BaseMultiChannelNetwork model...\")\n",
    "base_multi_channel_large_history = train_model(\n",
    "    model=base_multi_channel_large_model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optimizer=base_multi_channel_large_optimizer,\n",
    "    criterion=criterion,\n",
    "    num_epochs=num_epochs,\n",
    "    patience=patience,\n",
    "    model_name=\"BaseMultiChannelNetwork\"\n",
    ")\n",
    "\n",
    "# Train multi_channel_resnet50 model\n",
    "print(\"\\n🏋️‍♀️ Training MultiChannelResNetNetwork model...\")\n",
    "multi_channel_resnet50_history = train_model(\n",
    "    model=multi_channel_resnet50_model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optimizer=multi_channel_resnet50_optimizer,\n",
    "    criterion=criterion,\n",
    "    num_epochs=num_epochs,\n",
    "    patience=patience,\n",
    "    model_name=\"MultiChannelResNetNetwork\"\n",
    ")\n",
    "\n",
    "print(\"\\n✅ All models trained successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b017e5fc",
   "metadata": {},
   "source": [
    "## 15. Evaluate Models\n",
    "\n",
    "Evaluate the trained models on the test set and compare their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a8186a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Models\n",
    "print(\"📊 Evaluating models on the test set...\")\n",
    "\n",
    "def evaluate_model(model, test_loader, criterion, model_name=\"Model\"):\n",
    "    \"\"\"\n",
    "    Evaluate a model on the test set.\n",
    "    \n",
    "    Args:\n",
    "        model: The model to evaluate\n",
    "        test_loader: DataLoader for test data\n",
    "        criterion: Loss function\n",
    "        model_name: Name for logging\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with evaluation metrics\n",
    "    \"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "    model.eval()\n",
    "    \n",
    "    test_loss = 0.0\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        test_bar = tqdm(test_loader, desc=f\"{model_name} Testing\")\n",
    "        \n",
    "        for batch_idx, (rgb, brightness, targets) in enumerate(test_bar):\n",
    "            # Move to device\n",
    "            rgb, brightness, targets = rgb.to(device), brightness.to(device), targets.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(rgb, brightness)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            # Track statistics\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            test_total += targets.size(0)\n",
    "            test_correct += predicted.eq(targets).sum().item()\n",
    "            \n",
    "            # Store predictions and targets for detailed metrics\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "            \n",
    "            # Update progress bar\n",
    "            test_bar.set_postfix({\n",
    "                'loss': test_loss/(batch_idx+1), \n",
    "                'acc': 100.*test_correct/test_total\n",
    "            })\n",
    "    \n",
    "    test_acc = 100. * test_correct / test_total\n",
    "    test_loss = test_loss / len(test_loader)\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\n📈 {model_name} Test Results:\")\n",
    "    print(f\"   Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"   Test Accuracy: {test_acc:.2f}%\")\n",
    "    \n",
    "    # Generate classification report\n",
    "    report = classification_report(\n",
    "        all_targets, \n",
    "        all_predictions, \n",
    "        target_names=[CIFAR100_FINE_LABELS[i] for i in range(100)],\n",
    "        output_dict=True\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'test_loss': test_loss,\n",
    "        'test_acc': test_acc,\n",
    "        'classification_report': report,\n",
    "        'predictions': all_predictions,\n",
    "        'targets': all_targets\n",
    "    }\n",
    "\n",
    "# Evaluate baseline model\n",
    "baseline_results = evaluate_model(\n",
    "    model=baseline_model,\n",
    "    test_loader=test_loader,\n",
    "    criterion=baseline_criterion,\n",
    "    model_name=\"Baseline ResNet50\"\n",
    ")\n",
    "\n",
    "# Evaluate base_multi_channel_large model\n",
    "base_multi_channel_large_results = evaluate_model(\n",
    "    model=base_multi_channel_large_model,\n",
    "    test_loader=test_loader,\n",
    "    criterion=criterion,\n",
    "    model_name=\"BaseMultiChannelNetwork\"\n",
    ")\n",
    "\n",
    "# Evaluate multi_channel_resnet50 model\n",
    "multi_channel_resnet50_results = evaluate_model(\n",
    "    model=multi_channel_resnet50_model,\n",
    "    test_loader=test_loader,\n",
    "    criterion=criterion,\n",
    "    model_name=\"MultiChannelResNetNetwork\"\n",
    ")\n",
    "\n",
    "# Compare models\n",
    "print(\"\\n🔍 Model Comparison on Test Set:\")\n",
    "print(f\"   Baseline ResNet50: {baseline_results['test_acc']:.2f}%\")\n",
    "print(f\"   BaseMultiChannelNetwork: {base_multi_channel_large_results['test_acc']:.2f}%\")\n",
    "print(f\"   MultiChannelResNetNetwork: {multi_channel_resnet50_results['test_acc']:.2f}%\")\n",
    "\n",
    "# Visualize learning curves and check for overfitting\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "# Accuracy curves\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(baseline_history['train_acc'], label='Baseline Train')\n",
    "plt.plot(baseline_history['val_acc'], label='Baseline Val')\n",
    "plt.title('Baseline ResNet50 Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(base_multi_channel_large_history['train_acc'], label='Train')\n",
    "plt.plot(base_multi_channel_large_history['val_acc'], label='Val')\n",
    "plt.title('BaseMultiChannelNetwork Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Loss curves\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(baseline_history['train_loss'], label='Baseline Train')\n",
    "plt.plot(baseline_history['val_loss'], label='Baseline Val')\n",
    "plt.title('Baseline ResNet50 Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(multi_channel_resnet50_history['train_acc'], label='Train')\n",
    "plt.plot(multi_channel_resnet50_history['val_acc'], label='Val')\n",
    "plt.title('MultiChannelResNetNetwork Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Overfitting analysis \n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Training vs validation gap analysis\n",
    "for i, (model_name, history) in enumerate([\n",
    "    (\"Baseline ResNet50\", baseline_history),\n",
    "    (\"BaseMultiChannelNetwork\", base_multi_channel_large_history),\n",
    "    (\"MultiChannelResNetNetwork\", multi_channel_resnet50_history)\n",
    "]):\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    \n",
    "    # Calculate generalization gap (difference between train and val accuracy)\n",
    "    gap = np.array(history['train_acc']) - np.array(history['val_acc'])\n",
    "    epochs = range(1, len(gap) + 1)\n",
    "    \n",
    "    plt.plot(epochs, gap, 'r-', label='Generalization Gap')\n",
    "    plt.axhline(y=0, color='g', linestyle='--', alpha=0.7)\n",
    "    plt.title(f'{model_name}\\nOverfitting Analysis')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Train-Val Accuracy Gap (%)')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Calculate average generalization gap in last 5 epochs\n",
    "    last_5_gap = np.mean(gap[-5:]) if len(gap) >= 5 else np.mean(gap)\n",
    "    plt.text(\n",
    "        0.5, 0.9, \n",
    "        f'Final gap: {gap[-1]:.2f}%\\nAvg last 5: {last_5_gap:.2f}%',\n",
    "        horizontalalignment='center',\n",
    "        verticalalignment='center', \n",
    "        transform=plt.gca().transAxes,\n",
    "        bbox=dict(facecolor='white', alpha=0.8)\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✅ Model evaluation complete!\")\n",
    "\n",
    "# Summarize overfitting analysis\n",
    "print(\"\\n🔍 Overfitting Analysis:\")\n",
    "for model_name, history in [\n",
    "    (\"Baseline ResNet50\", baseline_history),\n",
    "    (\"BaseMultiChannelNetwork\", base_multi_channel_large_history),\n",
    "    (\"MultiChannelResNetNetwork\", multi_channel_resnet50_history)\n",
    "]:\n",
    "    gap = np.array(history['train_acc']) - np.array(history['val_acc'])\n",
    "    last_gap = gap[-1]\n",
    "    max_gap = np.max(gap)\n",
    "    avg_last_5 = np.mean(gap[-5:]) if len(gap) >= 5 else np.mean(gap)\n",
    "    \n",
    "    print(f\"\\n   {model_name}:\")\n",
    "    print(f\"      Final train-val gap: {last_gap:.2f}%\")\n",
    "    print(f\"      Maximum gap during training: {max_gap:.2f}%\")\n",
    "    print(f\"      Average gap in last 5 epochs: {avg_last_5:.2f}%\")\n",
    "    \n",
    "    # Evaluate overfitting level\n",
    "    if avg_last_5 < 3:\n",
    "        print(f\"      ✅ No significant overfitting (gap < 3%)\")\n",
    "    elif avg_last_5 < 7:\n",
    "        print(f\"      ⚠️ Mild overfitting (3% ≤ gap < 7%)\")\n",
    "    elif avg_last_5 < 15:\n",
    "        print(f\"      🔴 Moderate overfitting (7% ≤ gap < 15%)\")\n",
    "    else:\n",
    "        print(f\"      ❌ Severe overfitting (gap ≥ 15%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483e3a42",
   "metadata": {},
   "source": [
    "## 16. Pathway Analysis\n",
    "\n",
    "Analyze the contribution of each pathway (RGB and brightness) to the model's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702b32d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pathway Analysis\n",
    "print(\"🔍 Analyzing pathway contributions for multi-stream models...\")\n",
    "\n",
    "def analyze_pathways(model, test_loader, num_samples=100, model_name=\"Model\"):\n",
    "    \"\"\"\n",
    "    Analyze contributions of RGB and brightness pathways.\n",
    "    \n",
    "    Args:\n",
    "        model: The multi-stream model to analyze\n",
    "        test_loader: DataLoader for test data\n",
    "        num_samples: Number of samples to analyze\n",
    "        model_name: Name for logging\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with pathway analysis results\n",
    "    \"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "    model.eval()\n",
    "    \n",
    "    # Get a batch of data\n",
    "    all_rgb = []\n",
    "    all_brightness = []\n",
    "    all_targets = []\n",
    "    all_combined_outputs = []\n",
    "    all_rgb_outputs = []\n",
    "    all_brightness_outputs = []\n",
    "    \n",
    "    # Collect sample data\n",
    "    sample_count = 0\n",
    "    with torch.no_grad():\n",
    "        for rgb, brightness, targets in test_loader:\n",
    "            # Break when we have enough samples\n",
    "            if sample_count >= num_samples:\n",
    "                break\n",
    "            \n",
    "            # Collect only the samples we need\n",
    "            remaining = num_samples - sample_count\n",
    "            if remaining < len(rgb):\n",
    "                rgb = rgb[:remaining]\n",
    "                brightness = brightness[:remaining]\n",
    "                targets = targets[:remaining]\n",
    "            \n",
    "            # Move to device\n",
    "            rgb, brightness, targets = rgb.to(device), brightness.to(device), targets.to(device)\n",
    "            \n",
    "            # Get outputs from combined and individual pathways\n",
    "            combined_outputs = model(rgb, brightness)\n",
    "            \n",
    "            # Use the analyze_pathways method to get individual pathway outputs\n",
    "            # This is a key feature of our multi-stream models\n",
    "            rgb_outputs, brightness_outputs = model.analyze_pathways(rgb, brightness)\n",
    "            \n",
    "            all_rgb.append(rgb.cpu())\n",
    "            all_brightness.append(brightness.cpu())\n",
    "            all_targets.append(targets.cpu())\n",
    "            all_combined_outputs.append(combined_outputs.cpu())\n",
    "            all_rgb_outputs.append(rgb_outputs.cpu())\n",
    "            all_brightness_outputs.append(brightness_outputs.cpu())\n",
    "            \n",
    "            sample_count += len(rgb)\n",
    "    \n",
    "    # Concatenate all data\n",
    "    all_rgb = torch.cat(all_rgb)\n",
    "    all_brightness = torch.cat(all_brightness)\n",
    "    all_targets = torch.cat(all_targets)\n",
    "    all_combined_outputs = torch.cat(all_combined_outputs)\n",
    "    all_rgb_outputs = torch.cat(all_rgb_outputs)\n",
    "    all_brightness_outputs = torch.cat(all_brightness_outputs)\n",
    "    \n",
    "    # Calculate accuracy for each pathway\n",
    "    _, combined_preds = all_combined_outputs.max(1)\n",
    "    _, rgb_preds = all_rgb_outputs.max(1)\n",
    "    _, brightness_preds = all_brightness_outputs.max(1)\n",
    "    \n",
    "    combined_acc = 100. * (combined_preds == all_targets).sum().item() / len(all_targets)\n",
    "    rgb_acc = 100. * (rgb_preds == all_targets).sum().item() / len(all_targets)\n",
    "    brightness_acc = 100. * (brightness_preds == all_targets).sum().item() / len(all_targets)\n",
    "    \n",
    "    print(f\"\\n📊 {model_name} Pathway Analysis:\")\n",
    "    print(f\"   Combined accuracy: {combined_acc:.2f}%\")\n",
    "    print(f\"   RGB pathway accuracy: {rgb_acc:.2f}%\")\n",
    "    print(f\"   Brightness pathway accuracy: {brightness_acc:.2f}%\")\n",
    "    \n",
    "    # Calculate pathway agreement\n",
    "    rgb_brightness_agreement = 100. * (rgb_preds == brightness_preds).sum().item() / len(all_targets)\n",
    "    combined_rgb_agreement = 100. * (combined_preds == rgb_preds).sum().item() / len(all_targets)\n",
    "    combined_brightness_agreement = 100. * (combined_preds == brightness_preds).sum().item() / len(all_targets)\n",
    "    \n",
    "    print(f\"\\n🤝 Pathway Agreement:\")\n",
    "    print(f\"   RGB-Brightness agreement: {rgb_brightness_agreement:.2f}%\")\n",
    "    print(f\"   Combined-RGB agreement: {combined_rgb_agreement:.2f}%\")\n",
    "    print(f\"   Combined-Brightness agreement: {combined_brightness_agreement:.2f}%\")\n",
    "    \n",
    "    return {\n",
    "        'combined_acc': combined_acc,\n",
    "        'rgb_acc': rgb_acc,\n",
    "        'brightness_acc': brightness_acc,\n",
    "        'rgb_brightness_agreement': rgb_brightness_agreement,\n",
    "        'combined_rgb_agreement': combined_rgb_agreement,\n",
    "        'combined_brightness_agreement': combined_brightness_agreement\n",
    "    }\n",
    "\n",
    "# Analyze BaseMultiChannelNetwork\n",
    "base_multi_channel_large_pathway_analysis = analyze_pathways(\n",
    "    model=base_multi_channel_large_model,\n",
    "    test_loader=test_loader,\n",
    "    num_samples=200,  # Use a subset for demonstration\n",
    "    model_name=\"BaseMultiChannelNetwork\"\n",
    ")\n",
    "\n",
    "# Analyze MultiChannelResNetNetwork\n",
    "multi_channel_resnet50_pathway_analysis = analyze_pathways(\n",
    "    model=multi_channel_resnet50_model,\n",
    "    test_loader=test_loader,\n",
    "    num_samples=200,  # Use a subset for demonstration\n",
    "    model_name=\"MultiChannelResNetNetwork\"\n",
    ")\n",
    "\n",
    "# Visualize pathway contributions\n",
    "models = ['BaseMultiChannelNetwork', 'MultiChannelResNetNetwork']\n",
    "combined_acc = [\n",
    "    base_multi_channel_large_pathway_analysis['combined_acc'],\n",
    "    multi_channel_resnet50_pathway_analysis['combined_acc']\n",
    "]\n",
    "rgb_acc = [\n",
    "    base_multi_channel_large_pathway_analysis['rgb_acc'],\n",
    "    multi_channel_resnet50_pathway_analysis['rgb_acc']\n",
    "]\n",
    "brightness_acc = [\n",
    "    base_multi_channel_large_pathway_analysis['brightness_acc'],\n",
    "    multi_channel_resnet50_pathway_analysis['brightness_acc']\n",
    "]\n",
    "\n",
    "# Bar chart comparing pathway accuracies\n",
    "plt.figure(figsize=(10, 6))\n",
    "x = np.arange(len(models))\n",
    "width = 0.25\n",
    "\n",
    "plt.bar(x - width, combined_acc, width, label='Combined', color='purple', alpha=0.7)\n",
    "plt.bar(x, rgb_acc, width, label='RGB Pathway', color='blue', alpha=0.7)\n",
    "plt.bar(x + width, brightness_acc, width, label='Brightness Pathway', color='gray', alpha=0.7)\n",
    "\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Pathway Contribution Analysis')\n",
    "plt.xticks(x, models)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✅ Pathway analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55adcd79",
   "metadata": {},
   "source": [
    "## 17. Save Models\n",
    "\n",
    "Save the trained models for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89734e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Models\n",
    "print(\"💾 Saving trained models...\")\n",
    "\n",
    "# Create models directory if it doesn't exist\n",
    "models_dir = Path('./models')\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Function to save model with metadata\n",
    "def save_model(model, file_name, metadata=None):\n",
    "    \"\"\"\n",
    "    Save a model with its metadata.\n",
    "    \n",
    "    Args:\n",
    "        model: The model to save\n",
    "        file_name: File name to save as\n",
    "        metadata: Dictionary with metadata to save alongside the model\n",
    "    \"\"\"\n",
    "    file_path = models_dir / file_name\n",
    "    \n",
    "    # Prepare data to save\n",
    "    save_data = {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'metadata': metadata or {},\n",
    "        'timestamp': str(pd.Timestamp.now())\n",
    "    }\n",
    "    \n",
    "    torch.save(save_data, file_path)\n",
    "    print(f\"   ✅ Saved model to {file_path}\")\n",
    "\n",
    "# Save baseline model\n",
    "baseline_metadata = {\n",
    "    'model_type': 'ResNet50',\n",
    "    'accuracy': baseline_results['test_acc'],\n",
    "    'num_classes': 100,\n",
    "    'input_channels': 3\n",
    "}\n",
    "save_model(baseline_model, 'baseline_resnet50_cifar100.pth', baseline_metadata)\n",
    "\n",
    "# Save BaseMultiChannelNetwork model\n",
    "base_multi_channel_large_metadata = {\n",
    "    'model_type': 'BaseMultiChannelNetwork',\n",
    "    'accuracy': base_multi_channel_large_results['test_acc'],\n",
    "    'num_classes': 100,\n",
    "    'rgb_input_size': rgb_input_size,\n",
    "    'brightness_input_size': brightness_input_size,\n",
    "    'pathway_analysis': base_multi_channel_large_pathway_analysis\n",
    "}\n",
    "save_model(base_multi_channel_large_model, 'base_multi_channel_large_cifar100.pth', base_multi_channel_large_metadata)\n",
    "\n",
    "# Save MultiChannelResNetNetwork model\n",
    "multi_channel_resnet50_metadata = {\n",
    "    'model_type': 'MultiChannelResNetNetwork',\n",
    "    'accuracy': multi_channel_resnet50_results['test_acc'],\n",
    "    'num_classes': 100,\n",
    "    'rgb_channels': input_channels_rgb,\n",
    "    'brightness_channels': input_channels_brightness,\n",
    "    'pathway_analysis': multi_channel_resnet50_pathway_analysis\n",
    "}\n",
    "save_model(multi_channel_resnet50_model, 'multi_channel_resnet50_cifar100.pth', multi_channel_resnet50_metadata)\n",
    "\n",
    "print(\"\\n✅ All models saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fdbb77",
   "metadata": {},
   "source": [
    "## 18. Summary\n",
    "\n",
    "Summarize the results and findings from our multi-stream neural network experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334c5338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "print(\"📋 Multi-Stream Neural Networks CIFAR-100 Training Summary\")\n",
    "\n",
    "# Training summary\n",
    "print(\"\\n🏋️‍♀️ Training Results:\")\n",
    "print(f\"   Baseline ResNet50 final validation accuracy: {baseline_history['val_acc'][-1]:.2f}%\")\n",
    "print(f\"   BaseMultiChannelNetwork final validation accuracy: {base_multi_channel_large_history['val_acc'][-1]:.2f}%\")\n",
    "print(f\"   MultiChannelResNetNetwork final validation accuracy: {multi_channel_resnet50_history['val_acc'][-1]:.2f}%\")\n",
    "\n",
    "# Testing summary\n",
    "print(\"\\n🧪 Testing Results:\")\n",
    "print(f\"   Baseline ResNet50 test accuracy: {baseline_results['test_acc']:.2f}%\")\n",
    "print(f\"   BaseMultiChannelNetwork test accuracy: {base_multi_channel_large_results['test_acc']:.2f}%\")\n",
    "print(f\"   MultiChannelResNetNetwork test accuracy: {multi_channel_resnet50_results['test_acc']:.2f}%\")\n",
    "\n",
    "# Pathway analysis summary\n",
    "print(\"\\n🔍 Pathway Analysis Summary:\")\n",
    "print(\"   BaseMultiChannelNetwork:\")\n",
    "print(f\"      Combined accuracy: {base_multi_channel_large_pathway_analysis['combined_acc']:.2f}%\")\n",
    "print(f\"      RGB pathway: {base_multi_channel_large_pathway_analysis['rgb_acc']:.2f}%, Brightness pathway: {base_multi_channel_large_pathway_analysis['brightness_acc']:.2f}%\")\n",
    "\n",
    "print(\"   MultiChannelResNetNetwork:\")\n",
    "print(f\"      Combined accuracy: {multi_channel_resnet50_pathway_analysis['combined_acc']:.2f}%\")\n",
    "print(f\"      RGB pathway: {multi_channel_resnet50_pathway_analysis['rgb_acc']:.2f}%, Brightness pathway: {multi_channel_resnet50_pathway_analysis['brightness_acc']:.2f}%\")\n",
    "\n",
    "# Create a summary table\n",
    "summary_data = {\n",
    "    'Model': ['Baseline ResNet50', 'BaseMultiChannelNetwork', 'MultiChannelResNetNetwork'],\n",
    "    'Test Acc (%)': [\n",
    "        f\"{baseline_results['test_acc']:.2f}\",\n",
    "        f\"{base_multi_channel_large_results['test_acc']:.2f}\",\n",
    "        f\"{multi_channel_resnet50_results['test_acc']:.2f}\"\n",
    "    ],\n",
    "    'RGB Pathway (%)': [\n",
    "        'N/A',\n",
    "        f\"{base_multi_channel_large_pathway_analysis['rgb_acc']:.2f}\",\n",
    "        f\"{multi_channel_resnet50_pathway_analysis['rgb_acc']:.2f}\"\n",
    "    ],\n",
    "    'Brightness Pathway (%)': [\n",
    "        'N/A',\n",
    "        f\"{base_multi_channel_large_pathway_analysis['brightness_acc']:.2f}\",\n",
    "        f\"{multi_channel_resnet50_pathway_analysis['brightness_acc']:.2f}\"\n",
    "    ],\n",
    "    'Parameters': [\n",
    "        f\"{sum(p.numel() for p in baseline_model.parameters()):,}\",\n",
    "        f\"{large_dense_params:,}\",\n",
    "        f\"{resnet50_params:,}\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Use pandas to create a nice table\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "display(summary_df)\n",
    "\n",
    "print(\"\\n📝 Key Findings:\")\n",
    "print(\"   1. Multi-stream models can leverage both RGB and brightness information\")\n",
    "print(\"   2. The RGB pathway typically contributes more to accuracy than brightness\")\n",
    "print(\"   3. The combined model performs better than individual pathways\")\n",
    "print(\"   4. MultiChannelResNetNetwork architecture is more powerful but requires more parameters\")\n",
    "\n",
    "print(\"\\n🎯 Next Steps:\")\n",
    "print(\"   1. Try different fusion strategies\")\n",
    "print(\"   2. Experiment with balancing pathway contributions\")\n",
    "print(\"   3. Apply to more complex datasets\")\n",
    "print(\"   4. Optimize model architectures based on pathway analysis\")\n",
    "\n",
    "print(\"\\n✨ Thank you for exploring Multi-Stream Neural Networks! ✨\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
