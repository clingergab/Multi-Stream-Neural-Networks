{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# DMNet Training on SUN RGB-D - Google Colab\n",
    "\n",
    "**Complete end-to-end training pipeline for Direct Mixing ResNet (DMNet) on Google Colab with A100 GPU**\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“‹ Checklist Before Running:\n",
    "\n",
    "- [ ] **Enable A100 GPU:** Runtime â†’ Change runtime type â†’ Hardware accelerator: GPU â†’ GPU type: A100\n",
    "- [ ] **Mount Google Drive:** Your code and dataset will be stored on Drive\n",
    "- [ ] **Upload dataset to Drive:** `MyDrive/datasets/sunrgbd_15/` (preprocessed 15-category dataset)\n",
    "- [ ] **Expected Runtime:** ~2-3 hours for training\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ What This Notebook Does:\n",
    "\n",
    "1. âœ… Verify A100 GPU is available\n",
    "2. âœ… Mount Google Drive\n",
    "3. âœ… Clone your repository to local disk (fast I/O)\n",
    "4. âœ… Copy SUN RGB-D dataset to local disk (10-20x faster than Drive)\n",
    "5. âœ… Install dependencies\n",
    "6. âœ… Train DMNet (Direct Mixing ResNet) with all optimizations\n",
    "7. âœ… Save checkpoints to Drive (persistent storage)\n",
    "8. âœ… Generate training curves and analysis\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§  About DMNet:\n",
    "\n",
    "**DMNet** (Direct Mixing Network) is a 2-stream neural network architecture where:\n",
    "- **RGB stream** processes color images\n",
    "- **Depth stream** processes depth maps\n",
    "- **Integrated Stream** combines both streams using learned scalar mixing weights at every layer\n",
    "\n",
    "Unlike traditional fusion methods, DMNet performs integration **inside each convolution neuron** through scalar-based direct mixing:\n",
    "- Per-stream weights (full kernels for RGB and Depth)\n",
    "- Integrated weight (1Ã—1 channel-wise for integrated features)\n",
    "- Scalar mixing coefficients (Î±, Î³) learned per layer to combine stream outputs\n",
    "\n",
    "This allows the network to learn optimal integration strategies at every layer with minimal computational overhead!\n",
    "\n",
    "---\n",
    "\n",
    "**Let's get started!** ðŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-1"
   },
   "source": [
    "## 1. Environment Setup & GPU Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "check-gpu",
    "outputId": "146e5720-50a8-43b5-d772-93fbc608458b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "GPU VERIFICATION\n",
      "============================================================\n",
      "PyTorch version: 2.10.0+cu128\n",
      "CUDA available: True\n",
      "CUDA version: 12.8\n",
      "GPU Device: NVIDIA A100-SXM4-80GB\n",
      "GPU Memory: 79.25 GB\n",
      "\n",
      "âœ… A100 GPU detected - PERFECT for training!\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Check GPU availability and specs\n",
    "import torch\n",
    "import subprocess\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"GPU VERIFICATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check PyTorch and CUDA\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "\n",
    "    # Check if it's A100\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    if 'A100' in gpu_name:\n",
    "        print(\"\\nâœ… A100 GPU detected - PERFECT for training!\")\n",
    "    elif 'V100' in gpu_name:\n",
    "        print(\"\\nâœ… V100 GPU detected - Good for training (slower than A100)\")\n",
    "    elif 'T4' in gpu_name:\n",
    "        print(\"\\nâš ï¸  T4 GPU detected - Will be slower, consider upgrading to A100\")\n",
    "    else:\n",
    "        print(f\"\\nâš ï¸  GPU: {gpu_name} - Consider using A100 for best performance\")\n",
    "else:\n",
    "    print(\"\\nâŒ NO GPU DETECTED!\")\n",
    "    print(\"Please enable GPU: Runtime â†’ Change runtime type â†’ Hardware accelerator: GPU\")\n",
    "    raise RuntimeError(\"GPU is required for training\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nvidia-smi",
    "outputId": "94b42a1f-f835-45ce-8791-b57d1479ef44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Feb 25 01:19:02 2026       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 580.82.07              Driver Version: 580.82.07      CUDA Version: 13.0     |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100-SXM4-80GB          Off |   00000000:00:05.0 Off |                    0 |\n",
      "| N/A   32C    P0             54W /  400W |       6MiB /  81920MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Detailed GPU info\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-2"
   },
   "source": [
    "## 2. Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mount-drive",
    "outputId": "e8800f9b-5101-4100-80bb-f30e7722811d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "\n",
      "âœ… Google Drive mounted successfully!\n",
      "\n",
      "Drive contents:\n",
      "total 3079872\n",
      "-rw------- 1 root root        176 Sep 21  2019 06-lab2.gdoc\n",
      "-rw------- 1 root root      21621 Sep 30  2024 113-1363667-3121001@USSR24093000064918@pre-paid.png\n",
      "-rw------- 1 root root        176 Aug 13  2020 2020 summer final (1).gdoc\n",
      "-rw------- 1 root root        176 Aug 13  2020 2020 summer final (2).gdoc\n",
      "-rw------- 1 root root        176 Aug 13  2020 2020 summer final (3).gdoc\n",
      "-rw------- 1 root root        176 Aug 13  2020 2020 summer final.gdoc\n",
      "-rw------- 1 root root        176 Jul 11  2025 2025_Gabriel_Clinger_Contractor Agreement_BASE copy.gdoc\n",
      "-rw------- 1 root root      32204 Apr 18  2022 2900 On First- Welcome Home Next Steps.docx\n",
      "-rw------- 1 root root       8822 Jun 24  2017 A6.docx\n",
      "-rw------- 1 root root      22204 Jan 21  2023 activity (1).xlsx\n",
      "-rw------- 1 root root      22161 Jan 21  2023 activity (2).xlsx\n",
      "-rw------- 1 root root        176 Jan 21  2023 activity.gsheet\n",
      "-rw------- 1 root root      13104 Jan 21  2023 activity.xlsx\n",
      "-rw------- 1 root root        176 Feb 17 23:40 AI closes the fabrication â†’ behavior loop.gdoc\n",
      "-rw------- 1 root root        176 May  5  2025 AI law & ethics cheat sheet.gdoc\n",
      "-rw------- 1 root root        176 May  7  2025 AI law & ethics final cheat sheet.gdoc\n",
      "-rw------- 1 root root        176 May  6  2025 AI Law & Ethics.gdoc\n",
      "-rw------- 1 root root        176 Apr 12  2025 AI law midterm questions.gdoc\n",
      "-rw------- 1 root root        176 May 10  2025 ailawquestions.gdoc\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "print(\"\\nâœ… Google Drive mounted successfully!\")\n",
    "print(f\"\\nDrive contents:\")\n",
    "!ls -la /content/drive/MyDrive/ | head -20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-3"
   },
   "source": [
    "## 3. Clone Repository to Local Disk (Fast I/O)\n",
    "\n",
    "**Important:** We clone to `/content/` (local SSD) instead of Drive for 10-20x faster I/O\n",
    "\n",
    "**Default:** Clone from GitHub (recommended - always gets latest code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "clone-repo",
    "outputId": "13d10897-d96c-433d-f584-ae557357450d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "REPOSITORY SETUP\n",
      "============================================================\n",
      "Starting in: /content\n",
      "\n",
      "ðŸ“ Repo already exists: /content/Multi-Stream-Neural-Networks\n",
      "ðŸ”„ Pulling latest changes...\n",
      "Already up to date.\n",
      "âœ… Repo updated\n",
      "\n",
      "ðŸ“‚ Repository structure:\n",
      "total 84\n",
      "drwxr-xr-x 12 root root  4096 Feb 25 01:18 .\n",
      "drwxr-xr-x  1 root root  4096 Feb 25 01:18 ..\n",
      "drwxr-xr-x  5 root root  4096 Feb 25 01:18 configs\n",
      "drwxr-xr-x  2 root root  4096 Feb 25 01:18 data\n",
      "drwxr-xr-x  2 root root  4096 Feb 25 01:18 docs\n",
      "drwxr-xr-x  3 root root  4096 Feb 25 01:18 experiments\n",
      "drwxr-xr-x  9 root root  4096 Feb 25 01:19 .git\n",
      "-rw-r--r--  1 root root   732 Feb 25 01:18 .gitattributes\n",
      "drwxr-xr-x  3 root root  4096 Feb 25 01:18 .github\n",
      "-rw-r--r--  1 root root   768 Feb 25 01:18 .gitignore\n",
      "-rw-r--r--  1 root root  1084 Feb 25 01:18 LICENSE\n",
      "drwxr-xr-x  2 root root  4096 Feb 25 01:18 notebooks\n",
      "-rw-r--r--  1 root root   198 Feb 25 01:18 pytest.ini\n",
      "-rw-r--r--  1 root root  3884 Feb 25 01:18 README.md\n",
      "-rw-r--r--  1 root root   126 Feb 25 01:18 requirements.txt\n",
      "drwxr-xr-x  2 root root  4096 Feb 25 01:18 scripts\n",
      "-rw-r--r--  1 root root   566 Feb 25 01:18 setup.py\n",
      "drwxr-xr-x  7 root root  4096 Feb 25 01:18 src\n",
      "drwxr-xr-x  3 root root 12288 Feb 25 01:18 tests\n",
      "\n",
      "âœ… Working directory: /content/Multi-Stream-Neural-Networks\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Configuration\n",
    "PROJECT_NAME = \"Multi-Stream-Neural-Networks\"\n",
    "GITHUB_REPO = \"https://github.com/clingergab/Multi-Stream-Neural-Networks.git\"  # UPDATE THIS\n",
    "LOCAL_REPO_PATH = f\"/content/{PROJECT_NAME}\"  # Local copy for fast I/O\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"REPOSITORY SETUP\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Ensure we're in a valid directory\n",
    "os.chdir('/content')\n",
    "print(f\"Starting in: {os.getcwd()}\")\n",
    "\n",
    "# Check if repo already exists (same session, rerunning cell)\n",
    "if Path(LOCAL_REPO_PATH).exists() and Path(f\"{LOCAL_REPO_PATH}/.git\").exists():\n",
    "    print(f\"\\nðŸ“ Repo already exists: {LOCAL_REPO_PATH}\")\n",
    "    print(f\"ðŸ”„ Pulling latest changes...\")\n",
    "\n",
    "    os.chdir(LOCAL_REPO_PATH)\n",
    "    !git pull\n",
    "    print(\"âœ… Repo updated\")\n",
    "\n",
    "# Clone from GitHub (first run)\n",
    "else:\n",
    "    # Remove old incomplete copy if exists\n",
    "    if Path(LOCAL_REPO_PATH).exists():\n",
    "        print(f\"\\nðŸ—‘ï¸  Removing incomplete repo copy...\")\n",
    "        !rm -rf {LOCAL_REPO_PATH}\n",
    "\n",
    "    print(f\"\\nðŸ”„ Cloning from GitHub...\")\n",
    "    print(f\"   Repo: {GITHUB_REPO}\")\n",
    "    print(f\"   Destination: {LOCAL_REPO_PATH}\")\n",
    "\n",
    "    !git clone {GITHUB_REPO} {LOCAL_REPO_PATH}\n",
    "\n",
    "    # Verify clone succeeded\n",
    "    if not Path(LOCAL_REPO_PATH).exists():\n",
    "        raise RuntimeError(f\"Failed to clone repository to {LOCAL_REPO_PATH}\")\n",
    "\n",
    "    print(\"âœ… Repo cloned successfully\")\n",
    "    os.chdir(LOCAL_REPO_PATH)\n",
    "\n",
    "# Verify repo structure\n",
    "print(f\"\\nðŸ“‚ Repository structure:\")\n",
    "!ls -la {LOCAL_REPO_PATH}\n",
    "\n",
    "print(f\"\\nâœ… Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-4"
   },
   "source": [
    "## 4. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "install-deps",
    "outputId": "dcc4f6d8-a1bd-4f4c-9422-a13044897e7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing dependencies...\n",
      "âœ… All dependencies installed!\n",
      "   h5py: 3.15.1\n",
      "   matplotlib: 3.10.0\n",
      "   ray: 2.54.0\n",
      "   kornia: 0.8.2\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "print(\"Installing dependencies...\")\n",
    "\n",
    "!pip install -q h5py tqdm matplotlib seaborn ray[tune] kornia\n",
    "\n",
    "# Verify installations\n",
    "import h5py\n",
    "import tqdm\n",
    "import matplotlib\n",
    "import seaborn\n",
    "import ray\n",
    "import kornia\n",
    "\n",
    "print(\"âœ… All dependencies installed!\")\n",
    "print(f\"   h5py: {h5py.__version__}\")\n",
    "print(f\"   matplotlib: {matplotlib.__version__}\")\n",
    "print(f\"   ray: {ray.__version__}\")\n",
    "print(f\"   kornia: {kornia.__version__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-5"
   },
   "source": [
    "## 5. Copy SUN RGB-D Dataset to Local Disk\n",
    "\n",
    "**Performance Note:** Local disk I/O is ~10-20x faster than Drive!\n",
    "\n",
    "**Dataset:** SUN RGB-D 15-category preprocessed dataset with RGB + Depth (~2.5 GB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "download-dataset",
    "outputId": "758ba025-5b22-4b62-d426-d6b472f5ac01"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Paths â€” 3-way split (train/val/test) for standalone training\n",
    "DRIVE_DATASET_TAR = \"/content/drive/MyDrive/datasets/sunrgbd_15.tar.gz\"  # Compressed file (2-stream: RGB + Depth)\n",
    "LOCAL_DATASET_PATH = \"/dev/shm/sunrgbd_15\"  # Extracted location\n",
    "\n",
    "# Paths â€” trainval (train/test only, no val) for k-fold CV in Ray Tune HPO\n",
    "DRIVE_TRAINVAL_TAR = \"/content/drive/MyDrive/datasets/sunrgbd_15_trainval.tar.gz\"\n",
    "LOCAL_TRAINVAL_PATH = \"/dev/shm/sunrgbd_15_trainval\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SUN RGB-D 15-CATEGORY DATASET SETUP (2-STREAM: RGB + DEPTH)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def extract_dataset(drive_tar, local_path, label):\n",
    "    \"\"\"Extract a dataset tarball from Drive to local disk if needed.\"\"\"\n",
    "    if Path(local_path).exists():\n",
    "        print(f\"\\n[{label}] Already on local disk: {local_path}\")\n",
    "        train_rgb_count = len(list(Path(f\"{local_path}/train/rgb\").glob(\"*.png\")))\n",
    "        print(f\"   Train samples: {train_rgb_count}\")\n",
    "        if Path(f\"{local_path}/val\").exists():\n",
    "            val_rgb_count = len(list(Path(f\"{local_path}/val/rgb\").glob(\"*.png\")))\n",
    "            print(f\"   Val samples: {val_rgb_count}\")\n",
    "        return True\n",
    "\n",
    "    if Path(drive_tar).exists():\n",
    "        print(f\"\\n[{label}] Found on Drive: {drive_tar}\")\n",
    "        print(f\"   Copying compressed file to local disk...\")\n",
    "\n",
    "        tar_name = Path(drive_tar).name\n",
    "        local_tar = f\"/dev/shm/{tar_name}\"\n",
    "\n",
    "        !rsync -ah --info=progress2 {drive_tar} {local_tar}\n",
    "\n",
    "        print(f\"\\n   Extracting dataset to local disk...\")\n",
    "        !tar -xzf {local_tar} -C /dev/shm/ 2>&1 | grep -v \"Ignoring unknown extended header\"\n",
    "\n",
    "        !rm {local_tar}\n",
    "\n",
    "        train_rgb_count = len(list(Path(f\"{local_path}/train/rgb\").glob(\"*.png\")))\n",
    "        print(f\"   Extracted. Train samples: {train_rgb_count}\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"\\n[{label}] NOT FOUND on Drive: {drive_tar}\")\n",
    "        return False\n",
    "\n",
    "# Extract both datasets\n",
    "extract_dataset(DRIVE_DATASET_TAR, LOCAL_DATASET_PATH, \"3-way split\")\n",
    "ok = extract_dataset(DRIVE_TRAINVAL_TAR, LOCAL_TRAINVAL_PATH, \"trainval (k-fold)\")\n",
    "\n",
    "if not ok:\n",
    "    print(f\"\\n   To create trainval dataset:\")\n",
    "    print(f\"   1. Run: python3 scripts/preprocess_sunrgbd_15.py --no-val-split\")\n",
    "    print(f\"   2. Create tarball: tar -czf sunrgbd_15_trainval.tar.gz -C data sunrgbd_15_trainval\")\n",
    "    print(f\"   3. Upload to Google Drive at: {DRIVE_TRAINVAL_TAR}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"3-way dataset: {LOCAL_DATASET_PATH}\")\n",
    "print(f\"Trainval dataset: {LOCAL_TRAINVAL_PATH}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-6"
   },
   "source": [
    "## 6. Setup Python Path & Import DMNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "setup-imports",
    "outputId": "2b37a0e7-493f-49df-e658-28283153cfc3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project structure:\n",
      "total 48\n",
      "drwxr-xr-x 11 root root 4096 Feb 25 01:18 .\n",
      "drwxr-xr-x  7 root root 4096 Feb 25 01:18 ..\n",
      "drwxr-xr-x  2 root root 4096 Feb 25 01:18 abstracts\n",
      "drwxr-xr-x  2 root root 4096 Feb 25 01:18 common\n",
      "drwxr-xr-x  2 root root 4096 Feb 25 01:18 core\n",
      "drwxr-xr-x  2 root root 4096 Feb 25 01:18 direct_mixing_activation\n",
      "drwxr-xr-x  2 root root 4096 Feb 25 01:18 direct_mixing_bn\n",
      "drwxr-xr-x  2 root root 4096 Feb 25 01:18 direct_mixing_conv\n",
      "-rw-r--r--  1 root root 1076 Feb 25 01:18 __init__.py\n",
      "drwxr-xr-x  4 root root 4096 Feb 25 01:18 linear_integration\n",
      "drwxr-xr-x  2 root root 4096 Feb 25 01:18 multi_channel\n",
      "drwxr-xr-x  2 root root 4096 Feb 25 01:18 utils\n",
      "\n",
      "Importing LiNet and dataloaders...\n",
      "âœ… LINet3, dataloaders, and Ray Tune imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Remove cached modules\n",
    "modules_to_reload = [k for k in sys.modules.keys() if k.startswith('src.')]\n",
    "for module in modules_to_reload:\n",
    "    del sys.modules[module]\n",
    "\n",
    "# Add project to Python path\n",
    "project_root = '/content/Multi-Stream-Neural-Networks'\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "# Verify project structure\n",
    "print(\"Project structure:\")\n",
    "!ls -la {project_root}/src/models/\n",
    "\n",
    "# Import LiNet and SUN RGB-D dataloader\n",
    "print(\"\\nImporting LiNet and dataloaders...\")\n",
    "from src.models.linear_integration.li_net3 import li_resnet18\n",
    "from src.data_utils.sunrgbd_dataset import get_sunrgbd_dataloaders\n",
    "from src.training.augmentation_config import AugmentationConfig\n",
    "\n",
    "\n",
    "# Import Ray Tune\n",
    "from ray import train, tune\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "\n",
    "print(\"âœ… LINet3, dataloaders, and Ray Tune imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "xBtf06YvJrKJ"
   },
   "outputs": [],
   "source": [
    "# from scripts.benchmark_padding_vs_sequential import run_benchmark_suite\n",
    "# run_benchmark_suite([3, 1], batch_size=96, use_torch_compile=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Acbzx08zFfEw"
   },
   "outputs": [],
   "source": [
    "# from scripts.benchmark_padding_vs_sequential import run_benchmark_suite\n",
    "# run_benchmark_suite([3, 1], batch_size=96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Oy2hse8LZNCi"
   },
   "outputs": [],
   "source": [
    "# # Set random seed for reproducibility\n",
    "# from src.utils.seed import set_seed\n",
    "\n",
    "# SEED = 42\n",
    "# DETERMINISTIC = False  # False = faster, True = fully reproducible\n",
    "\n",
    "# print(\"=\" * 60)\n",
    "# print(\"RANDOM SEED CONFIGURATION\")\n",
    "# print(\"=\" * 60)\n",
    "\n",
    "# set_seed(SEED, deterministic=DETERMINISTIC)\n",
    "\n",
    "# print(f\"\\nâœ… Seed: {SEED}\")\n",
    "# print(f\"   Deterministic: {DETERMINISTIC}\")\n",
    "# if DETERMINISTIC:\n",
    "#     print(\"   Mode: Fully reproducible (slower)\")\n",
    "# else:\n",
    "#     print(\"   Mode: Fast reproducible\")\n",
    "\n",
    "# print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-7"
   },
   "source": [
    "## 7. Load SUN RGB-D Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "rZCuJeYE3eQx"
   },
   "outputs": [],
   "source": [
    "# # Verify dataset structure\n",
    "# from pathlib import Path\n",
    "\n",
    "# print(\"=\" * 60)\n",
    "# print(\"DATASET STRUCTURE VERIFICATION\")\n",
    "# print(\"=\" * 60)\n",
    "\n",
    "# dataset_root = Path(LOCAL_DATASET_PATH)\n",
    "\n",
    "# print(\"\\nDirectory structure:\")\n",
    "# print(f\"  {dataset_root}/\")\n",
    "# print(f\"    train/\")\n",
    "# print(f\"      rgb/ - {len(list((dataset_root / 'train' / 'rgb').glob('*.png')))} images\")\n",
    "# print(f\"      depth/ - {len(list((dataset_root / 'train' / 'depth').glob('*.png')))} images\")\n",
    "# print(f\"      labels.txt\")\n",
    "# print(f\"    val/\")\n",
    "# print(f\"      rgb/ - {len(list((dataset_root / 'val' / 'rgb').glob('*.png')))} images\")\n",
    "# print(f\"      depth/ - {len(list((dataset_root / 'val' / 'depth').glob('*.png')))} images\")\n",
    "# print(f\"      labels.txt\")\n",
    "# print(f\"    class_names.txt\")\n",
    "# print(f\"    dataset_info.txt\")\n",
    "\n",
    "# # Read class names\n",
    "# with open(dataset_root / 'class_names.txt', 'r') as f:\n",
    "#     class_names = [line.strip() for line in f]\n",
    "\n",
    "# print(f\"\\nClasses ({len(class_names)}):\")\n",
    "# for i, name in enumerate(class_names):\n",
    "#     print(f\"  {i}: {name}\")\n",
    "\n",
    "# print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "load-dataset"
   },
   "outputs": [],
   "source": [
    "# print(\"=\" * 60)\n",
    "# print(\"LOADING SUN RGB-D 15-CATEGORY DATASET (2-STREAM: RGB + DEPTH)\")\n",
    "# print(\"=\" * 60)\n",
    "\n",
    "# # Dataset configuration\n",
    "# DATASET_CONFIG = {\n",
    "#     'data_root': LOCAL_DATASET_PATH,\n",
    "#     'batch_size': 96,  # Good balance for A100\n",
    "#     'num_workers': 8,\n",
    "#     'target_size': (416, 544),\n",
    "#     'num_classes': 15,  # SUN RGB-D merged to 15 categories (labels 0-14)\n",
    "#     'seed': SEED  # For reproducible data loading\n",
    "# }\n",
    "\n",
    "# # Augmentation configuration (per-stream control)\n",
    "# # Set to 1.0 for baseline behavior, adjust to tune augmentation strength\n",
    "# AUGMENTATION_CONFIG = AugmentationConfig(\n",
    "#     rgb_aug_prob=1.0,    # Scales probability of RGB augmentations\n",
    "#     rgb_aug_mag=1.0,     # Scales magnitude of RGB augmentations\n",
    "#     depth_aug_prob=1.0,  # Scales probability of Depth augmentations\n",
    "#     depth_aug_mag=1.0,   # Scales magnitude of Depth augmentations\n",
    "# )\n",
    "\n",
    "# print(f\"Configuration:\")\n",
    "# for key, value in DATASET_CONFIG.items():\n",
    "#     print(f\"  {key}: {value}\")\n",
    "\n",
    "# print(f\"\\nAugmentation Configuration:\")\n",
    "# print(f\"  {AUGMENTATION_CONFIG}\")\n",
    "\n",
    "# print(f\"\\nLoading dataset from: {DATASET_CONFIG['data_root']}\")\n",
    "\n",
    "# # Create reproducible dataloaders\n",
    "# # normalize=False because GPU augmentation will handle normalization after augmentation\n",
    "# train_loader, val_loader = get_sunrgbd_dataloaders(\n",
    "#     data_root=DATASET_CONFIG['data_root'],\n",
    "#     batch_size=DATASET_CONFIG['batch_size'],\n",
    "#     num_workers=DATASET_CONFIG['num_workers'],\n",
    "#     target_size=DATASET_CONFIG['target_size'],\n",
    "#     seed=DATASET_CONFIG['seed'],  # Pass seed for reproducibility\n",
    "#     normalize=False,  # GPU will normalize after augmentation\n",
    "#     **AUGMENTATION_CONFIG.to_dict(),  # Pass augmentation params\n",
    "# )\n",
    "\n",
    "# print(f\"\\nâœ… Dataset loaded successfully!\")\n",
    "# print(f\"\\nDataset Statistics:\")\n",
    "# print(f\"  Train batches: {len(train_loader)}\")\n",
    "# print(f\"  Val batches: {len(val_loader)}\")\n",
    "# print(f\"  Train samples: {len(train_loader.dataset)}\")\n",
    "# print(f\"  Val samples: {len(val_loader.dataset)}\")\n",
    "# print(f\"  Batch size: {DATASET_CONFIG['batch_size']}\")\n",
    "\n",
    "# # Test loading a batch\n",
    "# print(f\"\\nTesting batch loading...\")\n",
    "# rgb_batch, depth_batch, label_batch = next(iter(train_loader))\n",
    "# print(f\"  RGB shape: {rgb_batch.shape}\")\n",
    "# print(f\"  Depth shape: {depth_batch.shape}\")\n",
    "# print(f\"  Labels shape: {label_batch.shape}\")\n",
    "# print(f\"  Labels min: {label_batch.min().item()}, max: {label_batch.max().item()}\")\n",
    "\n",
    "# print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-8"
   },
   "source": [
    "## 8. Visualize Sample Data\n",
    "\n",
    "Shows RGB images, depth maps, and scene labels from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "visualize-data"
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# import torch\n",
    "\n",
    "# # Visualize some samples from TRAINING set\n",
    "# # Note: With normalize=False, data is in [0, 1] range (no denormalization needed)\n",
    "# # GPU augmentation will normalize on-device during training, but raw data is [0, 1]\n",
    "# print(\"Loading samples from TRAINING set...\")\n",
    "# rgb_batch, depth_batch, label_batch = next(iter(train_loader))\n",
    "\n",
    "# print(\"\\n\" + \"=\"*60)\n",
    "# print(\"Creating visualization...\")\n",
    "# print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# fig, axes = plt.subplots(2, 4, figsize=(14, 7))\n",
    "\n",
    "# for i in range(4):\n",
    "#     rgb = rgb_batch[i].cpu()\n",
    "#     depth = depth_batch[i].cpu()\n",
    "#     label = label_batch[i].item()\n",
    "\n",
    "#     # Data is already in [0, 1] range (normalize=False in dataloader)\n",
    "#     # Clamp to handle any edge cases\n",
    "#     rgb_vis = torch.clamp(rgb, 0, 1)\n",
    "#     depth_vis = torch.clamp(depth, 0, 1)\n",
    "\n",
    "#     # Plot RGB\n",
    "#     axes[0, i].imshow(rgb_vis.permute(1, 2, 0))\n",
    "#     axes[0, i].set_title(f\"RGB - Class {label}\", fontsize=10)\n",
    "#     axes[0, i].axis('off')\n",
    "\n",
    "#     # Plot Depth\n",
    "#     axes[1, i].imshow(depth_vis.squeeze(), cmap='viridis')\n",
    "#     axes[1, i].set_title(f\"Depth - Class {label}\", fontsize=10)\n",
    "#     axes[1, i].axis('off')\n",
    "\n",
    "# plt.suptitle('SUN RGB-D Training Data (RGB + Depth)', fontsize=14, fontweight='bold')\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# print(\"âœ… Sample visualization complete!\")\n",
    "# print(\"\\nNote: Data is shown in raw [0, 1] range (before GPU augmentation/normalization).\")\n",
    "# print(\"During training, GPU augmentation applies: color jitter, blur, grayscale,\")\n",
    "# print(\"normalization, and random erasing on-device.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QdXy6Gl43eQy"
   },
   "source": [
    "## 8b. Hyperparameter Tuning with Ray Tune (Optional)\n",
    "\n",
    "Perform a wide search for optimal hyperparameters using Ray Tune.\n",
    "- **Parallel Trials:** Run multiple configurations simultaneously\n",
    "- **Data Subset:** Use 50% of data for fast iteration\n",
    "- **Short Duration:** Train for 10 epochs per trial\n",
    "- **ASHA Scheduler:** Early stopping for bad trials\n",
    "- **Uses fit() method:** Ensures consistency with main training (no custom training loop!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BNE0b7ssK--2",
    "outputId": "bef69604-f7c4-4280-fb10-50db6932ff84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting GPU to Exclusive Process Mode...\n",
      "Set compute mode to EXCLUSIVE_PROCESS for GPU 00000000:00:05.0.\n",
      "All done.\n",
      "Starting MPS Daemon...\n",
      "Verifying Daemon Status...\n",
      "root        5290       1  0 01:21 ?        00:00:00 nvidia-cuda-mps-control -d\n",
      "root        5296    4513  0 01:21 ?        00:00:00 /bin/bash -c ps -ef | grep mps\n",
      "root        5298    5296  0 01:21 ?        00:00:00 grep mps\n",
      "âœ… MPS Control Pipe found. Setup success.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "# 1. Define Paths explicitly\n",
    "mps_pipe_dir = \"/tmp/nvidia-mps\"\n",
    "mps_log_dir = \"/tmp/nvidia-log\"\n",
    "\n",
    "# 2. Create the directories (CRITICAL: Daemon fails if log dir doesn't exist)\n",
    "os.makedirs(mps_pipe_dir, exist_ok=True)\n",
    "os.makedirs(mps_log_dir, exist_ok=True)\n",
    "\n",
    "# 3. Set Environment Variables for the current Python process\n",
    "os.environ[\"CUDA_MPS_PIPE_DIRECTORY\"] = mps_pipe_dir\n",
    "os.environ[\"CUDA_MPS_LOG_DIRECTORY\"] = mps_log_dir\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "\n",
    "# 4. Configure GPU and Start Daemon using the SAME environment variables\n",
    "# We use f-strings to pass the python variables into the shell command\n",
    "print(\"Setting GPU to Exclusive Process Mode...\")\n",
    "!nvidia-smi -i 0 -c EXCLUSIVE_PROCESS\n",
    "\n",
    "print(\"Starting MPS Daemon...\")\n",
    "# We explicitly pass the env vars to the shell command\n",
    "!export CUDA_MPS_PIPE_DIRECTORY={mps_pipe_dir} && \\\n",
    " export CUDA_MPS_LOG_DIRECTORY={mps_log_dir} && \\\n",
    " nvidia-cuda-mps-control -d\n",
    "\n",
    "# 5. Verify it is running\n",
    "print(\"Verifying Daemon Status...\")\n",
    "time.sleep(1) # Give it a second to start\n",
    "!ps -ef | grep mps\n",
    "\n",
    "# Check if the pipe file actually exists\n",
    "if os.path.exists(os.path.join(mps_pipe_dir, \"control\")):\n",
    "    print(\"âœ… MPS Control Pipe found. Setup success.\")\n",
    "else:\n",
    "    print(\"âŒ MPS Control Pipe NOT found. Check /tmp/nvidia-log for errors.\")\n",
    "    # Optional: Print logs if it failed\n",
    "    !cat {mps_log_dir}/control.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HW1_2wH63eQy"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "import torch\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from src.models.linear_integration.li_net3 import li_resnet18\n",
    "from src.training.optimizers import create_stream_optimizer\n",
    "from src.training.schedulers import setup_scheduler\n",
    "from src.data_utils.sunrgbd_dataset import SUNRGBDDataset\n",
    "from src.training.augmentation_config import AugmentationConfig\n",
    "from src.utils.seed import set_seed\n",
    "\n",
    "\n",
    "class TrialTerminated(Exception):\n",
    "    \"\"\"Raised when a trial should be terminated early.\"\"\"\n",
    "    pass\n",
    "\n",
    "class RayTuneReporter:\n",
    "    \"\"\"Callback for reporting metrics to Ray Tune during training.\"\"\"\n",
    "\n",
    "    def __init__(self, fold_idx=None):\n",
    "        self.best_accuracy = 0.0\n",
    "        self.best_loss = float('inf')\n",
    "        self.fold_idx = fold_idx\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        \"\"\"Report current AND best metrics to Ray Tune.\"\"\"\n",
    "        # Track best metrics\n",
    "        if logs['val_accuracy'] > self.best_accuracy:\n",
    "            self.best_accuracy = logs['val_accuracy']\n",
    "        if logs['val_loss'] < self.best_loss:\n",
    "            self.best_loss = logs['val_loss']\n",
    "\n",
    "        # Report both current and best metrics\n",
    "        metrics = {\n",
    "            \"accuracy\": logs['val_accuracy'],        # Current epoch\n",
    "            \"loss\": logs['val_loss'],                # Current epoch\n",
    "            \"best_accuracy\": self.best_accuracy,     # Best so far\n",
    "            \"best_loss\": self.best_loss,             # Best so far\n",
    "            \"train_loss\": logs['train_loss'],\n",
    "            \"train_accuracy\": logs['train_accuracy'],\n",
    "        }\n",
    "        if self.fold_idx is not None:\n",
    "            metrics[\"fold\"] = self.fold_idx\n",
    "\n",
    "        tune.report(metrics)\n",
    "\n",
    "        # Early termination for bad trials\n",
    "        if epoch == 31 and self.best_accuracy < 0.6:\n",
    "            raise TrialTerminated(\n",
    "                f\"Trial terminated: best_accuracy={self.best_accuracy:.1%} < 0.6 \"\n",
    "                f\"at epoch 31\"\n",
    "            )\n",
    "        if epoch == 81 and self.best_accuracy < 0.7:\n",
    "            raise TrialTerminated(\n",
    "                f\"Trial terminated: best_accuracy={self.best_accuracy:.1%} < 0.7 \"\n",
    "                f\"at epoch 81\"\n",
    "            )\n",
    "\n",
    "\n",
    "def train_linet_tune(config, data_root=None, target_size=None, seed=42):\n",
    "    \"\"\"\n",
    "    Trainable function for Ray Tune using fit() method with k-fold CV.\n",
    "\n",
    "    Each trial trains on 1 randomly-assigned fold (out of 5). The fold index\n",
    "    is assigned BEFORE set_seed() so that different trials get different folds\n",
    "    (set_seed() would make random.randint deterministic otherwise).\n",
    "\n",
    "    Args:\n",
    "        config: Ray Tune configuration dict with hyperparameters\n",
    "        data_root: Path to trainval dataset root (no val/ split)\n",
    "        target_size: Target image size (H, W)\n",
    "        seed: Random seed for reproducible trials\n",
    "    \"\"\"\n",
    "    # CRITICAL: Assign fold BEFORE set_seed() â€” set_seed() seeds Python's\n",
    "    # random module, so random.randint() after it returns the same fold\n",
    "    # for every trial. This must come first for true randomness.\n",
    "    fold_idx = random.randint(0, 4)\n",
    "\n",
    "    # Seed this worker process for reproducibility (after fold assignment)\n",
    "    set_seed(seed, deterministic=False)\n",
    "    g = torch.Generator().manual_seed(seed)\n",
    "\n",
    "    # Per-trial augmentation config\n",
    "    aug_config = AugmentationConfig(\n",
    "        rgb_aug_prob=config.get(\"rgb_aug_prob\", 1.0),\n",
    "        rgb_aug_mag=config.get(\"rgb_aug_mag\", 1.0),\n",
    "        depth_aug_prob=config.get(\"depth_aug_prob\", 1.0),\n",
    "        depth_aug_mag=config.get(\"depth_aug_mag\", 1.0),\n",
    "    )\n",
    "\n",
    "    # Two dataset instances from the same train/ directory:\n",
    "    # 1) train_dataset: augmentation ON (split='train')\n",
    "    # 2) val_dataset:   augmentation OFF (split overridden to 'val')\n",
    "    # Both load from the same trainval train/ dir â€” mmap shares OS pages.\n",
    "    train_dataset = SUNRGBDDataset(\n",
    "        data_root=data_root,\n",
    "        split='train',\n",
    "        target_size=target_size,\n",
    "        normalize=False,  # GPU will normalize after augmentation\n",
    "        **aug_config.to_dict(),\n",
    "    )\n",
    "    val_dataset = SUNRGBDDataset(\n",
    "        data_root=data_root,\n",
    "        split='train',\n",
    "        target_size=target_size,\n",
    "        normalize=False,\n",
    "    )\n",
    "    val_dataset.split = 'val'  # Disable augmentation in __getitem__\n",
    "\n",
    "    # K-fold split (deterministic given seed â€” same fold definitions every trial)\n",
    "    all_labels = train_dataset.labels\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "    folds = list(skf.split(range(len(all_labels)), all_labels))\n",
    "    train_indices, val_indices = folds[fold_idx]\n",
    "\n",
    "    # Create fold subsets\n",
    "    train_subset = torch.utils.data.Subset(train_dataset, train_indices)\n",
    "    val_subset = torch.utils.data.Subset(val_dataset, val_indices)\n",
    "\n",
    "    # Stratified sampling for training fold\n",
    "    subset_labels = [all_labels[i] for i in train_indices]\n",
    "    label_counts = Counter(subset_labels)\n",
    "\n",
    "    # Compute class weights (inverse frequency)\n",
    "    num_samples = len(subset_labels)\n",
    "    class_weights = {label: num_samples / count for label, count in label_counts.items()}\n",
    "    sample_weights = torch.tensor([class_weights[label] for label in subset_labels], dtype=torch.float32)\n",
    "\n",
    "    # Create sampler\n",
    "    train_sampler = torch.utils.data.WeightedRandomSampler(\n",
    "        weights=sample_weights,\n",
    "        num_samples=num_samples,\n",
    "        replacement=True,\n",
    "        generator=g\n",
    "    )\n",
    "\n",
    "    # Worker init function for reproducible data loading\n",
    "    def worker_init_fn(worker_id):\n",
    "        worker_seed = seed + worker_id\n",
    "        np.random.seed(worker_seed)\n",
    "        random.seed(worker_seed)\n",
    "\n",
    "    # Create subset dataloaders\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_subset,\n",
    "        batch_size=96,\n",
    "        shuffle=False,  # Disabled when using sampler\n",
    "        sampler=train_sampler,\n",
    "        num_workers=2,\n",
    "        prefetch_factor=2,\n",
    "        persistent_workers=True,\n",
    "        pin_memory=True,\n",
    "        worker_init_fn=worker_init_fn\n",
    "    )\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_subset,\n",
    "        batch_size=96,\n",
    "        shuffle=False,\n",
    "        num_workers=2,\n",
    "        prefetch_factor=2,\n",
    "        persistent_workers=False,\n",
    "        pin_memory=True,\n",
    "        worker_init_fn=worker_init_fn\n",
    "    )\n",
    "\n",
    "    # 2. Create Model (2-stream: RGB + Depth)\n",
    "    model = li_resnet18(\n",
    "        num_classes=15,\n",
    "        stream_input_channels=[3, 1],  # RGB=3, Depth=1\n",
    "        dropout_p=config[\"dropout_p\"],\n",
    "        device=\"cuda\",\n",
    "        use_amp=True\n",
    "    )\n",
    "\n",
    "    # 3. Create Optimizer with stream-specific learning rates\n",
    "    optimizer = create_stream_optimizer(\n",
    "        model,\n",
    "        optimizer_type='adamw',\n",
    "        stream_lrs=[config[\"lr_rgb\"], config[\"lr_depth\"]],\n",
    "        stream_weight_decays=[config[\"wd_rgb\"], config[\"wd_depth\"]],\n",
    "        shared_lr=config[\"lr_shared\"],\n",
    "        shared_weight_decay=config[\"wd_shared\"]\n",
    "    )\n",
    "\n",
    "    # 4. Create Scheduler\n",
    "    warmup_epochs = 5\n",
    "\n",
    "    scheduler = setup_scheduler(\n",
    "        optimizer,\n",
    "        scheduler_type='cosine',\n",
    "        eta_min=[config['s1_eta_min'], config['s2_eta_min'], config['eta_min']],\n",
    "        t_max=config['t_max'],\n",
    "        train_loader_len=len(train_loader),\n",
    "        warmup_epochs=warmup_epochs,\n",
    "        warmup_start_factor=0.2\n",
    "    )\n",
    "\n",
    "    # 5. Compile model (Keras-style API)\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        loss='cross_entropy',\n",
    "        label_smoothing=config[\"label_smoothing\"],\n",
    "        gpu_augmentation=True,\n",
    "        **aug_config.to_dict(),\n",
    "    )\n",
    "\n",
    "    # 6. Train using fit() with Ray Tune callback\n",
    "    try:\n",
    "        model.fit(\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            epochs=120,\n",
    "            early_stopping=True,\n",
    "            patience=15,\n",
    "            grad_clip_norm=config[\"grad_clip_norm\"],\n",
    "            modality_dropout=True,\n",
    "            modality_dropout_start=0,\n",
    "            modality_dropout_ramp=20,\n",
    "            modality_dropout_rate=config['modality_dropout_rate'],\n",
    "            callbacks=[RayTuneReporter(fold_idx=fold_idx)],\n",
    "            verbose=False\n",
    "        )\n",
    "    except TrialTerminated as e:\n",
    "        print(f\"\\n{e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rxf1AxrqJoRy",
    "outputId": "a3edc7b6-e018-42af-ca49-1779a886aa6a"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# WARM-START CONFIGURATION (Optional)\n",
    "# =============================================================================\n",
    "# Enable warm-starting to continue exploration from previous runs.\n",
    "# HyperOptSearch's full TPE model state is saved/restored via cloudpickle,\n",
    "# so the surrogate model retains all (config, result) observations across\n",
    "# Colab sessions. If the search space changes, the checkpoint is ignored\n",
    "# and exploration starts fresh.\n",
    "#\n",
    "# IMPORTANT: Configs are tagged with a search space hash. If you change your\n",
    "# search space, old data is automatically ignored (fresh exploration starts).\n",
    "# =============================================================================\n",
    "\n",
    "import hashlib\n",
    "import json as json_module  # avoid conflict with pandas\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from ray.tune.search.sample import Domain\n",
    "\n",
    "WARM_START_ENABLED = True  # Set to True to load previous results\n",
    "WARM_START_CSV_PATH = \"/content/drive/MyDrive/ray_tune_results/ray_tune_results.csv\"  # Path on Google Drive (persistent)\n",
    "EPOCH_HISTORY_CSV_PATH = \"/content/drive/MyDrive/ray_tune_results/epoch_history.csv\"  # Per-epoch metrics for HistoricalMedianStoppingRule\n",
    "HYPEROPT_CHECKPOINT_DIR = \"/content/drive/MyDrive/ray_tune_results\"  # Directory for TPE model checkpoints\n",
    "\n",
    "\n",
    "def get_search_space_hash(search_space: dict) -> str:\n",
    "    \"\"\"\n",
    "    Generate a short hash to identify a search space configuration.\n",
    "\n",
    "    This allows us to track which configs came from which search space,\n",
    "    so we only warm-start from compatible configs.\n",
    "\n",
    "    Args:\n",
    "        search_space: Ray Tune search space dict\n",
    "\n",
    "    Returns:\n",
    "        8-character hash string\n",
    "    \"\"\"\n",
    "\n",
    "    def _serialize_value(v):\n",
    "        if isinstance(v, Domain):\n",
    "            if hasattr(v, \"categories\"):\n",
    "                return sorted([repr(c) for c in v.categories])\n",
    "            # Include sampler type to distinguish loguniform from uniform etc.\n",
    "            sampler_name = type(v.sampler).__name__ if hasattr(v, \"sampler\") else \"\"\n",
    "            domain_str = repr(v.domain_str) if hasattr(v, \"domain_str\") else type(v).__name__\n",
    "            return f\"{sampler_name}:{domain_str}\"\n",
    "        return repr(v)\n",
    "\n",
    "    space_repr = {k: _serialize_value(v) for k, v in sorted(search_space.items())}\n",
    "    space_str = json_module.dumps(space_repr, sort_keys=True)\n",
    "    return hashlib.md5(space_str.encode()).hexdigest()[:8]\n",
    "\n",
    "\n",
    "# Ensure the directory exists on Google Drive\n",
    "Path(WARM_START_CSV_PATH).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Note: Warm-start loading happens AFTER search_space is defined (in next cell)\n",
    "# We just set the flag here\n",
    "print(f\"Warm-start: {'ENABLED' if WARM_START_ENABLED else 'DISABLED'}\")\n",
    "if WARM_START_ENABLED:\n",
    "    print(f\"   Results CSV: {WARM_START_CSV_PATH}\")\n",
    "    print(f\"   Epoch history: {EPOCH_HISTORY_CSV_PATH}\")\n",
    "    print(f\"   HyperOpt checkpoint dir: {HYPEROPT_CHECKPOINT_DIR}\")\n",
    "else:\n",
    "    print(\"   To enable: set WARM_START_ENABLED = True\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2Lc7KUfB3eQz",
    "outputId": "6a3926dc-56b6-48d9-9fb8-3fead58d5538"
   },
   "outputs": [],
   "source": [
    "# Initialize Ray\n",
    "from ray.tune.search.hyperopt import HyperOptSearch\n",
    "from ray.tune.search import ConcurrencyLimiter\n",
    "from src.training.historical_median_stopping import HistoricalMedianStoppingRule\n",
    "\n",
    "ray.shutdown()  # Clean shutdown of any previous Ray instance\n",
    "ray.init(\n",
    "    ignore_reinit_error=True,\n",
    "    runtime_env={\n",
    "        \"env_vars\": {\n",
    "            \"CUDA_MPS_PIPE_DIRECTORY\": \"/tmp/nvidia-mps\",\n",
    "            \"CUDA_MPS_LOG_DIRECTORY\": \"/tmp/nvidia-log\",\n",
    "            \"CUDA_DEVICE_ORDER\": \"PCI_BUS_ID\",\n",
    "            # Ensure workers see the GPU as Device 0\n",
    "            \"CUDA_VISIBLE_DEVICES\": \"0\"\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "# Continuous search space for TPE optimization (k-fold CV)\n",
    "search_space = {\n",
    "    # Learning rates (log-uniform for order-of-magnitude exploration)\n",
    "    \"lr_rgb\": tune.loguniform(5e-6, 1e-3),\n",
    "    \"lr_depth\": tune.loguniform(2e-5, 1e-3),\n",
    "    \"lr_shared\": tune.loguniform(1e-6, 5e-5),\n",
    "\n",
    "    # Weight decay (log-uniform)\n",
    "    \"wd_rgb\": tune.loguniform(1e-6, 5e-4),\n",
    "    \"wd_depth\": tune.loguniform(5e-6, 1e-3),\n",
    "    \"wd_shared\": tune.loguniform(1e-4, 1e-2),\n",
    "\n",
    "    # Scheduler eta_min (log-uniform)\n",
    "    \"s1_eta_min\": tune.loguniform(5e-8, 2e-6),\n",
    "    \"s2_eta_min\": tune.loguniform(1e-7, 3e-6),\n",
    "    \"eta_min\": tune.loguniform(1e-8, 1e-6),\n",
    "\n",
    "    # Scheduler t_max (integer)\n",
    "    \"t_max\": tune.randint(80, 111),\n",
    "\n",
    "    # Regularization (uniform)\n",
    "    \"dropout_p\": tune.uniform(0.3, 0.7),\n",
    "    \"label_smoothing\": tune.uniform(0.01, 0.15),\n",
    "    \"grad_clip_norm\": tune.uniform(0.5, 2.0),\n",
    "\n",
    "    # Augmentation parameters (per-stream control)\n",
    "    \"rgb_aug_prob\": tune.uniform(0.9, 1.8),\n",
    "    \"rgb_aug_mag\": tune.uniform(0.9, 1.8),\n",
    "    \"depth_aug_prob\": tune.uniform(0.9, 1.8),\n",
    "    \"depth_aug_mag\": tune.uniform(0.9, 1.8),\n",
    "\n",
    "    # Modality dropout\n",
    "    \"modality_dropout_rate\": tune.uniform(0.1, 0.3),\n",
    "}\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"K-FOLD CV SEARCH SPACE (CONTINUOUS, 2-STREAM: RGB + DEPTH)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# HyperOptSearch uses best_accuracy for the TPE surrogate model\n",
    "# (so it optimizes for running-best, not noisy per-epoch accuracy)\n",
    "hyperopt_searcher = HyperOptSearch(\n",
    "    metric=\"best_accuracy\",\n",
    "    mode=\"max\",\n",
    "    random_state_seed=SEED,\n",
    ")\n",
    "\n",
    "if WARM_START_ENABLED:\n",
    "    current_hash = get_search_space_hash(search_space)\n",
    "    hyperopt_checkpoint_path = os.path.join(HYPEROPT_CHECKPOINT_DIR, f\"hyperopt_searcher_{current_hash}.pkl\")\n",
    "    _restored = False\n",
    "\n",
    "    # Migrate old-format checkpoint (hyperopt_searcher.pkl + .hash) to new format\n",
    "    old_checkpoint = os.path.join(HYPEROPT_CHECKPOINT_DIR, \"hyperopt_searcher.pkl\")\n",
    "    old_hash_file = old_checkpoint + \".hash\"\n",
    "    if not os.path.exists(hyperopt_checkpoint_path) and os.path.exists(old_checkpoint) and os.path.exists(old_hash_file):\n",
    "        with open(old_hash_file, \"r\") as f:\n",
    "            old_hash = f.read().strip()\n",
    "        if old_hash == current_hash:\n",
    "            os.rename(old_checkpoint, hyperopt_checkpoint_path)\n",
    "            os.remove(old_hash_file)\n",
    "            print(f\"   Migrated old checkpoint to {hyperopt_checkpoint_path}\")\n",
    "\n",
    "    if os.path.exists(hyperopt_checkpoint_path):\n",
    "        try:\n",
    "            hyperopt_searcher.restore(hyperopt_checkpoint_path)\n",
    "            # Clear stale trial mappings from previous session.\n",
    "            hyperopt_searcher._live_trial_mapping = {}\n",
    "            # Remove any instance-level _setup_hyperopt that may have been\n",
    "            # pickled from a previous session's monkey-patch.\n",
    "            if '_setup_hyperopt' in hyperopt_searcher.__dict__:\n",
    "                del hyperopt_searcher.__dict__['_setup_hyperopt']\n",
    "            # Wrap _setup_hyperopt to preserve restored trials.\n",
    "            # set_search_properties() (called by Tuner.fit()) triggers\n",
    "            # _setup_hyperopt() which creates a NEW _hpopt_trials, destroying\n",
    "            # our restored history. The wrapper re-injects the saved trials.\n",
    "            _restored_trials = hyperopt_searcher._hpopt_trials\n",
    "            _original_setup = hyperopt_searcher._setup_hyperopt\n",
    "            def _patched_setup():\n",
    "                _original_setup()\n",
    "                hyperopt_searcher._hpopt_trials = _restored_trials\n",
    "            hyperopt_searcher._setup_hyperopt = _patched_setup\n",
    "            # Clear saved search space so Tuner can re-initialize it\n",
    "            # from param_space via set_search_properties().\n",
    "            hyperopt_searcher._space = None\n",
    "            hyperopt_searcher.domain = None\n",
    "            hyperopt_searcher._points_to_evaluate = None\n",
    "            n_prev = len(hyperopt_searcher._hpopt_trials.trials)\n",
    "            print(f\"   Restored HyperOptSearch TPE model with {n_prev} previous trials (hash: {current_hash})\")\n",
    "            _restored = True\n",
    "        except Exception as e:\n",
    "            print(f\"   Failed to restore HyperOptSearch checkpoint: {e}\")\n",
    "            print(\"   Starting fresh exploration.\")\n",
    "    if not _restored:\n",
    "        print(f\"Starting HyperOptSearch from scratch (hash: {current_hash})\")\n",
    "\n",
    "print(f\"\\nUsing trainval dataset (k-fold): {LOCAL_TRAINVAL_PATH}\")\n",
    "\n",
    "# Concurrency limiter for parallel trials\n",
    "limited_search_alg = ConcurrencyLimiter(\n",
    "    hyperopt_searcher,\n",
    "    max_concurrent=3\n",
    ")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STARTING HYPERPARAMETER TUNING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Configure Tuner â€” uses trainval dataset (no val/ split)\n",
    "tuner = tune.Tuner(\n",
    "    tune.with_resources(\n",
    "        tune.with_parameters(\n",
    "            train_linet_tune,\n",
    "            data_root=LOCAL_TRAINVAL_PATH,\n",
    "            target_size=(416, 544),\n",
    "            seed=SEED\n",
    "        ),\n",
    "        resources={\"cpu\": 3, \"gpu\": 0.333}\n",
    "    ),\n",
    "    param_space=search_space,\n",
    "    tune_config=tune.TuneConfig(\n",
    "        # HistoricalMedianStoppingRule uses per-epoch \"accuracy\" for stopping\n",
    "        # decisions (current epoch accuracy compared against running medians)\n",
    "        scheduler = HistoricalMedianStoppingRule(\n",
    "            historical_csv_path=EPOCH_HISTORY_CSV_PATH if WARM_START_ENABLED else None,\n",
    "            search_space_hash=get_search_space_hash(search_space) if WARM_START_ENABLED else None,\n",
    "            min_historical_epochs=20,\n",
    "            metric=\"accuracy\",\n",
    "            mode=\"max\",\n",
    "            time_attr=\"training_iteration\",\n",
    "            grace_period=25,\n",
    "            min_samples_required=5,\n",
    "        ),\n",
    "        search_alg=limited_search_alg,\n",
    "        num_samples=13\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Run Tuning\n",
    "results = tuner.fit()\n",
    "\n",
    "# Remove monkey-patched _setup_hyperopt so save() doesn't pickle the closure\n",
    "if '_setup_hyperopt' in hyperopt_searcher.__dict__:\n",
    "    del hyperopt_searcher.__dict__['_setup_hyperopt']\n",
    "\n",
    "# Get Best Result\n",
    "best_result = results.get_best_result(\"best_accuracy\", \"max\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TUNING COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Best Trial Config: {best_result.config}\")\n",
    "print(f\"Best Trial Accuracy: {best_result.metrics['best_accuracy']:.4f}\")\n",
    "print(f\"Best Trial Loss: {best_result.metrics['best_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wUYrvLppu451"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SAVE RESULTS FOR FUTURE WARM-STARTS\n",
    "# =============================================================================\n",
    "# Save the full results DataFrame to CSV for warm-starting future runs.\n",
    "# Results are tagged with a search space hash so only compatible configs\n",
    "# are used when warm-starting (if you change search space, old configs ignored).\n",
    "# =============================================================================\n",
    "\n",
    "import datetime\n",
    "\n",
    "# Save with timestamp for history, plus a 'latest' version for easy warm-start\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Get full results DataFrame\n",
    "results_df = results.get_dataframe()\n",
    "\n",
    "# Tag with current search space hash for future compatibility filtering\n",
    "current_hash = get_search_space_hash(search_space)\n",
    "results_df['search_space_hash'] = current_hash\n",
    "\n",
    "# Save timestamped version (for history)\n",
    "timestamped_path = f\"/content/drive/MyDrive/ray_tune_results/trial_{timestamp}.csv\"\n",
    "results_df.to_csv(timestamped_path, index=False)\n",
    "print(f\"ðŸ“ Saved results to: {timestamped_path}\")\n",
    "print(f\"   Search space hash: {current_hash}\")\n",
    "\n",
    "# Save/update 'latest' version (for easy warm-start)\n",
    "latest_path = WARM_START_CSV_PATH\n",
    "\n",
    "# If a previous results file exists, merge with new results\n",
    "if os.path.exists(latest_path):\n",
    "    previous_df = pd.read_csv(latest_path)\n",
    "    # Combine previous and new results\n",
    "    combined_df = pd.concat([previous_df, results_df], ignore_index=True)\n",
    "    # Remove duplicates based on config columns AND search_space_hash (keep best accuracy)\n",
    "    config_cols = [c for c in combined_df.columns if c.startswith('config/')]\n",
    "    dedup_cols = config_cols + ['search_space_hash']\n",
    "    combined_df = combined_df.sort_values('accuracy', ascending=False)\n",
    "    combined_df = combined_df.drop_duplicates(subset=dedup_cols, keep='first')\n",
    "    combined_df.to_csv(latest_path, index=False)\n",
    "\n",
    "    # Count configs per search space\n",
    "    hash_counts = combined_df['search_space_hash'].value_counts()\n",
    "    print(f\"ðŸ“ Updated {latest_path} with {len(results_df)} new trials\")\n",
    "    print(f\"   Total unique configs: {len(combined_df)}\")\n",
    "    print(f\"   Configs per search space:\")\n",
    "    for h, count in hash_counts.items():\n",
    "        marker = \" (current)\" if h == current_hash else \"\"\n",
    "        print(f\"      {h}: {count} configs{marker}\")\n",
    "else:\n",
    "    results_df.to_csv(latest_path, index=False)\n",
    "    print(f\"ðŸ“ Created {latest_path}\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# SAVE PER-EPOCH METRICS FOR HISTORICAL MEDIAN STOPPING\n",
    "# =============================================================================\n",
    "# Save per-epoch metrics (one row per trial per epoch) so that\n",
    "# HistoricalMedianStoppingRule can use them as a median baseline\n",
    "# in future sessions.\n",
    "# =============================================================================\n",
    "\n",
    "all_epoch_rows = []\n",
    "for result in results:\n",
    "    if result.metrics_dataframe is not None:\n",
    "        epoch_df = result.metrics_dataframe.copy()\n",
    "        epoch_df[\"trial_id\"] = result.metrics.get(\"trial_id\", id(result))\n",
    "        epoch_df[\"search_space_hash\"] = current_hash\n",
    "        all_epoch_rows.append(epoch_df)\n",
    "\n",
    "if all_epoch_rows:\n",
    "    new_epoch_df = pd.concat(all_epoch_rows, ignore_index=True)\n",
    "\n",
    "    # Merge with previous epoch history (single source of truth)\n",
    "    if os.path.exists(EPOCH_HISTORY_CSV_PATH):\n",
    "        previous_epoch_df = pd.read_csv(EPOCH_HISTORY_CSV_PATH)\n",
    "        combined_epoch_df = pd.concat([previous_epoch_df, new_epoch_df], ignore_index=True)\n",
    "        # Dedup on (trial_id, training_iteration) keeping last occurrence\n",
    "        combined_epoch_df = combined_epoch_df.drop_duplicates(\n",
    "            subset=[\"trial_id\", \"training_iteration\"], keep=\"last\"\n",
    "        )\n",
    "        combined_epoch_df.to_csv(EPOCH_HISTORY_CSV_PATH, index=False)\n",
    "        print(f\"\\nðŸ“ Updated epoch history: {EPOCH_HISTORY_CSV_PATH}\")\n",
    "        print(f\"   Added {len(new_epoch_df)} epoch rows from {len(all_epoch_rows)} trials\")\n",
    "        print(f\"   Total epoch rows: {len(combined_epoch_df)}\")\n",
    "    else:\n",
    "        new_epoch_df.to_csv(EPOCH_HISTORY_CSV_PATH, index=False)\n",
    "        print(f\"\\nðŸ“ Created epoch history: {EPOCH_HISTORY_CSV_PATH}\")\n",
    "        print(f\"   Saved {len(new_epoch_df)} epoch rows from {len(all_epoch_rows)} trials\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸  No per-epoch metrics available (trials may have failed)\")\n",
    "\n",
    "# =============================================================================\n",
    "# SAVE HYPEROPTSEARCH CHECKPOINT (TPE model state)\n",
    "# =============================================================================\n",
    "# Save the full TPE surrogate model so that next session's HyperOptSearch\n",
    "# retains all (config, result) observations without re-running them.\n",
    "# Each search space hash gets its own checkpoint file, so switching\n",
    "# between search spaces preserves history for each one independently.\n",
    "# =============================================================================\n",
    "\n",
    "if WARM_START_ENABLED:\n",
    "    hyperopt_searcher.save(hyperopt_checkpoint_path)\n",
    "    n_trials = len(hyperopt_searcher._hpopt_trials.trials)\n",
    "    print(f\"\\nðŸ“ Saved HyperOptSearch checkpoint ({n_trials} trials) to {hyperopt_checkpoint_path}\")\n",
    "\n",
    "print(f\"\\nðŸ’¡ To warm-start next run:\")\n",
    "print(f\"   1. Set WARM_START_ENABLED = True in the warm-start cell\")\n",
    "print(f\"   2. Run the notebook\")\n",
    "print(f\"   3. Each search space gets its own TPE checkpoint â†’ switch freely between configs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Rcvbufm3eQz"
   },
   "outputs": [],
   "source": [
    "# Analyze Top 10 Trials from Ray Tune (ranked by best_accuracy)\n",
    "# With continuous search spaces, each trial has unique float values â€”\n",
    "# no grouping by config. Treat fold assignment as noise.\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"TOP 10 TRIALS BY BEST ACCURACY (K-FOLD CV, CONTINUOUS SEARCH)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Get all trials and convert to DataFrame\n",
    "df = results.get_dataframe()\n",
    "\n",
    "# Sort by best_accuracy (descending)\n",
    "df_sorted = df.sort_values('best_accuracy', ascending=False)\n",
    "\n",
    "# Select relevant columns for display\n",
    "display_cols = [\n",
    "    'best_accuracy',\n",
    "    'config/lr_rgb', 'config/lr_depth', 'config/lr_shared',\n",
    "    'config/wd_rgb', 'config/wd_depth', 'config/wd_shared',\n",
    "    'config/s1_eta_min', 'config/s2_eta_min', 'config/eta_min', 'config/t_max',\n",
    "    'config/dropout_p', 'config/label_smoothing', 'config/grad_clip_norm',\n",
    "    'config/rgb_aug_prob', 'config/rgb_aug_mag',\n",
    "    'config/depth_aug_prob', 'config/depth_aug_mag',\n",
    "    'config/modality_dropout_rate',\n",
    "]\n",
    "\n",
    "# Get top 10 trials\n",
    "top_10 = df_sorted[display_cols].head(10)\n",
    "\n",
    "# Format for better display\n",
    "top_10_formatted = top_10.copy()\n",
    "top_10_formatted['best_accuracy'] = top_10_formatted['best_accuracy'].apply(lambda x: f\"{x*100:.2f}%\")\n",
    "\n",
    "# Format scientific notation columns\n",
    "sci_cols = [\n",
    "    'config/lr_rgb', 'config/lr_depth', 'config/lr_shared',\n",
    "    'config/wd_rgb', 'config/wd_depth', 'config/wd_shared',\n",
    "    'config/s1_eta_min', 'config/s2_eta_min', 'config/eta_min',\n",
    "]\n",
    "for col in sci_cols:\n",
    "    if col in top_10_formatted.columns:\n",
    "        top_10_formatted[col] = top_10_formatted[col].apply(lambda x: f\"{x:.2e}\")\n",
    "\n",
    "# Format float columns\n",
    "float_cols = [\n",
    "    'config/dropout_p', 'config/label_smoothing', 'config/grad_clip_norm',\n",
    "    'config/rgb_aug_prob', 'config/rgb_aug_mag',\n",
    "    'config/depth_aug_prob', 'config/depth_aug_mag',\n",
    "    'config/modality_dropout_rate',\n",
    "]\n",
    "for col in float_cols:\n",
    "    if col in top_10_formatted.columns:\n",
    "        top_10_formatted[col] = top_10_formatted[col].apply(lambda x: f\"{x:.3f}\")\n",
    "\n",
    "print(top_10_formatted.to_string(index=False))\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\nNote: Each trial trained on 1 random fold (out of 5). To get robust\")\n",
    "print(\"estimates, revalidate top configs with full 5-fold CV.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
