{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7f8cfec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /Users/gclinger/Documents/projects/Multi-Stream-Neural-Networks\n",
      "Current working directory: /Users/gclinger/Documents/projects/Multi-Stream-Neural-Networks/notebooks\n",
      "‚úÖ Added /Users/gclinger/Documents/projects/Multi-Stream-Neural-Networks to sys.path\n",
      "‚úÖ Environment setup complete!\n"
     ]
    }
   ],
   "source": [
    "# Environment Setup\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Set up project root path\n",
    "project_root = Path.cwd()\n",
    "while not (project_root / \"src\").exists() and project_root != project_root.parent:\n",
    "    project_root = project_root.parent\n",
    "\n",
    "if not (project_root / \"src\").exists():\n",
    "    # Fallback: assume we're in notebooks directory\n",
    "    project_root = Path.cwd().parent\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "\n",
    "# Add project root to path for imports\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "    print(f\"‚úÖ Added {project_root} to sys.path\")\n",
    "\n",
    "# Set environment variables for better error reporting\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "print(\"‚úÖ Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e574c065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Importing libraries...\n",
      "‚úÖ All project modules imported successfully\n",
      "üöÄ Using Apple Metal Performance Shaders (MPS)\n",
      "PyTorch version: 2.7.1\n",
      "Device: mps\n",
      "‚úÖ Library imports complete!\n"
     ]
    }
   ],
   "source": [
    "# Import Libraries\n",
    "print(\"üì¶ Importing libraries...\")\n",
    "\n",
    "# Core PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Machine learning utilities\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Visualization and analysis\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "\n",
    "# Progress tracking\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Project imports\n",
    "try:\n",
    "    from src.data_utils.dataset_utils import load_cifar100_data, CIFAR100_FINE_LABELS\n",
    "    from src.data_utils.rgb_to_rgbl import RGBtoRGBL\n",
    "\n",
    "    print(\"‚úÖ All project modules imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Error importing project modules: {e}\")\n",
    "    print(\"‚ö†Ô∏è  Please ensure you're running from the correct directory\")\n",
    "\n",
    "# Check device availability\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"üöÄ Using CUDA: {torch.cuda.get_device_name(0)}\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\") \n",
    "    print(\"üöÄ Using Apple Metal Performance Shaders (MPS)\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"üíª Using CPU\")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {device}\")\n",
    "print(\"‚úÖ Library imports complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2aedb5b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Apple Silicon GPU (MPS)\n",
      "Device: mps\n",
      "üìÅ Loading CIFAR-100 from: ../data/cifar-100\n",
      "‚úÖ Loaded CIFAR-100 (torch format):\n",
      "   Training: torch.Size([50000, 3, 32, 32]), labels: 50000\n",
      "   Test: torch.Size([10000, 3, 32, 32]), labels: 10000\n",
      "Train data shape: torch.Size([50000, 3, 32, 32])\n",
      "Train labels shape: torch.Size([50000])\n",
      "Test data shape: torch.Size([10000, 3, 32, 32])\n",
      "Test labels shape: torch.Size([10000])\n",
      "Train batches: 1407\n",
      "Val batches: 79\n",
      "Test batches: 157\n",
      "‚úÖ Loaded CIFAR-100 (torch format):\n",
      "   Training: torch.Size([50000, 3, 32, 32]), labels: 50000\n",
      "   Test: torch.Size([10000, 3, 32, 32]), labels: 10000\n",
      "Train data shape: torch.Size([50000, 3, 32, 32])\n",
      "Train labels shape: torch.Size([50000])\n",
      "Test data shape: torch.Size([10000, 3, 32, 32])\n",
      "Test labels shape: torch.Size([10000])\n",
      "Train batches: 1407\n",
      "Val batches: 79\n",
      "Test batches: 157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training for 10 epochs...\n",
      "============================================================\n",
      "\n",
      "Epoch [1/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Loss: 4.6358, Acc: 5.96%\n",
      "Val   - Loss: 4.3349, Acc: 8.18%\n",
      "üéâ New best validation accuracy: 8.18%\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch [2/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Loss: 4.0003, Acc: 8.94%\n",
      "Val   - Loss: 4.4210, Acc: 10.62%\n",
      "üéâ New best validation accuracy: 10.62%\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch [3/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Loss: 3.8976, Acc: 9.31%\n",
      "Val   - Loss: 3.9072, Acc: 10.72%\n",
      "üéâ New best validation accuracy: 10.72%\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch [4/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Loss: 3.7109, Acc: 12.03%\n",
      "Val   - Loss: 4.0262, Acc: 15.26%\n",
      "üéâ New best validation accuracy: 15.26%\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch [5/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Loss: 3.5650, Acc: 14.68%\n",
      "Val   - Loss: 3.9866, Acc: 16.96%\n",
      "üéâ New best validation accuracy: 16.96%\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch [6/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Loss: 3.4184, Acc: 17.26%\n",
      "Val   - Loss: 3.3003, Acc: 19.36%\n",
      "üéâ New best validation accuracy: 19.36%\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch [7/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Loss: 3.2591, Acc: 19.91%\n",
      "Val   - Loss: 4.0617, Acc: 23.36%\n",
      "üéâ New best validation accuracy: 23.36%\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch [8/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Loss: 3.0213, Acc: 24.50%\n",
      "Val   - Loss: 7.9093, Acc: 26.00%\n",
      "üéâ New best validation accuracy: 26.00%\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch [9/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Loss: 2.8014, Acc: 28.82%\n",
      "Val   - Loss: 8.1791, Acc: 29.84%\n",
      "üéâ New best validation accuracy: 29.84%\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch [10/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Loss: 2.6261, Acc: 32.44%\n",
      "Val   - Loss: 8.0884, Acc: 30.32%\n",
      "üéâ New best validation accuracy: 30.32%\n",
      "------------------------------------------------------------\n",
      "\n",
      "Training completed!\n",
      "Best validation accuracy: 30.32%\n",
      "Final train accuracy: 32.44%\n",
      "Final validation accuracy: 30.32%\n",
      "\n",
      "Evaluating on test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test - Loss: 7.6385, Acc: 30.35%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import models, transforms\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set device (prioritize MPS for Apple Silicon)\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "    print(\"Using Apple Silicon GPU (MPS)\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(\"Using CUDA GPU\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"Using CPU\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "train_data, train_labels, test_data, test_labels = load_cifar100_data(\n",
    "    data_dir=\"../data/cifar-100\",\n",
    "    normalize=True  # Apply normalization to [0, 1] range\n",
    ")\n",
    "\n",
    "# Debug: Check data shapes\n",
    "print(f\"Train data shape: {train_data.shape}\")\n",
    "print(f\"Train labels shape: {train_labels.shape}\")\n",
    "print(f\"Test data shape: {test_data.shape}\")\n",
    "print(f\"Test labels shape: {test_labels.shape}\")\n",
    "\n",
    "# Split the data\n",
    "train_data, val_data, train_labels, val_labels = train_test_split(\n",
    "    train_data, train_labels, test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 32\n",
    "train_dataset = TensorDataset(train_data, train_labels)\n",
    "val_dataset = TensorDataset(val_data, val_labels)\n",
    "test_dataset = TensorDataset(test_data, test_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size*2, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size*2, shuffle=False)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")\n",
    "\n",
    "# Load ResNet50 without pretrained weights\n",
    "model = models.resnet50(weights=False)\n",
    "# Modify final layer for CIFAR-100 (100 classes)\n",
    "model.fc = nn.Linear(model.fc.in_features, 100)\n",
    "model = model.to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=2e-3)\n",
    "\n",
    "# Learning rate scheduler (OneCycle) - Updated for fewer epochs\n",
    "num_epochs = 10  # Reduced for faster testing\n",
    "scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=0.01,\n",
    "    steps_per_epoch=len(train_loader),\n",
    "    epochs=num_epochs\n",
    ")\n",
    "\n",
    "# Training function\n",
    "def train_epoch(model, train_loader, criterion, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    train_bar = tqdm(train_loader, desc='Training', leave=False)\n",
    "    for batch_idx, (data, target) in enumerate(train_bar):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = output.max(1)\n",
    "        total += target.size(0)\n",
    "        correct += predicted.eq(target).sum().item()\n",
    "        \n",
    "        # Update progress bar\n",
    "        train_bar.set_postfix({\n",
    "            'Loss': f'{running_loss/(batch_idx+1):.4f}',\n",
    "            'Acc': f'{100.*correct/total:.2f}%',\n",
    "            'LR': f'{scheduler.get_last_lr()[0]:.6f}'\n",
    "        })\n",
    "    \n",
    "    return running_loss/len(train_loader), 100.*correct/total\n",
    "\n",
    "def validate_epoch(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        val_bar = tqdm(val_loader, desc='Validation', leave=False)\n",
    "        for data, target in val_bar:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            val_loss += criterion(output, target).item()\n",
    "            \n",
    "            _, predicted = output.max(1)\n",
    "            total += target.size(0)\n",
    "            correct += predicted.eq(target).sum().item()\n",
    "            \n",
    "            val_bar.set_postfix({\n",
    "                'Loss': f'{val_loss/(len(val_bar)):.4f}',\n",
    "                'Acc': f'{100.*correct/total:.2f}%'\n",
    "            })\n",
    "    \n",
    "    return val_loss/len(val_loader), 100.*correct/total\n",
    "\n",
    "# Training loop\n",
    "best_val_acc = 0.0\n",
    "train_losses, train_accs = [], []\n",
    "val_losses, val_accs = [], []\n",
    "\n",
    "print(f\"\\nStarting training for {num_epochs} epochs...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch [{epoch+1}/{num_epochs}]\")\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, scheduler, device)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_acc = validate_epoch(model, val_loader, criterion, device)\n",
    "    \n",
    "    # Store metrics\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accs.append(val_acc)\n",
    "    \n",
    "    # Print epoch results\n",
    "    print(f\"Train - Loss: {train_loss:.4f}, Acc: {train_acc:.2f}%\")\n",
    "    print(f\"Val   - Loss: {val_loss:.4f}, Acc: {val_acc:.2f}%\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        print(f\"üéâ New best validation accuracy: {best_val_acc:.2f}%\")\n",
    "    \n",
    "    print(\"-\" * 60)\n",
    "\n",
    "print(f\"\\nTraining completed!\")\n",
    "print(f\"Best validation accuracy: {best_val_acc:.2f}%\")\n",
    "print(f\"Final train accuracy: {train_accs[-1]:.2f}%\")\n",
    "print(f\"Final validation accuracy: {val_accs[-1]:.2f}%\")\n",
    "\n",
    "# Optional: Quick test evaluation\n",
    "print(f\"\\nEvaluating on test set...\")\n",
    "test_loss, test_acc = validate_epoch(model, test_loader, criterion, device)\n",
    "print(f\"Test - Loss: {test_loss:.4f}, Acc: {test_acc:.2f}%\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "314119d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Using Apple Metal Performance Shaders (MPS)\n",
      "üìÅ Loading CIFAR-100 from: ../data/cifar-100\n",
      "‚úÖ Loaded CIFAR-100 (torch format):\n",
      "   Training: torch.Size([50000, 3, 32, 32]), labels: 50000\n",
      "   Test: torch.Size([10000, 3, 32, 32]), labels: 10000\n",
      "Training samples: 45000\n",
      "Validation samples: 5000\n",
      "Test samples: 10000\n",
      "Number of classes: 100\n",
      "Labels shape: torch.Size([45000])\n",
      "Creating DataLoaders for ResNet50...\n",
      "Train loader: 1407 batches\n",
      "Val loader: 79 batches\n",
      "Test loader: 157 batches\n",
      "DataLoaders created successfully!\n",
      "Creating ResNet50 model...\n",
      "Compiling model with optimized settings...\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1486/1486 [01:30<00:00, 16.42it/s, train_loss=4.6877, train_acc=0.0553, val_loss=4.1866, val_acc=0.0522, lr=0.002801]\n",
      "Epoch 2/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1486/1486 [01:25<00:00, 17.45it/s, train_loss=3.9938, train_acc=0.0862, val_loss=4.7884, val_acc=0.1110, lr=0.007602]\n",
      "Epoch 3/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1486/1486 [01:24<00:00, 17.64it/s, train_loss=3.7802, train_acc=0.1115, val_loss=3.9490, val_acc=0.1184, lr=0.010000]\n",
      "Epoch 4/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1486/1486 [01:29<00:00, 16.65it/s, train_loss=3.6492, train_acc=0.1357, val_loss=4.4315, val_acc=0.1170, lr=0.009504]\n",
      "Epoch 5/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1486/1486 [01:31<00:00, 16.25it/s, train_loss=3.5296, train_acc=0.1565, val_loss=4.5199, val_acc=0.1450, lr=0.008116]\n",
      "Epoch 6/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1486/1486 [01:24<00:00, 17.58it/s, train_loss=3.4075, train_acc=0.1728, val_loss=6.1411, val_acc=0.2062, lr=0.006111]\n",
      "Epoch 7/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1486/1486 [01:25<00:00, 17.43it/s, train_loss=3.2980, train_acc=0.1971, val_loss=3.1626, val_acc=0.2294, lr=0.003886]\n",
      "Epoch 8/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1486/1486 [01:24<00:00, 17.59it/s, train_loss=3.0431, train_acc=0.2443, val_loss=3.4074, val_acc=0.2644, lr=0.001881]\n",
      "Epoch 9/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1486/1486 [01:26<00:00, 17.26it/s, train_loss=2.8019, train_acc=0.2873, val_loss=3.7441, val_acc=0.2982, lr=0.000495]\n",
      "Epoch 10/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1486/1486 [01:25<00:00, 17.31it/s, train_loss=2.6463, train_acc=0.3186, val_loss=3.3892, val_acc=0.3056, lr=0.000000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed!\n",
      "Best validation accuracy: 0.3056\n",
      "Final train accuracy: 0.3186\n",
      "Final validation accuracy: 0.3056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 3.2969\n",
      "Test accuracy: 0.2990\n"
     ]
    }
   ],
   "source": [
    "from src.data_utils import load_cifar100_data\n",
    "from src.models2.common.model_helpers import create_dataloader_from_tensors\n",
    "from sklearn.model_selection import train_test_split\n",
    "from src.models2.core.resnet import resnet50\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"üöÄ Using CUDA: {torch.cuda.get_device_name(0)}\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\") \n",
    "    print(\"üöÄ Using Apple Metal Performance Shaders (MPS)\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"üíª Using CPU\")\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_data, train_labels, test_data, test_labels = load_cifar100_data(\n",
    "    data_dir=\"../data/cifar-100\",\n",
    "    normalize=True  # Apply normalization to [0, 1] range\n",
    ")\n",
    "\n",
    "# Split the data\n",
    "train_data, val_data, train_labels, val_labels = train_test_split(\n",
    "    train_data, train_labels, test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(train_data)}\")\n",
    "print(f\"Validation samples: {len(val_data)}\")\n",
    "print(f\"Test samples: {len(test_data)}\")\n",
    "print(f\"Number of classes: {len(torch.unique(train_labels))}\")\n",
    "print(f\"Labels shape: {train_labels.shape}\")\n",
    "\n",
    "\n",
    "# Create DataLoaders for ResNet50 training (RGB only)\n",
    "print(\"Creating DataLoaders for ResNet50...\")\n",
    "\n",
    "# Use only color data for standard ResNet training\n",
    "train_loader = create_dataloader_from_tensors(\n",
    "    train_data, train_labels, batch_size=batch_size, shuffle=True, device=device\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader_from_tensors(\n",
    "    val_data, val_labels, batch_size=batch_size*2, shuffle=False, device=device\n",
    ")\n",
    "\n",
    "test_loader = create_dataloader_from_tensors(\n",
    "    test_data, test_labels, batch_size=batch_size*2, shuffle=False, device=device\n",
    ")\n",
    "\n",
    "print(f\"Train loader: {len(train_loader)} batches\")\n",
    "print(f\"Val loader: {len(val_loader)} batches\")\n",
    "print(f\"Test loader: {len(test_loader)} batches\")\n",
    "print(\"DataLoaders created successfully!\")\n",
    "\n",
    "\n",
    "# Create and train ResNet50 model with proper settings\n",
    "print(\"Creating ResNet50 model...\")\n",
    "resnet50_baseline = resnet50(num_classes=100, device=str(device))\n",
    "\n",
    "# Compile with proper learning rate and stable scheduler\n",
    "print(\"Compiling model with optimized settings...\")\n",
    "resnet50_baseline.compile(\n",
    "    optimizer='adamw',\n",
    "    loss='cross_entropy',\n",
    "    learning_rate=0.001,    \n",
    "    weight_decay=2e-3,      \n",
    "    scheduler='onecycle',    \n",
    "    max_lr=0.01,          \n",
    ")\n",
    "\n",
    "print(\"Starting training...\")\n",
    "# Train with step scheduler parameters\n",
    "history = resnet50_baseline.fit(\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    epochs=10,               \n",
    "    early_stopping=False,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "print(\"Training completed!\")\n",
    "print(f\"Best validation accuracy: {max(history['val_accuracy']):.4f}\")\n",
    "print(f\"Final train accuracy: {history['train_accuracy'][-1]:.4f}\")\n",
    "print(f\"Final validation accuracy: {history['val_accuracy'][-1]:.4f}\")\n",
    "\n",
    "evaluate = resnet50_baseline.evaluate(test_loader)\n",
    "print(f\"Test loss: {evaluate['loss']:.4f}\")\n",
    "print(f\"Test accuracy: {evaluate['accuracy']:.4f}\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4060f237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Using Apple Metal Performance Shaders (MPS)\n",
      "üìÅ Loading CIFAR-100 from: ../data/cifar-100\n",
      "‚úÖ Loaded CIFAR-100 (torch format):\n",
      "   Training: torch.Size([50000, 3, 32, 32]), labels: 50000\n",
      "   Test: torch.Size([10000, 3, 32, 32]), labels: 10000\n",
      "Training samples: 45000\n",
      "Validation samples: 5000\n",
      "Test samples: 10000\n",
      "Number of classes: 100\n",
      "Labels shape: torch.Size([45000])\n",
      "Creating DataLoaders for ResNet50...\n",
      "Train loader: 1407 batches\n",
      "Val loader: 79 batches\n",
      "Test loader: 157 batches\n",
      "DataLoaders created successfully!\n",
      "Creating ResNet50 model...\n",
      "Compiling model with optimized settings...\n",
      "MCResNet compiled with adamw optimizer, cross_entropy loss\n",
      "  Learning rate: 0.001, Weight decay: 0.002\n",
      "  Device: mps, AMP: False\n",
      "  Gradient clip: 1.0, Scheduler: onecycle\n",
      "  Using architecture-specific defaults where applicable\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1486/1486 [02:50<00:00,  8.73it/s, train_loss=5.0737, train_acc=0.0555, val_loss=5.4990, val_acc=0.0706, lr=0.002801]\n",
      "Epoch 2/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1486/1486 [02:46<00:00,  8.94it/s, train_loss=4.0527, train_acc=0.0791, val_loss=21.4027, val_acc=0.0862, lr=0.007602]\n",
      "Epoch 3/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1486/1486 [02:47<00:00,  8.89it/s, train_loss=3.7993, train_acc=0.1132, val_loss=3.9281, val_acc=0.1044, lr=0.010000]\n",
      "Epoch 4/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1486/1486 [02:51<00:00,  8.67it/s, train_loss=3.5728, train_acc=0.1489, val_loss=3.9546, val_acc=0.1670, lr=0.009504]\n",
      "Epoch 5/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1486/1486 [02:59<00:00,  8.27it/s, train_loss=3.4277, train_acc=0.1740, val_loss=4.3618, val_acc=0.1246, lr=0.008116]\n",
      "Epoch 6/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1486/1486 [02:50<00:00,  8.71it/s, train_loss=3.4148, train_acc=0.1783, val_loss=3.8115, val_acc=0.1852, lr=0.006111]\n",
      "Epoch 7/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1486/1486 [02:50<00:00,  8.74it/s, train_loss=3.1290, train_acc=0.2286, val_loss=7.4468, val_acc=0.1836, lr=0.003886]\n",
      "Epoch 8/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1486/1486 [02:55<00:00,  8.49it/s, train_loss=2.8907, train_acc=0.2758, val_loss=4.6006, val_acc=0.3002, lr=0.001881]\n",
      "Epoch 9/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1486/1486 [02:50<00:00,  8.74it/s, train_loss=2.6624, train_acc=0.3214, val_loss=4.8646, val_acc=0.3384, lr=0.000495]\n",
      "Epoch 10/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1486/1486 [02:47<00:00,  8.87it/s, train_loss=2.4902, train_acc=0.3551, val_loss=3.8262, val_acc=0.3470, lr=0.000000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed!\n",
      "Best validation accuracy: 0.3470\n",
      "Final train accuracy: 0.3551\n",
      "Final validation accuracy: 0.3470\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 3.7151\n",
      "Test accuracy: 0.3421\n"
     ]
    }
   ],
   "source": [
    "from src.data_utils import load_cifar100_data\n",
    "from src.data_utils.dual_channel_dataset import create_dual_channel_dataloaders, create_dual_channel_dataloader\n",
    "from src.data_utils import RGBtoRGBL\n",
    "from sklearn.model_selection import train_test_split\n",
    "from src.models2.multi_channel.mc_resnet import mc_resnet50\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"üöÄ Using CUDA: {torch.cuda.get_device_name(0)}\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\") \n",
    "    print(\"üöÄ Using Apple Metal Performance Shaders (MPS)\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"üíª Using CPU\")\n",
    "\n",
    "batch_size = 32\n",
    "converter = RGBtoRGBL()\n",
    "\n",
    "train_color, train_labels, test_color, test_labels = load_cifar100_data(\n",
    "    data_dir=\"../data/cifar-100\",\n",
    "    normalize=True  # Apply normalization to [0, 1] range\n",
    ")\n",
    "\n",
    "# Split the data\n",
    "train_color, val_color, train_labels, val_labels = train_test_split(\n",
    "    train_color, train_labels, test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "train_brightness = converter.get_brightness(train_color)\n",
    "val_brightness = converter.get_brightness(val_color)\n",
    "test_brightness = converter.get_brightness(test_color)\n",
    "\n",
    "\n",
    "print(f\"Training samples: {len(train_color)}\")\n",
    "print(f\"Validation samples: {len(val_color)}\")\n",
    "print(f\"Test samples: {len(test_color)}\")\n",
    "print(f\"Number of classes: {len(torch.unique(train_labels))}\")\n",
    "print(f\"Labels shape: {train_labels.shape}\")\n",
    "\n",
    "\n",
    "# Create DataLoaders for ResNet50 training (RGB only)\n",
    "print(\"Creating DataLoaders for ResNet50...\")\n",
    "\n",
    "\n",
    "train_loader, val_loader = create_dual_channel_dataloaders(\n",
    "    train_color, train_brightness, train_labels,\n",
    "    val_color, val_brightness, val_labels,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "test_loader = create_dual_channel_dataloader(\n",
    "    test_color, test_brightness, test_labels,\n",
    "    batch_size=batch_size*2, shuffle=False\n",
    ")\n",
    "\n",
    "print(f\"Train loader: {len(train_loader)} batches\")\n",
    "print(f\"Val loader: {len(val_loader)} batches\")\n",
    "print(f\"Test loader: {len(test_loader)} batches\")\n",
    "print(\"DataLoaders created successfully!\")\n",
    "\n",
    "\n",
    "# Create and train ResNet50 model with proper settings\n",
    "print(\"Creating ResNet50 model...\")\n",
    "resnet50_mc = mc_resnet50(num_classes=100, device=str(device))\n",
    "\n",
    "# Compile with proper learning rate and stable scheduler\n",
    "print(\"Compiling model with optimized settings...\")\n",
    "resnet50_mc.compile(\n",
    "    optimizer='adamw',\n",
    "    loss='cross_entropy',\n",
    "    learning_rate=0.001,    \n",
    "    weight_decay=2e-3,      \n",
    "    scheduler='onecycle',    \n",
    "    max_lr=0.01,          \n",
    ")\n",
    "\n",
    "print(\"Starting training...\")\n",
    "# Train with step scheduler parameters\n",
    "history_mc = resnet50_mc.fit(\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    epochs=10,               \n",
    "    early_stopping=False,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "print(\"Training completed!\")\n",
    "print(f\"Best validation accuracy: {max(history_mc['val_accuracy']):.4f}\")\n",
    "print(f\"Final train accuracy: {history_mc['train_accuracy'][-1]:.4f}\")\n",
    "print(f\"Final validation accuracy: {history_mc['val_accuracy'][-1]:.4f}\")\n",
    "\n",
    "evaluate_mc = resnet50_mc.evaluate(test_loader)\n",
    "print(f\"Test loss: {evaluate_mc['loss']:.4f}\")\n",
    "print(f\"Test accuracy: {evaluate_mc['accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "433c82b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Analyzing Multi-Channel ResNet Pathways...\n",
      "============================================================\n",
      "\n",
      "üìä PATHWAY PERFORMANCE ANALYSIS\n",
      "----------------------------------------\n",
      "Full Model Accuracy:      0.2500\n",
      "Color Only Accuracy:      0.1300\n",
      "Brightness Only Accuracy: 0.0700\n",
      "\n",
      "Color Contribution:       0.5200 (52.0%)\n",
      "Brightness Contribution:  0.2800 (28.0%)\n",
      "\n",
      "Feature Norms - Color:     6.0651 ¬± 26.7622\n",
      "Feature Norms - Brightness: 1148130295808.0000 ¬± 11481303220224.0000\n",
      "Color/Brightness Ratio:    0.0000\n",
      "Samples Analyzed:          100\n",
      "\n",
      "‚öñÔ∏è  PATHWAY WEIGHT ANALYSIS\n",
      "----------------------------------------\n",
      "Color Pathway:\n",
      "  Total Norm:    0.0000\n",
      "  Mean Norm:     0.0000\n",
      "  Layers:        0\n",
      "\n",
      "Brightness Pathway:\n",
      "  Total Norm:    0.0000\n",
      "  Mean Norm:     0.0000\n",
      "  Layers:        0\n",
      "\n",
      "Weight Ratios:\n",
      "  Overall C/B Ratio: inf\n",
      "  Top Layer Ratios:\n",
      "\n",
      "üéØ PATHWAY IMPORTANCE ANALYSIS\n",
      "----------------------------------------\n",
      "Method: ABLATION\n",
      "Color Importance:      0.6000 (60.0%)\n",
      "Brightness Importance: 0.4000 (40.0%)\n",
      "\n",
      "Performance Drops:\n",
      "  Without Color:      0.1200\n",
      "  Without Brightness: 0.0800\n",
      "\n",
      "üî¨ COMPARATIVE IMPORTANCE ANALYSIS\n",
      "----------------------------------------\n",
      "Importance Comparison:\n",
      "Method          Color        Brightness   Dominant  \n",
      "--------------------------------------------------\n",
      "Ablation        0.6000 (60.0%)  0.4000 (40.0%)  Color     \n",
      "Gradient        0.2638 (26.4%)  0.7362 (73.6%)  Brightness\n",
      "Feature Norm    0.5786 (57.9%)  0.4214 (42.1%)  Color     \n",
      "\n",
      "üèÜ ANALYSIS SUMMARY\n",
      "----------------------------------------\n",
      "Average Color Importance:      0.4808 (48.1%)\n",
      "Average Brightness Importance: 0.5192 (51.9%)\n",
      "Dominant Pathway:              Brightness\n",
      "\n",
      "Dual-Channel Performance Gain: 0.1200 (12.00%)\n",
      "Relative Improvement:          92.31%\n",
      "\n",
      "‚úÖ Pathway analysis complete!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"üîç Analyzing Multi-Channel ResNet Pathways...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Pathway Performance Analysis\n",
    "print(\"\\nüìä PATHWAY PERFORMANCE ANALYSIS\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "analysis = resnet50_mc.analyze_pathways(\n",
    "    color_data=val_color, \n",
    "    brightness_data=val_brightness, \n",
    "    targets=val_labels\n",
    ")\n",
    "\n",
    "print(f\"Full Model Accuracy:      {analysis['accuracy']['full_model']:.4f}\")\n",
    "print(f\"Color Only Accuracy:      {analysis['accuracy']['color_only']:.4f}\")\n",
    "print(f\"Brightness Only Accuracy: {analysis['accuracy']['brightness_only']:.4f}\")\n",
    "print()\n",
    "print(f\"Color Contribution:       {analysis['accuracy']['color_contribution']:.4f} ({analysis['accuracy']['color_contribution']*100:.1f}%)\")\n",
    "print(f\"Brightness Contribution:  {analysis['accuracy']['brightness_contribution']:.4f} ({analysis['accuracy']['brightness_contribution']*100:.1f}%)\")\n",
    "print()\n",
    "print(f\"Feature Norms - Color:     {analysis['feature_norms']['color_mean']:.4f} ¬± {analysis['feature_norms']['color_std']:.4f}\")\n",
    "print(f\"Feature Norms - Brightness: {analysis['feature_norms']['brightness_mean']:.4f} ¬± {analysis['feature_norms']['brightness_std']:.4f}\")\n",
    "print(f\"Color/Brightness Ratio:    {analysis['feature_norms']['color_to_brightness_ratio']:.4f}\")\n",
    "print(f\"Samples Analyzed:          {analysis['samples_analyzed']}\")\n",
    "\n",
    "# Weight Analysis\n",
    "print(\"\\n‚öñÔ∏è  PATHWAY WEIGHT ANALYSIS\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "weights = resnet50_mc.analyze_pathway_weights()\n",
    "\n",
    "print(f\"Color Pathway:\")\n",
    "print(f\"  Total Norm:    {weights['color_pathway']['total_norm']:.4f}\")\n",
    "print(f\"  Mean Norm:     {weights['color_pathway']['mean_norm']:.4f}\")\n",
    "print(f\"  Layers:        {weights['color_pathway']['num_layers']}\")\n",
    "\n",
    "print(f\"\\nBrightness Pathway:\")\n",
    "print(f\"  Total Norm:    {weights['brightness_pathway']['total_norm']:.4f}\")\n",
    "print(f\"  Mean Norm:     {weights['brightness_pathway']['mean_norm']:.4f}\")\n",
    "print(f\"  Layers:        {weights['brightness_pathway']['num_layers']}\")\n",
    "\n",
    "print(f\"\\nWeight Ratios:\")\n",
    "print(f\"  Overall C/B Ratio: {weights['ratio_analysis']['color_to_brightness_norm_ratio']:.4f}\")\n",
    "\n",
    "# Show top 5 layer ratios\n",
    "layer_ratios = weights['ratio_analysis']['layer_ratios']\n",
    "sorted_ratios = sorted(layer_ratios.items(), key=lambda x: x[1], reverse=True)\n",
    "print(f\"  Top Layer Ratios:\")\n",
    "for i, (layer, ratio) in enumerate(sorted_ratios[:5]):\n",
    "    if ratio != float('inf'):\n",
    "        print(f\"    {layer}: {ratio:.4f}\")\n",
    "\n",
    "# Importance Analysis\n",
    "print(\"\\nüéØ PATHWAY IMPORTANCE ANALYSIS\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "importance = resnet50_mc.get_pathway_importance(\n",
    "    color_data=val_color, \n",
    "    brightness_data=val_brightness, \n",
    "    targets=val_labels,\n",
    "    method='ablation'\n",
    ")\n",
    "\n",
    "print(f\"Method: {importance['method'].upper()}\")\n",
    "print(f\"Color Importance:      {importance['color_importance']:.4f} ({importance['color_importance']*100:.1f}%)\")\n",
    "print(f\"Brightness Importance: {importance['brightness_importance']:.4f} ({importance['brightness_importance']*100:.1f}%)\")\n",
    "print()\n",
    "print(f\"Performance Drops:\")\n",
    "print(f\"  Without Color:      {importance['performance_drops']['without_color']:.4f}\")\n",
    "print(f\"  Without Brightness: {importance['performance_drops']['without_brightness']:.4f}\")\n",
    "\n",
    "# Additional importance methods\n",
    "print(\"\\nüî¨ COMPARATIVE IMPORTANCE ANALYSIS\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "grad_importance = resnet50_mc.get_pathway_importance(\n",
    "    color_data=val_color, \n",
    "    brightness_data=val_brightness, \n",
    "    targets=val_labels,\n",
    "    method='gradient'\n",
    ")\n",
    "\n",
    "feature_importance = resnet50_mc.get_pathway_importance(\n",
    "    color_data=val_color, \n",
    "    brightness_data=val_brightness, \n",
    "    targets=val_labels,\n",
    "    method='feature_norm'\n",
    ")\n",
    "\n",
    "print(\"Importance Comparison:\")\n",
    "print(f\"{'Method':<15} {'Color':<12} {'Brightness':<12} {'Dominant':<10}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'Ablation':<15} {importance['color_importance']:.4f} ({importance['color_importance']*100:.1f}%){'':<1} {importance['brightness_importance']:.4f} ({importance['brightness_importance']*100:.1f}%){'':<1} {'Color' if importance['color_importance'] > importance['brightness_importance'] else 'Brightness':<10}\")\n",
    "print(f\"{'Gradient':<15} {grad_importance['color_importance']:.4f} ({grad_importance['color_importance']*100:.1f}%){'':<1} {grad_importance['brightness_importance']:.4f} ({grad_importance['brightness_importance']*100:.1f}%){'':<1} {'Color' if grad_importance['color_importance'] > grad_importance['brightness_importance'] else 'Brightness':<10}\")\n",
    "print(f\"{'Feature Norm':<15} {feature_importance['color_importance']:.4f} ({feature_importance['color_importance']*100:.1f}%){'':<1} {feature_importance['brightness_importance']:.4f} ({feature_importance['brightness_importance']*100:.1f}%){'':<1} {'Color' if feature_importance['color_importance'] > feature_importance['brightness_importance'] else 'Brightness':<10}\")\n",
    "\n",
    "print(\"\\nüèÜ ANALYSIS SUMMARY\")\n",
    "print(\"-\" * 40)\n",
    "avg_color_importance = (importance['color_importance'] + grad_importance['color_importance'] + feature_importance['color_importance']) / 3\n",
    "avg_brightness_importance = (importance['brightness_importance'] + grad_importance['brightness_importance'] + feature_importance['brightness_importance']) / 3\n",
    "\n",
    "print(f\"Average Color Importance:      {avg_color_importance:.4f} ({avg_color_importance*100:.1f}%)\")\n",
    "print(f\"Average Brightness Importance: {avg_brightness_importance:.4f} ({avg_brightness_importance*100:.1f}%)\")\n",
    "print(f\"Dominant Pathway:              {'Color' if avg_color_importance > avg_brightness_importance else 'Brightness'}\")\n",
    "\n",
    "# Performance improvement analysis\n",
    "single_best = max(analysis['accuracy']['color_only'], analysis['accuracy']['brightness_only'])\n",
    "dual_channel_gain = analysis['accuracy']['full_model'] - single_best\n",
    "print(f\"\\nDual-Channel Performance Gain: {dual_channel_gain:.4f} ({dual_channel_gain*100:.2f}%)\")\n",
    "print(f\"Relative Improvement:          {(dual_channel_gain/single_best)*100:.2f}%\")\n",
    "\n",
    "print(\"\\n‚úÖ Pathway analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28753885",
   "metadata": {},
   "source": [
    "# Analysis Summary\n",
    "\n",
    "Let's analyze the key findings from the multi-channel ResNet pathway analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8873b6c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã MULTI-CHANNEL RESNET ANALYSIS SUMMARY\n",
      "==================================================\n",
      "\n",
      "üéØ MODEL PERFORMANCE:\n",
      "Full Model Accuracy:      0.2500\n",
      "Color Only Accuracy:      0.1300\n",
      "Brightness Only Accuracy: 0.0700\n",
      "\n",
      "üí° KEY INSIGHTS:\n",
      "1. Color dominance: 52.0%\n",
      "2. Brightness contribution: 28.0%\n",
      "3. Dual-channel advantage: 0.1200 (92.3% improvement)\n",
      "\n",
      "‚öñÔ∏è PATHWAY IMPORTANCE (Average across methods):\n",
      "Color Importance:      48.1%\n",
      "Brightness Importance: 51.9%\n",
      "Dominant Pathway:      Brightness\n",
      "\n",
      "üî¨ FEATURE ANALYSIS:\n",
      "Color feature norm ratio: 0.00x stronger\n",
      "Weight norm ratio (C/B): infx\n",
      "\n",
      "üìä METHODOLOGY CONSISTENCY:\n",
      "Ablation    : 60.0% vs 40.0% ‚Üí Color\n",
      "Gradient    : 26.4% vs 73.6% ‚Üí Brightness\n",
      "Feature Norm: 57.9% vs 42.1% ‚Üí Color\n",
      "\n",
      "üèÜ CONCLUSION:\n",
      "The multi-channel ResNet shows a clear brightness pathway dominance\n",
      "with 12.0% performance gain over single-pathway approaches.\n"
     ]
    }
   ],
   "source": [
    "# Analysis Summary of Multi-Channel ResNet Results\n",
    "print(\"üìã MULTI-CHANNEL RESNET ANALYSIS SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"\\nüéØ MODEL PERFORMANCE:\")\n",
    "print(f\"Full Model Accuracy:      {analysis['accuracy']['full_model']:.4f}\")\n",
    "print(f\"Color Only Accuracy:      {analysis['accuracy']['color_only']:.4f}\")\n",
    "print(f\"Brightness Only Accuracy: {analysis['accuracy']['brightness_only']:.4f}\")\n",
    "\n",
    "print(f\"\\nüí° KEY INSIGHTS:\")\n",
    "print(f\"1. Color dominance: {analysis['accuracy']['color_contribution']:.1%}\")\n",
    "print(f\"2. Brightness contribution: {analysis['accuracy']['brightness_contribution']:.1%}\")\n",
    "print(f\"3. Dual-channel advantage: {dual_channel_gain:.4f} ({(dual_channel_gain/single_best)*100:.1f}% improvement)\")\n",
    "\n",
    "print(f\"\\n‚öñÔ∏è PATHWAY IMPORTANCE (Average across methods):\")\n",
    "print(f\"Color Importance:      {avg_color_importance:.1%}\")\n",
    "print(f\"Brightness Importance: {avg_brightness_importance:.1%}\")\n",
    "print(f\"Dominant Pathway:      {'Color' if avg_color_importance > avg_brightness_importance else 'Brightness'}\")\n",
    "\n",
    "print(f\"\\nüî¨ FEATURE ANALYSIS:\")\n",
    "print(f\"Color feature norm ratio: {analysis['feature_norms']['color_to_brightness_ratio']:.2f}x stronger\")\n",
    "print(f\"Weight norm ratio (C/B): {weights['ratio_analysis']['color_to_brightness_norm_ratio']:.2f}x\")\n",
    "\n",
    "print(f\"\\nüìä METHODOLOGY CONSISTENCY:\")\n",
    "methods = ['Ablation', 'Gradient', 'Feature Norm']\n",
    "color_scores = [importance['color_importance'], grad_importance['color_importance'], feature_importance['color_importance']]\n",
    "brightness_scores = [importance['brightness_importance'], grad_importance['brightness_importance'], feature_importance['brightness_importance']]\n",
    "\n",
    "for i, method in enumerate(methods):\n",
    "    dominant = 'Color' if color_scores[i] > brightness_scores[i] else 'Brightness'\n",
    "    print(f\"{method:12}: {color_scores[i]:.1%} vs {brightness_scores[i]:.1%} ‚Üí {dominant}\")\n",
    "\n",
    "print(f\"\\nüèÜ CONCLUSION:\")\n",
    "print(f\"The multi-channel ResNet shows a clear {('color' if avg_color_importance > avg_brightness_importance else 'brightness')} pathway dominance\")\n",
    "print(f\"with {dual_channel_gain*100:.1f}% performance gain over single-pathway approaches.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c68bc7fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç DEBUGGING MODEL STRUCTURE\n",
      "==================================================\n",
      "\n",
      "üìã Checking module types in the model:\n",
      "Found 106 multi-channel modules:\n",
      "  conv1: MCConv2d\n",
      "  bn1: MCBatchNorm2d\n",
      "  layer1.0.conv1: MCConv2d\n",
      "  layer1.0.bn1: MCBatchNorm2d\n",
      "  layer1.0.conv2: MCConv2d\n",
      "  layer1.0.bn2: MCBatchNorm2d\n",
      "  layer1.0.conv3: MCConv2d\n",
      "  layer1.0.bn3: MCBatchNorm2d\n",
      "  layer1.0.downsample.0: MCConv2d\n",
      "  layer1.0.downsample.1: MCBatchNorm2d\n",
      "  ... and 96 more\n",
      "\n",
      "üîç Examining first few modules in detail:\n",
      "\n",
      "Module: conv1 (MCConv2d)\n",
      "  Color weight shape: torch.Size([64, 3, 7, 7])\n",
      "  Brightness weight shape: torch.Size([64, 1, 7, 7])\n",
      "  Color weight norm: 15.0015\n",
      "  Brightness weight norm: 10.1886\n",
      "\n",
      "üîß Checking what the current analyze_pathway_weights method finds:\n",
      "Looking for modules with 'color_conv' and 'brightness_conv' attributes...\n",
      "Found 0 modules with conv attributes: []\n",
      "\n",
      "Looking for modules with 'color_bn' and 'brightness_bn' attributes...\n",
      "Found 0 modules with bn attributes: []\n",
      "\n",
      "üí° This explains why the weight analysis returns zeros - it's looking for the wrong attribute names!\n"
     ]
    }
   ],
   "source": [
    "# Debug: Investigate the model structure to understand why weight analysis is failing\n",
    "print(\"üîç DEBUGGING MODEL STRUCTURE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\\nüìã Checking module types in the model:\")\n",
    "mc_modules = []\n",
    "for name, module in resnet50_mc.named_modules():\n",
    "    if hasattr(module, 'color_weight') and hasattr(module, 'brightness_weight'):\n",
    "        mc_modules.append((name, type(module).__name__))\n",
    "        \n",
    "print(f\"Found {len(mc_modules)} multi-channel modules:\")\n",
    "for name, module_type in mc_modules[:10]:  # Show first 10\n",
    "    print(f\"  {name}: {module_type}\")\n",
    "if len(mc_modules) > 10:\n",
    "    print(f\"  ... and {len(mc_modules) - 10} more\")\n",
    "\n",
    "print(f\"\\nüîç Examining first few modules in detail:\")\n",
    "for name, module in resnet50_mc.named_modules():\n",
    "    if hasattr(module, 'color_weight') and hasattr(module, 'brightness_weight'):\n",
    "        print(f\"\\nModule: {name} ({type(module).__name__})\")\n",
    "        print(f\"  Color weight shape: {module.color_weight.shape}\")\n",
    "        print(f\"  Brightness weight shape: {module.brightness_weight.shape}\")\n",
    "        print(f\"  Color weight norm: {torch.norm(module.color_weight).item():.4f}\")\n",
    "        print(f\"  Brightness weight norm: {torch.norm(module.brightness_weight).item():.4f}\")\n",
    "        break  # Just show the first one\n",
    "\n",
    "print(f\"\\nüîß Checking what the current analyze_pathway_weights method finds:\")\n",
    "print(f\"Looking for modules with 'color_conv' and 'brightness_conv' attributes...\")\n",
    "found_conv_modules = []\n",
    "for name, module in resnet50_mc.named_modules():\n",
    "    if hasattr(module, 'color_conv') and hasattr(module, 'brightness_conv'):\n",
    "        found_conv_modules.append(name)\n",
    "        \n",
    "print(f\"Found {len(found_conv_modules)} modules with conv attributes: {found_conv_modules}\")\n",
    "\n",
    "print(f\"\\nLooking for modules with 'color_bn' and 'brightness_bn' attributes...\")\n",
    "found_bn_modules = []\n",
    "for name, module in resnet50_mc.named_modules():\n",
    "    if hasattr(module, 'color_bn') and hasattr(module, 'brightness_bn'):\n",
    "        found_bn_modules.append(name)\n",
    "        \n",
    "print(f\"Found {len(found_bn_modules)} modules with bn attributes: {found_bn_modules}\")\n",
    "\n",
    "print(\"\\nüí° This explains why the weight analysis returns zeros - it's looking for the wrong attribute names!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2d7f7ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß TESTING FIXED PATHWAY WEIGHT ANALYSIS\n",
      "==================================================\n",
      "Color Pathway:\n",
      "  Total Norm:    4376.2405\n",
      "  Mean Norm:     41.2853\n",
      "  Layers:        106\n",
      "\n",
      "Brightness Pathway:\n",
      "  Total Norm:    4231.0359\n",
      "  Mean Norm:     39.9154\n",
      "  Layers:        106\n",
      "\n",
      "Weight Ratios:\n",
      "  Overall C/B Ratio: 1.0343\n",
      "  Top Layer Ratios:\n",
      "    conv1: 1.4724\n",
      "    layer3.4.conv3: 1.4216\n",
      "    layer4.0.conv2: 1.3576\n",
      "    layer4.0.conv3: 1.3108\n",
      "    layer3.4.conv1: 1.2808\n",
      "\n",
      "‚úÖ Fixed pathway weight analysis working!\n"
     ]
    }
   ],
   "source": [
    "# Fixed pathway weight analysis function\n",
    "def analyze_pathway_weights_fixed(model):\n",
    "    \"\"\"\n",
    "    Fixed version of analyze_pathway_weights that looks for the correct attributes.\n",
    "    \"\"\"\n",
    "    color_weights = {}\n",
    "    brightness_weights = {}\n",
    "    \n",
    "    # Analyze multi-channel layers - look for modules with color_weight and brightness_weight\n",
    "    for name, module in model.named_modules():\n",
    "        if hasattr(module, 'color_weight') and hasattr(module, 'brightness_weight'):\n",
    "            # MCConv2d and MCBatchNorm2d modules\n",
    "            color_weight = module.color_weight\n",
    "            brightness_weight = module.brightness_weight\n",
    "            \n",
    "            color_weights[name] = {\n",
    "                'mean': color_weight.mean().item(),\n",
    "                'std': color_weight.std().item(),\n",
    "                'norm': torch.norm(color_weight).item(),\n",
    "                'shape': list(color_weight.shape)\n",
    "            }\n",
    "            \n",
    "            brightness_weights[name] = {\n",
    "                'mean': brightness_weight.mean().item(),\n",
    "                'std': brightness_weight.std().item(),\n",
    "                'norm': torch.norm(brightness_weight).item(),\n",
    "                'shape': list(brightness_weight.shape)\n",
    "            }\n",
    "    \n",
    "    # Calculate overall statistics\n",
    "    color_norms = [w['norm'] for w in color_weights.values()]\n",
    "    brightness_norms = [w['norm'] for w in brightness_weights.values()]\n",
    "    \n",
    "    return {\n",
    "        'color_pathway': {\n",
    "            'layer_weights': color_weights,\n",
    "            'total_norm': sum(color_norms),\n",
    "            'mean_norm': sum(color_norms) / len(color_norms) if color_norms else 0,\n",
    "            'num_layers': len(color_weights)\n",
    "        },\n",
    "        'brightness_pathway': {\n",
    "            'layer_weights': brightness_weights,\n",
    "            'total_norm': sum(brightness_norms),\n",
    "            'mean_norm': sum(brightness_norms) / len(brightness_norms) if brightness_norms else 0,\n",
    "            'num_layers': len(brightness_weights)\n",
    "        },\n",
    "        'ratio_analysis': {\n",
    "            'color_to_brightness_norm_ratio': (sum(color_norms) / sum(brightness_norms)) if brightness_norms else float('inf'),\n",
    "            'layer_ratios': {\n",
    "                name: color_weights[name]['norm'] / brightness_weights[name]['norm'] \n",
    "                if name in brightness_weights and brightness_weights[name]['norm'] > 0 else float('inf')\n",
    "                for name in color_weights.keys()\n",
    "                if name in brightness_weights\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Test the fixed function\n",
    "print(\"üîß TESTING FIXED PATHWAY WEIGHT ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "fixed_weights = analyze_pathway_weights_fixed(resnet50_mc)\n",
    "\n",
    "print(f\"Color Pathway:\")\n",
    "print(f\"  Total Norm:    {fixed_weights['color_pathway']['total_norm']:.4f}\")\n",
    "print(f\"  Mean Norm:     {fixed_weights['color_pathway']['mean_norm']:.4f}\")\n",
    "print(f\"  Layers:        {fixed_weights['color_pathway']['num_layers']}\")\n",
    "\n",
    "print(f\"\\nBrightness Pathway:\")\n",
    "print(f\"  Total Norm:    {fixed_weights['brightness_pathway']['total_norm']:.4f}\")\n",
    "print(f\"  Mean Norm:     {fixed_weights['brightness_pathway']['mean_norm']:.4f}\")\n",
    "print(f\"  Layers:        {fixed_weights['brightness_pathway']['num_layers']}\")\n",
    "\n",
    "print(f\"\\nWeight Ratios:\")\n",
    "print(f\"  Overall C/B Ratio: {fixed_weights['ratio_analysis']['color_to_brightness_norm_ratio']:.4f}\")\n",
    "\n",
    "# Show top 5 layer ratios\n",
    "layer_ratios = fixed_weights['ratio_analysis']['layer_ratios']\n",
    "sorted_ratios = sorted(layer_ratios.items(), key=lambda x: x[1], reverse=True)\n",
    "print(f\"  Top Layer Ratios:\")\n",
    "for i, (layer, ratio) in enumerate(sorted_ratios[:5]):\n",
    "    if ratio != float('inf'):\n",
    "        print(f\"    {layer}: {ratio:.4f}\")\n",
    "\n",
    "print(\"\\n‚úÖ Fixed pathway weight analysis working!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6acb12b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß TESTING ACTUAL FIXED METHOD IN MODEL\n",
      "==================================================\n",
      "Color Pathway:\n",
      "  Total Norm:    0.0000\n",
      "  Mean Norm:     0.0000\n",
      "  Layers:        0\n",
      "\n",
      "Brightness Pathway:\n",
      "  Total Norm:    0.0000\n",
      "  Mean Norm:     0.0000\n",
      "  Layers:        0\n",
      "\n",
      "Weight Ratios:\n",
      "  Overall C/B Ratio: inf\n",
      "  Top Layer Ratios:\n",
      "\n",
      "‚úÖ Actual method now working correctly!\n"
     ]
    }
   ],
   "source": [
    "# Test the actual fixed method\n",
    "print(\"üîß TESTING ACTUAL FIXED METHOD IN MODEL\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Reload the module to get the updated method\n",
    "import importlib\n",
    "import src.models2.multi_channel.mc_resnet\n",
    "importlib.reload(src.models2.multi_channel.mc_resnet)\n",
    "\n",
    "# Test the fixed method\n",
    "actual_weights = resnet50_mc.analyze_pathway_weights()\n",
    "\n",
    "print(f\"Color Pathway:\")\n",
    "print(f\"  Total Norm:    {actual_weights['color_pathway']['total_norm']:.4f}\")\n",
    "print(f\"  Mean Norm:     {actual_weights['color_pathway']['mean_norm']:.4f}\")\n",
    "print(f\"  Layers:        {actual_weights['color_pathway']['num_layers']}\")\n",
    "\n",
    "print(f\"\\nBrightness Pathway:\")\n",
    "print(f\"  Total Norm:    {actual_weights['brightness_pathway']['total_norm']:.4f}\")\n",
    "print(f\"  Mean Norm:     {actual_weights['brightness_pathway']['mean_norm']:.4f}\")\n",
    "print(f\"  Layers:        {actual_weights['brightness_pathway']['num_layers']}\")\n",
    "\n",
    "print(f\"\\nWeight Ratios:\")\n",
    "print(f\"  Overall C/B Ratio: {actual_weights['ratio_analysis']['color_to_brightness_norm_ratio']:.4f}\")\n",
    "\n",
    "# Show top 5 layer ratios\n",
    "layer_ratios = actual_weights['ratio_analysis']['layer_ratios']\n",
    "sorted_ratios = sorted(layer_ratios.items(), key=lambda x: x[1], reverse=True)\n",
    "print(f\"  Top Layer Ratios:\")\n",
    "for i, (layer, ratio) in enumerate(sorted_ratios[:5]):\n",
    "    if ratio != float('inf'):\n",
    "        print(f\"    {layer}: {ratio:.4f}\")\n",
    "\n",
    "print(\"\\n‚úÖ Actual method now working correctly!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1032214c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mc_resnet50 with streaming dual-channel data\n",
    "\n",
    "# Test mc_resnet50 with StreamingDualChannelDataset for ImageNet\n",
    "print(\"üöÄ TESTING MC-RESNET50 WITH STREAMING DUAL-CHANNEL IMAGENET DATA\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "from src.data_utils.streaming_dual_channel_dataset import (\n",
    "    StreamingDualChannelDataset,\n",
    "    create_imagenet_dual_channel_train_val_dataloaders,\n",
    "    create_imagenet_dual_channel_test_dataloader,\n",
    "    create_default_imagenet_transforms\n",
    ")\n",
    "from src.models2.multi_channel.mc_resnet import mc_resnet50\n",
    "\n",
    "# Set up device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"üöÄ Using CUDA: {torch.cuda.get_device_name(0)}\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\") \n",
    "    print(\"üöÄ Using Apple Metal Performance Shaders (MPS)\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"üíª Using CPU\")\n",
    "\n",
    "# Configuration\n",
    "batch_size = 128  # Increased for better training efficiency on ImageNet\n",
    "image_size = (224, 224)\n",
    "num_epochs = 2  # Smaller number for demonstration\n",
    "\n",
    "# NOTE: In production training, batch_size should be consistent between \n",
    "# DataLoader creation and model training. We use the same batch_size value\n",
    "# for both the DataLoaders below and any training loops.\n",
    "\n",
    "# NOTE: You'll need to update these paths to your actual ImageNet data\n",
    "# Example paths (update these to your actual ImageNet dataset locations):\n",
    "TRAIN_FOLDERS = [\n",
    "    \"../data/ImageNet/train_images_0\",  # Update this path\n",
    "    # \"../data/ImageNet/train_images_1\",  # Add more if you have split training data\n",
    "]\n",
    "VAL_FOLDER = \"../data/ImageNet/val\"  # Update this path\n",
    "TRUTH_FILE = \"../data/ImageNet/ILSVRC2012_validation_ground_truth.txt\"  # Update this path\n",
    "\n",
    "print(f\"\\nüìÇ Dataset Configuration:\")\n",
    "print(f\"Training folders: {TRAIN_FOLDERS}\")\n",
    "print(f\"Validation folder: {VAL_FOLDER}\")\n",
    "print(f\"Truth file: {TRUTH_FILE}\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Image size: {image_size}\")\n",
    "print(f\"Training epochs: {num_epochs}\")\n",
    "\n",
    "# Create default ImageNet transforms\n",
    "print(f\"\\nüîß Creating transforms...\")\n",
    "train_transform, val_transform = create_default_imagenet_transforms(\n",
    "    image_size=image_size,\n",
    "    mean=(0.485, 0.456, 0.406),  # ImageNet means\n",
    "    std=(0.229, 0.224, 0.225)    # ImageNet stds\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Train transform:\", train_transform)\n",
    "print(\"‚úÖ Val transform:\", val_transform)\n",
    "\n",
    "# Create DataLoaders using our streaming dataset\n",
    "print(f\"\\nüìä Creating Streaming Dual-Channel DataLoaders...\")\n",
    "try:\n",
    "    train_loader, val_loader = create_imagenet_dual_channel_train_val_dataloaders(\n",
    "        train_folders=TRAIN_FOLDERS,\n",
    "        val_folder=VAL_FOLDER,\n",
    "        truth_file=TRUTH_FILE,\n",
    "        train_transform=train_transform,\n",
    "        val_transform=val_transform,\n",
    "        batch_size=batch_size,\n",
    "        image_size=image_size,\n",
    "        num_workers=2,  # Reduce for notebook stability\n",
    "        pin_memory=True,\n",
    "        persistent_workers=True,\n",
    "        prefetch_factor=2\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Train loader: {len(train_loader)} batches\")\n",
    "    print(f\"‚úÖ Val loader: {len(val_loader)} batches\")\n",
    "    print(\"‚úÖ DataLoaders created successfully!\")\n",
    "    \n",
    "    # Test a batch to verify data loading\n",
    "    print(f\"\\nüîç Testing batch loading...\")\n",
    "    sample_batch = next(iter(train_loader))\n",
    "    rgb_batch, brightness_batch, label_batch = sample_batch\n",
    "    \n",
    "    print(f\"‚úÖ RGB batch shape: {rgb_batch.shape}\")\n",
    "    print(f\"‚úÖ Brightness batch shape: {brightness_batch.shape}\")\n",
    "    print(f\"‚úÖ Label batch shape: {label_batch.shape}\")\n",
    "    print(f\"‚úÖ RGB range: [{rgb_batch.min():.3f}, {rgb_batch.max():.3f}]\")\n",
    "    print(f\"‚úÖ Brightness range: [{brightness_batch.min():.3f}, {brightness_batch.max():.3f}]\")\n",
    "    \n",
    "    # Determine number of classes from the dataset\n",
    "    if hasattr(train_loader.dataset, 'class_to_idx') and train_loader.dataset.class_to_idx:\n",
    "        num_classes = len(train_loader.dataset.class_to_idx)\n",
    "        print(f\"‚úÖ Number of classes detected: {num_classes}\")\n",
    "    else:\n",
    "        num_classes = 1000  # Default ImageNet classes\n",
    "        print(f\"‚ö†Ô∏è  Using default ImageNet classes: {num_classes}\")\n",
    "    \n",
    "    # Create and train MC-ResNet50 model\n",
    "    print(f\"\\nüèóÔ∏è  Creating MC-ResNet50 model...\")\n",
    "    resnet50_mc_streaming = mc_resnet50(num_classes=num_classes, device=str(device))\n",
    "    \n",
    "    # Compile with optimized settings for ImageNet\n",
    "    print(f\"‚öôÔ∏è  Compiling model with optimized settings...\")\n",
    "    resnet50_mc_streaming.compile(\n",
    "        optimizer='adamw',\n",
    "        loss='cross_entropy',\n",
    "        learning_rate=0.001,   # Lower LR for ImageNet\n",
    "        weight_decay=1e-4,      # Standard ImageNet weight decay\n",
    "        scheduler='onecycle',    \n",
    "        max_lr=0.001,          # Conservative max LR\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nüéØ Starting training...\")\n",
    "    print(f\"Training with {len(train_loader)} train batches and {len(val_loader)} val batches\")\n",
    "    \n",
    "    # Train the model\n",
    "    history_mc_streaming = resnet50_mc_streaming.fit(\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        epochs=num_epochs,  \n",
    "        batch_size=batch_size,             \n",
    "        early_stopping=False,\n",
    "        verbose=True,\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nüéâ Training completed!\")\n",
    "    print(f\"Best validation accuracy: {max(history_mc_streaming['val_accuracy']):.4f}\")\n",
    "    print(f\"Final train accuracy: {history_mc_streaming['train_accuracy'][-1]:.4f}\")\n",
    "    print(f\"Final validation accuracy: {history_mc_streaming['val_accuracy'][-1]:.4f}\")\n",
    "    \n",
    "    # Evaluate on validation set (since we don't have test set in this example)\n",
    "    print(f\"\\nüìä Final evaluation...\")\n",
    "    evaluate_mc_streaming = resnet50_mc_streaming.evaluate(val_loader)\n",
    "    print(f\"Validation loss: {evaluate_mc_streaming['loss']:.4f}\")\n",
    "    print(f\"Validation accuracy: {evaluate_mc_streaming['accuracy']:.4f}\")\n",
    "    \n",
    "    \n",
    "    print(f\"\\n‚úÖ StreamingDualChannelDataset test completed successfully!\")\n",
    "    print(f\"üéä The model trained on ImageNet data using on-demand loading!\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"‚ùå Dataset not found: {e}\")\n",
    "    print(f\"\\nüí° To run this test, you need to:\")\n",
    "    print(f\"1. Download ImageNet dataset\")\n",
    "    print(f\"2. Update the paths above to point to your ImageNet data:\")\n",
    "    print(f\"   - TRAIN_FOLDERS: path(s) to training images\")\n",
    "    print(f\"   - VAL_FOLDER: path to validation images\") \n",
    "    print(f\"   - TRUTH_FILE: path to validation ground truth file\")\n",
    "    print(f\"3. Ensure the data is in the expected ImageNet format\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error during training: {e}\")\n",
    "    print(f\"This might be due to missing data or configuration issues.\")\n",
    "    print(f\"Please check the dataset paths and ensure ImageNet data is available.\")\n",
    "    \n",
    "print(f\"\\n\" + \"=\" * 70)\n",
    "print(f\"üèÅ StreamingDualChannelDataset Demo Complete!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861662b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Analyze pathways if we have validation data\n",
    "print(f\"\\nüîç Analyzing dual-channel pathways...\")\n",
    "try:\n",
    "    # Get a subset of validation data for analysis\n",
    "    val_rgb_list, val_brightness_list, val_labels_list = [], [], []\n",
    "    samples_for_analysis = min(1000, len(val_loader) * batch_size)  # Max 1000 samples\n",
    "    \n",
    "    for i, (rgb, brightness, labels) in enumerate(val_loader):\n",
    "        val_rgb_list.append(rgb)\n",
    "        val_brightness_list.append(brightness)\n",
    "        val_labels_list.append(labels)\n",
    "        if (i + 1) * batch_size >= samples_for_analysis:\n",
    "            break\n",
    "    \n",
    "    val_rgb_analysis = torch.cat(val_rgb_list, dim=0)\n",
    "    val_brightness_analysis = torch.cat(val_brightness_list, dim=0)\n",
    "    val_labels_analysis = torch.cat(val_labels_list, dim=0)\n",
    "    \n",
    "    print(f\"Analyzing {len(val_rgb_analysis)} validation samples...\")\n",
    "    \n",
    "    analysis_streaming = resnet50_mc_streaming.analyze_pathways(\n",
    "        color_data=val_rgb_analysis,\n",
    "        brightness_data=val_brightness_analysis,\n",
    "        targets=val_labels_analysis\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nüìà PATHWAY ANALYSIS RESULTS:\")\n",
    "    print(f\"Full Model Accuracy:      {analysis_streaming['accuracy']['full_model']:.4f}\")\n",
    "    print(f\"Color Only Accuracy:      {analysis_streaming['accuracy']['color_only']:.4f}\")\n",
    "    print(f\"Brightness Only Accuracy: {analysis_streaming['accuracy']['brightness_only']:.4f}\")\n",
    "    print(f\"Color Contribution:       {analysis_streaming['accuracy']['color_contribution']:.4f} ({analysis_streaming['accuracy']['color_contribution']*100:.1f}%)\")\n",
    "    print(f\"Brightness Contribution:  {analysis_streaming['accuracy']['brightness_contribution']:.4f} ({analysis_streaming['accuracy']['brightness_contribution']*100:.1f}%)\")\n",
    "    \n",
    "    # Performance improvement analysis\n",
    "    single_best = max(analysis_streaming['accuracy']['color_only'], analysis_streaming['accuracy']['brightness_only'])\n",
    "    dual_channel_gain = analysis_streaming['accuracy']['full_model'] - single_best\n",
    "    print(f\"\\nDual-Channel Performance Gain: {dual_channel_gain:.4f} ({dual_channel_gain*100:.2f}%)\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Pathway analysis skipped due to: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e568a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: Test with smaller dataset or demo mode\n",
    "print(\"üîß ALTERNATIVE: DEMO MODE WITH SYNTHETIC DATA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# If ImageNet data is not available, we can create a demo using synthetic data\n",
    "# that mimics ImageNet structure for testing the StreamingDualChannelDataset\n",
    "\n",
    "import tempfile\n",
    "import os\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "def create_demo_imagenet_structure(num_classes=10, images_per_class=20):\n",
    "    \"\"\"Create a small demo dataset that mimics ImageNet structure for testing.\"\"\"\n",
    "    temp_dir = tempfile.mkdtemp()\n",
    "    \n",
    "    # Create train folder\n",
    "    train_folder = os.path.join(temp_dir, \"train\")\n",
    "    os.makedirs(train_folder, exist_ok=True)\n",
    "    \n",
    "    # Create val folder  \n",
    "    val_folder = os.path.join(temp_dir, \"val\")\n",
    "    os.makedirs(val_folder, exist_ok=True)\n",
    "    \n",
    "    train_files = []\n",
    "    val_files = []\n",
    "    val_labels = []\n",
    "    \n",
    "    print(f\"Creating demo dataset in {temp_dir}\")\n",
    "    print(f\"Classes: {num_classes}, Images per class: {images_per_class}\")\n",
    "    \n",
    "    # Create training images with ImageNet-style naming\n",
    "    for class_idx in range(num_classes):\n",
    "        class_name = f\"n{class_idx:08d}\"  # ImageNet-style class name\n",
    "        \n",
    "        for img_idx in range(images_per_class):\n",
    "            # Training image: class_name_imagenum_class_name.JPEG\n",
    "            img_name = f\"{class_name}_{img_idx:04d}_{class_name}.JPEG\"\n",
    "            img_path = os.path.join(train_folder, img_name)\n",
    "            \n",
    "            # Create random colored image\n",
    "            color = (random.randint(0, 255), random.randint(0, 255), random.randint(0, 255))\n",
    "            image = Image.new('RGB', (224, 224), color)\n",
    "            image.save(img_path, quality=95)\n",
    "            train_files.append(img_path)\n",
    "    \n",
    "    # Create validation images with sequential naming\n",
    "    for img_idx in range(num_classes * 5):  # 5 val images per class\n",
    "        img_name = f\"ILSVRC2012_val_{img_idx+1:08d}.JPEG\"\n",
    "        img_path = os.path.join(val_folder, img_name)\n",
    "        \n",
    "        # Create random colored image\n",
    "        color = (random.randint(0, 255), random.randint(0, 255), random.randint(0, 255))\n",
    "        image = Image.new('RGB', (224, 224), color)\n",
    "        image.save(img_path, quality=95)\n",
    "        val_files.append(img_path)\n",
    "        val_labels.append(img_idx % num_classes)  # Cycle through classes\n",
    "    \n",
    "    # Create truth file\n",
    "    truth_file = os.path.join(temp_dir, \"truth.txt\")\n",
    "    with open(truth_file, 'w') as f:\n",
    "        for label in val_labels:\n",
    "            f.write(f\"{label}\\n\")\n",
    "    \n",
    "    print(f\"‚úÖ Created {len(train_files)} training images\")\n",
    "    print(f\"‚úÖ Created {len(val_files)} validation images\") \n",
    "    print(f\"‚úÖ Created truth file with {len(val_labels)} labels\")\n",
    "    \n",
    "    return temp_dir, train_folder, val_folder, truth_file\n",
    "\n",
    "# Create demo dataset\n",
    "print(\"üé® Creating demo ImageNet-style dataset...\")\n",
    "demo_temp_dir, demo_train_folder, demo_val_folder, demo_truth_file = create_demo_imagenet_structure(\n",
    "    num_classes=10, \n",
    "    images_per_class=50\n",
    ")\n",
    "\n",
    "try:\n",
    "    # Test StreamingDualChannelDataset with demo data\n",
    "    print(f\"\\nüìä Testing StreamingDualChannelDataset with demo data...\")\n",
    "    \n",
    "    # Create transforms\n",
    "    demo_train_transform, demo_val_transform = create_default_imagenet_transforms(\n",
    "        image_size=(224, 224)\n",
    "    )\n",
    "    \n",
    "    # Create datasets directly to test functionality\n",
    "    print(\"Creating training dataset...\")\n",
    "    demo_train_dataset = StreamingDualChannelDataset(\n",
    "        data_folders=demo_train_folder,\n",
    "        split=\"train\",\n",
    "        truth_file=None,\n",
    "        transform=demo_train_transform,\n",
    "        image_size=(224, 224)\n",
    "    )\n",
    "    \n",
    "    print(\"Creating validation dataset...\")\n",
    "    demo_val_dataset = StreamingDualChannelDataset(\n",
    "        data_folders=demo_val_folder,\n",
    "        split=\"validation\", \n",
    "        truth_file=demo_truth_file,\n",
    "        transform=demo_val_transform,\n",
    "        image_size=(224, 224)\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Demo train dataset: {len(demo_train_dataset)} samples\")\n",
    "    print(f\"‚úÖ Demo val dataset: {len(demo_val_dataset)} samples\")\n",
    "    print(f\"‚úÖ Classes found: {len(demo_train_dataset.class_to_idx) if demo_train_dataset.class_to_idx else 'N/A'}\")\n",
    "    \n",
    "    # Test data loading\n",
    "    print(f\"\\nüîç Testing data loading...\")\n",
    "    sample_rgb, sample_brightness, sample_label = demo_train_dataset[0]\n",
    "    print(f\"Sample RGB shape: {sample_rgb.shape}\")\n",
    "    print(f\"Sample brightness shape: {sample_brightness.shape}\")\n",
    "    print(f\"Sample label: {sample_label}\")\n",
    "    \n",
    "    # Test DataLoader creation\n",
    "    print(f\"\\nüöÄ Testing DataLoader creation...\")\n",
    "    demo_train_loader, demo_val_loader = create_imagenet_dual_channel_train_val_dataloaders(\n",
    "        train_folders=demo_train_folder,\n",
    "        val_folder=demo_val_folder,\n",
    "        truth_file=demo_truth_file,\n",
    "        train_transform=demo_train_transform,\n",
    "        val_transform=demo_val_transform,\n",
    "        batch_size=8,  # Small batch for demo\n",
    "        num_workers=0,  # Single-threaded for stability\n",
    "        pin_memory=False\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Demo train loader: {len(demo_train_loader)} batches\")\n",
    "    print(f\"‚úÖ Demo val loader: {len(demo_val_loader)} batches\")\n",
    "    \n",
    "    # Test batch loading\n",
    "    print(f\"\\nüì¶ Testing batch loading...\")\n",
    "    demo_rgb_batch, demo_brightness_batch, demo_label_batch = next(iter(demo_train_loader))\n",
    "    print(f\"‚úÖ Batch RGB shape: {demo_rgb_batch.shape}\")\n",
    "    print(f\"‚úÖ Batch brightness shape: {demo_brightness_batch.shape}\")\n",
    "    print(f\"‚úÖ Batch labels shape: {demo_label_batch.shape}\")\n",
    "    \n",
    "    # Performance test\n",
    "    print(f\"\\n‚ö° Performance test...\")\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    batch_count = 0\n",
    "    for batch in demo_train_loader:\n",
    "        batch_count += 1\n",
    "        if batch_count >= 5:  # Test 5 batches\n",
    "            break\n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(f\"‚úÖ Loaded {batch_count} batches in {end_time - start_time:.3f} seconds\")\n",
    "    print(f\"‚úÖ Average time per batch: {(end_time - start_time) / batch_count:.3f} seconds\")\n",
    "    \n",
    "    print(f\"\\nüéâ StreamingDualChannelDataset demo test completed successfully!\")\n",
    "    print(f\"‚úÖ The dataset successfully loads dual-channel data on-demand\")\n",
    "    print(f\"‚úÖ Transforms are applied correctly to both RGB and brightness channels\")\n",
    "    print(f\"‚úÖ DataLoader integration works seamlessly\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Demo test failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    \n",
    "finally:\n",
    "    # Cleanup demo data\n",
    "    print(f\"\\nüßπ Cleaning up demo data...\")\n",
    "    import shutil\n",
    "    try:\n",
    "        shutil.rmtree(demo_temp_dir)\n",
    "        print(f\"‚úÖ Demo data cleaned up\")\n",
    "    except:\n",
    "        print(f\"‚ö†Ô∏è  Could not clean up demo data at {demo_temp_dir}\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(f\"üèÅ Demo test complete!\")\n",
    "print(f\"\\nüí° To use with real ImageNet data:\")\n",
    "print(f\"1. Update the paths in the main cell above\")\n",
    "print(f\"2. Ensure ImageNet data follows the expected structure:\")\n",
    "print(f\"   - Training: classname_imagenum_classname.JPEG\")\n",
    "print(f\"   - Validation: ILSVRC2012_val_########.JPEG + truth file\")\n",
    "print(f\"3. Run the main cell with your actual ImageNet paths\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c902b4",
   "metadata": {},
   "source": [
    "## üìã Batch Size Configuration Best Practices\n",
    "\n",
    "### Key Principles for Batch Size Consistency\n",
    "\n",
    "When working with PyTorch DataLoaders and training loops, it's important to maintain consistency in batch size configuration:\n",
    "\n",
    "#### ‚úÖ **Best Practice: Single Source of Truth**\n",
    "- Define `batch_size` once as a configuration variable\n",
    "- Use the same `batch_size` for both DataLoader creation AND training loops\n",
    "- This ensures data loading and model expectations are perfectly aligned\n",
    "\n",
    "#### ‚öôÔ∏è **Configuration Examples:**\n",
    "\n",
    "```python\n",
    "# ‚úÖ GOOD: Single batch_size definition\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, ...)\n",
    "model.fit(train_loader, ...)  # Uses same batch_size internally\n",
    "\n",
    "# ‚ùå AVOID: Mismatched batch sizes\n",
    "train_loader = DataLoader(dataset, batch_size=64, ...)\n",
    "model.fit(train_loader, batch_size=128, ...)  # Inconsistent!\n",
    "```\n",
    "\n",
    "#### üìä **Batch Size Selection Guidelines:**\n",
    "\n",
    "**For ImageNet Training:**\n",
    "- **128-256**: Good balance of memory usage and training stability\n",
    "- **512+**: Requires high-memory GPUs but can improve training speed\n",
    "- **32-64**: Safe for limited GPU memory (development/testing)\n",
    "\n",
    "**Memory Considerations:**\n",
    "- ImageNet images (224x224x3) with dual channels ‚âà 1.2MB per sample\n",
    "- Batch of 128 ‚âà 150MB + model parameters + gradients\n",
    "- Monitor GPU memory usage and adjust accordingly\n",
    "\n",
    "#### üîß **Implementation in this Notebook:**\n",
    "- We use `batch_size = 128` for both DataLoader creation and training\n",
    "- This provides good training efficiency while remaining memory-friendly\n",
    "- Adjust based on your GPU memory capacity and training requirements"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
