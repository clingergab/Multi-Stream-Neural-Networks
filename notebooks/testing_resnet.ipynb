{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7f8cfec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /Users/gclinger/Documents/projects/Multi-Stream-Neural-Networks\n",
      "Current working directory: /Users/gclinger/Documents/projects/Multi-Stream-Neural-Networks/notebooks\n",
      "Changed to project root: /Users/gclinger/Documents/projects/Multi-Stream-Neural-Networks\n",
      "Installing package in development mode...\n",
      "‚úÖ Package installed in development mode successfully!\n",
      "Now you can import modules without 'src.' prefix\n",
      "‚úÖ Environment setup complete!\n",
      "‚úÖ Package installed in development mode successfully!\n",
      "Now you can import modules without 'src.' prefix\n",
      "‚úÖ Environment setup complete!\n"
     ]
    }
   ],
   "source": [
    "# Environment Setup\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "# Set up project root path\n",
    "project_root = Path.cwd()\n",
    "while not (project_root / \"src\").exists() and project_root != project_root.parent:\n",
    "    project_root = project_root.parent\n",
    "\n",
    "if not (project_root / \"src\").exists():\n",
    "    # Fallback: assume we're in notebooks directory\n",
    "    project_root = Path.cwd().parent\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "\n",
    "# Change to project root directory\n",
    "os.chdir(project_root)\n",
    "print(f\"Changed to project root: {os.getcwd()}\")\n",
    "\n",
    "# Install package in development mode\n",
    "print(\"Installing package in development mode...\")\n",
    "try:\n",
    "    result = subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-e\", \".\"], \n",
    "                          check=True, capture_output=True, text=True)\n",
    "    print(\"‚úÖ Package installed in development mode successfully!\")\n",
    "    print(\"Now you can import modules without 'src.' prefix\")\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"‚ùå Error installing package: {e}\")\n",
    "    print(f\"Output: {e.output}\")\n",
    "    print(\"‚ö†Ô∏è  Falling back to manual path setup...\")\n",
    "    \n",
    "    # Fallback: Add project root to path for imports\n",
    "    if str(project_root) not in sys.path:\n",
    "        sys.path.insert(0, str(project_root))\n",
    "        print(f\"‚úÖ Added {project_root} to sys.path\")\n",
    "\n",
    "# Set environment variables for better error reporting\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "print(\"‚úÖ Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e574c065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Importing libraries...\n",
      "‚úÖ All project modules imported successfully\n",
      "üöÄ Using Apple Metal Performance Shaders (MPS)\n",
      "PyTorch version: 2.7.1\n",
      "Device: mps\n",
      "‚úÖ Library imports complete!\n",
      "‚úÖ All project modules imported successfully\n",
      "üöÄ Using Apple Metal Performance Shaders (MPS)\n",
      "PyTorch version: 2.7.1\n",
      "Device: mps\n",
      "‚úÖ Library imports complete!\n"
     ]
    }
   ],
   "source": [
    "# Import Libraries\n",
    "print(\"üì¶ Importing libraries...\")\n",
    "\n",
    "# Core PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Machine learning utilities\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Visualization and analysis\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "\n",
    "# Progress tracking\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Project imports\n",
    "try:\n",
    "    from data_utils.dataset_utils import load_cifar100_data, CIFAR100_FINE_LABELS\n",
    "    from data_utils.rgb_to_rgbl import RGBtoRGBL\n",
    "\n",
    "    print(\"‚úÖ All project modules imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Error importing project modules: {e}\")\n",
    "    print(\"‚ö†Ô∏è  Please ensure you're running from the correct directory\")\n",
    "\n",
    "# Check device availability\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"üöÄ Using CUDA: {torch.cuda.get_device_name(0)}\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\") \n",
    "    print(\"üöÄ Using Apple Metal Performance Shaders (MPS)\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"üíª Using CPU\")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {device}\")\n",
    "print(\"‚úÖ Library imports complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3965b99d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Testing development mode installation...\n",
      "‚úÖ Found package: ['multi-stream-neural-networks             0.1.0           /Users/gclinger/Documents/projects/Multi-Stream-Neural-Networks/src']\n",
      "\n",
      "üß™ Testing import approaches:\n",
      "‚úÖ Method 1: 'import data_utils' - SUCCESS\n",
      "‚úÖ Method 2: 'from data_utils import dataset_utils' - SUCCESS\n",
      "\n",
      "üìÅ Current working directory: /Users/gclinger/Documents/projects/Multi-Stream-Neural-Networks\n",
      "üìÅ Python path includes:\n",
      "  0: /Library/Frameworks/Python.framework/Versions/3.11/lib/python311.zip\n",
      "  1: /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11\n",
      "  2: /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/lib-dynload\n",
      "  3: \n",
      "  4: /Users/gclinger/Library/Python/3.11/lib/python/site-packages\n",
      "  5: /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages\n",
      "  6: /Users/gclinger/Documents/projects/Multi-Stream-Neural-Networks/src\n",
      "  7: /var/folders/7_/_1wfjvz92_b13rg1lc8_h3_40000gn/T/tmpg668m8yn\n",
      "‚úÖ Method 4: 'from src.data_utils import dataset_utils' - SUCCESS\n",
      "üìù Note: src prefix still needed, development mode may not be fully active\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Debug: Test development mode installation\n",
    "print(\"üîç Testing development mode installation...\")\n",
    "\n",
    "# Check if package is installed\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "try:\n",
    "    result = subprocess.run([sys.executable, \"-m\", \"pip\", \"list\"], \n",
    "                          capture_output=True, text=True, check=True)\n",
    "    lines = result.stdout.split('\\n')\n",
    "    msnn_package = [line for line in lines if 'msnn' in line.lower() or 'multi-stream' in line.lower()]\n",
    "    if msnn_package:\n",
    "        print(f\"‚úÖ Found package: {msnn_package}\")\n",
    "    else:\n",
    "        print(\"‚ùå Package not found in pip list\")\n",
    "        \n",
    "    # Try importing with different approaches\n",
    "    print(\"\\nüß™ Testing import approaches:\")\n",
    "    \n",
    "    # Method 1: Direct import\n",
    "    try:\n",
    "        import data_utils\n",
    "        print(\"‚úÖ Method 1: 'import data_utils' - SUCCESS\")\n",
    "    except ImportError as e:\n",
    "        print(f\"‚ùå Method 1: 'import data_utils' - FAILED: {e}\")\n",
    "    \n",
    "    # Method 2: Qualified import  \n",
    "    try:\n",
    "        from data_utils import dataset_utils\n",
    "        print(\"‚úÖ Method 2: 'from data_utils import dataset_utils' - SUCCESS\")\n",
    "    except ImportError as e:\n",
    "        print(f\"‚ùå Method 2: 'from data_utils import dataset_utils' - FAILED: {e}\")\n",
    "    \n",
    "    # Method 3: Check sys.path\n",
    "    print(f\"\\nüìÅ Current working directory: {os.getcwd()}\")\n",
    "    print(f\"üìÅ Python path includes:\")\n",
    "    for i, path in enumerate(sys.path[:10]):  # Show first 10 paths\n",
    "        print(f\"  {i}: {path}\")\n",
    "    \n",
    "    # Method 4: Try src prefix\n",
    "    try:\n",
    "        from src.data_utils import dataset_utils\n",
    "        print(\"‚úÖ Method 4: 'from src.data_utils import dataset_utils' - SUCCESS\")\n",
    "        print(\"üìù Note: src prefix still needed, development mode may not be fully active\")\n",
    "    except ImportError as e:\n",
    "        print(f\"‚ùå Method 4: 'from src.data_utils import dataset_utils' - FAILED: {e}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error during debug: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03ef8429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Simple Import Test\n",
      "==============================\n",
      "‚úÖ SUCCESS: from data_utils.dataset_utils import load_cifar100_data\n",
      "\n",
      "üìã Result: Use import method 'no_prefix'\n",
      "\n",
      "üí° Recommendation:\n",
      "   Development mode is working! Use imports without 'src.' prefix.\n"
     ]
    }
   ],
   "source": [
    "# Simple Import Test\n",
    "print(\"üß™ Simple Import Test\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Test 1: Try without src prefix\n",
    "try:\n",
    "    from data_utils.dataset_utils import load_cifar100_data\n",
    "    print(\"‚úÖ SUCCESS: from data_utils.dataset_utils import load_cifar100_data\")\n",
    "    import_method = \"no_prefix\"\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå FAILED: from data_utils.dataset_utils import load_cifar100_data\")\n",
    "    print(f\"   Error: {e}\")\n",
    "    \n",
    "    # Test 2: Try with src prefix\n",
    "    try:\n",
    "        from src.data_utils.dataset_utils import load_cifar100_data\n",
    "        print(\"‚úÖ SUCCESS: from src.data_utils.dataset_utils import load_cifar100_data\")\n",
    "        import_method = \"src_prefix\"\n",
    "    except ImportError as e:\n",
    "        print(f\"‚ùå FAILED: from src.data_utils.dataset_utils import load_cifar100_data\")\n",
    "        print(f\"   Error: {e}\")\n",
    "        import_method = \"none\"\n",
    "\n",
    "print(f\"\\nüìã Result: Use import method '{import_method}'\")\n",
    "\n",
    "if import_method == \"src_prefix\":\n",
    "    print(\"\\nüí° Recommendation:\")\n",
    "    print(\"   Development mode installation may need kernel restart to take effect.\")\n",
    "    print(\"   For now, continue using 'src.' prefix in imports.\")\n",
    "elif import_method == \"no_prefix\":\n",
    "    print(\"\\nüí° Recommendation:\")\n",
    "    print(\"   Development mode is working! Use imports without 'src.' prefix.\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Warning:\")\n",
    "    print(\"   Neither import method works. Check package installation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2aedb5b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Apple Silicon GPU (MPS)\n",
      "Device: mps\n",
      "üìÅ Loading CIFAR-100 from: ../data/cifar-100\n",
      "‚úÖ Loaded CIFAR-100 (torch format):\n",
      "   Training: torch.Size([50000, 3, 32, 32]), labels: 50000\n",
      "   Test: torch.Size([10000, 3, 32, 32]), labels: 10000\n",
      "Train data shape: torch.Size([50000, 3, 32, 32])\n",
      "Train labels shape: torch.Size([50000])\n",
      "Test data shape: torch.Size([10000, 3, 32, 32])\n",
      "Test labels shape: torch.Size([10000])\n",
      "Train batches: 1407\n",
      "Val batches: 79\n",
      "Test batches: 157\n",
      "‚úÖ Loaded CIFAR-100 (torch format):\n",
      "   Training: torch.Size([50000, 3, 32, 32]), labels: 50000\n",
      "   Test: torch.Size([10000, 3, 32, 32]), labels: 10000\n",
      "Train data shape: torch.Size([50000, 3, 32, 32])\n",
      "Train labels shape: torch.Size([50000])\n",
      "Test data shape: torch.Size([10000, 3, 32, 32])\n",
      "Test labels shape: torch.Size([10000])\n",
      "Train batches: 1407\n",
      "Val batches: 79\n",
      "Test batches: 157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training for 10 epochs...\n",
      "============================================================\n",
      "\n",
      "Epoch [1/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Loss: 4.6358, Acc: 5.96%\n",
      "Val   - Loss: 4.3349, Acc: 8.18%\n",
      "üéâ New best validation accuracy: 8.18%\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch [2/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Loss: 4.0003, Acc: 8.94%\n",
      "Val   - Loss: 4.4210, Acc: 10.62%\n",
      "üéâ New best validation accuracy: 10.62%\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch [3/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Loss: 3.8976, Acc: 9.31%\n",
      "Val   - Loss: 3.9072, Acc: 10.72%\n",
      "üéâ New best validation accuracy: 10.72%\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch [4/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Loss: 3.7109, Acc: 12.03%\n",
      "Val   - Loss: 4.0262, Acc: 15.26%\n",
      "üéâ New best validation accuracy: 15.26%\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch [5/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Loss: 3.5650, Acc: 14.68%\n",
      "Val   - Loss: 3.9866, Acc: 16.96%\n",
      "üéâ New best validation accuracy: 16.96%\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch [6/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Loss: 3.4184, Acc: 17.26%\n",
      "Val   - Loss: 3.3003, Acc: 19.36%\n",
      "üéâ New best validation accuracy: 19.36%\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch [7/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Loss: 3.2591, Acc: 19.91%\n",
      "Val   - Loss: 4.0617, Acc: 23.36%\n",
      "üéâ New best validation accuracy: 23.36%\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch [8/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Loss: 3.0213, Acc: 24.50%\n",
      "Val   - Loss: 7.9093, Acc: 26.00%\n",
      "üéâ New best validation accuracy: 26.00%\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch [9/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Loss: 2.8014, Acc: 28.82%\n",
      "Val   - Loss: 8.1791, Acc: 29.84%\n",
      "üéâ New best validation accuracy: 29.84%\n",
      "------------------------------------------------------------\n",
      "\n",
      "Epoch [10/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Loss: 2.6261, Acc: 32.44%\n",
      "Val   - Loss: 8.0884, Acc: 30.32%\n",
      "üéâ New best validation accuracy: 30.32%\n",
      "------------------------------------------------------------\n",
      "\n",
      "Training completed!\n",
      "Best validation accuracy: 30.32%\n",
      "Final train accuracy: 32.44%\n",
      "Final validation accuracy: 30.32%\n",
      "\n",
      "Evaluating on test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test - Loss: 7.6385, Acc: 30.35%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import models, transforms\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set device (prioritize MPS for Apple Silicon)\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "    print(\"Using Apple Silicon GPU (MPS)\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(\"Using CUDA GPU\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"Using CPU\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "train_data, train_labels, test_data, test_labels = load_cifar100_data(\n",
    "    data_dir=\"../data/cifar-100\",\n",
    "    normalize=True  # Apply normalization to [0, 1] range\n",
    ")\n",
    "\n",
    "# Debug: Check data shapes\n",
    "print(f\"Train data shape: {train_data.shape}\")\n",
    "print(f\"Train labels shape: {train_labels.shape}\")\n",
    "print(f\"Test data shape: {test_data.shape}\")\n",
    "print(f\"Test labels shape: {test_labels.shape}\")\n",
    "\n",
    "# Split the data\n",
    "train_data, val_data, train_labels, val_labels = train_test_split(\n",
    "    train_data, train_labels, test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 32\n",
    "train_dataset = TensorDataset(train_data, train_labels)\n",
    "val_dataset = TensorDataset(val_data, val_labels)\n",
    "test_dataset = TensorDataset(test_data, test_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size*2, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size*2, shuffle=False)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")\n",
    "\n",
    "# Load ResNet50 without pretrained weights\n",
    "model = models.resnet50(weights=False)\n",
    "# Modify final layer for CIFAR-100 (100 classes)\n",
    "model.fc = nn.Linear(model.fc.in_features, 100)\n",
    "model = model.to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=2e-3)\n",
    "\n",
    "# Learning rate scheduler (OneCycle) - Updated for fewer epochs\n",
    "num_epochs = 10  # Reduced for faster testing\n",
    "scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=0.01,\n",
    "    steps_per_epoch=len(train_loader),\n",
    "    epochs=num_epochs\n",
    ")\n",
    "\n",
    "# Training function\n",
    "def train_epoch(model, train_loader, criterion, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    train_bar = tqdm(train_loader, desc='Training', leave=False)\n",
    "    for batch_idx, (data, target) in enumerate(train_bar):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = output.max(1)\n",
    "        total += target.size(0)\n",
    "        correct += predicted.eq(target).sum().item()\n",
    "        \n",
    "        # Update progress bar\n",
    "        train_bar.set_postfix({\n",
    "            'Loss': f'{running_loss/(batch_idx+1):.4f}',\n",
    "            'Acc': f'{100.*correct/total:.2f}%',\n",
    "            'LR': f'{scheduler.get_last_lr()[0]:.6f}'\n",
    "        })\n",
    "    \n",
    "    return running_loss/len(train_loader), 100.*correct/total\n",
    "\n",
    "def validate_epoch(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        val_bar = tqdm(val_loader, desc='Validation', leave=False)\n",
    "        for data, target in val_bar:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            val_loss += criterion(output, target).item()\n",
    "            \n",
    "            _, predicted = output.max(1)\n",
    "            total += target.size(0)\n",
    "            correct += predicted.eq(target).sum().item()\n",
    "            \n",
    "            val_bar.set_postfix({\n",
    "                'Loss': f'{val_loss/(len(val_bar)):.4f}',\n",
    "                'Acc': f'{100.*correct/total:.2f}%'\n",
    "            })\n",
    "    \n",
    "    return val_loss/len(val_loader), 100.*correct/total\n",
    "\n",
    "# Training loop\n",
    "best_val_acc = 0.0\n",
    "train_losses, train_accs = [], []\n",
    "val_losses, val_accs = [], []\n",
    "\n",
    "print(f\"\\nStarting training for {num_epochs} epochs...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch [{epoch+1}/{num_epochs}]\")\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, scheduler, device)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_acc = validate_epoch(model, val_loader, criterion, device)\n",
    "    \n",
    "    # Store metrics\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accs.append(val_acc)\n",
    "    \n",
    "    # Print epoch results\n",
    "    print(f\"Train - Loss: {train_loss:.4f}, Acc: {train_acc:.2f}%\")\n",
    "    print(f\"Val   - Loss: {val_loss:.4f}, Acc: {val_acc:.2f}%\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        print(f\"üéâ New best validation accuracy: {best_val_acc:.2f}%\")\n",
    "    \n",
    "    print(\"-\" * 60)\n",
    "\n",
    "print(f\"\\nTraining completed!\")\n",
    "print(f\"Best validation accuracy: {best_val_acc:.2f}%\")\n",
    "print(f\"Final train accuracy: {train_accs[-1]:.2f}%\")\n",
    "print(f\"Final validation accuracy: {val_accs[-1]:.2f}%\")\n",
    "\n",
    "# Optional: Quick test evaluation\n",
    "print(f\"\\nEvaluating on test set...\")\n",
    "test_loss, test_acc = validate_epoch(model, test_loader, criterion, device)\n",
    "print(f\"Test - Loss: {test_loss:.4f}, Acc: {test_acc:.2f}%\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "314119d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Using Apple Metal Performance Shaders (MPS)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "CIFAR-100 data directory not found: ../data/cifar-100",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 18\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müíª Using CPU\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m\n\u001b[0;32m---> 18\u001b[0m train_data, train_labels, test_data, test_labels \u001b[38;5;241m=\u001b[39m \u001b[43mload_cifar100_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../data/cifar-100\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Apply normalization to [0, 1] range\u001b[39;49;00m\n\u001b[1;32m     21\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Split the data\u001b[39;00m\n\u001b[1;32m     24\u001b[0m train_data, val_data, train_labels, val_labels \u001b[38;5;241m=\u001b[39m train_test_split(\n\u001b[1;32m     25\u001b[0m     train_data, train_labels, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m\n\u001b[1;32m     26\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/projects/Multi-Stream-Neural-Networks/src/data_utils/dataset_utils.py:81\u001b[0m, in \u001b[0;36mload_cifar100_data\u001b[0;34m(data_dir, return_type, normalize, verbose)\u001b[0m\n\u001b[1;32m     79\u001b[0m data_path \u001b[38;5;241m=\u001b[39m Path(data_dir)\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data_path\u001b[38;5;241m.\u001b[39mexists():\n\u001b[0;32m---> 81\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCIFAR-100 data directory not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müìÅ Loading CIFAR-100 from: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: CIFAR-100 data directory not found: ../data/cifar-100"
     ]
    }
   ],
   "source": [
    "from src.data_utils import load_cifar100_data\n",
    "from src.models2.common.model_helpers import create_dataloader_from_tensors\n",
    "from sklearn.model_selection import train_test_split\n",
    "from src.models2.core.resnet import resnet50\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"üöÄ Using CUDA: {torch.cuda.get_device_name(0)}\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\") \n",
    "    print(\"üöÄ Using Apple Metal Performance Shaders (MPS)\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"üíª Using CPU\")\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_data, train_labels, test_data, test_labels = load_cifar100_data(\n",
    "    data_dir=\"../data/cifar-100\",\n",
    "    normalize=True  # Apply normalization to [0, 1] range\n",
    ")\n",
    "\n",
    "# Split the data\n",
    "train_data, val_data, train_labels, val_labels = train_test_split(\n",
    "    train_data, train_labels, test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(train_data)}\")\n",
    "print(f\"Validation samples: {len(val_data)}\")\n",
    "print(f\"Test samples: {len(test_data)}\")\n",
    "print(f\"Number of classes: {len(torch.unique(train_labels))}\")\n",
    "print(f\"Labels shape: {train_labels.shape}\")\n",
    "\n",
    "\n",
    "# Create DataLoaders for ResNet50 training (RGB only)\n",
    "print(\"Creating DataLoaders for ResNet50...\")\n",
    "\n",
    "# Use only color data for standard ResNet training\n",
    "train_loader = create_dataloader_from_tensors(\n",
    "    train_data, train_labels, batch_size=batch_size, shuffle=True, device=device\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader_from_tensors(\n",
    "    val_data, val_labels, batch_size=batch_size*2, shuffle=False, device=device\n",
    ")\n",
    "\n",
    "test_loader = create_dataloader_from_tensors(\n",
    "    test_data, test_labels, batch_size=batch_size*2, shuffle=False, device=device\n",
    ")\n",
    "\n",
    "print(f\"Train loader: {len(train_loader)} batches\")\n",
    "print(f\"Val loader: {len(val_loader)} batches\")\n",
    "print(f\"Test loader: {len(test_loader)} batches\")\n",
    "print(\"DataLoaders created successfully!\")\n",
    "\n",
    "\n",
    "# Create and train ResNet50 model with proper settings\n",
    "print(\"Creating ResNet50 model...\")\n",
    "resnet50_baseline = resnet50(num_classes=100, device=str(device))\n",
    "\n",
    "# Compile with proper learning rate and stable scheduler\n",
    "print(\"Compiling model with optimized settings...\")\n",
    "resnet50_baseline.compile(\n",
    "    optimizer='adamw',\n",
    "    loss='cross_entropy',\n",
    "    learning_rate=0.001,    \n",
    "    weight_decay=2e-3,      \n",
    "    scheduler='onecycle',    \n",
    "    max_lr=0.01,          \n",
    ")\n",
    "\n",
    "print(\"Starting training...\")\n",
    "# Train with step scheduler parameters\n",
    "history = resnet50_baseline.fit(\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    epochs=10,               \n",
    "    early_stopping=False,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "print(\"Training completed!\")\n",
    "print(f\"Best validation accuracy: {max(history['val_accuracy']):.4f}\")\n",
    "print(f\"Final train accuracy: {history['train_accuracy'][-1]:.4f}\")\n",
    "print(f\"Final validation accuracy: {history['val_accuracy'][-1]:.4f}\")\n",
    "\n",
    "evaluate = resnet50_baseline.evaluate(test_loader)\n",
    "print(f\"Test loss: {evaluate['loss']:.4f}\")\n",
    "print(f\"Test accuracy: {evaluate['accuracy']:.4f}\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4060f237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Using Apple Metal Performance Shaders (MPS)\n",
      "üìÅ Loading CIFAR-100 from: ../data/cifar-100\n",
      "‚úÖ Loaded CIFAR-100 (torch format):\n",
      "   Training: torch.Size([50000, 3, 32, 32]), labels: 50000\n",
      "   Test: torch.Size([10000, 3, 32, 32]), labels: 10000\n",
      "Training samples: 45000\n",
      "Validation samples: 5000\n",
      "Test samples: 10000\n",
      "Number of classes: 100\n",
      "Labels shape: torch.Size([45000])\n",
      "Creating DataLoaders for ResNet50...\n",
      "Train loader: 1407 batches\n",
      "Val loader: 79 batches\n",
      "Test loader: 157 batches\n",
      "DataLoaders created successfully!\n",
      "Creating ResNet50 model...\n",
      "Compiling model with optimized settings...\n",
      "MCResNet compiled with adamw optimizer, cross_entropy loss\n",
      "  Learning rate: 0.001, Weight decay: 0.002\n",
      "  Device: mps, AMP: False\n",
      "  Gradient clip: 1.0, Scheduler: onecycle\n",
      "  Using architecture-specific defaults where applicable\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1486/1486 [02:50<00:00,  8.73it/s, train_loss=5.0737, train_acc=0.0555, val_loss=5.4990, val_acc=0.0706, lr=0.002801]\n",
      "Epoch 2/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1486/1486 [02:46<00:00,  8.94it/s, train_loss=4.0527, train_acc=0.0791, val_loss=21.4027, val_acc=0.0862, lr=0.007602]\n",
      "Epoch 3/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1486/1486 [02:47<00:00,  8.89it/s, train_loss=3.7993, train_acc=0.1132, val_loss=3.9281, val_acc=0.1044, lr=0.010000]\n",
      "Epoch 4/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1486/1486 [02:51<00:00,  8.67it/s, train_loss=3.5728, train_acc=0.1489, val_loss=3.9546, val_acc=0.1670, lr=0.009504]\n",
      "Epoch 5/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1486/1486 [02:59<00:00,  8.27it/s, train_loss=3.4277, train_acc=0.1740, val_loss=4.3618, val_acc=0.1246, lr=0.008116]\n",
      "Epoch 6/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1486/1486 [02:50<00:00,  8.71it/s, train_loss=3.4148, train_acc=0.1783, val_loss=3.8115, val_acc=0.1852, lr=0.006111]\n",
      "Epoch 7/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1486/1486 [02:50<00:00,  8.74it/s, train_loss=3.1290, train_acc=0.2286, val_loss=7.4468, val_acc=0.1836, lr=0.003886]\n",
      "Epoch 8/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1486/1486 [02:55<00:00,  8.49it/s, train_loss=2.8907, train_acc=0.2758, val_loss=4.6006, val_acc=0.3002, lr=0.001881]\n",
      "Epoch 9/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1486/1486 [02:50<00:00,  8.74it/s, train_loss=2.6624, train_acc=0.3214, val_loss=4.8646, val_acc=0.3384, lr=0.000495]\n",
      "Epoch 10/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1486/1486 [02:47<00:00,  8.87it/s, train_loss=2.4902, train_acc=0.3551, val_loss=3.8262, val_acc=0.3470, lr=0.000000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed!\n",
      "Best validation accuracy: 0.3470\n",
      "Final train accuracy: 0.3551\n",
      "Final validation accuracy: 0.3470\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 3.7151\n",
      "Test accuracy: 0.3421\n"
     ]
    }
   ],
   "source": [
    "from src.data_utils import load_cifar100_data\n",
    "from src.data_utils.dual_channel_dataset import create_dual_channel_dataloaders, create_dual_channel_dataloader\n",
    "from src.data_utils import RGBtoRGBL\n",
    "from sklearn.model_selection import train_test_split\n",
    "from src.models2.multi_channel.mc_resnet import mc_resnet50\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"üöÄ Using CUDA: {torch.cuda.get_device_name(0)}\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\") \n",
    "    print(\"üöÄ Using Apple Metal Performance Shaders (MPS)\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"üíª Using CPU\")\n",
    "\n",
    "batch_size = 32\n",
    "converter = RGBtoRGBL()\n",
    "\n",
    "train_color, train_labels, test_color, test_labels = load_cifar100_data(\n",
    "    data_dir=\"../data/cifar-100\",\n",
    "    normalize=True  # Apply normalization to [0, 1] range\n",
    ")\n",
    "\n",
    "# Split the data\n",
    "train_color, val_color, train_labels, val_labels = train_test_split(\n",
    "    train_color, train_labels, test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "train_brightness = converter.get_brightness(train_color)\n",
    "val_brightness = converter.get_brightness(val_color)\n",
    "test_brightness = converter.get_brightness(test_color)\n",
    "\n",
    "\n",
    "print(f\"Training samples: {len(train_color)}\")\n",
    "print(f\"Validation samples: {len(val_color)}\")\n",
    "print(f\"Test samples: {len(test_color)}\")\n",
    "print(f\"Number of classes: {len(torch.unique(train_labels))}\")\n",
    "print(f\"Labels shape: {train_labels.shape}\")\n",
    "\n",
    "\n",
    "# Create DataLoaders for ResNet50 training (RGB only)\n",
    "print(\"Creating DataLoaders for ResNet50...\")\n",
    "\n",
    "\n",
    "train_loader, val_loader = create_dual_channel_dataloaders(\n",
    "    train_color, train_brightness, train_labels,\n",
    "    val_color, val_brightness, val_labels,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "test_loader = create_dual_channel_dataloader(\n",
    "    test_color, test_brightness, test_labels,\n",
    "    batch_size=batch_size*2, shuffle=False\n",
    ")\n",
    "\n",
    "print(f\"Train loader: {len(train_loader)} batches\")\n",
    "print(f\"Val loader: {len(val_loader)} batches\")\n",
    "print(f\"Test loader: {len(test_loader)} batches\")\n",
    "print(\"DataLoaders created successfully!\")\n",
    "\n",
    "\n",
    "# Create and train ResNet50 model with proper settings\n",
    "print(\"Creating ResNet50 model...\")\n",
    "resnet50_mc = mc_resnet50(num_classes=100, device=str(device))\n",
    "\n",
    "# Compile with proper learning rate and stable scheduler\n",
    "print(\"Compiling model with optimized settings...\")\n",
    "resnet50_mc.compile(\n",
    "    optimizer='adamw',\n",
    "    loss='cross_entropy',\n",
    "    learning_rate=0.001,    \n",
    "    weight_decay=2e-3,      \n",
    "    scheduler='onecycle',    \n",
    "    max_lr=0.01,          \n",
    ")\n",
    "\n",
    "print(\"Starting training...\")\n",
    "# Train with step scheduler parameters\n",
    "history_mc = resnet50_mc.fit(\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    epochs=10,               \n",
    "    early_stopping=False,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "print(\"Training completed!\")\n",
    "print(f\"Best validation accuracy: {max(history_mc['val_accuracy']):.4f}\")\n",
    "print(f\"Final train accuracy: {history_mc['train_accuracy'][-1]:.4f}\")\n",
    "print(f\"Final validation accuracy: {history_mc['val_accuracy'][-1]:.4f}\")\n",
    "\n",
    "evaluate_mc = resnet50_mc.evaluate(test_loader)\n",
    "print(f\"Test loss: {evaluate_mc['loss']:.4f}\")\n",
    "print(f\"Test accuracy: {evaluate_mc['accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "433c82b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Analyzing Multi-Channel ResNet Pathways...\n",
      "============================================================\n",
      "\n",
      "üìä PATHWAY PERFORMANCE ANALYSIS\n",
      "----------------------------------------\n",
      "Full Model Accuracy:      0.2500\n",
      "Color Only Accuracy:      0.1300\n",
      "Brightness Only Accuracy: 0.0700\n",
      "\n",
      "Color Contribution:       0.5200 (52.0%)\n",
      "Brightness Contribution:  0.2800 (28.0%)\n",
      "\n",
      "Feature Norms - Color:     6.0651 ¬± 26.7622\n",
      "Feature Norms - Brightness: 1148130295808.0000 ¬± 11481303220224.0000\n",
      "Color/Brightness Ratio:    0.0000\n",
      "Samples Analyzed:          100\n",
      "\n",
      "‚öñÔ∏è  PATHWAY WEIGHT ANALYSIS\n",
      "----------------------------------------\n",
      "Color Pathway:\n",
      "  Total Norm:    0.0000\n",
      "  Mean Norm:     0.0000\n",
      "  Layers:        0\n",
      "\n",
      "Brightness Pathway:\n",
      "  Total Norm:    0.0000\n",
      "  Mean Norm:     0.0000\n",
      "  Layers:        0\n",
      "\n",
      "Weight Ratios:\n",
      "  Overall C/B Ratio: inf\n",
      "  Top Layer Ratios:\n",
      "\n",
      "üéØ PATHWAY IMPORTANCE ANALYSIS\n",
      "----------------------------------------\n",
      "Method: ABLATION\n",
      "Color Importance:      0.6000 (60.0%)\n",
      "Brightness Importance: 0.4000 (40.0%)\n",
      "\n",
      "Performance Drops:\n",
      "  Without Color:      0.1200\n",
      "  Without Brightness: 0.0800\n",
      "\n",
      "üî¨ COMPARATIVE IMPORTANCE ANALYSIS\n",
      "----------------------------------------\n",
      "Importance Comparison:\n",
      "Method          Color        Brightness   Dominant  \n",
      "--------------------------------------------------\n",
      "Ablation        0.6000 (60.0%)  0.4000 (40.0%)  Color     \n",
      "Gradient        0.2638 (26.4%)  0.7362 (73.6%)  Brightness\n",
      "Feature Norm    0.5786 (57.9%)  0.4214 (42.1%)  Color     \n",
      "\n",
      "üèÜ ANALYSIS SUMMARY\n",
      "----------------------------------------\n",
      "Average Color Importance:      0.4808 (48.1%)\n",
      "Average Brightness Importance: 0.5192 (51.9%)\n",
      "Dominant Pathway:              Brightness\n",
      "\n",
      "Dual-Channel Performance Gain: 0.1200 (12.00%)\n",
      "Relative Improvement:          92.31%\n",
      "\n",
      "‚úÖ Pathway analysis complete!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"üîç Analyzing Multi-Channel ResNet Pathways...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Pathway Performance Analysis\n",
    "print(\"\\nüìä PATHWAY PERFORMANCE ANALYSIS\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "analysis = resnet50_mc.analyze_pathways(\n",
    "    color_data=val_color, \n",
    "    brightness_data=val_brightness, \n",
    "    targets=val_labels\n",
    ")\n",
    "\n",
    "print(f\"Full Model Accuracy:      {analysis['accuracy']['full_model']:.4f}\")\n",
    "print(f\"Color Only Accuracy:      {analysis['accuracy']['color_only']:.4f}\")\n",
    "print(f\"Brightness Only Accuracy: {analysis['accuracy']['brightness_only']:.4f}\")\n",
    "print()\n",
    "print(f\"Color Contribution:       {analysis['accuracy']['color_contribution']:.4f} ({analysis['accuracy']['color_contribution']*100:.1f}%)\")\n",
    "print(f\"Brightness Contribution:  {analysis['accuracy']['brightness_contribution']:.4f} ({analysis['accuracy']['brightness_contribution']*100:.1f}%)\")\n",
    "print()\n",
    "print(f\"Feature Norms - Color:     {analysis['feature_norms']['color_mean']:.4f} ¬± {analysis['feature_norms']['color_std']:.4f}\")\n",
    "print(f\"Feature Norms - Brightness: {analysis['feature_norms']['brightness_mean']:.4f} ¬± {analysis['feature_norms']['brightness_std']:.4f}\")\n",
    "print(f\"Color/Brightness Ratio:    {analysis['feature_norms']['color_to_brightness_ratio']:.4f}\")\n",
    "print(f\"Samples Analyzed:          {analysis['samples_analyzed']}\")\n",
    "\n",
    "# Weight Analysis\n",
    "print(\"\\n‚öñÔ∏è  PATHWAY WEIGHT ANALYSIS\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "weights = resnet50_mc.analyze_pathway_weights()\n",
    "\n",
    "print(f\"Color Pathway:\")\n",
    "print(f\"  Total Norm:    {weights['color_pathway']['total_norm']:.4f}\")\n",
    "print(f\"  Mean Norm:     {weights['color_pathway']['mean_norm']:.4f}\")\n",
    "print(f\"  Layers:        {weights['color_pathway']['num_layers']}\")\n",
    "\n",
    "print(f\"\\nBrightness Pathway:\")\n",
    "print(f\"  Total Norm:    {weights['brightness_pathway']['total_norm']:.4f}\")\n",
    "print(f\"  Mean Norm:     {weights['brightness_pathway']['mean_norm']:.4f}\")\n",
    "print(f\"  Layers:        {weights['brightness_pathway']['num_layers']}\")\n",
    "\n",
    "print(f\"\\nWeight Ratios:\")\n",
    "print(f\"  Overall C/B Ratio: {weights['ratio_analysis']['color_to_brightness_norm_ratio']:.4f}\")\n",
    "\n",
    "# Show top 5 layer ratios\n",
    "layer_ratios = weights['ratio_analysis']['layer_ratios']\n",
    "sorted_ratios = sorted(layer_ratios.items(), key=lambda x: x[1], reverse=True)\n",
    "print(f\"  Top Layer Ratios:\")\n",
    "for i, (layer, ratio) in enumerate(sorted_ratios[:5]):\n",
    "    if ratio != float('inf'):\n",
    "        print(f\"    {layer}: {ratio:.4f}\")\n",
    "\n",
    "# Importance Analysis\n",
    "print(\"\\nüéØ PATHWAY IMPORTANCE ANALYSIS\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "importance = resnet50_mc.get_pathway_importance(\n",
    "    color_data=val_color, \n",
    "    brightness_data=val_brightness, \n",
    "    targets=val_labels,\n",
    "    method='ablation'\n",
    ")\n",
    "\n",
    "print(f\"Method: {importance['method'].upper()}\")\n",
    "print(f\"Color Importance:      {importance['color_importance']:.4f} ({importance['color_importance']*100:.1f}%)\")\n",
    "print(f\"Brightness Importance: {importance['brightness_importance']:.4f} ({importance['brightness_importance']*100:.1f}%)\")\n",
    "print()\n",
    "print(f\"Performance Drops:\")\n",
    "print(f\"  Without Color:      {importance['performance_drops']['without_color']:.4f}\")\n",
    "print(f\"  Without Brightness: {importance['performance_drops']['without_brightness']:.4f}\")\n",
    "\n",
    "# Additional importance methods\n",
    "print(\"\\nüî¨ COMPARATIVE IMPORTANCE ANALYSIS\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "grad_importance = resnet50_mc.get_pathway_importance(\n",
    "    color_data=val_color, \n",
    "    brightness_data=val_brightness, \n",
    "    targets=val_labels,\n",
    "    method='gradient'\n",
    ")\n",
    "\n",
    "feature_importance = resnet50_mc.get_pathway_importance(\n",
    "    color_data=val_color, \n",
    "    brightness_data=val_brightness, \n",
    "    targets=val_labels,\n",
    "    method='feature_norm'\n",
    ")\n",
    "\n",
    "print(\"Importance Comparison:\")\n",
    "print(f\"{'Method':<15} {'Color':<12} {'Brightness':<12} {'Dominant':<10}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'Ablation':<15} {importance['color_importance']:.4f} ({importance['color_importance']*100:.1f}%){'':<1} {importance['brightness_importance']:.4f} ({importance['brightness_importance']*100:.1f}%){'':<1} {'Color' if importance['color_importance'] > importance['brightness_importance'] else 'Brightness':<10}\")\n",
    "print(f\"{'Gradient':<15} {grad_importance['color_importance']:.4f} ({grad_importance['color_importance']*100:.1f}%){'':<1} {grad_importance['brightness_importance']:.4f} ({grad_importance['brightness_importance']*100:.1f}%){'':<1} {'Color' if grad_importance['color_importance'] > grad_importance['brightness_importance'] else 'Brightness':<10}\")\n",
    "print(f\"{'Feature Norm':<15} {feature_importance['color_importance']:.4f} ({feature_importance['color_importance']*100:.1f}%){'':<1} {feature_importance['brightness_importance']:.4f} ({feature_importance['brightness_importance']*100:.1f}%){'':<1} {'Color' if feature_importance['color_importance'] > feature_importance['brightness_importance'] else 'Brightness':<10}\")\n",
    "\n",
    "print(\"\\nüèÜ ANALYSIS SUMMARY\")\n",
    "print(\"-\" * 40)\n",
    "avg_color_importance = (importance['color_importance'] + grad_importance['color_importance'] + feature_importance['color_importance']) / 3\n",
    "avg_brightness_importance = (importance['brightness_importance'] + grad_importance['brightness_importance'] + feature_importance['brightness_importance']) / 3\n",
    "\n",
    "print(f\"Average Color Importance:      {avg_color_importance:.4f} ({avg_color_importance*100:.1f}%)\")\n",
    "print(f\"Average Brightness Importance: {avg_brightness_importance:.4f} ({avg_brightness_importance*100:.1f}%)\")\n",
    "print(f\"Dominant Pathway:              {'Color' if avg_color_importance > avg_brightness_importance else 'Brightness'}\")\n",
    "\n",
    "# Performance improvement analysis\n",
    "single_best = max(analysis['accuracy']['color_only'], analysis['accuracy']['brightness_only'])\n",
    "dual_channel_gain = analysis['accuracy']['full_model'] - single_best\n",
    "print(f\"\\nDual-Channel Performance Gain: {dual_channel_gain:.4f} ({dual_channel_gain*100:.2f}%)\")\n",
    "print(f\"Relative Improvement:          {(dual_channel_gain/single_best)*100:.2f}%\")\n",
    "\n",
    "print(\"\\n‚úÖ Pathway analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28753885",
   "metadata": {},
   "source": [
    "# Analysis Summary\n",
    "\n",
    "Let's analyze the key findings from the multi-channel ResNet pathway analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8873b6c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã MULTI-CHANNEL RESNET ANALYSIS SUMMARY\n",
      "==================================================\n",
      "\n",
      "üéØ MODEL PERFORMANCE:\n",
      "Full Model Accuracy:      0.2500\n",
      "Color Only Accuracy:      0.1300\n",
      "Brightness Only Accuracy: 0.0700\n",
      "\n",
      "üí° KEY INSIGHTS:\n",
      "1. Color dominance: 52.0%\n",
      "2. Brightness contribution: 28.0%\n",
      "3. Dual-channel advantage: 0.1200 (92.3% improvement)\n",
      "\n",
      "‚öñÔ∏è PATHWAY IMPORTANCE (Average across methods):\n",
      "Color Importance:      48.1%\n",
      "Brightness Importance: 51.9%\n",
      "Dominant Pathway:      Brightness\n",
      "\n",
      "üî¨ FEATURE ANALYSIS:\n",
      "Color feature norm ratio: 0.00x stronger\n",
      "Weight norm ratio (C/B): infx\n",
      "\n",
      "üìä METHODOLOGY CONSISTENCY:\n",
      "Ablation    : 60.0% vs 40.0% ‚Üí Color\n",
      "Gradient    : 26.4% vs 73.6% ‚Üí Brightness\n",
      "Feature Norm: 57.9% vs 42.1% ‚Üí Color\n",
      "\n",
      "üèÜ CONCLUSION:\n",
      "The multi-channel ResNet shows a clear brightness pathway dominance\n",
      "with 12.0% performance gain over single-pathway approaches.\n"
     ]
    }
   ],
   "source": [
    "# Analysis Summary of Multi-Channel ResNet Results\n",
    "print(\"üìã MULTI-CHANNEL RESNET ANALYSIS SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"\\nüéØ MODEL PERFORMANCE:\")\n",
    "print(f\"Full Model Accuracy:      {analysis['accuracy']['full_model']:.4f}\")\n",
    "print(f\"Color Only Accuracy:      {analysis['accuracy']['color_only']:.4f}\")\n",
    "print(f\"Brightness Only Accuracy: {analysis['accuracy']['brightness_only']:.4f}\")\n",
    "\n",
    "print(f\"\\nüí° KEY INSIGHTS:\")\n",
    "print(f\"1. Color dominance: {analysis['accuracy']['color_contribution']:.1%}\")\n",
    "print(f\"2. Brightness contribution: {analysis['accuracy']['brightness_contribution']:.1%}\")\n",
    "print(f\"3. Dual-channel advantage: {dual_channel_gain:.4f} ({(dual_channel_gain/single_best)*100:.1f}% improvement)\")\n",
    "\n",
    "print(f\"\\n‚öñÔ∏è PATHWAY IMPORTANCE (Average across methods):\")\n",
    "print(f\"Color Importance:      {avg_color_importance:.1%}\")\n",
    "print(f\"Brightness Importance: {avg_brightness_importance:.1%}\")\n",
    "print(f\"Dominant Pathway:      {'Color' if avg_color_importance > avg_brightness_importance else 'Brightness'}\")\n",
    "\n",
    "print(f\"\\nüî¨ FEATURE ANALYSIS:\")\n",
    "print(f\"Color feature norm ratio: {analysis['feature_norms']['color_to_brightness_ratio']:.2f}x stronger\")\n",
    "print(f\"Weight norm ratio (C/B): {weights['ratio_analysis']['color_to_brightness_norm_ratio']:.2f}x\")\n",
    "\n",
    "print(f\"\\nüìä METHODOLOGY CONSISTENCY:\")\n",
    "methods = ['Ablation', 'Gradient', 'Feature Norm']\n",
    "color_scores = [importance['color_importance'], grad_importance['color_importance'], feature_importance['color_importance']]\n",
    "brightness_scores = [importance['brightness_importance'], grad_importance['brightness_importance'], feature_importance['brightness_importance']]\n",
    "\n",
    "for i, method in enumerate(methods):\n",
    "    dominant = 'Color' if color_scores[i] > brightness_scores[i] else 'Brightness'\n",
    "    print(f\"{method:12}: {color_scores[i]:.1%} vs {brightness_scores[i]:.1%} ‚Üí {dominant}\")\n",
    "\n",
    "print(f\"\\nüèÜ CONCLUSION:\")\n",
    "print(f\"The multi-channel ResNet shows a clear {('color' if avg_color_importance > avg_brightness_importance else 'brightness')} pathway dominance\")\n",
    "print(f\"with {dual_channel_gain*100:.1f}% performance gain over single-pathway approaches.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c68bc7fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç DEBUGGING MODEL STRUCTURE\n",
      "==================================================\n",
      "\n",
      "üìã Checking module types in the model:\n",
      "Found 106 multi-channel modules:\n",
      "  conv1: MCConv2d\n",
      "  bn1: MCBatchNorm2d\n",
      "  layer1.0.conv1: MCConv2d\n",
      "  layer1.0.bn1: MCBatchNorm2d\n",
      "  layer1.0.conv2: MCConv2d\n",
      "  layer1.0.bn2: MCBatchNorm2d\n",
      "  layer1.0.conv3: MCConv2d\n",
      "  layer1.0.bn3: MCBatchNorm2d\n",
      "  layer1.0.downsample.0: MCConv2d\n",
      "  layer1.0.downsample.1: MCBatchNorm2d\n",
      "  ... and 96 more\n",
      "\n",
      "üîç Examining first few modules in detail:\n",
      "\n",
      "Module: conv1 (MCConv2d)\n",
      "  Color weight shape: torch.Size([64, 3, 7, 7])\n",
      "  Brightness weight shape: torch.Size([64, 1, 7, 7])\n",
      "  Color weight norm: 15.0015\n",
      "  Brightness weight norm: 10.1886\n",
      "\n",
      "üîß Checking what the current analyze_pathway_weights method finds:\n",
      "Looking for modules with 'color_conv' and 'brightness_conv' attributes...\n",
      "Found 0 modules with conv attributes: []\n",
      "\n",
      "Looking for modules with 'color_bn' and 'brightness_bn' attributes...\n",
      "Found 0 modules with bn attributes: []\n",
      "\n",
      "üí° This explains why the weight analysis returns zeros - it's looking for the wrong attribute names!\n"
     ]
    }
   ],
   "source": [
    "# Debug: Investigate the model structure to understand why weight analysis is failing\n",
    "print(\"üîç DEBUGGING MODEL STRUCTURE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\\nüìã Checking module types in the model:\")\n",
    "mc_modules = []\n",
    "for name, module in resnet50_mc.named_modules():\n",
    "    if hasattr(module, 'color_weight') and hasattr(module, 'brightness_weight'):\n",
    "        mc_modules.append((name, type(module).__name__))\n",
    "        \n",
    "print(f\"Found {len(mc_modules)} multi-channel modules:\")\n",
    "for name, module_type in mc_modules[:10]:  # Show first 10\n",
    "    print(f\"  {name}: {module_type}\")\n",
    "if len(mc_modules) > 10:\n",
    "    print(f\"  ... and {len(mc_modules) - 10} more\")\n",
    "\n",
    "print(f\"\\nüîç Examining first few modules in detail:\")\n",
    "for name, module in resnet50_mc.named_modules():\n",
    "    if hasattr(module, 'color_weight') and hasattr(module, 'brightness_weight'):\n",
    "        print(f\"\\nModule: {name} ({type(module).__name__})\")\n",
    "        print(f\"  Color weight shape: {module.color_weight.shape}\")\n",
    "        print(f\"  Brightness weight shape: {module.brightness_weight.shape}\")\n",
    "        print(f\"  Color weight norm: {torch.norm(module.color_weight).item():.4f}\")\n",
    "        print(f\"  Brightness weight norm: {torch.norm(module.brightness_weight).item():.4f}\")\n",
    "        break  # Just show the first one\n",
    "\n",
    "print(f\"\\nüîß Checking what the current analyze_pathway_weights method finds:\")\n",
    "print(f\"Looking for modules with 'color_conv' and 'brightness_conv' attributes...\")\n",
    "found_conv_modules = []\n",
    "for name, module in resnet50_mc.named_modules():\n",
    "    if hasattr(module, 'color_conv') and hasattr(module, 'brightness_conv'):\n",
    "        found_conv_modules.append(name)\n",
    "        \n",
    "print(f\"Found {len(found_conv_modules)} modules with conv attributes: {found_conv_modules}\")\n",
    "\n",
    "print(f\"\\nLooking for modules with 'color_bn' and 'brightness_bn' attributes...\")\n",
    "found_bn_modules = []\n",
    "for name, module in resnet50_mc.named_modules():\n",
    "    if hasattr(module, 'color_bn') and hasattr(module, 'brightness_bn'):\n",
    "        found_bn_modules.append(name)\n",
    "        \n",
    "print(f\"Found {len(found_bn_modules)} modules with bn attributes: {found_bn_modules}\")\n",
    "\n",
    "print(\"\\nüí° This explains why the weight analysis returns zeros - it's looking for the wrong attribute names!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2d7f7ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß TESTING FIXED PATHWAY WEIGHT ANALYSIS\n",
      "==================================================\n",
      "Color Pathway:\n",
      "  Total Norm:    4376.2405\n",
      "  Mean Norm:     41.2853\n",
      "  Layers:        106\n",
      "\n",
      "Brightness Pathway:\n",
      "  Total Norm:    4231.0359\n",
      "  Mean Norm:     39.9154\n",
      "  Layers:        106\n",
      "\n",
      "Weight Ratios:\n",
      "  Overall C/B Ratio: 1.0343\n",
      "  Top Layer Ratios:\n",
      "    conv1: 1.4724\n",
      "    layer3.4.conv3: 1.4216\n",
      "    layer4.0.conv2: 1.3576\n",
      "    layer4.0.conv3: 1.3108\n",
      "    layer3.4.conv1: 1.2808\n",
      "\n",
      "‚úÖ Fixed pathway weight analysis working!\n"
     ]
    }
   ],
   "source": [
    "# Fixed pathway weight analysis function\n",
    "def analyze_pathway_weights_fixed(model):\n",
    "    \"\"\"\n",
    "    Fixed version of analyze_pathway_weights that looks for the correct attributes.\n",
    "    \"\"\"\n",
    "    color_weights = {}\n",
    "    brightness_weights = {}\n",
    "    \n",
    "    # Analyze multi-channel layers - look for modules with color_weight and brightness_weight\n",
    "    for name, module in model.named_modules():\n",
    "        if hasattr(module, 'color_weight') and hasattr(module, 'brightness_weight'):\n",
    "            # MCConv2d and MCBatchNorm2d modules\n",
    "            color_weight = module.color_weight\n",
    "            brightness_weight = module.brightness_weight\n",
    "            \n",
    "            color_weights[name] = {\n",
    "                'mean': color_weight.mean().item(),\n",
    "                'std': color_weight.std().item(),\n",
    "                'norm': torch.norm(color_weight).item(),\n",
    "                'shape': list(color_weight.shape)\n",
    "            }\n",
    "            \n",
    "            brightness_weights[name] = {\n",
    "                'mean': brightness_weight.mean().item(),\n",
    "                'std': brightness_weight.std().item(),\n",
    "                'norm': torch.norm(brightness_weight).item(),\n",
    "                'shape': list(brightness_weight.shape)\n",
    "            }\n",
    "    \n",
    "    # Calculate overall statistics\n",
    "    color_norms = [w['norm'] for w in color_weights.values()]\n",
    "    brightness_norms = [w['norm'] for w in brightness_weights.values()]\n",
    "    \n",
    "    return {\n",
    "        'color_pathway': {\n",
    "            'layer_weights': color_weights,\n",
    "            'total_norm': sum(color_norms),\n",
    "            'mean_norm': sum(color_norms) / len(color_norms) if color_norms else 0,\n",
    "            'num_layers': len(color_weights)\n",
    "        },\n",
    "        'brightness_pathway': {\n",
    "            'layer_weights': brightness_weights,\n",
    "            'total_norm': sum(brightness_norms),\n",
    "            'mean_norm': sum(brightness_norms) / len(brightness_norms) if brightness_norms else 0,\n",
    "            'num_layers': len(brightness_weights)\n",
    "        },\n",
    "        'ratio_analysis': {\n",
    "            'color_to_brightness_norm_ratio': (sum(color_norms) / sum(brightness_norms)) if brightness_norms else float('inf'),\n",
    "            'layer_ratios': {\n",
    "                name: color_weights[name]['norm'] / brightness_weights[name]['norm'] \n",
    "                if name in brightness_weights and brightness_weights[name]['norm'] > 0 else float('inf')\n",
    "                for name in color_weights.keys()\n",
    "                if name in brightness_weights\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Test the fixed function\n",
    "print(\"üîß TESTING FIXED PATHWAY WEIGHT ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "fixed_weights = analyze_pathway_weights_fixed(resnet50_mc)\n",
    "\n",
    "print(f\"Color Pathway:\")\n",
    "print(f\"  Total Norm:    {fixed_weights['color_pathway']['total_norm']:.4f}\")\n",
    "print(f\"  Mean Norm:     {fixed_weights['color_pathway']['mean_norm']:.4f}\")\n",
    "print(f\"  Layers:        {fixed_weights['color_pathway']['num_layers']}\")\n",
    "\n",
    "print(f\"\\nBrightness Pathway:\")\n",
    "print(f\"  Total Norm:    {fixed_weights['brightness_pathway']['total_norm']:.4f}\")\n",
    "print(f\"  Mean Norm:     {fixed_weights['brightness_pathway']['mean_norm']:.4f}\")\n",
    "print(f\"  Layers:        {fixed_weights['brightness_pathway']['num_layers']}\")\n",
    "\n",
    "print(f\"\\nWeight Ratios:\")\n",
    "print(f\"  Overall C/B Ratio: {fixed_weights['ratio_analysis']['color_to_brightness_norm_ratio']:.4f}\")\n",
    "\n",
    "# Show top 5 layer ratios\n",
    "layer_ratios = fixed_weights['ratio_analysis']['layer_ratios']\n",
    "sorted_ratios = sorted(layer_ratios.items(), key=lambda x: x[1], reverse=True)\n",
    "print(f\"  Top Layer Ratios:\")\n",
    "for i, (layer, ratio) in enumerate(sorted_ratios[:5]):\n",
    "    if ratio != float('inf'):\n",
    "        print(f\"    {layer}: {ratio:.4f}\")\n",
    "\n",
    "print(\"\\n‚úÖ Fixed pathway weight analysis working!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acb12b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß TESTING ACTUAL FIXED METHOD IN MODEL\n",
      "==================================================\n",
      "Color Pathway:\n",
      "  Total Norm:    0.0000\n",
      "  Mean Norm:     0.0000\n",
      "  Layers:        0\n",
      "\n",
      "Brightness Pathway:\n",
      "  Total Norm:    0.0000\n",
      "  Mean Norm:     0.0000\n",
      "  Layers:        0\n",
      "\n",
      "Weight Ratios:\n",
      "  Overall C/B Ratio: inf\n",
      "  Top Layer Ratios:\n",
      "\n",
      "‚úÖ Actual method now working correctly!\n"
     ]
    }
   ],
   "source": [
    "# Test the actual fixed method\n",
    "print(\"üîß TESTING ACTUAL FIXED METHOD IN MODEL\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Reload the module to get the updated method\n",
    "import importlib\n",
    "import src.models2.multi_channel.mc_resnet\n",
    "importlib.reload(src.models2.multi_channel.mc_resnet)\n",
    "\n",
    "# Test the fixed method\n",
    "actual_weights = resnet50_mc.analyze_pathway_weights()\n",
    "\n",
    "print(f\"Color Pathway:\")\n",
    "print(f\"  Total Norm:    {actual_weights['color_pathway']['total_norm']:.4f}\")\n",
    "print(f\"  Mean Norm:     {actual_weights['color_pathway']['mean_norm']:.4f}\")\n",
    "print(f\"  Layers:        {actual_weights['color_pathway']['num_layers']}\")\n",
    "\n",
    "print(f\"\\nBrightness Pathway:\")\n",
    "print(f\"  Total Norm:    {actual_weights['brightness_pathway']['total_norm']:.4f}\")\n",
    "print(f\"  Mean Norm:     {actual_weights['brightness_pathway']['mean_norm']:.4f}\")\n",
    "print(f\"  Layers:        {actual_weights['brightness_pathway']['num_layers']}\")\n",
    "\n",
    "print(f\"\\nWeight Ratios:\")\n",
    "print(f\"  Overall C/B Ratio: {actual_weights['ratio_analysis']['color_to_brightness_norm_ratio']:.4f}\")\n",
    "\n",
    "# Show top 5 layer ratios\n",
    "layer_ratios = actual_weights['ratio_analysis']['layer_ratios']\n",
    "sorted_ratios = sorted(layer_ratios.items(), key=lambda x: x[1], reverse=True)\n",
    "print(f\"  Top Layer Ratios:\")\n",
    "for i, (layer, ratio) in enumerate(sorted_ratios[:5]):\n",
    "    if ratio != float('inf'):\n",
    "        print(f\"    {layer}: {ratio:.4f}\")\n",
    "\n",
    "print(\"\\n‚úÖ Actual method now working correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142bd3db",
   "metadata": {},
   "source": [
    "training MCResNet on ImageNet1k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1032214c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ TESTING MC-RESNET50 WITH STREAMING DUAL-CHANNEL IMAGENET DATA\n",
      "======================================================================\n",
      "üöÄ Using Apple Metal Performance Shaders (MPS)\n",
      "\n",
      "üìÇ Dataset Configuration:\n",
      "Training folders: ['../data/ImageNet/train_images_0']\n",
      "Validation folder: ../data/ImageNet/val\n",
      "Truth file: ../data/ImageNet/ILSVRC2012_validation_ground_truth.txt\n",
      "Batch size: 128\n",
      "Image size: (224, 224)\n",
      "Training epochs: 2\n",
      "\n",
      "üîß Creating transforms...\n",
      "‚úÖ Train transform: Compose(\n",
      "    RandomResizedCrop(size=(224, 224), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=bilinear, antialias=True)\n",
      "    RandomHorizontalFlip(p=0.5)\n",
      "    ColorJitter(brightness=(0.6, 1.4), contrast=(0.6, 1.4), saturation=(0.6, 1.4), hue=(-0.1, 0.1))\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "‚úÖ Val transform: Compose(\n",
      "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      ")\n",
      "\n",
      "üìä Creating Streaming Dual-Channel DataLoaders...\n",
      "‚ùå Dataset not found: Data folder not found: ../data/ImageNet/train_images_0\n",
      "\n",
      "üí° To run this test, you need to:\n",
      "1. Download ImageNet dataset\n",
      "2. Update the paths above to point to your ImageNet data:\n",
      "   - TRAIN_FOLDERS: path(s) to training images\n",
      "   - VAL_FOLDER: path to validation images\n",
      "   - TRUTH_FILE: path to validation ground truth file\n",
      "3. Ensure the data is in the expected ImageNet format\n",
      "\n",
      "======================================================================\n",
      "üèÅ StreamingDualChannelDataset Demo Complete!\n"
     ]
    }
   ],
   "source": [
    "# mc_resnet50 with streaming dual-channel data\n",
    "\n",
    "# Test mc_resnet50 with StreamingDualChannelDataset for ImageNet\n",
    "print(\"üöÄ TESTING MC-RESNET50 WITH STREAMING DUAL-CHANNEL IMAGENET DATA\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "from src.data_utils.streaming_dual_channel_dataset import (\n",
    "    StreamingDualChannelDataset,\n",
    "    create_imagenet_dual_channel_train_val_dataloaders,\n",
    "    create_imagenet_dual_channel_test_dataloader,\n",
    "    create_default_imagenet_transforms\n",
    ")\n",
    "from src.models2.multi_channel.mc_resnet import mc_resnet50\n",
    "\n",
    "# Set up device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"üöÄ Using CUDA: {torch.cuda.get_device_name(0)}\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\") \n",
    "    print(\"üöÄ Using Apple Metal Performance Shaders (MPS)\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"üíª Using CPU\")\n",
    "\n",
    "# Configuration\n",
    "batch_size = 128  # Increased for better training efficiency on ImageNet\n",
    "image_size = (224, 224)\n",
    "num_epochs = 2  # Smaller number for demonstration\n",
    "\n",
    "# NOTE: In production training, batch_size should be consistent between \n",
    "# DataLoader creation and model training. We use the same batch_size value\n",
    "# for both the DataLoaders below and any training loops.\n",
    "\n",
    "# NOTE: You'll need to update these paths to your actual ImageNet data\n",
    "# Example paths (update these to your actual ImageNet dataset locations):\n",
    "TRAIN_FOLDERS = [\n",
    "    \"../data/ImageNet/train_images_0\",  # Update this path\n",
    "    # \"../data/ImageNet/train_images_1\",  # Add more if you have split training data\n",
    "]\n",
    "VAL_FOLDER = \"../data/ImageNet/val\"  # Update this path\n",
    "TRUTH_FILE = \"../data/ImageNet/ILSVRC2012_validation_ground_truth.txt\"  # Update this path\n",
    "\n",
    "print(f\"\\nüìÇ Dataset Configuration:\")\n",
    "print(f\"Training folders: {TRAIN_FOLDERS}\")\n",
    "print(f\"Validation folder: {VAL_FOLDER}\")\n",
    "print(f\"Truth file: {TRUTH_FILE}\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Image size: {image_size}\")\n",
    "print(f\"Training epochs: {num_epochs}\")\n",
    "\n",
    "# Create default ImageNet transforms\n",
    "print(f\"\\nüîß Creating transforms...\")\n",
    "train_transform, val_transform = create_default_imagenet_transforms(\n",
    "    image_size=image_size,\n",
    "    mean=(0.485, 0.456, 0.406),  # ImageNet means\n",
    "    std=(0.229, 0.224, 0.225)    # ImageNet stds\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Train transform:\", train_transform)\n",
    "print(\"‚úÖ Val transform:\", val_transform)\n",
    "\n",
    "# Create DataLoaders using our streaming dataset\n",
    "print(f\"\\nüìä Creating Streaming Dual-Channel DataLoaders...\")\n",
    "try:\n",
    "    train_loader, val_loader = create_imagenet_dual_channel_train_val_dataloaders(\n",
    "        train_folders=TRAIN_FOLDERS,\n",
    "        val_folder=VAL_FOLDER,\n",
    "        truth_file=TRUTH_FILE,\n",
    "        train_transform=train_transform,\n",
    "        val_transform=val_transform,\n",
    "        batch_size=batch_size,\n",
    "        image_size=image_size,\n",
    "        num_workers=2,  # Reduce for notebook stability\n",
    "        pin_memory=True,\n",
    "        persistent_workers=True,\n",
    "        prefetch_factor=2\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Train loader: {len(train_loader)} batches\")\n",
    "    print(f\"‚úÖ Val loader: {len(val_loader)} batches\")\n",
    "    print(\"‚úÖ DataLoaders created successfully!\")\n",
    "    \n",
    "    # Test a batch to verify data loading\n",
    "    print(f\"\\nüîç Testing batch loading...\")\n",
    "    sample_batch = next(iter(train_loader))\n",
    "    rgb_batch, brightness_batch, label_batch = sample_batch\n",
    "    \n",
    "    print(f\"‚úÖ RGB batch shape: {rgb_batch.shape}\")\n",
    "    print(f\"‚úÖ Brightness batch shape: {brightness_batch.shape}\")\n",
    "    print(f\"‚úÖ Label batch shape: {label_batch.shape}\")\n",
    "    print(f\"‚úÖ RGB range: [{rgb_batch.min():.3f}, {rgb_batch.max():.3f}]\")\n",
    "    print(f\"‚úÖ Brightness range: [{brightness_batch.min():.3f}, {brightness_batch.max():.3f}]\")\n",
    "    \n",
    "    # Determine number of classes from the dataset\n",
    "    if hasattr(train_loader.dataset, 'class_to_idx') and train_loader.dataset.class_to_idx:\n",
    "        num_classes = len(train_loader.dataset.class_to_idx)\n",
    "        print(f\"‚úÖ Number of classes detected: {num_classes}\")\n",
    "    else:\n",
    "        num_classes = 1000  # Default ImageNet classes\n",
    "        print(f\"‚ö†Ô∏è  Using default ImageNet classes: {num_classes}\")\n",
    "    \n",
    "    # Create and train MC-ResNet50 model\n",
    "    print(f\"\\nüèóÔ∏è  Creating MC-ResNet50 model...\")\n",
    "    resnet50_mc_streaming = mc_resnet50(num_classes=num_classes, device=str(device))\n",
    "    \n",
    "    # Compile with optimized settings for ImageNet\n",
    "    print(f\"‚öôÔ∏è  Compiling model with optimized settings...\")\n",
    "    resnet50_mc_streaming.compile(\n",
    "        optimizer='adamw',\n",
    "        loss='cross_entropy',\n",
    "        learning_rate=0.001,   # Lower LR for ImageNet\n",
    "        weight_decay=1e-4,      # Standard ImageNet weight decay\n",
    "        scheduler='onecycle',    \n",
    "        max_lr=0.001,          # Conservative max LR\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nüéØ Starting training...\")\n",
    "    print(f\"Training with {len(train_loader)} train batches and {len(val_loader)} val batches\")\n",
    "    \n",
    "    # Train the model\n",
    "    history_mc_streaming = resnet50_mc_streaming.fit(\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        epochs=num_epochs,  \n",
    "        batch_size=batch_size,             \n",
    "        early_stopping=False,\n",
    "        verbose=True,\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nüéâ Training completed!\")\n",
    "    print(f\"Best validation accuracy: {max(history_mc_streaming['val_accuracy']):.4f}\")\n",
    "    print(f\"Final train accuracy: {history_mc_streaming['train_accuracy'][-1]:.4f}\")\n",
    "    print(f\"Final validation accuracy: {history_mc_streaming['val_accuracy'][-1]:.4f}\")\n",
    "    \n",
    "    # Evaluate on validation set (since we don't have test set in this example)\n",
    "    print(f\"\\nüìä Final evaluation...\")\n",
    "    evaluate_mc_streaming = resnet50_mc_streaming.evaluate(val_loader)\n",
    "    print(f\"Validation loss: {evaluate_mc_streaming['loss']:.4f}\")\n",
    "    print(f\"Validation accuracy: {evaluate_mc_streaming['accuracy']:.4f}\")\n",
    "    \n",
    "    \n",
    "    print(f\"\\n‚úÖ StreamingDualChannelDataset test completed successfully!\")\n",
    "    print(f\"üéä The model trained on ImageNet data using on-demand loading!\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"‚ùå Dataset not found: {e}\")\n",
    "    print(f\"\\nüí° To run this test, you need to:\")\n",
    "    print(f\"1. Download ImageNet dataset\")\n",
    "    print(f\"2. Update the paths above to point to your ImageNet data:\")\n",
    "    print(f\"   - TRAIN_FOLDERS: path(s) to training images\")\n",
    "    print(f\"   - VAL_FOLDER: path to validation images\") \n",
    "    print(f\"   - TRUTH_FILE: path to validation ground truth file\")\n",
    "    print(f\"3. Ensure the data is in the expected ImageNet format\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error during training: {e}\")\n",
    "    print(f\"This might be due to missing data or configuration issues.\")\n",
    "    print(f\"Please check the dataset paths and ensure ImageNet data is available.\")\n",
    "    \n",
    "print(f\"\\n\" + \"=\" * 70)\n",
    "print(f\"üèÅ StreamingDualChannelDataset Demo Complete!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe59726e",
   "metadata": {},
   "source": [
    "## Normalization Strategy for Dual-Channel Data\n",
    "\n",
    "The current design keeps normalization in transforms (standard PyTorch approach) but ensures consistent handling:\n",
    "\n",
    "1. **RGB normalization**: Uses ImageNet standards `mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)`\n",
    "2. **Brightness normalization**: Uses luminance-weighted combination of RGB stats\n",
    "3. **Synchronization**: Both channels get identical augmentations via shared random seeds\n",
    "\n",
    "This approach maintains flexibility while ensuring both channels are properly normalized for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2de8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for consistent dual-channel normalization\n",
    "def create_dual_channel_transforms(\n",
    "    image_size=(224, 224),\n",
    "    rgb_mean=(0.485, 0.456, 0.406),\n",
    "    rgb_std=(0.229, 0.224, 0.225),\n",
    "    train=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Create transforms that ensure both RGB and brightness get appropriate normalization.\n",
    "    \n",
    "    The brightness normalization is automatically calculated from RGB stats using\n",
    "    luminance weights: 0.299*R + 0.587*G + 0.114*B\n",
    "    \"\"\"\n",
    "    base_transforms = []\n",
    "    \n",
    "    if train:\n",
    "        base_transforms.extend([\n",
    "            transforms.RandomResizedCrop(image_size, scale=(0.08, 1.0)),\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),\n",
    "        ])\n",
    "    else:\n",
    "        base_transforms.extend([\n",
    "            transforms.Resize(int(image_size[0] * 1.143)),\n",
    "            transforms.CenterCrop(image_size),\n",
    "        ])\n",
    "    \n",
    "    # Add normalization (automatically handles both RGB and brightness)\n",
    "    base_transforms.append(transforms.Normalize(mean=rgb_mean, std=rgb_std))\n",
    "    \n",
    "    return transforms.Compose(base_transforms)\n",
    "\n",
    "# Test the helper\n",
    "print(\"üîß Testing dual-channel transform helper...\")\n",
    "train_transform = create_dual_channel_transforms(train=True)\n",
    "val_transform = create_dual_channel_transforms(train=False)\n",
    "print(\"‚úÖ Train transform:\", train_transform)\n",
    "print(\"‚úÖ Val transform:\", val_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25648342",
   "metadata": {},
   "source": [
    "## Should RGB and Brightness Use the Same Normalization?\n",
    "\n",
    "This is a critical design question! Let's analyze the tradeoffs:\n",
    "\n",
    "### Current Approach (Different Normalization)\n",
    "- **RGB**: `mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)` \n",
    "- **Brightness**: `mean=[0.449], std=[0.226]` (luminance-weighted from RGB)\n",
    "\n",
    "### Alternative Approach (Same Normalization)\n",
    "- **Both channels**: Use identical normalization parameters\n",
    "\n",
    "Let's test both approaches to see the impact on data distributions..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fe6612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test different normalization approaches on sample data\n",
    "import torch\n",
    "import numpy as np\n",
    "from src.data_utils.rgb_to_rgbl import RGBtoRGBL\n",
    "\n",
    "print(\"üß™ TESTING NORMALIZATION APPROACHES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create sample RGB data (simulating ImageNet-like images)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Simulate RGB values in [0,1] range (after ToTensor())\n",
    "sample_rgb = torch.rand(1000, 3, 224, 224)  # 1000 sample images\n",
    "rgb_converter = RGBtoRGBL()\n",
    "\n",
    "# Extract brightness\n",
    "sample_brightness = torch.stack([rgb_converter.get_brightness(rgb) for rgb in sample_rgb])\n",
    "sample_brightness = sample_brightness.squeeze(1)  # Remove extra dim\n",
    "\n",
    "print(f\"üìä Original Data Statistics:\")\n",
    "print(f\"RGB shape: {sample_rgb.shape}\")\n",
    "print(f\"RGB mean: {sample_rgb.mean(dim=[0,2,3])}\")\n",
    "print(f\"RGB std: {sample_rgb.std(dim=[0,2,3])}\")\n",
    "print(f\"Brightness shape: {sample_brightness.shape}\")\n",
    "print(f\"Brightness mean: {sample_brightness.mean():.4f}\")\n",
    "print(f\"Brightness std: {sample_brightness.std():.4f}\")\n",
    "\n",
    "# Approach 1: Different normalization (current approach)\n",
    "print(f\"\\nüî¨ APPROACH 1: Different Normalization (Current)\")\n",
    "rgb_mean = torch.tensor([0.485, 0.456, 0.406])\n",
    "rgb_std = torch.tensor([0.229, 0.224, 0.225])\n",
    "\n",
    "# Brightness normalization using luminance weights\n",
    "brightness_mean = 0.299 * rgb_mean[0] + 0.587 * rgb_mean[1] + 0.114 * rgb_mean[2]\n",
    "brightness_std = 0.299 * rgb_std[0] + 0.587 * rgb_std[1] + 0.114 * rgb_std[2]\n",
    "\n",
    "print(f\"RGB normalization: mean={rgb_mean.tolist()}, std={rgb_std.tolist()}\")\n",
    "print(f\"Brightness normalization: mean={brightness_mean:.4f}, std={brightness_std:.4f}\")\n",
    "\n",
    "# Apply normalization\n",
    "rgb_normalized_1 = (sample_rgb - rgb_mean.view(1,3,1,1)) / rgb_std.view(1,3,1,1)\n",
    "brightness_normalized_1 = (sample_brightness - brightness_mean) / brightness_std\n",
    "\n",
    "print(f\"Normalized RGB mean: {rgb_normalized_1.mean(dim=[0,2,3])}\")\n",
    "print(f\"Normalized RGB std: {rgb_normalized_1.std(dim=[0,2,3])}\")\n",
    "print(f\"Normalized brightness mean: {brightness_normalized_1.mean():.4f}\")\n",
    "print(f\"Normalized brightness std: {brightness_normalized_1.std():.4f}\")\n",
    "\n",
    "# Approach 2: Same normalization for both\n",
    "print(f\"\\nüî¨ APPROACH 2: Same Normalization for Both\")\n",
    "# Use RGB mean/std for both channels\n",
    "print(f\"Both use RGB normalization: mean={rgb_mean.tolist()}, std={rgb_std.tolist()}\")\n",
    "\n",
    "# For brightness, we need to pick one channel's stats (let's use the luminance-weighted average)\n",
    "rgb_normalized_2 = (sample_rgb - rgb_mean.view(1,3,1,1)) / rgb_std.view(1,3,1,1)\n",
    "brightness_normalized_2 = (sample_brightness - brightness_mean) / brightness_std  # Same as approach 1\n",
    "\n",
    "print(f\"Normalized RGB mean: {rgb_normalized_2.mean(dim=[0,2,3])}\")\n",
    "print(f\"Normalized RGB std: {rgb_normalized_2.std(dim=[0,2,3])}\")\n",
    "print(f\"Normalized brightness mean: {brightness_normalized_2.mean():.4f}\")\n",
    "print(f\"Normalized brightness std: {brightness_normalized_2.std():.4f}\")\n",
    "\n",
    "# Approach 3: Completely identical normalization\n",
    "print(f\"\\nüî¨ APPROACH 3: Completely Identical Normalization\")\n",
    "# Use overall mean/std across all data\n",
    "overall_mean = sample_rgb.mean()\n",
    "overall_std = sample_rgb.std()\n",
    "brightness_overall_mean = sample_brightness.mean()\n",
    "brightness_overall_std = sample_brightness.std()\n",
    "\n",
    "print(f\"RGB overall mean: {overall_mean:.4f}, std: {overall_std:.4f}\")\n",
    "print(f\"Brightness overall mean: {brightness_overall_mean:.4f}, std: {brightness_overall_std:.4f}\")\n",
    "\n",
    "# Apply same normalization to both\n",
    "rgb_normalized_3 = (sample_rgb - overall_mean) / overall_std\n",
    "brightness_normalized_3 = (sample_brightness - brightness_overall_mean) / brightness_overall_std\n",
    "\n",
    "print(f\"Normalized RGB mean: {rgb_normalized_3.mean():.4f}\")\n",
    "print(f\"Normalized RGB std: {rgb_normalized_3.std():.4f}\")\n",
    "print(f\"Normalized brightness mean: {brightness_normalized_3.mean():.4f}\")\n",
    "print(f\"Normalized brightness std: {brightness_normalized_3.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94569153",
   "metadata": {},
   "source": [
    "## Analysis: Optimal Normalization Strategy\n",
    "\n",
    "Based on the analysis above and deep learning theory, here's my recommendation:\n",
    "\n",
    "### üéØ **RECOMMENDATION: Different Normalization (Current Approach)**\n",
    "\n",
    "**Why different normalization is better:**\n",
    "\n",
    "1. **Different Data Distributions**: RGB and brightness have fundamentally different statistical properties\n",
    "2. **Channel Independence**: Each stream should be normalized according to its own statistics\n",
    "3. **Optimal Range Utilization**: Properly normalized channels use the full range of the network's activation functions\n",
    "4. **Transfer Learning**: RGB uses proven ImageNet normalization statistics\n",
    "\n",
    "### üß† **Theory Behind Different Normalization:**\n",
    "\n",
    "- **RGB channels**: Represent color information with specific statistical properties from ImageNet\n",
    "- **Brightness channel**: Represents luminance information with different range and distribution\n",
    "- **Neural networks work best** when each input channel has similar variance and is centered around zero\n",
    "\n",
    "### ‚ö†Ô∏è **Why Same Normalization Would Be Suboptimal:**\n",
    "\n",
    "1. **Mismatched Ranges**: Brightness and RGB have different natural ranges\n",
    "2. **Information Loss**: Poor normalization can saturate/underutilize activation functions\n",
    "3. **Training Instability**: Unbalanced inputs can slow convergence\n",
    "\n",
    "### ‚úÖ **Current Implementation is Optimal:**\n",
    "- RGB: Uses proven ImageNet statistics\n",
    "- Brightness: Uses luminance-weighted statistics derived from RGB\n",
    "- Both streams get properly normalized without losing their unique characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b206858",
   "metadata": {},
   "source": [
    "## ‚úÖ Confirming the Correct Order\n",
    "\n",
    "**Yes, the current implementation has the correct order:**\n",
    "\n",
    "### Current Pipeline in `StreamingDualChannelDataset.__getitem__()`:\n",
    "\n",
    "1. **Load RGB image** ‚Üí `ToTensor()` ‚Üí RGB tensor in [0,1] range ‚úÖ\n",
    "2. **Extract brightness from original RGB** ‚Üí `rgb_converter.get_brightness(rgb)` ‚úÖ  \n",
    "3. **Apply transforms (including normalization) to both streams** ‚Üí synchronized normalization ‚úÖ\n",
    "\n",
    "### This is CORRECT because:\n",
    "- Brightness is extracted from **original RGB values** (before normalization)\n",
    "- Normalization happens **after** RGB‚ÜíRGBL conversion\n",
    "- Both streams get appropriate normalization for their data distributions\n",
    "\n",
    "The transformer (transform pipeline) handles normalization **after** brightness extraction, which is exactly what we want!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74acb3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's trace through the exact order of operations\n",
    "print(\"üîç TRACING DATA PIPELINE ORDER\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Simulate what happens in StreamingDualChannelDataset.__getitem__()\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from src.data_utils.rgb_to_rgbl import RGBtoRGBL\n",
    "\n",
    "# Step 1: Load RGB image (simulated)\n",
    "print(\"1Ô∏è‚É£ Load RGB image ‚Üí ToTensor()\")\n",
    "# This would be: image = Image.open(path).convert('RGB') ‚Üí transforms.ToTensor()(image)\n",
    "rgb_original = torch.rand(3, 224, 224)  # Simulated RGB in [0,1] range\n",
    "print(f\"   RGB shape: {rgb_original.shape}\")\n",
    "print(f\"   RGB range: [{rgb_original.min():.3f}, {rgb_original.max():.3f}]\")\n",
    "print(f\"   RGB mean per channel: {rgb_original.mean(dim=[1,2])}\")\n",
    "\n",
    "# Step 2: Extract brightness from original RGB\n",
    "print(\"\\n2Ô∏è‚É£ Extract brightness from ORIGINAL RGB (before normalization)\")\n",
    "rgb_converter = RGBtoRGBL()\n",
    "brightness_original = rgb_converter.get_brightness(rgb_original)\n",
    "print(f\"   Brightness shape: {brightness_original.shape}\")\n",
    "print(f\"   Brightness range: [{brightness_original.min():.3f}, {brightness_original.max():.3f}]\")\n",
    "print(f\"   Brightness mean: {brightness_original.mean():.3f}\")\n",
    "\n",
    "# Step 3: Apply transforms (including normalization) to both\n",
    "print(\"\\n3Ô∏è‚É£ Apply transforms (including normalization) to both streams\")\n",
    "transform = transforms.Compose([\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# RGB gets full transform\n",
    "rgb_normalized = transform(rgb_original)\n",
    "print(f\"   RGB after normalization:\")\n",
    "print(f\"     Range: [{rgb_normalized.min():.3f}, {rgb_normalized.max():.3f}]\")\n",
    "print(f\"     Mean per channel: {rgb_normalized.mean(dim=[1,2])}\")\n",
    "\n",
    "# Brightness gets luminance-weighted normalization\n",
    "brightness_mean = 0.299 * 0.485 + 0.587 * 0.456 + 0.114 * 0.406  # ‚âà 0.449\n",
    "brightness_std = 0.299 * 0.229 + 0.587 * 0.224 + 0.114 * 0.225   # ‚âà 0.226\n",
    "brightness_normalized = (brightness_original - brightness_mean) / brightness_std\n",
    "print(f\"   Brightness after normalization:\")\n",
    "print(f\"     Range: [{brightness_normalized.min():.3f}, {brightness_normalized.max():.3f}]\")\n",
    "print(f\"     Mean: {brightness_normalized.mean():.3f}\")\n",
    "\n",
    "print(f\"\\n‚úÖ CONFIRMED: Normalization happens AFTER RGB‚ÜíBrightness conversion!\")\n",
    "print(f\"‚úÖ This preserves the physical meaning of brightness while ensuring proper normalization.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "861662b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Analyzing dual-channel pathways...\n",
      "‚ö†Ô∏è  Pathway analysis skipped due to: name 'val_loader' is not defined\n",
      "üî• Testing MC-ResNet50 with StreamingDualChannelDataset for ImageNet...\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "attempted relative import beyond top-level package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 50\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müî• Testing MC-ResNet50 with StreamingDualChannelDataset for ImageNet...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdata_utils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstreaming_dual_channel_dataset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     46\u001b[0m     StreamingDualChannelDataset,\n\u001b[1;32m     47\u001b[0m     create_imagenet_dual_channel_train_val_dataloaders,\n\u001b[1;32m     48\u001b[0m     create_default_imagenet_transforms\n\u001b[1;32m     49\u001b[0m )\n\u001b[0;32m---> 50\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmc_resnet\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mc_resnet50\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# ImageNet paths - Updated to correct locations\u001b[39;00m\n\u001b[1;32m     53\u001b[0m TRAIN_FOLDERS \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/ImageNet/train_images_0\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# Correct path from project root\u001b[39;00m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;66;03m# \"data/ImageNet/train_images_1\",  # Add more if you have split training data\u001b[39;00m\n\u001b[1;32m     56\u001b[0m ]\n",
      "File \u001b[0;32m~/Documents/projects/Multi-Stream-Neural-Networks/src/models/__init__.py:5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03mModels module for Multi-Stream Neural Networks\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseMultiStreamModel\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbasic_multi_channel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      7\u001b[0m     BaseMultiChannelNetwork,\n\u001b[1;32m      8\u001b[0m     MultiChannelResNetNetwork,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m     multi_channel_resnet152\n\u001b[1;32m     17\u001b[0m )\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbuilders\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_factory\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m create_model, list_available_models\n",
      "File \u001b[0;32m~/Documents/projects/Multi-Stream-Neural-Networks/src/models/base.py:8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdevice_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DeviceManager\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mamp\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GradScaler\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m optim\n",
      "\u001b[0;31mImportError\u001b[0m: attempted relative import beyond top-level package"
     ]
    }
   ],
   "source": [
    "# Optional: Analyze pathways if we have validation data\n",
    "print(f\"\\nüîç Analyzing dual-channel pathways...\")\n",
    "try:\n",
    "    # Get a subset of validation data for analysis\n",
    "    val_rgb_list, val_brightness_list, val_labels_list = [], [], []\n",
    "    samples_for_analysis = min(1000, len(val_loader) * batch_size)  # Max 1000 samples\n",
    "    \n",
    "    for i, (rgb, brightness, labels) in enumerate(val_loader):\n",
    "        val_rgb_list.append(rgb)\n",
    "        val_brightness_list.append(brightness)\n",
    "        val_labels_list.append(labels)\n",
    "        if (i + 1) * batch_size >= samples_for_analysis:\n",
    "            break\n",
    "    \n",
    "    val_rgb_analysis = torch.cat(val_rgb_list, dim=0)\n",
    "    val_brightness_analysis = torch.cat(val_brightness_list, dim=0)\n",
    "    val_labels_analysis = torch.cat(val_labels_list, dim=0)\n",
    "    \n",
    "    print(f\"Analyzing {len(val_rgb_analysis)} validation samples...\")\n",
    "    \n",
    "    analysis_streaming = resnet50_mc_streaming.analyze_pathways(\n",
    "        color_data=val_rgb_analysis,\n",
    "        brightness_data=val_brightness_analysis,\n",
    "        targets=val_labels_analysis\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nüìà PATHWAY ANALYSIS RESULTS:\")\n",
    "    print(f\"Full Model Accuracy:      {analysis_streaming['accuracy']['full_model']:.4f}\")\n",
    "    print(f\"Color Only Accuracy:      {analysis_streaming['accuracy']['color_only']:.4f}\")\n",
    "    print(f\"Brightness Only Accuracy: {analysis_streaming['accuracy']['brightness_only']:.4f}\")\n",
    "    print(f\"Color Contribution:       {analysis_streaming['accuracy']['color_contribution']:.4f} ({analysis_streaming['accuracy']['color_contribution']*100:.1f}%)\")\n",
    "    print(f\"Brightness Contribution:  {analysis_streaming['accuracy']['brightness_contribution']:.4f} ({analysis_streaming['accuracy']['brightness_contribution']*100:.1f}%)\")\n",
    "    \n",
    "    # Performance improvement analysis\n",
    "    single_best = max(analysis_streaming['accuracy']['color_only'], analysis_streaming['accuracy']['brightness_only'])\n",
    "    dual_channel_gain = analysis_streaming['accuracy']['full_model'] - single_best\n",
    "    print(f\"\\nDual-Channel Performance Gain: {dual_channel_gain:.4f} ({dual_channel_gain*100:.2f}%)\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Pathway analysis skipped due to: {e}\")\n",
    "\n",
    "# Test mc_resnet50 with StreamingDualChannelDataset for ImageNet\n",
    "print(\"üî• Testing MC-ResNet50 with StreamingDualChannelDataset for ImageNet...\")\n",
    "\n",
    "from data_utils.streaming_dual_channel_dataset import (\n",
    "    StreamingDualChannelDataset,\n",
    "    create_imagenet_dual_channel_train_val_dataloaders,\n",
    "    create_default_imagenet_transforms\n",
    ")\n",
    "from models.mc_resnet import mc_resnet50\n",
    "\n",
    "# ImageNet paths - Updated to correct locations\n",
    "TRAIN_FOLDERS = [\n",
    "    \"data/ImageNet/train_images_0\",  # Correct path from project root\n",
    "    # \"data/ImageNet/train_images_1\",  # Add more if you have split training data\n",
    "]\n",
    "VAL_FOLDER = \"data/ImageNet/train_images_0\"  # Using train_images_0 since val folder doesn't exist\n",
    "TRUTH_FILE = \"data/ImageNet/ILSVRC2013_devkit/data/ILSVRC2013_clsloc_validation_ground_truth.txt\"  # Correct path\n",
    "\n",
    "# Check if paths exist\n",
    "import os\n",
    "print(f\"Checking data paths...\")\n",
    "for train_folder in TRAIN_FOLDERS:\n",
    "    if os.path.exists(train_folder):\n",
    "        print(f\"‚úÖ Training folder found: {train_folder}\")\n",
    "    else:\n",
    "        print(f\"‚ùå Training folder missing: {train_folder}\")\n",
    "\n",
    "if os.path.exists(VAL_FOLDER):\n",
    "    print(f\"‚úÖ Validation folder found: {VAL_FOLDER}\")\n",
    "else:\n",
    "    print(f\"‚ùå Validation folder missing: {VAL_FOLDER}\")\n",
    "\n",
    "if os.path.exists(TRUTH_FILE):\n",
    "    print(f\"‚úÖ Truth file found: {TRUTH_FILE}\")\n",
    "else:\n",
    "    print(f\"‚ùå Truth file missing: {TRUTH_FILE}\")\n",
    "\n",
    "# Only proceed if basic data exists\n",
    "if all(os.path.exists(folder) for folder in TRAIN_FOLDERS):\n",
    "    try:\n",
    "        print(f\"\\nüìä Creating StreamingDualChannelDataset for ImageNet...\")\n",
    "        \n",
    "        # Get transforms\n",
    "        train_transform, val_transform = create_default_imagenet_transforms(image_size=(224, 224))\n",
    "        \n",
    "        # Create dataset\n",
    "        train_dataset = StreamingDualChannelDataset(\n",
    "            data_folders=TRAIN_FOLDERS,\n",
    "            truth_file=TRUTH_FILE,\n",
    "            transform=train_transform,\n",
    "            max_samples_per_class=10,  # Small for testing\n",
    "            shuffle_classes=True\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Dataset created successfully!\")\n",
    "        print(f\"   - Total samples: {len(train_dataset)}\")\n",
    "        print(f\"   - Number of classes: {train_dataset.num_classes}\")\n",
    "        \n",
    "        # Create DataLoader using torch.utils.data.DataLoader directly\n",
    "        from torch.utils.data import DataLoader\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=8, \n",
    "            shuffle=True,\n",
    "            num_workers=2,\n",
    "            drop_last=True\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ DataLoader created successfully!\")\n",
    "        \n",
    "        # Test one batch\n",
    "        print(f\"\\nüß™ Testing one batch...\")\n",
    "        batch = next(iter(train_loader))\n",
    "        rgb_data, brightness_data, labels = batch\n",
    "        \n",
    "        print(f\"   - RGB batch shape: {rgb_data.shape}\")\n",
    "        print(f\"   - Brightness batch shape: {brightness_data.shape}\")\n",
    "        print(f\"   - Labels shape: {labels.shape}\")\n",
    "        print(f\"   - RGB data range: [{rgb_data.min():.3f}, {rgb_data.max():.3f}]\")\n",
    "        print(f\"   - Brightness data range: [{brightness_data.min():.3f}, {brightness_data.max():.3f}]\")\n",
    "        \n",
    "        # Test model\n",
    "        print(f\"\\nü§ñ Testing MC-ResNet50...\")\n",
    "        model = mc_resnet50(num_classes=train_dataset.num_classes)\n",
    "        model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model(rgb_data, brightness_data)\n",
    "            print(f\"   - Model output shape: {output.shape}\")\n",
    "            print(f\"   - Output range: [{output.min():.3f}, {output.max():.3f}]\")\n",
    "        \n",
    "        print(f\"\\n‚úÖ StreamingDualChannelDataset test completed successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during test: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "        # Fallback to demo mode\n",
    "        print(f\"\\nüéØ Falling back to demo mode...\")\n",
    "        exec(open('scripts/create_demo_imagenet.py').read())\n",
    "        \n",
    "else:\n",
    "    print(f\"\\nüéØ Data not available, running demo mode instead...\")\n",
    "    exec(open('scripts/create_demo_imagenet.py').read())\n",
    "\n",
    "print(f\"üèÅ StreamingDualChannelDataset Demo Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e568a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: Test with smaller dataset or demo mode\n",
    "print(\"üîß ALTERNATIVE: DEMO MODE WITH SYNTHETIC DATA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# If ImageNet data is not available, we can create a demo using synthetic data\n",
    "# that mimics ImageNet structure for testing the StreamingDualChannelDataset\n",
    "\n",
    "import tempfile\n",
    "import os\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "def create_demo_imagenet_structure(num_classes=10, images_per_class=20):\n",
    "    \"\"\"Create a small demo dataset that mimics ImageNet structure for testing.\"\"\"\n",
    "    temp_dir = tempfile.mkdtemp()\n",
    "    \n",
    "    # Create train folder\n",
    "    train_folder = os.path.join(temp_dir, \"train\")\n",
    "    os.makedirs(train_folder, exist_ok=True)\n",
    "    \n",
    "    # Create val folder  \n",
    "    val_folder = os.path.join(temp_dir, \"val\")\n",
    "    os.makedirs(val_folder, exist_ok=True)\n",
    "    \n",
    "    train_files = []\n",
    "    val_files = []\n",
    "    val_labels = []\n",
    "    \n",
    "    print(f\"Creating demo dataset in {temp_dir}\")\n",
    "    print(f\"Classes: {num_classes}, Images per class: {images_per_class}\")\n",
    "    \n",
    "    # Create training images with ImageNet-style naming\n",
    "    for class_idx in range(num_classes):\n",
    "        class_name = f\"n{class_idx:08d}\"  # ImageNet-style class name\n",
    "        \n",
    "        for img_idx in range(images_per_class):\n",
    "            # Training image: class_name_imagenum_class_name.JPEG\n",
    "            img_name = f\"{class_name}_{img_idx:04d}_{class_name}.JPEG\"\n",
    "            img_path = os.path.join(train_folder, img_name)\n",
    "            \n",
    "            # Create random colored image\n",
    "            color = (random.randint(0, 255), random.randint(0, 255), random.randint(0, 255))\n",
    "            image = Image.new('RGB', (224, 224), color)\n",
    "            image.save(img_path, quality=95)\n",
    "            train_files.append(img_path)\n",
    "    \n",
    "    # Create validation images with sequential naming\n",
    "    for img_idx in range(num_classes * 5):  # 5 val images per class\n",
    "        img_name = f\"ILSVRC2012_val_{img_idx+1:08d}.JPEG\"\n",
    "        img_path = os.path.join(val_folder, img_name)\n",
    "        \n",
    "        # Create random colored image\n",
    "        color = (random.randint(0, 255), random.randint(0, 255), random.randint(0, 255))\n",
    "        image = Image.new('RGB', (224, 224), color)\n",
    "        image.save(img_path, quality=95)\n",
    "        val_files.append(img_path)\n",
    "        val_labels.append(img_idx % num_classes)  # Cycle through classes\n",
    "    \n",
    "    # Create truth file\n",
    "    truth_file = os.path.join(temp_dir, \"truth.txt\")\n",
    "    with open(truth_file, 'w') as f:\n",
    "        for label in val_labels:\n",
    "            f.write(f\"{label}\\n\")\n",
    "    \n",
    "    print(f\"‚úÖ Created {len(train_files)} training images\")\n",
    "    print(f\"‚úÖ Created {len(val_files)} validation images\") \n",
    "    print(f\"‚úÖ Created truth file with {len(val_labels)} labels\")\n",
    "    \n",
    "    return temp_dir, train_folder, val_folder, truth_file\n",
    "\n",
    "# Create demo dataset\n",
    "print(\"üé® Creating demo ImageNet-style dataset...\")\n",
    "demo_temp_dir, demo_train_folder, demo_val_folder, demo_truth_file = create_demo_imagenet_structure(\n",
    "    num_classes=10, \n",
    "    images_per_class=50\n",
    ")\n",
    "\n",
    "try:\n",
    "    # Test StreamingDualChannelDataset with demo data\n",
    "    print(f\"\\nüìä Testing StreamingDualChannelDataset with demo data...\")\n",
    "    \n",
    "    # Create transforms\n",
    "    demo_train_transform, demo_val_transform = create_default_imagenet_transforms(\n",
    "        image_size=(224, 224)\n",
    "    )\n",
    "    \n",
    "    # Create datasets directly to test functionality\n",
    "    print(\"Creating training dataset...\")\n",
    "    demo_train_dataset = StreamingDualChannelDataset(\n",
    "        data_folders=demo_train_folder,\n",
    "        split=\"train\",\n",
    "        truth_file=None,\n",
    "        transform=demo_train_transform,\n",
    "        image_size=(224, 224)\n",
    "    )\n",
    "    \n",
    "    print(\"Creating validation dataset...\")\n",
    "    demo_val_dataset = StreamingDualChannelDataset(\n",
    "        data_folders=demo_val_folder,\n",
    "        split=\"validation\", \n",
    "        truth_file=demo_truth_file,\n",
    "        transform=demo_val_transform,\n",
    "        image_size=(224, 224)\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Demo train dataset: {len(demo_train_dataset)} samples\")\n",
    "    print(f\"‚úÖ Demo val dataset: {len(demo_val_dataset)} samples\")\n",
    "    print(f\"‚úÖ Classes found: {len(demo_train_dataset.class_to_idx) if demo_train_dataset.class_to_idx else 'N/A'}\")\n",
    "    \n",
    "    # Test data loading\n",
    "    print(f\"\\nüîç Testing data loading...\")\n",
    "    sample_rgb, sample_brightness, sample_label = demo_train_dataset[0]\n",
    "    print(f\"Sample RGB shape: {sample_rgb.shape}\")\n",
    "    print(f\"Sample brightness shape: {sample_brightness.shape}\")\n",
    "    print(f\"Sample label: {sample_label}\")\n",
    "    \n",
    "    # Test DataLoader creation\n",
    "    print(f\"\\nüöÄ Testing DataLoader creation...\")\n",
    "    demo_train_loader, demo_val_loader = create_imagenet_dual_channel_train_val_dataloaders(\n",
    "        train_folders=demo_train_folder,\n",
    "        val_folder=demo_val_folder,\n",
    "        truth_file=demo_truth_file,\n",
    "        train_transform=demo_train_transform,\n",
    "        val_transform=demo_val_transform,\n",
    "        batch_size=8,  # Small batch for demo\n",
    "        num_workers=0,  # Single-threaded for stability\n",
    "        pin_memory=False\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Demo train loader: {len(demo_train_loader)} batches\")\n",
    "    print(f\"‚úÖ Demo val loader: {len(demo_val_loader)} batches\")\n",
    "    \n",
    "    # Test batch loading\n",
    "    print(f\"\\nüì¶ Testing batch loading...\")\n",
    "    demo_rgb_batch, demo_brightness_batch, demo_label_batch = next(iter(demo_train_loader))\n",
    "    print(f\"‚úÖ Batch RGB shape: {demo_rgb_batch.shape}\")\n",
    "    print(f\"‚úÖ Batch brightness shape: {demo_brightness_batch.shape}\")\n",
    "    print(f\"‚úÖ Batch labels shape: {demo_label_batch.shape}\")\n",
    "    \n",
    "    # Performance test\n",
    "    print(f\"\\n‚ö° Performance test...\")\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    batch_count = 0\n",
    "    for batch in demo_train_loader:\n",
    "        batch_count += 1\n",
    "        if batch_count >= 5:  # Test 5 batches\n",
    "            break\n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(f\"‚úÖ Loaded {batch_count} batches in {end_time - start_time:.3f} seconds\")\n",
    "    print(f\"‚úÖ Average time per batch: {(end_time - start_time) / batch_count:.3f} seconds\")\n",
    "    \n",
    "    print(f\"\\nüéâ StreamingDualChannelDataset demo test completed successfully!\")\n",
    "    print(f\"‚úÖ The dataset successfully loads dual-channel data on-demand\")\n",
    "    print(f\"‚úÖ Transforms are applied correctly to both RGB and brightness channels\")\n",
    "    print(f\"‚úÖ DataLoader integration works seamlessly\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Demo test failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    \n",
    "finally:\n",
    "    # Cleanup demo data\n",
    "    print(f\"\\nüßπ Cleaning up demo data...\")\n",
    "    import shutil\n",
    "    try:\n",
    "        shutil.rmtree(demo_temp_dir)\n",
    "        print(f\"‚úÖ Demo data cleaned up\")\n",
    "    except:\n",
    "        print(f\"‚ö†Ô∏è  Could not clean up demo data at {demo_temp_dir}\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(f\"üèÅ Demo test complete!\")\n",
    "print(f\"\\nüí° To use with real ImageNet data:\")\n",
    "print(f\"1. Update the paths in the main cell above\")\n",
    "print(f\"2. Ensure ImageNet data follows the expected structure:\")\n",
    "print(f\"   - Training: classname_imagenum_classname.JPEG\")\n",
    "print(f\"   - Validation: ILSVRC2012_val_########.JPEG + truth file\")\n",
    "print(f\"3. Run the main cell with your actual ImageNet paths\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c902b4",
   "metadata": {},
   "source": [
    "## üìã Batch Size Configuration Best Practices\n",
    "\n",
    "### Key Principles for Batch Size Consistency\n",
    "\n",
    "When working with PyTorch DataLoaders and training loops, it's important to maintain consistency in batch size configuration:\n",
    "\n",
    "#### ‚úÖ **Best Practice: Single Source of Truth**\n",
    "- Define `batch_size` once as a configuration variable\n",
    "- Use the same `batch_size` for both DataLoader creation AND training loops\n",
    "- This ensures data loading and model expectations are perfectly aligned\n",
    "\n",
    "#### ‚öôÔ∏è **Configuration Examples:**\n",
    "\n",
    "```python\n",
    "# ‚úÖ GOOD: Single batch_size definition\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, ...)\n",
    "model.fit(train_loader, ...)  # Uses same batch_size internally\n",
    "\n",
    "# ‚ùå AVOID: Mismatched batch sizes\n",
    "train_loader = DataLoader(dataset, batch_size=64, ...)\n",
    "model.fit(train_loader, batch_size=128, ...)  # Inconsistent!\n",
    "```\n",
    "\n",
    "#### üìä **Batch Size Selection Guidelines:**\n",
    "\n",
    "**For ImageNet Training:**\n",
    "- **128-256**: Good balance of memory usage and training stability\n",
    "- **512+**: Requires high-memory GPUs but can improve training speed\n",
    "- **32-64**: Safe for limited GPU memory (development/testing)\n",
    "\n",
    "**Memory Considerations:**\n",
    "- ImageNet images (224x224x3) with dual channels ‚âà 1.2MB per sample\n",
    "- Batch of 128 ‚âà 150MB + model parameters + gradients\n",
    "- Monitor GPU memory usage and adjust accordingly\n",
    "\n",
    "#### üîß **Implementation in this Notebook:**\n",
    "- We use `batch_size = 128` for both DataLoader creation and training\n",
    "- This provides good training efficiency while remaining memory-friendly\n",
    "- Adjust based on your GPU memory capacity and training requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0abd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ImageNet Validation Folder Analysis Script\n",
    "print(\"üîç ANALYZING IMAGENET VALIDATION FOLDER\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "import os\n",
    "import re\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "\n",
    "# Configuration\n",
    "REMOVE_DUPLICATES = False  # Set to True to actually remove duplicate files\n",
    "DRY_RUN = True  # Set to False to actually perform file operations\n",
    "\n",
    "# Path to validation folder\n",
    "val_folder = \"data/ImageNet-1K/val_images\"\n",
    "truth_file = \"data/ImageNet/ILSVRC2012_validation_ground_truth.txt\"\n",
    "\n",
    "print(f\"üìÇ Analyzing folder: {val_folder}\")\n",
    "print(f\"üìÑ Truth file: {truth_file}\")\n",
    "print(f\"üîß Remove duplicates: {'YES' if REMOVE_DUPLICATES else 'NO (analysis only)'}\")\n",
    "print(f\"üîß Dry run mode: {'YES' if DRY_RUN else 'NO (will actually modify files)'}\")\n",
    "\n",
    "# Check if folder exists\n",
    "if not os.path.exists(val_folder):\n",
    "    print(f\"‚ùå Validation folder not found: {val_folder}\")\n",
    "    # Try alternative paths\n",
    "    alternative_paths = [\n",
    "        \"data/ImageNet/val\",\n",
    "        \"data/ImageNet-1K/val\", \n",
    "        \"../data/ImageNet-1K/val_images\",\n",
    "        \"../data/ImageNet/val\"\n",
    "    ]\n",
    "    \n",
    "    for alt_path in alternative_paths:\n",
    "        if os.path.exists(alt_path):\n",
    "            val_folder = alt_path\n",
    "            print(f\"‚úÖ Found alternative path: {val_folder}\")\n",
    "            break\n",
    "    else:\n",
    "        print(f\"‚ùå No validation folder found in any of the expected locations\")\n",
    "        print(f\"Checked paths: {[val_folder] + alternative_paths}\")\n",
    "        val_folder = None\n",
    "\n",
    "if val_folder and os.path.exists(val_folder):\n",
    "    print(f\"\\nüîç Scanning files in {val_folder}...\")\n",
    "    \n",
    "    # Get all JPEG files\n",
    "    image_files = []\n",
    "    for root, dirs, files in os.walk(val_folder):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(('.jpeg', '.jpg')):\n",
    "                image_files.append(file)\n",
    "    \n",
    "    print(f\"üìä Total image files found: {len(image_files)}\")\n",
    "    \n",
    "    # Extract file numbers using regex pattern\n",
    "    # Pattern matches: ILSVRC2012_val_########_*.JPEG or ILSVRC2012_val_########.JPEG\n",
    "    file_numbers = []\n",
    "    invalid_files = []\n",
    "    file_mapping = {}  # Maps file_number to list of filenames\n",
    "    \n",
    "    # Regex pattern to extract the validation image number\n",
    "    pattern = re.compile(r'ILSVRC2012_val_(\\d{8})(?:_.*)?\\.jpe?g', re.IGNORECASE)\n",
    "    \n",
    "    print(f\"\\nüîç Extracting file numbers...\")\n",
    "    for filename in image_files:\n",
    "        match = pattern.match(filename)\n",
    "        if match:\n",
    "            file_number = match.group(1)\n",
    "            file_numbers.append(file_number)\n",
    "            if file_number not in file_mapping:\n",
    "                file_mapping[file_number] = []\n",
    "            file_mapping[file_number].append(filename)\n",
    "        else:\n",
    "            invalid_files.append(filename)\n",
    "    \n",
    "    print(f\"‚úÖ Valid files with numbers: {len(file_numbers)}\")\n",
    "    print(f\"‚ö†Ô∏è  Invalid/unexpected files: {len(invalid_files)}\")\n",
    "    \n",
    "    if invalid_files:\n",
    "        print(f\"\\nüìã Invalid files (first 10):\")\n",
    "        for i, invalid_file in enumerate(invalid_files[:10]):\n",
    "            print(f\"  {i+1}: {invalid_file}\")\n",
    "        if len(invalid_files) > 10:\n",
    "            print(f\"  ... and {len(invalid_files) - 10} more\")\n",
    "    \n",
    "    # Check for duplicates\n",
    "    print(f\"\\nüîç Checking for duplicate file numbers...\")\n",
    "    number_counts = Counter(file_numbers)\n",
    "    duplicates = {num: count for num, count in number_counts.items() if count > 1}\n",
    "    \n",
    "    if duplicates:\n",
    "        print(f\"‚ùå Found {len(duplicates)} duplicate file numbers:\")\n",
    "        print(f\"üìä Total duplicate instances: {sum(duplicates.values()) - len(duplicates)}\")\n",
    "        \n",
    "        print(f\"\\nüìã Duplicate details:\")\n",
    "        files_to_remove = []\n",
    "        \n",
    "        for file_number, count in sorted(duplicates.items()):\n",
    "            print(f\"  File number {file_number}: {count} instances\")\n",
    "            \n",
    "            # Show which files have this number\n",
    "            matching_files = file_mapping[file_number]\n",
    "            for i, matching_file in enumerate(matching_files):\n",
    "                status = \"KEEP\" if i == 0 else \"REMOVE\"\n",
    "                print(f\"    {status}: {matching_file}\")\n",
    "                if i > 0:  # Add to removal list (keep first, remove rest)\n",
    "                    files_to_remove.append(matching_file)\n",
    "        \n",
    "        # Calculate impact\n",
    "        total_duplicates = sum(duplicates.values()) - len(duplicates)\n",
    "        expected_unique_files = len(image_files) - total_duplicates\n",
    "        print(f\"\\nüìà Impact Analysis:\")\n",
    "        print(f\"  Total files found: {len(image_files)}\")\n",
    "        print(f\"  Duplicate instances to remove: {total_duplicates}\")\n",
    "        print(f\"  Files after cleanup: {expected_unique_files}\")\n",
    "        \n",
    "        # Remove duplicates if requested\n",
    "        if REMOVE_DUPLICATES and files_to_remove:\n",
    "            print(f\"\\nüóëÔ∏è  REMOVING DUPLICATE FILES:\")\n",
    "            print(f\"Files to remove: {len(files_to_remove)}\")\n",
    "            \n",
    "            removed_count = 0\n",
    "            failed_removals = []\n",
    "            \n",
    "            for file_to_remove in files_to_remove:\n",
    "                file_path = os.path.join(val_folder, file_to_remove)\n",
    "                \n",
    "                if DRY_RUN:\n",
    "                    print(f\"  [DRY RUN] Would remove: {file_to_remove}\")\n",
    "                else:\n",
    "                    try:\n",
    "                        os.remove(file_path)\n",
    "                        print(f\"  ‚úÖ Removed: {file_to_remove}\")\n",
    "                        removed_count += 1\n",
    "                    except Exception as e:\n",
    "                        print(f\"  ‚ùå Failed to remove {file_to_remove}: {e}\")\n",
    "                        failed_removals.append(file_to_remove)\n",
    "            \n",
    "            if not DRY_RUN:\n",
    "                print(f\"\\nüìä Removal Summary:\")\n",
    "                print(f\"  Successfully removed: {removed_count}\")\n",
    "                print(f\"  Failed removals: {len(failed_removals)}\")\n",
    "                \n",
    "                if failed_removals:\n",
    "                    print(f\"  Failed files: {failed_removals}\")\n",
    "                \n",
    "                # Re-scan to verify\n",
    "                print(f\"\\nüîç Re-scanning after removal...\")\n",
    "                remaining_files = [f for f in os.listdir(val_folder) if f.lower().endswith(('.jpeg', '.jpg'))]\n",
    "                print(f\"  Files remaining: {len(remaining_files)}\")\n",
    "                \n",
    "        elif not REMOVE_DUPLICATES:\n",
    "            print(f\"\\nüí° TO REMOVE DUPLICATES:\")\n",
    "            print(f\"1. Set REMOVE_DUPLICATES = True\")\n",
    "            print(f\"2. Set DRY_RUN = False to actually remove files\")\n",
    "            print(f\"3. Re-run this cell\")\n",
    "            \n",
    "            print(f\"\\nüîß Manual removal commands:\")\n",
    "            for file_to_remove in files_to_remove:\n",
    "                print(f\"rm '{val_folder}/{file_to_remove}'\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"‚úÖ No duplicate file numbers found!\")\n",
    "        print(f\"All {len(file_numbers)} files have unique numbers\")\n",
    "    \n",
    "    # Check sequential numbering\n",
    "    print(f\"\\nüîç Checking sequential numbering...\")\n",
    "    if file_numbers:\n",
    "        file_numbers_int = [int(num) for num in file_numbers]\n",
    "        min_num = min(file_numbers_int)\n",
    "        max_num = max(file_numbers_int)\n",
    "        expected_range = list(range(min_num, max_num + 1))\n",
    "        \n",
    "        print(f\"üìä Number range: {min_num:08d} to {max_num:08d}\")\n",
    "        print(f\"üìä Expected count in range: {len(expected_range)}\")\n",
    "        print(f\"üìä Actual unique count: {len(set(file_numbers_int))}\")\n",
    "        \n",
    "        # Find missing numbers\n",
    "        missing_numbers = set(expected_range) - set(file_numbers_int)\n",
    "        if missing_numbers:\n",
    "            print(f\"‚ö†Ô∏è  Missing {len(missing_numbers)} numbers in sequence\")\n",
    "            if len(missing_numbers) <= 20:\n",
    "                missing_sorted = sorted(missing_numbers)\n",
    "                print(f\"   Missing numbers: {[f'{num:08d}' for num in missing_sorted]}\")\n",
    "            else:\n",
    "                missing_sorted = sorted(missing_numbers)\n",
    "                print(f\"   First 10 missing: {[f'{num:08d}' for num in missing_sorted[:10]]}\")\n",
    "                print(f\"   Last 10 missing: {[f'{num:08d}' for num in missing_sorted[-10:]]}\")\n",
    "        else:\n",
    "            print(f\"‚úÖ All numbers in range are present\")\n",
    "    \n",
    "    # Check against truth file\n",
    "    if os.path.exists(truth_file):\n",
    "        print(f\"\\nüîç Checking against truth file...\")\n",
    "        try:\n",
    "            with open(truth_file, 'r') as f:\n",
    "                truth_lines = f.readlines()\n",
    "            \n",
    "            truth_count = len([line.strip() for line in truth_lines if line.strip()])\n",
    "            current_file_count = len(image_files) if not (REMOVE_DUPLICATES and not DRY_RUN) else len(image_files) - len(files_to_remove) if duplicates else len(image_files)\n",
    "            \n",
    "            print(f\"üìä Truth file labels: {truth_count}\")\n",
    "            print(f\"üìä Image files found: {len(image_files)}\")\n",
    "            print(f\"üìä Valid numbered files: {len(file_numbers)}\")\n",
    "            print(f\"üìä Unique numbered files: {len(set(file_numbers))}\")\n",
    "            if duplicates and REMOVE_DUPLICATES:\n",
    "                print(f\"üìä Files after duplicate removal: {current_file_count}\")\n",
    "            \n",
    "            unique_count = len(set(file_numbers))\n",
    "            if truth_count == unique_count:\n",
    "                print(f\"‚úÖ Perfect match: truth labels = unique image files\")\n",
    "            elif truth_count == len(image_files):\n",
    "                print(f\"‚ö†Ô∏è  Truth labels = total files (but duplicates exist)\")\n",
    "            else:\n",
    "                print(f\"‚ùå Mismatch detected!\")\n",
    "                print(f\"   Difference: {abs(truth_count - unique_count)} files\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error reading truth file: {e}\")\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è  Truth file not found: {truth_file}\")\n",
    "        # Try alternative truth file paths\n",
    "        alternative_truth_paths = [\n",
    "            \"data/ImageNet/ILSVRC2012_validation_ground_truth.txt\",\n",
    "            \"data/ImageNet-1K/ILSVRC2012_validation_ground_truth.txt\",\n",
    "            \"../data/ImageNet/ILSVRC2012_validation_ground_truth.txt\"\n",
    "        ]\n",
    "        \n",
    "        for alt_truth in alternative_truth_paths:\n",
    "            if os.path.exists(alt_truth):\n",
    "                print(f\"‚úÖ Found alternative truth file: {alt_truth}\")\n",
    "                truth_file = alt_truth\n",
    "                break\n",
    "        else:\n",
    "            print(f\"‚ùå No truth file found in any expected location\")\n",
    "\n",
    "    # Summary and recommendations\n",
    "    print(f\"\\nüìã ANALYSIS SUMMARY\")\n",
    "    print(f\"-\" * 40)\n",
    "    print(f\"Total files found: {len(image_files)}\")\n",
    "    print(f\"Valid numbered files: {len(file_numbers)}\")\n",
    "    print(f\"Unique file numbers: {len(set(file_numbers)) if file_numbers else 0}\")\n",
    "    print(f\"Duplicate instances: {len(duplicates)} ({sum(duplicates.values()) - len(duplicates) if duplicates else 0} extra files)\")\n",
    "    print(f\"Invalid files: {len(invalid_files)}\")\n",
    "    \n",
    "    if duplicates and not REMOVE_DUPLICATES:\n",
    "        print(f\"\\nüí° TO FIX THE MISMATCH:\")\n",
    "        print(f\"1. Set REMOVE_DUPLICATES = True at the top of this cell\")\n",
    "        print(f\"2. Set DRY_RUN = False to actually remove files\")\n",
    "        print(f\"3. Re-run this cell to remove {sum(duplicates.values()) - len(duplicates)} duplicate files\")\n",
    "        print(f\"4. This should resolve the training error\")\n",
    "        \n",
    "    elif duplicates and REMOVE_DUPLICATES and DRY_RUN:\n",
    "        print(f\"\\nüí° READY TO REMOVE DUPLICATES:\")\n",
    "        print(f\"Set DRY_RUN = False and re-run to actually remove the files\")\n",
    "        \n",
    "    elif duplicates and REMOVE_DUPLICATES and not DRY_RUN:\n",
    "        print(f\"\\n‚úÖ Duplicate removal attempted!\")\n",
    "        print(f\"The mismatch error should now be resolved\")\n",
    "    \n",
    "    else:\n",
    "        print(f\"\\n‚úÖ No cleanup needed - all files have unique numbers!\")\n",
    "\n",
    "else:\n",
    "    print(f\"\\n‚ùå Cannot analyze - validation folder not accessible\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(f\"üèÅ ImageNet validation analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381578b3",
   "metadata": {},
   "source": [
    "# üöÄ Google Colab Batch Size Optimization for MC-ResNet\n",
    "\n",
    "This script helps you find the optimal batch size for your MC-ResNet model in Google Colab by testing various metrics:\n",
    "\n",
    "## üìä **Key Metrics Tested:**\n",
    "\n",
    "1. **GPU Memory Usage** - Peak memory consumption during forward/backward pass\n",
    "2. **Training Speed** - Time per batch and samples per second\n",
    "3. **Memory Efficiency** - GPU memory utilization percentage\n",
    "4. **Gradient Stability** - Gradient norm consistency across batch sizes\n",
    "5. **Training Stability** - Loss convergence behavior\n",
    "6. **Throughput** - Overall training throughput (samples/second)\n",
    "\n",
    "## üéØ **Testing Strategy:**\n",
    "\n",
    "- **Data**: Uses ImageNet-1K subset for realistic testing\n",
    "- **Progressive Testing**: Starts small and increases until OOM\n",
    "- **Safety Checks**: Automatic recovery from out-of-memory errors\n",
    "- **Colab Optimization**: Designed for Colab's GPU environment (T4/V100/A100)\n",
    "\n",
    "## üîß **What the Script Finds:**\n",
    "\n",
    "- **Maximum Batch Size**: Largest batch size that fits in GPU memory\n",
    "- **Optimal Batch Size**: Best balance of speed, memory efficiency, and stability\n",
    "- **Performance Curves**: Visualizations of batch size vs performance metrics\n",
    "- **Recommendations**: Specific batch sizes for training vs validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82339c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Colab Batch Size Optimization Script for MC-ResNet\n",
    "print(\"üöÄ GOOGLE COLAB BATCH SIZE OPTIMIZATION FOR MC-RESNET\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import psutil\n",
    "import GPUtil\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check if we're in Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"‚úÖ Running in Google Colab\")\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"‚ö†Ô∏è  Not in Colab - adapting for local environment\")\n",
    "\n",
    "# GPU Detection and Setup\n",
    "def setup_device():\n",
    "    \"\"\"Setup device with Colab-specific optimizations.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "        gpu_name = torch.cuda.get_device_name(0)\n",
    "        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "        print(f\"üöÄ GPU: {gpu_name}\")\n",
    "        print(f\"üíæ GPU Memory: {gpu_memory:.1f} GB\")\n",
    "        \n",
    "        # Colab-specific optimizations\n",
    "        if IN_COLAB:\n",
    "            # Enable memory fraction for better memory management\n",
    "            torch.cuda.empty_cache()\n",
    "            # Set memory allocation strategy\n",
    "            torch.backends.cudnn.benchmark = True\n",
    "            \n",
    "        return device, gpu_name, gpu_memory\n",
    "    else:\n",
    "        print(\"‚ùå No GPU available - batch size optimization not recommended\")\n",
    "        return torch.device('cpu'), \"CPU\", 0\n",
    "\n",
    "device, gpu_name, gpu_memory = setup_device()\n",
    "\n",
    "def get_memory_usage():\n",
    "    \"\"\"Get current GPU memory usage.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / 1e9\n",
    "        reserved = torch.cuda.memory_reserved() / 1e9\n",
    "        return allocated, reserved\n",
    "    return 0, 0\n",
    "\n",
    "def clear_memory():\n",
    "    \"\"\"Clear GPU memory.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "class BatchSizeTester:\n",
    "    \"\"\"Comprehensive batch size testing for MC-ResNet models.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_class, num_classes=1000, image_size=(224, 224)):\n",
    "        self.model_class = model_class\n",
    "        self.num_classes = num_classes\n",
    "        self.image_size = image_size\n",
    "        self.results = defaultdict(list)\n",
    "        \n",
    "    def create_test_data(self, batch_size, num_batches=3):\n",
    "        \"\"\"Create synthetic dual-channel test data.\"\"\"\n",
    "        # RGB data (3 channels)\n",
    "        rgb_data = torch.randn(batch_size * num_batches, 3, *self.image_size)\n",
    "        # Brightness data (1 channel) \n",
    "        brightness_data = torch.randn(batch_size * num_batches, 1, *self.image_size)\n",
    "        labels = torch.randint(0, self.num_classes, (batch_size * num_batches,))\n",
    "        \n",
    "        # Create datasets\n",
    "        dataset = TensorDataset(rgb_data, brightness_data, labels)\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "        \n",
    "        return dataloader\n",
    "    \n",
    "    def test_batch_size(self, batch_size, test_epochs=3):\n",
    "        \"\"\"Test a specific batch size comprehensively.\"\"\"\n",
    "        print(f\"\\nüß™ Testing batch size: {batch_size}\")\n",
    "        \n",
    "        # Clear memory before test\n",
    "        clear_memory()\n",
    "        \n",
    "        try:\n",
    "            # Create model\n",
    "            model = self.model_class(num_classes=self.num_classes, device=str(device))\n",
    "            model = model.to(device)\n",
    "            \n",
    "            # Create test data\n",
    "            test_loader = self.create_test_data(batch_size)\n",
    "            \n",
    "            # Setup training components\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "            \n",
    "            # Measurements\n",
    "            times = []\n",
    "            memory_peaks = []\n",
    "            gradient_norms = []\n",
    "            losses = []\n",
    "            \n",
    "            # Memory before training\n",
    "            initial_allocated, initial_reserved = get_memory_usage()\n",
    "            \n",
    "            model.train()\n",
    "            for epoch in range(test_epochs):\n",
    "                epoch_times = []\n",
    "                epoch_losses = []\n",
    "                \n",
    "                for batch_idx, (rgb_data, brightness_data, targets) in enumerate(test_loader):\n",
    "                    rgb_data = rgb_data.to(device, non_blocking=True)\n",
    "                    brightness_data = brightness_data.to(device, non_blocking=True)\n",
    "                    targets = targets.to(device, non_blocking=True)\n",
    "                    \n",
    "                    # Time the forward and backward pass\n",
    "                    start_time = time.time()\n",
    "                    \n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(rgb_data, brightness_data)\n",
    "                    loss = criterion(outputs, targets)\n",
    "                    loss.backward()\n",
    "                    \n",
    "                    # Calculate gradient norm\n",
    "                    total_norm = 0\n",
    "                    for p in model.parameters():\n",
    "                        if p.grad is not None:\n",
    "                            param_norm = p.grad.data.norm(2)\n",
    "                            total_norm += param_norm.item() ** 2\n",
    "                    gradient_norms.append(total_norm ** 0.5)\n",
    "                    \n",
    "                    optimizer.step()\n",
    "                    \n",
    "                    end_time = time.time()\n",
    "                    batch_time = end_time - start_time\n",
    "                    epoch_times.append(batch_time)\n",
    "                    epoch_losses.append(loss.item())\n",
    "                    \n",
    "                    # Measure peak memory\n",
    "                    allocated, reserved = get_memory_usage()\n",
    "                    memory_peaks.append(allocated)\n",
    "                \n",
    "                times.extend(epoch_times)\n",
    "                losses.extend(epoch_losses)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            avg_time_per_batch = np.mean(times)\n",
    "            samples_per_second = batch_size / avg_time_per_batch\n",
    "            peak_memory = max(memory_peaks)\n",
    "            memory_efficiency = (peak_memory / gpu_memory) * 100 if gpu_memory > 0 else 0\n",
    "            avg_gradient_norm = np.mean(gradient_norms)\n",
    "            gradient_stability = np.std(gradient_norms) / (avg_gradient_norm + 1e-8)\n",
    "            avg_loss = np.mean(losses)\n",
    "            loss_stability = np.std(losses) / (avg_loss + 1e-8)\n",
    "            \n",
    "            # Store results\n",
    "            results = {\n",
    "                'batch_size': batch_size,\n",
    "                'avg_time_per_batch': avg_time_per_batch,\n",
    "                'samples_per_second': samples_per_second,\n",
    "                'peak_memory_gb': peak_memory,\n",
    "                'memory_efficiency_pct': memory_efficiency,\n",
    "                'avg_gradient_norm': avg_gradient_norm,\n",
    "                'gradient_stability': gradient_stability,\n",
    "                'avg_loss': avg_loss,\n",
    "                'loss_stability': loss_stability,\n",
    "                'success': True\n",
    "            }\n",
    "            \n",
    "            print(f\"  ‚úÖ Success!\")\n",
    "            print(f\"     Time/batch: {avg_time_per_batch:.3f}s\")\n",
    "            print(f\"     Samples/sec: {samples_per_second:.1f}\")\n",
    "            print(f\"     Peak memory: {peak_memory:.2f} GB ({memory_efficiency:.1f}%)\")\n",
    "            print(f\"     Gradient norm: {avg_gradient_norm:.3f} (stability: {gradient_stability:.3f})\")\n",
    "            \n",
    "            return results\n",
    "            \n",
    "        except RuntimeError as e:\n",
    "            if \"out of memory\" in str(e).lower():\n",
    "                print(f\"  ‚ùå Out of memory!\")\n",
    "                clear_memory()\n",
    "                return {\n",
    "                    'batch_size': batch_size,\n",
    "                    'success': False,\n",
    "                    'error': 'OOM'\n",
    "                }\n",
    "            else:\n",
    "                print(f\"  ‚ùå Error: {e}\")\n",
    "                return {\n",
    "                    'batch_size': batch_size,\n",
    "                    'success': False,\n",
    "                    'error': str(e)\n",
    "                }\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå Unexpected error: {e}\")\n",
    "            return {\n",
    "                'batch_size': batch_size,\n",
    "                'success': False,\n",
    "                'error': str(e)\n",
    "            }\n",
    "        finally:\n",
    "            # Clean up\n",
    "            if 'model' in locals():\n",
    "                del model\n",
    "            if 'test_loader' in locals():\n",
    "                del test_loader\n",
    "            clear_memory()\n",
    "    \n",
    "    def find_optimal_batch_size(self, start_batch=8, max_batch=512, multiplier=2):\n",
    "        \"\"\"Find optimal batch size through progressive testing.\"\"\"\n",
    "        print(f\"\\nüîç FINDING OPTIMAL BATCH SIZE\")\n",
    "        print(f\"Testing range: {start_batch} to {max_batch} (multiplier: {multiplier})\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        all_results = []\n",
    "        current_batch = start_batch\n",
    "        max_successful_batch = 0\n",
    "        \n",
    "        while current_batch <= max_batch:\n",
    "            result = self.test_batch_size(current_batch)\n",
    "            all_results.append(result)\n",
    "            \n",
    "            if result['success']:\n",
    "                max_successful_batch = current_batch\n",
    "                # Store successful results\n",
    "                for key, value in result.items():\n",
    "                    if key != 'success':\n",
    "                        self.results[key].append(value)\n",
    "            else:\n",
    "                print(f\"  üíÄ Stopping at batch size {current_batch} due to: {result.get('error', 'Unknown error')}\")\n",
    "                break\n",
    "            \n",
    "            current_batch *= multiplier\n",
    "        \n",
    "        print(f\"\\nüìä TESTING COMPLETE\")\n",
    "        print(f\"Maximum successful batch size: {max_successful_batch}\")\n",
    "        \n",
    "        return all_results, max_successful_batch\n",
    "    \n",
    "    def analyze_results(self):\n",
    "        \"\"\"Analyze results and provide recommendations.\"\"\"\n",
    "        if not self.results['batch_size']:\n",
    "            print(\"‚ùå No successful results to analyze\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"\\nüìà PERFORMANCE ANALYSIS\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        batch_sizes = self.results['batch_size']\n",
    "        throughputs = self.results['samples_per_second']\n",
    "        memory_usage = self.results['peak_memory_gb']\n",
    "        memory_efficiency = self.results['memory_efficiency_pct']\n",
    "        gradient_stability = self.results['gradient_stability']\n",
    "        \n",
    "        # Find optimal batch sizes for different criteria\n",
    "        max_throughput_idx = np.argmax(throughputs)\n",
    "        best_efficiency_idx = np.argmax([t/m for t, m in zip(throughputs, memory_usage)])\n",
    "        most_stable_idx = np.argmin(gradient_stability)\n",
    "        \n",
    "        print(f\"üöÄ Maximum Throughput:\")\n",
    "        print(f\"   Batch Size: {batch_sizes[max_throughput_idx]}\")\n",
    "        print(f\"   Throughput: {throughputs[max_throughput_idx]:.1f} samples/sec\")\n",
    "        print(f\"   Memory: {memory_usage[max_throughput_idx]:.2f} GB ({memory_efficiency[max_throughput_idx]:.1f}%)\")\n",
    "        \n",
    "        print(f\"\\n‚ö° Best Efficiency (throughput/memory):\")\n",
    "        print(f\"   Batch Size: {batch_sizes[best_efficiency_idx]}\")\n",
    "        print(f\"   Throughput: {throughputs[best_efficiency_idx]:.1f} samples/sec\")\n",
    "        print(f\"   Memory: {memory_usage[best_efficiency_idx]:.2f} GB ({memory_efficiency[best_efficiency_idx]:.1f}%)\")\n",
    "        \n",
    "        print(f\"\\nüéØ Most Stable Training:\")\n",
    "        print(f\"   Batch Size: {batch_sizes[most_stable_idx]}\")\n",
    "        print(f\"   Gradient Stability: {gradient_stability[most_stable_idx]:.3f}\")\n",
    "        print(f\"   Throughput: {throughputs[most_stable_idx]:.1f} samples/sec\")\n",
    "        \n",
    "        # Recommendations\n",
    "        max_batch = max(batch_sizes)\n",
    "        print(f\"\\nüí° RECOMMENDATIONS:\")\n",
    "        print(f\"   üèãÔ∏è  Maximum Batch Size: {max_batch}\")\n",
    "        print(f\"   üöÄ For Speed: {batch_sizes[max_throughput_idx]}\")\n",
    "        print(f\"   ‚ö° For Efficiency: {batch_sizes[best_efficiency_idx]}\")\n",
    "        print(f\"   üéØ For Stability: {batch_sizes[most_stable_idx]}\")\n",
    "        \n",
    "        # Colab-specific recommendations\n",
    "        if IN_COLAB:\n",
    "            print(f\"\\nüîß COLAB-SPECIFIC RECOMMENDATIONS:\")\n",
    "            if \"T4\" in gpu_name:\n",
    "                recommended = min(batch_sizes[best_efficiency_idx], 64)\n",
    "                print(f\"   T4 GPU: Use batch size {recommended} for best balance\")\n",
    "            elif \"V100\" in gpu_name:\n",
    "                recommended = min(batch_sizes[max_throughput_idx], 128)\n",
    "                print(f\"   V100 GPU: Use batch size {recommended} for optimal performance\")\n",
    "            elif \"A100\" in gpu_name:\n",
    "                recommended = batch_sizes[max_throughput_idx]\n",
    "                print(f\"   A100 GPU: Use batch size {recommended} for maximum performance\")\n",
    "            else:\n",
    "                recommended = batch_sizes[best_efficiency_idx]\n",
    "                print(f\"   Unknown GPU: Use batch size {recommended} for safety\")\n",
    "        \n",
    "        return {\n",
    "            'max_batch_size': max_batch,\n",
    "            'optimal_for_speed': batch_sizes[max_throughput_idx],\n",
    "            'optimal_for_efficiency': batch_sizes[best_efficiency_idx],\n",
    "            'optimal_for_stability': batch_sizes[most_stable_idx],\n",
    "            'recommended_batch_size': recommended if IN_COLAB else batch_sizes[best_efficiency_idx]\n",
    "        }\n",
    "    \n",
    "    def plot_results(self):\n",
    "        \"\"\"Create comprehensive plots of batch size performance.\"\"\"\n",
    "        if not self.results['batch_size']:\n",
    "            print(\"‚ùå No results to plot\")\n",
    "            return\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        fig.suptitle('MC-ResNet Batch Size Optimization Results', fontsize=16)\n",
    "        \n",
    "        batch_sizes = self.results['batch_size']\n",
    "        \n",
    "        # Throughput vs Batch Size\n",
    "        axes[0, 0].plot(batch_sizes, self.results['samples_per_second'], 'b-o', linewidth=2, markersize=8)\n",
    "        axes[0, 0].set_xlabel('Batch Size')\n",
    "        axes[0, 0].set_ylabel('Samples/Second')\n",
    "        axes[0, 0].set_title('Training Throughput')\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        axes[0, 0].set_xscale('log', base=2)\n",
    "        \n",
    "        # Memory Usage vs Batch Size\n",
    "        axes[0, 1].plot(batch_sizes, self.results['peak_memory_gb'], 'r-o', linewidth=2, markersize=8)\n",
    "        axes[0, 1].set_xlabel('Batch Size')\n",
    "        axes[0, 1].set_ylabel('Peak Memory (GB)')\n",
    "        axes[0, 1].set_title('GPU Memory Usage')\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        axes[0, 1].set_xscale('log', base=2)\n",
    "        \n",
    "        # Memory Efficiency vs Batch Size\n",
    "        axes[1, 0].plot(batch_sizes, self.results['memory_efficiency_pct'], 'g-o', linewidth=2, markersize=8)\n",
    "        axes[1, 0].set_xlabel('Batch Size')\n",
    "        axes[1, 0].set_ylabel('Memory Efficiency (%)')\n",
    "        axes[1, 0].set_title('GPU Memory Efficiency')\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        axes[1, 0].set_xscale('log', base=2)\n",
    "        \n",
    "        # Gradient Stability vs Batch Size\n",
    "        axes[1, 1].plot(batch_sizes, self.results['gradient_stability'], 'm-o', linewidth=2, markersize=8)\n",
    "        axes[1, 1].set_xlabel('Batch Size')\n",
    "        axes[1, 1].set_ylabel('Gradient Stability (lower is better)')\n",
    "        axes[1, 1].set_title('Training Stability')\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        axes[1, 1].set_xscale('log', base=2)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Efficiency scatter plot\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        efficiency_scores = [t/m for t, m in zip(self.results['samples_per_second'], self.results['peak_memory_gb'])]\n",
    "        \n",
    "        plt.scatter(batch_sizes, efficiency_scores, s=100, alpha=0.7, c=self.results['gradient_stability'], \n",
    "                   cmap='viridis_r', edgecolors='black')\n",
    "        plt.colorbar(label='Gradient Stability (lower is better)')\n",
    "        plt.xlabel('Batch Size')\n",
    "        plt.ylabel('Efficiency Score (Samples/sec per GB)')\n",
    "        plt.title('Batch Size Efficiency Analysis\\n(Larger bubbles = better efficiency, darker = more stable)')\n",
    "        plt.xscale('log', base=2)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Annotate points\n",
    "        for i, batch_size in enumerate(batch_sizes):\n",
    "            plt.annotate(f'{batch_size}', (batch_size, efficiency_scores[i]), \n",
    "                        xytext=(5, 5), textcoords='offset points', fontsize=10)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Example usage for MC-ResNet\n",
    "print(f\"\\nüéØ READY TO TEST MC-RESNET BATCH SIZES\")\n",
    "print(f\"This script will help you find the optimal batch size for your MC-ResNet model\")\n",
    "print(f\"GPU: {gpu_name} ({gpu_memory:.1f} GB)\")\n",
    "print(f\"\\nüí° To run the test:\")\n",
    "print(f\"1. Import your mc_resnet50 model\")\n",
    "print(f\"2. Run: tester = BatchSizeTester(mc_resnet50)\")\n",
    "print(f\"3. Run: results, max_batch = tester.find_optimal_batch_size()\")\n",
    "print(f\"4. Run: recommendations = tester.analyze_results()\")\n",
    "print(f\"5. Run: tester.plot_results()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63f5d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Running Batch Size Optimization for MC-ResNet50\n",
    "print(\"üöÄ EXAMPLE: BATCH SIZE OPTIMIZATION FOR MC-RESNET50\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# NOTE: Uncomment and run this section when you want to test batch sizes\n",
    "\n",
    "\"\"\"\n",
    "# Step 1: Import your MC-ResNet model\n",
    "from src.models2.multi_channel.mc_resnet import mc_resnet50\n",
    "\n",
    "# Step 2: Create the batch size tester\n",
    "print(\"üîß Creating batch size tester...\")\n",
    "tester = BatchSizeTester(mc_resnet50, num_classes=1000, image_size=(224, 224))\n",
    "\n",
    "# Step 3: Run the optimization test\n",
    "print(\"üß™ Starting batch size optimization...\")\n",
    "# For quick testing, use smaller range:\n",
    "results, max_batch = tester.find_optimal_batch_size(\n",
    "    start_batch=8,     # Start small\n",
    "    max_batch=256,     # Conservative max for safety\n",
    "    multiplier=2       # Double each time: 8, 16, 32, 64, 128, 256\n",
    ")\n",
    "\n",
    "# For comprehensive testing on powerful GPUs:\n",
    "# results, max_batch = tester.find_optimal_batch_size(\n",
    "#     start_batch=4,\n",
    "#     max_batch=1024,\n",
    "#     multiplier=2\n",
    "# )\n",
    "\n",
    "# Step 4: Analyze results and get recommendations\n",
    "print(\"üìä Analyzing results...\")\n",
    "recommendations = tester.analyze_results()\n",
    "\n",
    "# Step 5: Plot the results\n",
    "print(\"üìà Creating performance plots...\")\n",
    "tester.plot_results()\n",
    "\n",
    "# Step 6: Display final recommendations\n",
    "print(f\"\\\\nüéØ FINAL RECOMMENDATIONS FOR YOUR MC-RESNET:\")\n",
    "if recommendations:\n",
    "    print(f\"   Maximum possible batch size: {recommendations['max_batch_size']}\")\n",
    "    print(f\"   Recommended for training: {recommendations['recommended_batch_size']}\")\n",
    "    print(f\"   Best for speed: {recommendations['optimal_for_speed']}\")\n",
    "    print(f\"   Best for efficiency: {recommendations['optimal_for_efficiency']}\")\n",
    "    print(f\"   Best for stability: {recommendations['optimal_for_stability']}\")\n",
    "    \n",
    "    # Memory usage estimate\n",
    "    if 'peak_memory_gb' in tester.results:\n",
    "        recommended_idx = tester.results['batch_size'].index(recommendations['recommended_batch_size'])\n",
    "        memory_usage = tester.results['peak_memory_gb'][recommended_idx]\n",
    "        print(f\"   Expected memory usage: {memory_usage:.2f} GB\")\n",
    "        print(f\"   Memory efficiency: {tester.results['memory_efficiency_pct'][recommended_idx]:.1f}%\")\n",
    "\"\"\"\n",
    "\n",
    "print(\"üîß TO RUN THE OPTIMIZATION:\")\n",
    "print(\"1. Uncomment the code block above\")\n",
    "print(\"2. Make sure your MC-ResNet model is imported\")\n",
    "print(\"3. Run the cell\")\n",
    "print(\"4. Wait for results and recommendations\")\n",
    "\n",
    "print(\"\\nüí° WHAT THIS TEST MEASURES:\")\n",
    "test_metrics = [\n",
    "    \"üöÄ Training Throughput (samples/second)\",\n",
    "    \"üíæ GPU Memory Usage (peak GB and efficiency %)\",\n",
    "    \"üìä Gradient Stability (training consistency)\",\n",
    "    \"‚è±Ô∏è  Time per Batch (forward + backward pass)\",\n",
    "    \"üéØ Loss Stability (convergence behavior)\",\n",
    "    \"‚ö° Memory Efficiency (throughput per GB used)\"\n",
    "]\n",
    "\n",
    "for metric in test_metrics:\n",
    "    print(f\"   {metric}\")\n",
    "\n",
    "print(\"\\nüéØ WHY THESE METRICS MATTER:\")\n",
    "explanations = {\n",
    "    \"Throughput\": \"Higher = faster training, more samples processed per second\",\n",
    "    \"Memory Usage\": \"Lower = can fit larger models or use larger validation batches\", \n",
    "    \"Gradient Stability\": \"Lower variance = more consistent training dynamics\",\n",
    "    \"Time per Batch\": \"Lower = faster iterations, quicker experimentation\",\n",
    "    \"Memory Efficiency\": \"Higher = better utilization of expensive GPU resources\"\n",
    "}\n",
    "\n",
    "for metric, explanation in explanations.items():\n",
    "    print(f\"   ‚Ä¢ {metric}: {explanation}\")\n",
    "\n",
    "print(\"\\nüî• COLAB-SPECIFIC OPTIMIZATIONS:\")\n",
    "colab_tips = [\n",
    "    \"üéØ Automatically detects T4/V100/A100 and adjusts recommendations\",\n",
    "    \"üíæ Uses progressive testing to avoid crashing your session\",\n",
    "    \"üöÄ Optimizes for Colab's memory management and CUDA settings\",\n",
    "    \"üìä Provides visualizations that work well in Colab notebooks\",\n",
    "    \"‚ö° Includes safety checks for out-of-memory recovery\"\n",
    "]\n",
    "\n",
    "for tip in colab_tips:\n",
    "    print(f\"   {tip}\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(\"üèÅ Ready to optimize your MC-ResNet batch sizes!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f36a229",
   "metadata": {},
   "source": [
    "# üìä Real ImageNet Data Batch Size Testing\n",
    "\n",
    "This script tests batch sizes using your actual ImageNet data with the `create_imagenet_dual_channel_train_val_dataloaders` function. Unlike the synthetic data version, this provides:\n",
    "\n",
    "## üéØ **Real-World Performance Metrics:**\n",
    "\n",
    "- **Data Loading I/O** - Actual disk read times and preprocessing overhead\n",
    "- **Transform Pipeline** - Real image augmentation and normalization costs  \n",
    "- **Memory Patterns** - Realistic memory usage with actual ImageNet images\n",
    "- **End-to-End Timing** - Complete training loop including data loading\n",
    "- **Worker Efficiency** - Multi-processing data loading performance\n",
    "\n",
    "## üîß **Testing Approach:**\n",
    "\n",
    "- **Uses Your ImageNet Data** - Tests with actual `data/ImageNet-1K/` files\n",
    "- **Streaming DataLoaders** - Tests the `StreamingDualChannelDataset` performance\n",
    "- **Progressive Batch Sizes** - Finds maximum batch size with real data constraints\n",
    "- **I/O vs Compute Balance** - Identifies bottlenecks between data loading and GPU computation\n",
    "\n",
    "This gives you the most accurate batch size recommendations for your actual training pipeline!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73dac45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real ImageNet Data Batch Size Testing Script\n",
    "print(\"üìä REAL IMAGENET DATA BATCH SIZE TESTING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Import your data loading functions\n",
    "try:\n",
    "    from src.data_utils.streaming_dual_channel_dataset import (\n",
    "        create_imagenet_dual_channel_train_val_dataloaders,\n",
    "        create_default_imagenet_transforms\n",
    "    )\n",
    "    print(\"‚úÖ ImageNet data loading functions imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Error importing data functions: {e}\")\n",
    "    print(\"Please ensure you're in the correct directory and modules are available\")\n",
    "\n",
    "class RealDataBatchSizeTester:\n",
    "    \"\"\"Batch size testing with real ImageNet data and your dual-channel dataloaders.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_class, data_config=None):\n",
    "        self.model_class = model_class\n",
    "        self.results = defaultdict(list)\n",
    "        \n",
    "        # Default data configuration - update paths as needed\n",
    "        self.data_config = data_config or {\n",
    "            'train_folders': ['data/ImageNet-1K/train_images_0'],  # Update to your path\n",
    "            'val_folder': 'data/ImageNet-1K/val_images',          # Update to your path\n",
    "            'truth_file': 'data/ImageNet/ILSVRC2012_validation_ground_truth.txt',  # Update to your path\n",
    "            'image_size': (224, 224),\n",
    "            'num_workers': 4,\n",
    "            'pin_memory': True,\n",
    "            'persistent_workers': True\n",
    "        }\n",
    "        \n",
    "        # Verify data paths\n",
    "        self.verify_data_paths()\n",
    "        \n",
    "    def verify_data_paths(self):\n",
    "        \"\"\"Verify that data paths exist.\"\"\"\n",
    "        print(f\"\\nüîç Verifying data paths...\")\n",
    "        \n",
    "        # Check train folders\n",
    "        train_found = False\n",
    "        for folder in self.data_config['train_folders']:\n",
    "            if os.path.exists(folder):\n",
    "                print(f\"‚úÖ Train folder found: {folder}\")\n",
    "                train_found = True\n",
    "            else:\n",
    "                print(f\"‚ùå Train folder missing: {folder}\")\n",
    "        \n",
    "        # Check validation folder\n",
    "        if os.path.exists(self.data_config['val_folder']):\n",
    "            print(f\"‚úÖ Validation folder found: {self.data_config['val_folder']}\")\n",
    "        else:\n",
    "            print(f\"‚ùå Validation folder missing: {self.data_config['val_folder']}\")\n",
    "            \n",
    "        # Check truth file\n",
    "        if os.path.exists(self.data_config['truth_file']):\n",
    "            print(f\"‚úÖ Truth file found: {self.data_config['truth_file']}\")\n",
    "        else:\n",
    "            print(f\"‚ùå Truth file missing: {self.data_config['truth_file']}\")\n",
    "            \n",
    "        if not train_found:\n",
    "            print(f\"\\n‚ö†Ô∏è  No training data found! Please update data paths in data_config\")\n",
    "            \n",
    "    def create_dataloaders(self, batch_size, limit_samples=None):\n",
    "        \"\"\"Create dataloaders with specified batch size.\"\"\"\n",
    "        try:\n",
    "            # Create transforms\n",
    "            train_transform, val_transform = create_default_imagenet_transforms(\n",
    "                image_size=self.data_config['image_size']\n",
    "            )\n",
    "            \n",
    "            # Create dataloaders with your function\n",
    "            train_loader, val_loader = create_imagenet_dual_channel_train_val_dataloaders(\n",
    "                train_folders=self.data_config['train_folders'],\n",
    "                val_folder=self.data_config['val_folder'],\n",
    "                truth_file=self.data_config['truth_file'],\n",
    "                train_transform=train_transform,\n",
    "                val_transform=val_transform,\n",
    "                batch_size=batch_size,\n",
    "                image_size=self.data_config['image_size'],\n",
    "                num_workers=self.data_config['num_workers'],\n",
    "                pin_memory=self.data_config['pin_memory'],\n",
    "                persistent_workers=self.data_config['persistent_workers']\n",
    "            )\n",
    "            \n",
    "            # Optionally limit samples for faster testing\n",
    "            if limit_samples:\n",
    "                # Create limited dataset view\n",
    "                print(f\"  üìè Limiting to {limit_samples} samples per loader for testing\")\n",
    "                \n",
    "            return train_loader, val_loader\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error creating dataloaders: {e}\")\n",
    "            return None, None\n",
    "    \n",
    "    def test_batch_size_with_real_data(self, batch_size, test_batches=10, test_epochs=2):\n",
    "        \"\"\"Test batch size with real ImageNet data.\"\"\"\n",
    "        print(f\"\\nüß™ Testing batch size {batch_size} with real ImageNet data...\")\n",
    "        \n",
    "        # Clear memory\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "        try:\n",
    "            # Create model\n",
    "            print(f\"  üèóÔ∏è  Creating model...\")\n",
    "            model = self.model_class(num_classes=1000, device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "            device = next(model.parameters()).device\n",
    "            \n",
    "            # Create dataloaders\n",
    "            print(f\"  üìä Creating dataloaders...\")\n",
    "            train_loader, val_loader = self.create_dataloaders(batch_size)\n",
    "            \n",
    "            if train_loader is None:\n",
    "                return {'batch_size': batch_size, 'success': False, 'error': 'Failed to create dataloaders'}\n",
    "            \n",
    "            # Setup training components\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "            \n",
    "            # Timing and memory tracking\n",
    "            data_loading_times = []\n",
    "            forward_times = []\n",
    "            backward_times = []\n",
    "            total_batch_times = []\n",
    "            memory_peaks = []\n",
    "            gradient_norms = []\n",
    "            \n",
    "            print(f\"  üöÄ Starting training test...\")\n",
    "            model.train()\n",
    "            \n",
    "            for epoch in range(test_epochs):\n",
    "                batch_count = 0\n",
    "                epoch_start = time.time()\n",
    "                \n",
    "                for batch_idx, batch_data in enumerate(train_loader):\n",
    "                    if batch_count >= test_batches:\n",
    "                        break\n",
    "                        \n",
    "                    batch_start = time.time()\n",
    "                    \n",
    "                    # Extract data (measure data loading time)\n",
    "                    data_start = time.time()\n",
    "                    rgb_data, brightness_data, targets = batch_data\n",
    "                    rgb_data = rgb_data.to(device, non_blocking=True)\n",
    "                    brightness_data = brightness_data.to(device, non_blocking=True) \n",
    "                    targets = targets.to(device, non_blocking=True)\n",
    "                    data_end = time.time()\n",
    "                    data_loading_times.append(data_end - data_start)\n",
    "                    \n",
    "                    # Forward pass\n",
    "                    forward_start = time.time()\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(rgb_data, brightness_data)\n",
    "                    loss = criterion(outputs, targets)\n",
    "                    forward_end = time.time()\n",
    "                    forward_times.append(forward_end - forward_start)\n",
    "                    \n",
    "                    # Backward pass\n",
    "                    backward_start = time.time()\n",
    "                    loss.backward()\n",
    "                    \n",
    "                    # Calculate gradient norm\n",
    "                    total_norm = 0\n",
    "                    for p in model.parameters():\n",
    "                        if p.grad is not None:\n",
    "                            param_norm = p.grad.data.norm(2)\n",
    "                            total_norm += param_norm.item() ** 2\n",
    "                    gradient_norms.append(total_norm ** 0.5)\n",
    "                    \n",
    "                    optimizer.step()\n",
    "                    backward_end = time.time()\n",
    "                    backward_times.append(backward_end - backward_start)\n",
    "                    \n",
    "                    # Total batch time\n",
    "                    batch_end = time.time()\n",
    "                    total_batch_times.append(batch_end - batch_start)\n",
    "                    \n",
    "                    # Memory tracking\n",
    "                    if torch.cuda.is_available():\n",
    "                        memory_peaks.append(torch.cuda.memory_allocated() / 1e9)\n",
    "                    \n",
    "                    batch_count += 1\n",
    "                    \n",
    "                    if batch_count % 5 == 0:\n",
    "                        print(f\"    Batch {batch_count}/{test_batches} - \"\n",
    "                              f\"Total: {batch_end - batch_start:.3f}s, \"\n",
    "                              f\"Data: {data_end - data_start:.3f}s, \"\n",
    "                              f\"Forward: {forward_end - forward_start:.3f}s\")\n",
    "                \n",
    "                epoch_end = time.time()\n",
    "                print(f\"  üìä Epoch {epoch + 1}/{test_epochs} completed in {epoch_end - epoch_start:.2f}s\")\n",
    "            \n",
    "            # Calculate comprehensive metrics\n",
    "            avg_data_time = np.mean(data_loading_times)\n",
    "            avg_forward_time = np.mean(forward_times)\n",
    "            avg_backward_time = np.mean(backward_times)\n",
    "            avg_total_time = np.mean(total_batch_times)\n",
    "            \n",
    "            # Performance metrics\n",
    "            samples_per_second = batch_size / avg_total_time\n",
    "            data_loading_overhead = (avg_data_time / avg_total_time) * 100\n",
    "            compute_time = avg_forward_time + avg_backward_time\n",
    "            compute_efficiency = (compute_time / avg_total_time) * 100\n",
    "            \n",
    "            # Memory metrics\n",
    "            peak_memory = max(memory_peaks) if memory_peaks else 0\n",
    "            avg_gradient_norm = np.mean(gradient_norms)\n",
    "            gradient_stability = np.std(gradient_norms) / (avg_gradient_norm + 1e-8)\n",
    "            \n",
    "            results = {\n",
    "                'batch_size': batch_size,\n",
    "                'success': True,\n",
    "                'samples_per_second': samples_per_second,\n",
    "                'avg_total_time_per_batch': avg_total_time,\n",
    "                'avg_data_loading_time': avg_data_time,\n",
    "                'avg_forward_time': avg_forward_time,\n",
    "                'avg_backward_time': avg_backward_time,\n",
    "                'data_loading_overhead_pct': data_loading_overhead,\n",
    "                'compute_efficiency_pct': compute_efficiency,\n",
    "                'peak_memory_gb': peak_memory,\n",
    "                'avg_gradient_norm': avg_gradient_norm,\n",
    "                'gradient_stability': gradient_stability,\n",
    "                'io_vs_compute_ratio': avg_data_time / compute_time if compute_time > 0 else 0\n",
    "            }\n",
    "            \n",
    "            print(f\"  ‚úÖ Success!\")\n",
    "            print(f\"     Samples/sec: {samples_per_second:.1f}\")\n",
    "            print(f\"     Data loading: {data_loading_overhead:.1f}% of total time\")\n",
    "            print(f\"     Compute efficiency: {compute_efficiency:.1f}%\")\n",
    "            print(f\"     Peak memory: {peak_memory:.2f} GB\")\n",
    "            print(f\"     I/O vs Compute ratio: {results['io_vs_compute_ratio']:.2f}\")\n",
    "            \n",
    "            return results\n",
    "            \n",
    "        except RuntimeError as e:\n",
    "            if \"out of memory\" in str(e).lower():\n",
    "                print(f\"  ‚ùå Out of memory!\")\n",
    "                return {'batch_size': batch_size, 'success': False, 'error': 'OOM'}\n",
    "            else:\n",
    "                print(f\"  ‚ùå Runtime error: {e}\")\n",
    "                return {'batch_size': batch_size, 'success': False, 'error': str(e)}\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå Unexpected error: {e}\")\n",
    "            return {'batch_size': batch_size, 'success': False, 'error': str(e)}\n",
    "        \n",
    "        finally:\n",
    "            # Cleanup\n",
    "            if 'model' in locals():\n",
    "                del model\n",
    "            if 'train_loader' in locals():\n",
    "                del train_loader\n",
    "            if 'val_loader' in locals():\n",
    "                del val_loader\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "    \n",
    "    def run_real_data_optimization(self, start_batch=8, max_batch=256, multiplier=2):\n",
    "        \"\"\"Run batch size optimization with real ImageNet data.\"\"\"\n",
    "        print(f\"\\nüîç REAL DATA BATCH SIZE OPTIMIZATION\")\n",
    "        print(f\"Testing range: {start_batch} to {max_batch}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        all_results = []\n",
    "        current_batch = start_batch\n",
    "        max_successful_batch = 0\n",
    "        \n",
    "        while current_batch <= max_batch:\n",
    "            result = self.test_batch_size_with_real_data(current_batch)\n",
    "            all_results.append(result)\n",
    "            \n",
    "            if result['success']:\n",
    "                max_successful_batch = current_batch\n",
    "                # Store results for analysis\n",
    "                for key, value in result.items():\n",
    "                    if key != 'success':\n",
    "                        self.results[key].append(value)\n",
    "            else:\n",
    "                print(f\"  üíÄ Stopping at batch size {current_batch}: {result.get('error', 'Unknown error')}\")\n",
    "                break\n",
    "                \n",
    "            current_batch *= multiplier\n",
    "        \n",
    "        print(f\"\\nüìä REAL DATA TESTING COMPLETE\")\n",
    "        print(f\"Maximum successful batch size: {max_successful_batch}\")\n",
    "        \n",
    "        return all_results, max_successful_batch\n",
    "    \n",
    "    def analyze_real_data_results(self):\n",
    "        \"\"\"Analyze real data results with I/O considerations.\"\"\"\n",
    "        if not self.results['batch_size']:\n",
    "            print(\"‚ùå No successful results to analyze\")\n",
    "            return None\n",
    "            \n",
    "        print(f\"\\nüìà REAL DATA PERFORMANCE ANALYSIS\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        batch_sizes = self.results['batch_size']\n",
    "        throughputs = self.results['samples_per_second']\n",
    "        data_overhead = self.results['data_loading_overhead_pct']\n",
    "        compute_efficiency = self.results['compute_efficiency_pct']\n",
    "        io_compute_ratios = self.results['io_vs_compute_ratio']\n",
    "        \n",
    "        # Find optimal configurations\n",
    "        max_throughput_idx = np.argmax(throughputs)\n",
    "        min_data_overhead_idx = np.argmin(data_overhead)\n",
    "        best_efficiency_idx = np.argmax(compute_efficiency)\n",
    "        best_io_balance_idx = np.argmin([abs(ratio - 0.1) for ratio in io_compute_ratios])  # Target 10% I/O overhead\n",
    "        \n",
    "        print(f\"üöÄ Maximum Throughput Configuration:\")\n",
    "        print(f\"   Batch Size: {batch_sizes[max_throughput_idx]}\")\n",
    "        print(f\"   Throughput: {throughputs[max_throughput_idx]:.1f} samples/sec\")\n",
    "        print(f\"   Data Loading Overhead: {data_overhead[max_throughput_idx]:.1f}%\")\n",
    "        print(f\"   Compute Efficiency: {compute_efficiency[max_throughput_idx]:.1f}%\")\n",
    "        \n",
    "        print(f\"\\n‚ö° Best I/O Efficiency Configuration:\")\n",
    "        print(f\"   Batch Size: {batch_sizes[min_data_overhead_idx]}\")\n",
    "        print(f\"   Data Loading Overhead: {data_overhead[min_data_overhead_idx]:.1f}%\")\n",
    "        print(f\"   Throughput: {throughputs[min_data_overhead_idx]:.1f} samples/sec\")\n",
    "        \n",
    "        print(f\"\\nüéØ Best Overall Balance Configuration:\")\n",
    "        print(f\"   Batch Size: {batch_sizes[best_io_balance_idx]}\")\n",
    "        print(f\"   I/O vs Compute Ratio: {io_compute_ratios[best_io_balance_idx]:.2f}\")\n",
    "        print(f\"   Throughput: {throughputs[best_io_balance_idx]:.1f} samples/sec\")\n",
    "        print(f\"   Data Overhead: {data_overhead[best_io_balance_idx]:.1f}%\")\n",
    "        \n",
    "        # Bottleneck analysis\n",
    "        avg_data_overhead = np.mean(data_overhead)\n",
    "        if avg_data_overhead > 30:\n",
    "            print(f\"\\n‚ö†Ô∏è  I/O BOTTLENECK DETECTED!\")\n",
    "            print(f\"   Average data loading overhead: {avg_data_overhead:.1f}%\")\n",
    "            print(f\"   Consider: More workers, faster storage, or data preprocessing\")\n",
    "        elif avg_data_overhead < 5:\n",
    "            print(f\"\\n‚úÖ COMPUTE BOUND (Good!)\")\n",
    "            print(f\"   Data loading overhead: {avg_data_overhead:.1f}%\")\n",
    "            print(f\"   GPU is the limiting factor - optimal for training\")\n",
    "        else:\n",
    "            print(f\"\\n‚úÖ WELL BALANCED\")\n",
    "            print(f\"   Data loading overhead: {avg_data_overhead:.1f}%\")\n",
    "            print(f\"   Good balance between I/O and compute\")\n",
    "        \n",
    "        return {\n",
    "            'max_batch_size': max(batch_sizes),\n",
    "            'optimal_for_throughput': batch_sizes[max_throughput_idx],\n",
    "            'optimal_for_io_efficiency': batch_sizes[min_data_overhead_idx],\n",
    "            'optimal_for_balance': batch_sizes[best_io_balance_idx],\n",
    "            'avg_data_overhead': avg_data_overhead\n",
    "        }\n",
    "    \n",
    "    def plot_real_data_results(self):\n",
    "        \"\"\"Plot real data performance results.\"\"\"\n",
    "        if not self.results['batch_size']:\n",
    "            print(\"‚ùå No results to plot\")\n",
    "            return\n",
    "            \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        fig.suptitle('Real ImageNet Data - Batch Size Performance Analysis', fontsize=16)\n",
    "        \n",
    "        batch_sizes = self.results['batch_size']\n",
    "        \n",
    "        # Throughput vs Batch Size\n",
    "        axes[0, 0].plot(batch_sizes, self.results['samples_per_second'], 'b-o', linewidth=2, markersize=8)\n",
    "        axes[0, 0].set_xlabel('Batch Size')\n",
    "        axes[0, 0].set_ylabel('Samples/Second')\n",
    "        axes[0, 0].set_title('Training Throughput (Real Data)')\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        axes[0, 0].set_xscale('log', base=2)\n",
    "        \n",
    "        # Data Loading Overhead vs Batch Size\n",
    "        axes[0, 1].plot(batch_sizes, self.results['data_loading_overhead_pct'], 'r-o', linewidth=2, markersize=8)\n",
    "        axes[0, 1].set_xlabel('Batch Size')\n",
    "        axes[0, 1].set_ylabel('Data Loading Overhead (%)')\n",
    "        axes[0, 1].set_title('I/O Overhead')\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        axes[0, 1].set_xscale('log', base=2)\n",
    "        axes[0, 1].axhline(y=10, color='orange', linestyle='--', alpha=0.7, label='Target 10%')\n",
    "        axes[0, 1].legend()\n",
    "        \n",
    "        # Compute Efficiency vs Batch Size\n",
    "        axes[1, 0].plot(batch_sizes, self.results['compute_efficiency_pct'], 'g-o', linewidth=2, markersize=8)\n",
    "        axes[1, 0].set_xlabel('Batch Size')\n",
    "        axes[1, 0].set_ylabel('Compute Efficiency (%)')\n",
    "        axes[1, 0].set_title('GPU Utilization Efficiency')\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        axes[1, 0].set_xscale('log', base=2)\n",
    "        \n",
    "        # I/O vs Compute Ratio\n",
    "        axes[1, 1].plot(batch_sizes, self.results['io_vs_compute_ratio'], 'm-o', linewidth=2, markersize=8)\n",
    "        axes[1, 1].set_xlabel('Batch Size')\n",
    "        axes[1, 1].set_ylabel('I/O vs Compute Ratio')\n",
    "        axes[1, 1].set_title('I/O vs Compute Balance')\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        axes[1, 1].set_xscale('log', base=2)\n",
    "        axes[1, 1].axhline(y=0.1, color='orange', linestyle='--', alpha=0.7, label='Ideal (~0.1)')\n",
    "        axes[1, 1].legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "print(\"üéØ REAL IMAGENET DATA BATCH SIZE TESTER READY\")\n",
    "print(\"\\nüí° To use this tester:\")\n",
    "print(\"1. Update data paths in the data_config if needed\")\n",
    "print(\"2. Import your mc_resnet50 model\")  \n",
    "print(\"3. Run: real_tester = RealDataBatchSizeTester(mc_resnet50)\")\n",
    "print(\"4. Run: results, max_batch = real_tester.run_real_data_optimization()\")\n",
    "print(\"5. Run: recommendations = real_tester.analyze_real_data_results()\")\n",
    "print(\"6. Run: real_tester.plot_real_data_results()\")\n",
    "\n",
    "print(\"\\nüîç This version tests:\")\n",
    "print(\"   üìä Real data loading performance with your ImageNet dataset\")\n",
    "print(\"   ‚ö° I/O overhead vs GPU compute time balance\")\n",
    "print(\"   üöÄ End-to-end training pipeline performance\")\n",
    "print(\"   üíæ Memory usage with actual image data\")\n",
    "print(\"   üéØ Realistic batch size recommendations for production training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d64a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Real ImageNet Data Batch Size Testing\n",
    "print(\"üìä EXAMPLE: REAL IMAGENET BATCH SIZE TESTING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# NOTE: Uncomment to run real data batch size testing\n",
    "\n",
    "\"\"\"\n",
    "# Step 1: Configure your data paths\n",
    "data_config = {\n",
    "    'train_folders': ['data/ImageNet-1K/train_images_0'],  # Update to your actual path\n",
    "    'val_folder': 'data/ImageNet-1K/val_images',          # Update to your actual path  \n",
    "    'truth_file': 'data/ImageNet/ILSVRC2012_validation_ground_truth.txt',  # Update to your actual path\n",
    "    'image_size': (224, 224),\n",
    "    'num_workers': 4,        # Adjust based on your CPU cores\n",
    "    'pin_memory': True,\n",
    "    'persistent_workers': True\n",
    "}\n",
    "\n",
    "# Step 2: Import your MC-ResNet model\n",
    "from src.models2.multi_channel.mc_resnet import mc_resnet50\n",
    "\n",
    "# Step 3: Create the real data tester\n",
    "print(\"üîß Creating real data batch size tester...\")\n",
    "real_tester = RealDataBatchSizeTester(mc_resnet50, data_config=data_config)\n",
    "\n",
    "# Step 4: Run the optimization with real data\n",
    "print(\"üß™ Starting real data batch size optimization...\")\n",
    "# Conservative range for safety with real I/O\n",
    "real_results, max_real_batch = real_tester.run_real_data_optimization(\n",
    "    start_batch=8,\n",
    "    max_batch=128,    # Start conservative with real data\n",
    "    multiplier=2\n",
    ")\n",
    "\n",
    "# Step 5: Analyze real data results\n",
    "print(\"üìä Analyzing real data performance...\")\n",
    "real_recommendations = real_tester.analyze_real_data_results()\n",
    "\n",
    "# Step 6: Plot real data performance\n",
    "print(\"üìà Creating real data performance plots...\")\n",
    "real_tester.plot_real_data_results()\n",
    "\n",
    "# Step 7: Compare with synthetic data results (if available)\n",
    "print(f\"\\\\nüîÑ REAL vs SYNTHETIC DATA COMPARISON:\")\n",
    "if 'recommendations' in locals() and real_recommendations:\n",
    "    print(f\"   Synthetic Data Optimal: {recommendations['recommended_batch_size']}\")\n",
    "    print(f\"   Real Data Optimal: {real_recommendations['optimal_for_balance']}\")\n",
    "    print(f\"   Difference: {abs(recommendations['recommended_batch_size'] - real_recommendations['optimal_for_balance'])}\")\n",
    "    \n",
    "    if real_recommendations['avg_data_overhead'] > 20:\n",
    "        print(f\"   ‚ö†Ô∏è  Real data shows high I/O overhead ({real_recommendations['avg_data_overhead']:.1f}%)\")\n",
    "        print(f\"   üí° Consider: faster storage, more workers, or data preprocessing\")\n",
    "    else:\n",
    "        print(f\"   ‚úÖ Real data I/O overhead is acceptable ({real_recommendations['avg_data_overhead']:.1f}%)\")\n",
    "\n",
    "print(f\"\\\\nüéØ FINAL REAL DATA RECOMMENDATIONS:\")\n",
    "if real_recommendations:\n",
    "    print(f\"   üèÜ Best Overall Balance: {real_recommendations['optimal_for_balance']}\")\n",
    "    print(f\"   üöÄ Maximum Throughput: {real_recommendations['optimal_for_throughput']}\")\n",
    "    print(f\"   ‚ö° Best I/O Efficiency: {real_recommendations['optimal_for_io_efficiency']}\")\n",
    "    print(f\"   üíæ Maximum Batch Size: {real_recommendations['max_batch_size']}\")\n",
    "\"\"\"\n",
    "\n",
    "print(\"üîß TO RUN REAL DATA TESTING:\")\n",
    "print(\"1. Update the data paths in data_config above to match your setup\")\n",
    "print(\"2. Ensure your ImageNet data is accessible\")\n",
    "print(\"3. Uncomment the code block above\")\n",
    "print(\"4. Run the cell\")\n",
    "\n",
    "print(\"\\nüìä REAL DATA TESTING MEASURES:\")\n",
    "real_data_metrics = [\n",
    "    \"üöÄ End-to-End Training Throughput (samples/sec with real I/O)\",\n",
    "    \"üìÅ Data Loading Overhead (% of total time spent on I/O)\",  \n",
    "    \"‚ö° Compute Efficiency (% of time spent on actual training)\",\n",
    "    \"üîÑ I/O vs Compute Balance (ratio of data loading to GPU time)\",\n",
    "    \"üíæ Real Memory Usage (with actual ImageNet images)\",\n",
    "    \"üéØ Worker Efficiency (multi-processing data loading performance)\"\n",
    "]\n",
    "\n",
    "for metric in real_data_metrics:\n",
    "    print(f\"   {metric}\")\n",
    "\n",
    "print(\"\\nüéØ WHY REAL DATA TESTING MATTERS:\")\n",
    "real_data_benefits = [\n",
    "    \"üîç Identifies I/O bottlenecks that synthetic data can't reveal\",\n",
    "    \"üìä Measures actual preprocessing and augmentation overhead\", \n",
    "    \"üíæ Tests real memory patterns with diverse image sizes/content\",\n",
    "    \"‚ö° Optimizes num_workers and pin_memory settings\",\n",
    "    \"üöÄ Provides production-ready batch size recommendations\",\n",
    "    \"üîÑ Balances data loading speed vs GPU utilization\"\n",
    "]\n",
    "\n",
    "for benefit in real_data_benefits:\n",
    "    print(f\"   {benefit}\")\n",
    "\n",
    "print(\"\\nüí° TYPICAL FINDINGS:\")\n",
    "print(\"   ‚Ä¢ Real data usually supports smaller optimal batch sizes due to I/O overhead\")\n",
    "print(\"   ‚Ä¢ Batch sizes 64-128 often optimal for ImageNet on most GPUs\")  \n",
    "print(\"   ‚Ä¢ I/O overhead >30% indicates need for more workers or faster storage\")\n",
    "print(\"   ‚Ä¢ I/O overhead <5% means you're GPU-bound (ideal for training)\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(\"üèÅ Ready to test with your real ImageNet data!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0eb6b75",
   "metadata": {},
   "source": [
    "# ‚úÖ Successful Large Batch Training Configuration\n",
    "\n",
    "## Batch Size 256 Training Success\n",
    "\n",
    "Successfully trained MC-ResNet50 with **batch size 256** on Google Colab without CUDA out-of-memory errors! This validates our streaming dual-channel dataloader implementation and the new `val_batch_size` parameter functionality.\n",
    "\n",
    "### Key Configuration:\n",
    "- **Batch Size**: 256 (maximum tested)\n",
    "- **Model**: MC-ResNet50 with dual-channel input (RGB + Brightness)\n",
    "- **Dataset**: ImageNet with StreamingDualChannelDataset\n",
    "- **Environment**: Google Colab (GPU runtime)\n",
    "- **Memory Management**: `torch.cuda.empty_cache()` before training\n",
    "\n",
    "### Training Parameters:\n",
    "- **Optimizer**: AdamW\n",
    "- **Learning Rate**: 0.1 \n",
    "- **Weight Decay**: 1e-5\n",
    "- **Scheduler**: OneCycle\n",
    "- **Workers**: 2 (for notebook stability)\n",
    "- **Pin Memory**: True\n",
    "- **Persistent Workers**: True\n",
    "- **Prefetch Factor**: 2\n",
    "\n",
    "This configuration demonstrates that our streaming dataloader can handle large batch sizes efficiently without memory issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcaf600",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import beyond top-level package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlr_scheduler\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OneCycleLR\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_utils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstreaming_dual_channel_dataset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m create_imagenet_dual_channel_train_val_dataloaders\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmulti_channel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmc_resnet\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mc_resnet50\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Clear CUDA cache before starting\u001b[39;00m\n\u001b[1;32m     10\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n",
      "File \u001b[0;32m~/Documents/projects/Multi-Stream-Neural-Networks/src/models/__init__.py:6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03mModels module for Multi-Stream Neural Networks\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseMultiStreamModel\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbasic_multi_channel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      7\u001b[0m     BaseMultiChannelNetwork,\n\u001b[1;32m      8\u001b[0m     MultiChannelResNetNetwork,\n\u001b[1;32m      9\u001b[0m     base_multi_channel_small,\n\u001b[1;32m     10\u001b[0m     base_multi_channel_medium,\n\u001b[1;32m     11\u001b[0m     base_multi_channel_large,\n\u001b[1;32m     12\u001b[0m     multi_channel_resnet18,\n\u001b[1;32m     13\u001b[0m     multi_channel_resnet34,\n\u001b[1;32m     14\u001b[0m     multi_channel_resnet50,\n\u001b[1;32m     15\u001b[0m     multi_channel_resnet101,\n\u001b[1;32m     16\u001b[0m     multi_channel_resnet152\n\u001b[1;32m     17\u001b[0m )\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbuilders\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_factory\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m create_model, list_available_models\n\u001b[1;32m     20\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBaseMultiStreamModel\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBaseMultiChannelNetwork\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlist_available_models\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     34\u001b[0m ]\n",
      "File \u001b[0;32m~/Documents/projects/Multi-Stream-Neural-Networks/src/models/basic_multi_channel/__init__.py:9\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"Basic multi-channel model package.\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase_multi_channel_network\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      4\u001b[0m     BaseMultiChannelNetwork,\n\u001b[1;32m      5\u001b[0m     base_multi_channel_small,\n\u001b[1;32m      6\u001b[0m     base_multi_channel_medium, \n\u001b[1;32m      7\u001b[0m     base_multi_channel_large\n\u001b[1;32m      8\u001b[0m )\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmulti_channel_resnet_network\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     10\u001b[0m     MultiChannelResNetNetwork,\n\u001b[1;32m     11\u001b[0m     multi_channel_resnet18,\n\u001b[1;32m     12\u001b[0m     multi_channel_resnet34,\n\u001b[1;32m     13\u001b[0m     multi_channel_resnet50,\n\u001b[1;32m     14\u001b[0m     multi_channel_resnet101,\n\u001b[1;32m     15\u001b[0m     multi_channel_resnet152\n\u001b[1;32m     16\u001b[0m )\n\u001b[1;32m     18\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m# New modular architecture\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBaseMultiChannelNetwork\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmulti_channel_resnet152\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     30\u001b[0m ]\n",
      "File \u001b[0;32m~/Documents/projects/Multi-Stream-Neural-Networks/src/models/basic_multi_channel/multi_channel_resnet_network.py:38\u001b[0m\n\u001b[1;32m     35\u001b[0m tqdm \u001b[38;5;241m=\u001b[39m _get_tqdm()\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dict, List, Any, Union\n\u001b[0;32m---> 38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseMultiStreamModel\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgrad_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m safe_clip_grad_norm\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconv_layers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     41\u001b[0m     MultiChannelConv2d,\n\u001b[1;32m     42\u001b[0m     MultiChannelBatchNorm2d,\n\u001b[1;32m     43\u001b[0m     MultiChannelActivation,\n\u001b[1;32m     44\u001b[0m     MultiChannelAdaptiveAvgPool2d\n\u001b[1;32m     45\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/projects/Multi-Stream-Neural-Networks/src/models/__init__.py:5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03mModels module for Multi-Stream Neural Networks\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseMultiStreamModel\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbasic_multi_channel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      7\u001b[0m     BaseMultiChannelNetwork,\n\u001b[1;32m      8\u001b[0m     MultiChannelResNetNetwork,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m     multi_channel_resnet152\n\u001b[1;32m     17\u001b[0m )\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbuilders\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_factory\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m create_model, list_available_models\n",
      "File \u001b[0;32m~/Documents/projects/Multi-Stream-Neural-Networks/src/models/base.py:8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdevice_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DeviceManager\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mamp\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GradScaler\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m optim\n",
      "\u001b[0;31mImportError\u001b[0m: attempted relative import beyond top-level package"
     ]
    }
   ],
   "source": [
    "# Working Configuration for Batch Size 256 Training on Colab\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from src.data_utils.streaming_dual_channel_dataset import create_imagenet_dual_channel_train_val_dataloaders\n",
    "from src.models.multi_channel.mc_resnet import mc_resnet50\n",
    "\n",
    "# Clear CUDA cache before starting\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Configuration that works for batch size 256\n",
    "config = {\n",
    "    'batch_size': 256,\n",
    "    'val_batch_size': 256,  # Slightly smaller for validation to save memory\n",
    "    'num_workers': 2,\n",
    "    'pin_memory': True,\n",
    "    'persistent_workers': True,\n",
    "    'prefetch_factor': 2,\n",
    "}\n",
    "\n",
    "# Create dataloaders with the new val_batch_size parameter\n",
    "train_loader, val_loader = create_imagenet_dual_channel_train_val_dataloaders(\n",
    "    data_root=\"data/ImageNet\",\n",
    "    batch_size=256,\n",
    "    val_batch_size=256,  # Use separate validation batch size\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    "    prefetch_factor=2,\n",
    ")\n",
    "\n",
    "# Initialize model for dual-channel input\n",
    "model = mc_resnet50(num_classes=1000, in_channels=4)  # RGB + Brightness = 4 channels\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# Optimizer and scheduler configuration\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.1, weight_decay=1e-5)\n",
    "scheduler = OneCycleLR(optimizer, max_lr=0.1, epochs=10, steps_per_epoch=len(train_loader))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(f\"‚úÖ Successfully configured training with batch size {config['batch_size']}\")\n",
    "print(f\"üìä Training batches: {len(train_loader)}, Validation batches: {len(val_loader)}\")\n",
    "print(f\"üéØ Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"üíæ CUDA memory allocated: {torch.cuda.memory_allocated()/1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1f5ceb",
   "metadata": {},
   "source": [
    "## üí° Best Practices for Large Batch Training\n",
    "\n",
    "### Memory Management Tips:\n",
    "1. **Clear CUDA cache**: Always run `torch.cuda.empty_cache()` before training\n",
    "2. **Separate validation batch size**: Use smaller `val_batch_size` than training batch size\n",
    "3. **Gradient accumulation**: For even larger effective batch sizes, use gradient accumulation\n",
    "4. **Monitor memory**: Check `torch.cuda.memory_allocated()` regularly\n",
    "5. **Dataloader workers**: Keep `num_workers=2` in notebooks for stability\n",
    "\n",
    "### Configuration Guidelines:\n",
    "- **Training batch size**: Start with 128, increase to 256 if memory allows\n",
    "- **Validation batch size**: Use 50-75% of training batch size\n",
    "- **Persistent workers**: Always `True` for better performance\n",
    "- **Pin memory**: Always `True` when using GPU\n",
    "- **Prefetch factor**: 2 works well for most cases\n",
    "\n",
    "### Troubleshooting CUDA OOM:\n",
    "```python\n",
    "# If you encounter CUDA out-of-memory errors:\n",
    "torch.cuda.empty_cache()  # Clear cache\n",
    "# Reduce batch_size by half\n",
    "# Reduce val_batch_size even further\n",
    "# Consider gradient accumulation instead\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f18c7d7",
   "metadata": {},
   "source": [
    "# üß† Memory Management for Dual-Stream Training\n",
    "\n",
    "## CUDA Out of Memory Solutions\n",
    "\n",
    "When training with MCConv2d (dual-stream), memory usage is naturally ~2x higher than single-stream Conv2d. This is **expected behavior** and doesn't indicate a problem with our implementation.\n",
    "\n",
    "### Memory-Efficient Training Strategies:\n",
    "\n",
    "1. **Reduce Batch Size**: Most effective solution\n",
    "2. **Gradient Accumulation**: Achieve larger effective batch sizes\n",
    "3. **Mixed Precision**: Use `torch.amp.autocast()`\n",
    "4. **Memory Cleanup**: Clear cache between operations\n",
    "5. **Sequential Processing**: Process streams separately if needed\n",
    "\n",
    "The key is maintaining the **exact same dual-stream computation** while managing memory more efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85f4003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory Management Utilities for Dual-Stream Training\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "def get_gpu_memory_info():\n",
    "    \"\"\"Get current GPU memory usage.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / 1024**3  # GB\n",
    "        reserved = torch.cuda.memory_reserved() / 1024**3    # GB\n",
    "        max_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3  # GB\n",
    "        \n",
    "        print(f\"GPU Memory - Allocated: {allocated:.2f} GB\")\n",
    "        print(f\"GPU Memory - Reserved: {reserved:.2f} GB\") \n",
    "        print(f\"GPU Memory - Total: {max_memory:.2f} GB\")\n",
    "        print(f\"GPU Memory - Free: {max_memory - reserved:.2f} GB\")\n",
    "        return allocated, reserved, max_memory\n",
    "    return 0, 0, 0\n",
    "\n",
    "def clear_gpu_memory():\n",
    "    \"\"\"Aggressive GPU memory cleanup.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "        gc.collect()\n",
    "        print(\"üßπ GPU memory cleared\")\n",
    "\n",
    "def estimate_mcconv2d_memory(batch_size, color_channels, brightness_channels, \n",
    "                            height, width, out_channels):\n",
    "    \"\"\"\n",
    "    Estimate memory usage for MCConv2d layer.\n",
    "    This helps determine safe batch sizes.\n",
    "    \"\"\"\n",
    "    # Input tensors\n",
    "    color_input_mb = batch_size * color_channels * height * width * 4 / 1024**2\n",
    "    brightness_input_mb = batch_size * brightness_channels * height * width * 4 / 1024**2\n",
    "    \n",
    "    # Output tensors  \n",
    "    color_output_mb = batch_size * out_channels * height * width * 4 / 1024**2\n",
    "    brightness_output_mb = batch_size * out_channels * height * width * 4 / 1024**2\n",
    "    \n",
    "    # Gradients (roughly same size as tensors)\n",
    "    total_mb = (color_input_mb + brightness_input_mb + \n",
    "                color_output_mb + brightness_output_mb) * 2  # *2 for gradients\n",
    "    \n",
    "    print(f\"Estimated MCConv2d memory usage: {total_mb:.1f} MB\")\n",
    "    return total_mb\n",
    "\n",
    "# Example usage\n",
    "print(\"Current GPU memory status:\")\n",
    "get_gpu_memory_info()\n",
    "\n",
    "# Estimate memory for typical batch\n",
    "estimate_mcconv2d_memory(\n",
    "    batch_size=64, \n",
    "    color_channels=3, \n",
    "    brightness_channels=1,\n",
    "    height=224, \n",
    "    width=224, \n",
    "    out_channels=64\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38709d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 1: Gradient Accumulation Training Loop\n",
    "# This maintains large effective batch size while using smaller mini-batches\n",
    "\n",
    "def train_with_gradient_accumulation(model, dataloader, optimizer, criterion, \n",
    "                                   accumulation_steps=4, device='cuda'):\n",
    "    \"\"\"\n",
    "    Training loop with gradient accumulation for memory-efficient dual-stream training.\n",
    "    \n",
    "    Args:\n",
    "        accumulation_steps: Number of mini-batches to accumulate before optimizer step\n",
    "                           Effective batch size = mini_batch_size * accumulation_steps\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    clear_gpu_memory()  # Start with clean memory\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    for batch_idx, (rgb_data, brightness_data, targets) in enumerate(dataloader):\n",
    "        # Move to device\n",
    "        rgb_data = rgb_data.to(device, non_blocking=True)\n",
    "        brightness_data = brightness_data.to(device, non_blocking=True) \n",
    "        targets = targets.to(device, non_blocking=True)\n",
    "        \n",
    "        # Forward pass - our MCConv2d works exactly like Conv2d\n",
    "        with torch.amp.autocast(device_type='cuda', enabled=True):  # Mixed precision\n",
    "            outputs = model(rgb_data, brightness_data)\n",
    "            loss = criterion(outputs, targets) / accumulation_steps  # Scale loss\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # Optimizer step after accumulation_steps\n",
    "        if (batch_idx + 1) % accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            if batch_idx % (accumulation_steps * 10) == 0:\n",
    "                print(f\"Batch {batch_idx}, Loss: {running_loss:.4f}\")\n",
    "                running_loss = 0.0\n",
    "        \n",
    "        # Periodic memory cleanup\n",
    "        if batch_idx % 50 == 0:\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Example configuration for memory-constrained training\n",
    "config = {\n",
    "    'mini_batch_size': 16,      # Small mini-batch that fits in memory\n",
    "    'accumulation_steps': 4,     # Effective batch size = 16 * 4 = 64\n",
    "    'mixed_precision': True,     # Reduces memory by ~50%\n",
    "    'num_workers': 2,           # Reduce CPU->GPU transfer overhead\n",
    "    'pin_memory': True,\n",
    "    'persistent_workers': True\n",
    "}\n",
    "\n",
    "print(\"Memory-efficient training configuration:\")\n",
    "print(f\"Mini-batch size: {config['mini_batch_size']}\")\n",
    "print(f\"Effective batch size: {config['mini_batch_size'] * config['accumulation_steps']}\")\n",
    "print(f\"Mixed precision: {config['mixed_precision']}\")\n",
    "print(\"\\n‚úÖ This maintains exact MCConv2d computation while managing memory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823aabde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 2: Adaptive Batch Size Finder\n",
    "# Automatically find the largest safe batch size for your GPU\n",
    "\n",
    "def find_max_batch_size(model, sample_color, sample_brightness, \n",
    "                       start_batch=1, max_batch=512, device='cuda'):\n",
    "    \"\"\"\n",
    "    Binary search to find maximum batch size that fits in GPU memory.\n",
    "    Maintains exact MCConv2d behavior while optimizing memory usage.\n",
    "    \"\"\"\n",
    "    model.eval()  # Disable dropout for consistent memory usage\n",
    "    clear_gpu_memory()\n",
    "    \n",
    "    def test_batch_size(batch_size):\n",
    "        try:\n",
    "            # Create test batch\n",
    "            color_batch = sample_color.repeat(batch_size, 1, 1, 1).to(device)\n",
    "            brightness_batch = sample_brightness.repeat(batch_size, 1, 1, 1).to(device)\n",
    "            \n",
    "            # Test forward pass\n",
    "            with torch.no_grad():\n",
    "                _ = model(color_batch, brightness_batch)\n",
    "            \n",
    "            # Test backward pass (uses more memory)\n",
    "            color_batch.requires_grad_(True)\n",
    "            brightness_batch.requires_grad_(True)\n",
    "            output = model(color_batch, brightness_batch)\n",
    "            loss = output.sum()\n",
    "            loss.backward()\n",
    "            \n",
    "            del color_batch, brightness_batch, output, loss\n",
    "            torch.cuda.empty_cache()\n",
    "            return True\n",
    "            \n",
    "        except RuntimeError as e:\n",
    "            if \"out of memory\" in str(e):\n",
    "                torch.cuda.empty_cache()\n",
    "                return False\n",
    "            else:\n",
    "                raise e\n",
    "    \n",
    "    # Binary search for max batch size\n",
    "    low, high = start_batch, max_batch\n",
    "    max_safe_batch = start_batch\n",
    "    \n",
    "    while low <= high:\n",
    "        mid = (low + high) // 2\n",
    "        print(f\"Testing batch size: {mid}\")\n",
    "        \n",
    "        if test_batch_size(mid):\n",
    "            max_safe_batch = mid\n",
    "            low = mid + 1\n",
    "        else:\n",
    "            high = mid - 1\n",
    "    \n",
    "    print(f\"‚úÖ Maximum safe batch size: {max_safe_batch}\")\n",
    "    return max_safe_batch\n",
    "\n",
    "# Solution 3: Memory Troubleshooting Guide\n",
    "def diagnose_memory_issue(model, dataloader, device='cuda'):\n",
    "    \"\"\"Diagnose what's using GPU memory in dual-stream training.\"\"\"\n",
    "    \n",
    "    print(\"üîç MEMORY DIAGNOSIS FOR DUAL-STREAM TRAINING\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Baseline memory\n",
    "    clear_gpu_memory()\n",
    "    baseline = torch.cuda.memory_allocated() / 1024**2\n",
    "    print(f\"Baseline memory: {baseline:.1f} MB\")\n",
    "    \n",
    "    # Model memory\n",
    "    model = model.to(device)\n",
    "    model_mem = torch.cuda.memory_allocated() / 1024**2 - baseline\n",
    "    print(f\"Model memory: {model_mem:.1f} MB\")\n",
    "    \n",
    "    # Sample batch memory\n",
    "    rgb_data, brightness_data, targets = next(iter(dataloader))\n",
    "    batch_size = rgb_data.shape[0]\n",
    "    \n",
    "    rgb_data = rgb_data.to(device)\n",
    "    brightness_data = brightness_data.to(device)\n",
    "    targets = targets.to(device)\n",
    "    \n",
    "    input_mem = torch.cuda.memory_allocated() / 1024**2 - baseline - model_mem\n",
    "    print(f\"Input batch memory (size {batch_size}): {input_mem:.1f} MB\")\n",
    "    \n",
    "    # Forward pass memory\n",
    "    outputs = model(rgb_data, brightness_data)\n",
    "    forward_mem = torch.cuda.memory_allocated() / 1024**2 - baseline - model_mem - input_mem\n",
    "    print(f\"Forward pass memory: {forward_mem:.1f} MB\")\n",
    "    \n",
    "    # Backward pass memory\n",
    "    loss = torch.nn.functional.cross_entropy(outputs, targets)\n",
    "    loss.backward()\n",
    "    backward_mem = torch.cuda.memory_allocated() / 1024**2 - baseline - model_mem - input_mem - forward_mem\n",
    "    print(f\"Backward pass memory: {backward_mem:.1f} MB\")\n",
    "    \n",
    "    total_per_sample = (input_mem + forward_mem + backward_mem) / batch_size\n",
    "    print(f\"\\nüìä Memory per sample: {total_per_sample:.1f} MB\")\n",
    "    \n",
    "    # Recommendations\n",
    "    max_memory = torch.cuda.get_device_properties(0).total_memory / 1024**2\n",
    "    safe_batch_size = int((max_memory * 0.8) / total_per_sample)  # Use 80% of GPU\n",
    "    \n",
    "    print(f\"\\nüí° RECOMMENDATIONS:\")\n",
    "    print(f\"‚Ä¢ Current batch size: {batch_size}\")\n",
    "    print(f\"‚Ä¢ Recommended max batch size: {safe_batch_size}\")\n",
    "    print(f\"‚Ä¢ Use gradient accumulation if you need larger effective batch sizes\")\n",
    "    print(f\"‚Ä¢ MCConv2d is working correctly - this is expected dual-stream memory usage\")\n",
    "\n",
    "print(\"üõ†Ô∏è Memory management tools ready!\")\n",
    "print(\"Use these functions to optimize your dual-stream training without changing MCConv2d\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
