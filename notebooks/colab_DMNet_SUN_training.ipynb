{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# DMNet Training on SUN RGB-D - Google Colab\n",
    "\n",
    "**Complete end-to-end training pipeline for Direct Mixing ResNet (DMNet) on Google Colab with A100 GPU**\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Checklist Before Running:\n",
    "\n",
    "- [ ] **Enable A100 GPU:** Runtime ‚Üí Change runtime type ‚Üí Hardware accelerator: GPU ‚Üí GPU type: A100\n",
    "- [ ] **Mount Google Drive:** Your code and dataset will be stored on Drive\n",
    "- [ ] **Upload dataset to Drive:** `MyDrive/datasets/sunrgbd_15/` (preprocessed 15-category dataset)\n",
    "- [ ] **Expected Runtime:** ~2-3 hours for training\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ What This Notebook Does:\n",
    "\n",
    "1. ‚úÖ Verify A100 GPU is available\n",
    "2. ‚úÖ Mount Google Drive\n",
    "3. ‚úÖ Clone your repository to local disk (fast I/O)\n",
    "4. ‚úÖ Copy SUN RGB-D dataset to local disk (10-20x faster than Drive)\n",
    "5. ‚úÖ Install dependencies\n",
    "6. ‚úÖ Train DMNet (Direct Mixing ResNet) with all optimizations\n",
    "7. ‚úÖ Save checkpoints to Drive (persistent storage)\n",
    "8. ‚úÖ Generate training curves and analysis\n",
    "\n",
    "---\n",
    "\n",
    "## üß† About DMNet:\n",
    "\n",
    "**DMNet** (Direct Mixing Network) is a 2-stream neural network architecture where:\n",
    "- **RGB stream** processes color images\n",
    "- **Depth stream** processes depth maps\n",
    "- **Integrated Stream** combines both streams using learned scalar mixing weights at every layer\n",
    "\n",
    "Unlike traditional fusion methods, DMNet performs integration **inside each convolution neuron** through scalar-based direct mixing:\n",
    "- Per-stream weights (full kernels for RGB and Depth)\n",
    "- Integrated weight (1√ó1 channel-wise for integrated features)\n",
    "- Scalar mixing coefficients (Œ±, Œ≥) learned per layer to combine stream outputs\n",
    "\n",
    "This allows the network to learn optimal integration strategies at every layer with minimal computational overhead!\n",
    "\n",
    "---\n",
    "\n",
    "**Let's get started!** üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-1"
   },
   "source": [
    "## 1. Environment Setup & GPU Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "check-gpu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "GPU VERIFICATION\n",
      "============================================================\n",
      "PyTorch version: 2.7.1\n",
      "CUDA available: False\n",
      "\n",
      "‚ùå NO GPU DETECTED!\n",
      "Please enable GPU: Runtime ‚Üí Change runtime type ‚Üí Hardware accelerator: GPU\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "GPU is required for training",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 31\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m‚ùå NO GPU DETECTED!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease enable GPU: Runtime ‚Üí Change runtime type ‚Üí Hardware accelerator: GPU\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 31\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPU is required for training\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m60\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: GPU is required for training"
     ]
    }
   ],
   "source": [
    "# Check GPU availability and specs\n",
    "import torch\n",
    "import subprocess\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"GPU VERIFICATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check PyTorch and CUDA\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "    \n",
    "    # Check if it's A100\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    if 'A100' in gpu_name:\n",
    "        print(\"\\n‚úÖ A100 GPU detected - PERFECT for training!\")\n",
    "    elif 'V100' in gpu_name:\n",
    "        print(\"\\n‚úÖ V100 GPU detected - Good for training (slower than A100)\")\n",
    "    elif 'T4' in gpu_name:\n",
    "        print(\"\\n‚ö†Ô∏è  T4 GPU detected - Will be slower, consider upgrading to A100\")\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è  GPU: {gpu_name} - Consider using A100 for best performance\")\n",
    "else:\n",
    "    print(\"\\n‚ùå NO GPU DETECTED!\")\n",
    "    print(\"Please enable GPU: Runtime ‚Üí Change runtime type ‚Üí Hardware accelerator: GPU\")\n",
    "    raise RuntimeError(\"GPU is required for training\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nvidia-smi"
   },
   "outputs": [],
   "source": [
    "# Detailed GPU info\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-2"
   },
   "source": [
    "## 2. Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mount-drive"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "print(\"\\n‚úÖ Google Drive mounted successfully!\")\n",
    "print(f\"\\nDrive contents:\")\n",
    "!ls -la /content/drive/MyDrive/ | head -20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-3"
   },
   "source": [
    "## 3. Clone Repository to Local Disk (Fast I/O)\n",
    "\n",
    "**Important:** We clone to `/content/` (local SSD) instead of Drive for 10-20x faster I/O\n",
    "\n",
    "**Default:** Clone from GitHub (recommended - always gets latest code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clone-repo"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Configuration\n",
    "PROJECT_NAME = \"Multi-Stream-Neural-Networks\"\n",
    "GITHUB_REPO = \"https://github.com/clingergab/Multi-Stream-Neural-Networks.git\"  # UPDATE THIS\n",
    "LOCAL_REPO_PATH = f\"/content/{PROJECT_NAME}\"  # Local copy for fast I/O\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"REPOSITORY SETUP\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Ensure we're in a valid directory\n",
    "os.chdir('/content')\n",
    "print(f\"Starting in: {os.getcwd()}\")\n",
    "\n",
    "# Check if repo already exists (same session, rerunning cell)\n",
    "if Path(LOCAL_REPO_PATH).exists() and Path(f\"{LOCAL_REPO_PATH}/.git\").exists():\n",
    "    print(f\"\\nüìÅ Repo already exists: {LOCAL_REPO_PATH}\")\n",
    "    print(f\"üîÑ Pulling latest changes...\")\n",
    "    \n",
    "    os.chdir(LOCAL_REPO_PATH)\n",
    "    !git pull\n",
    "    print(\"‚úÖ Repo updated\")\n",
    "\n",
    "# Clone from GitHub (first run)\n",
    "else:\n",
    "    # Remove old incomplete copy if exists\n",
    "    if Path(LOCAL_REPO_PATH).exists():\n",
    "        print(f\"\\nüóëÔ∏è  Removing incomplete repo copy...\")\n",
    "        !rm -rf {LOCAL_REPO_PATH}\n",
    "    \n",
    "    print(f\"\\nüîÑ Cloning from GitHub...\")\n",
    "    print(f\"   Repo: {GITHUB_REPO}\")\n",
    "    print(f\"   Destination: {LOCAL_REPO_PATH}\")\n",
    "    \n",
    "    !git clone {GITHUB_REPO} {LOCAL_REPO_PATH}\n",
    "    \n",
    "    # Verify clone succeeded\n",
    "    if not Path(LOCAL_REPO_PATH).exists():\n",
    "        raise RuntimeError(f\"Failed to clone repository to {LOCAL_REPO_PATH}\")\n",
    "    \n",
    "    print(\"‚úÖ Repo cloned successfully\")\n",
    "    os.chdir(LOCAL_REPO_PATH)\n",
    "\n",
    "# Verify repo structure\n",
    "print(f\"\\nüìÇ Repository structure:\")\n",
    "!ls -la {LOCAL_REPO_PATH}\n",
    "\n",
    "print(f\"\\n‚úÖ Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-4"
   },
   "source": [
    "## 4. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install-deps"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "print(\"Installing dependencies...\")\n",
    "\n",
    "!pip install -q h5py tqdm matplotlib seaborn ray[tune]\n",
    "\n",
    "# Verify installations\n",
    "import h5py\n",
    "import tqdm\n",
    "import matplotlib\n",
    "import seaborn\n",
    "import ray\n",
    "\n",
    "print(\"‚úÖ All dependencies installed!\")\n",
    "print(f\"   h5py: {h5py.__version__}\")\n",
    "print(f\"   matplotlib: {matplotlib.__version__}\")\n",
    "print(f\"   ray: {ray.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-5"
   },
   "source": [
    "## 5. Copy SUN RGB-D Dataset to Local Disk\n",
    "\n",
    "**Performance Note:** Local disk I/O is ~10-20x faster than Drive!\n",
    "\n",
    "**Dataset:** SUN RGB-D 15-category preprocessed dataset with RGB + Depth (~2.5 GB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download-dataset"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Paths\n",
    "DRIVE_DATASET_TAR = \"/content/drive/MyDrive/datasets/sunrgbd_15.tar.gz\"  # Compressed file (2-stream: RGB + Depth)\n",
    "LOCAL_DATASET_PATH = \"/content/data/sunrgbd_15\"  # Extracted location\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SUN RGB-D 15-CATEGORY DATASET SETUP (2-STREAM: RGB + DEPTH)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check if already on local disk\n",
    "if Path(LOCAL_DATASET_PATH).exists():\n",
    "    print(f\"‚úÖ Dataset already on local disk: {LOCAL_DATASET_PATH}\")\n",
    "    \n",
    "    # Verify structure\n",
    "    train_rgb_count = len(list(Path(f\"{LOCAL_DATASET_PATH}/train/rgb\").glob(\"*.png\")))\n",
    "    val_rgb_count = len(list(Path(f\"{LOCAL_DATASET_PATH}/val/rgb\").glob(\"*.png\")))\n",
    "    print(f\"   Train samples: {train_rgb_count}\")\n",
    "    print(f\"   Val samples: {val_rgb_count}\")\n",
    "\n",
    "# Copy and extract from Drive\n",
    "elif Path(DRIVE_DATASET_TAR).exists():\n",
    "    print(f\"üìÅ Found compressed dataset on Drive: {DRIVE_DATASET_TAR}\")\n",
    "    print(f\"üì• Copying compressed file to local disk...\")\n",
    "    \n",
    "    # Create parent directory\n",
    "    !mkdir -p /content/data\n",
    "    \n",
    "    # Copy compressed file with progress\n",
    "    print(f\"\\nCopying compressed archive...\")\n",
    "    !rsync -ah --info=progress2 {DRIVE_DATASET_TAR} /content/data/sunrgbd_15.tar.gz\n",
    "    \n",
    "    # Extract to local disk (suppress macOS metadata warnings)\n",
    "    print(f\"\\nüì¶ Extracting dataset to local disk...\")\n",
    "    !tar -xzf /content/data/sunrgbd_15.tar.gz -C /content/data/ 2>&1 | grep -v \"Ignoring unknown extended header\"\n",
    "    \n",
    "    # Remove tar file to save space\n",
    "    !rm /content/data/sunrgbd_15.tar.gz\n",
    "    \n",
    "    print(f\"\\n‚úÖ Dataset extracted to local disk\")\n",
    "    \n",
    "    # Verify extraction\n",
    "    train_rgb_count = len(list(Path(f\"{LOCAL_DATASET_PATH}/train/rgb\").glob(\"*.png\")))\n",
    "    val_rgb_count = len(list(Path(f\"{LOCAL_DATASET_PATH}/val/rgb\").glob(\"*.png\")))\n",
    "    print(f\"   Train samples: {train_rgb_count}\")\n",
    "    print(f\"   Val samples: {val_rgb_count}\")\n",
    "\n",
    "else:\n",
    "    print(f\"‚ùå Compressed dataset not found on Drive!\")\n",
    "    print(f\"   Expected location: {DRIVE_DATASET_TAR}\")\n",
    "    print(f\"\\nüìã To fix this:\")\n",
    "    print(f\"   1. Run: tar -czf sunrgbd_15.tar.gz -C data sunrgbd_15\")\n",
    "    print(f\"   2. Upload sunrgbd_15.tar.gz to Google Drive\")\n",
    "    print(f\"   3. Place it at: {DRIVE_DATASET_TAR}\")\n",
    "    raise FileNotFoundError(f\"Compressed dataset not found at {DRIVE_DATASET_TAR}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"Dataset ready at: {LOCAL_DATASET_PATH}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-6"
   },
   "source": [
    "## 6. Setup Python Path & Import DMNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup-imports"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Remove cached modules\n",
    "modules_to_reload = [k for k in sys.modules.keys() if k.startswith('src.')]\n",
    "for module in modules_to_reload:\n",
    "    del sys.modules[module]\n",
    "    \n",
    "# Add project to Python path\n",
    "project_root = '/content/Multi-Stream-Neural-Networks'\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "# Verify project structure\n",
    "print(\"Project structure:\")\n",
    "!ls -la {project_root}/src/models/\n",
    "\n",
    "# Import DMNet and SUN RGB-D dataloader\n",
    "print(\"\\nImporting DMNet and dataloaders...\")\n",
    "from src.models.direct_mixing_conv.dm_net import dm_net18, dm_net50\n",
    "from src.data_utils.sunrgbd_dataset import get_sunrgbd_dataloaders\n",
    "\n",
    "# Import Ray Tune\n",
    "from ray import train, tune\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "\n",
    "print(\"‚úÖ DMNet, dataloaders, and Ray Tune imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "from src.utils.seed import set_seed\n",
    "\n",
    "SEED = 42\n",
    "DETERMINISTIC = False  # False = faster, True = fully reproducible\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"RANDOM SEED CONFIGURATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "set_seed(SEED, deterministic=DETERMINISTIC)\n",
    "\n",
    "print(f\"\\n‚úÖ Seed: {SEED}\")\n",
    "print(f\"   Deterministic: {DETERMINISTIC}\")\n",
    "if DETERMINISTIC:\n",
    "    print(\"   Mode: Fully reproducible (slower)\")\n",
    "else:\n",
    "    print(\"   Mode: Fast reproducible\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-7"
   },
   "source": [
    "## 7. Load SUN RGB-D Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify dataset structure\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DATASET STRUCTURE VERIFICATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "dataset_root = Path(LOCAL_DATASET_PATH)\n",
    "\n",
    "print(\"\\nDirectory structure:\")\n",
    "print(f\"  {dataset_root}/\")\n",
    "print(f\"    train/\")\n",
    "print(f\"      rgb/ - {len(list((dataset_root / 'train' / 'rgb').glob('*.png')))} images\")\n",
    "print(f\"      depth/ - {len(list((dataset_root / 'train' / 'depth').glob('*.png')))} images\")\n",
    "print(f\"      labels.txt\")\n",
    "print(f\"    val/\")\n",
    "print(f\"      rgb/ - {len(list((dataset_root / 'val' / 'rgb').glob('*.png')))} images\")\n",
    "print(f\"      depth/ - {len(list((dataset_root / 'val' / 'depth').glob('*.png')))} images\")\n",
    "print(f\"      labels.txt\")\n",
    "print(f\"    class_names.txt\")\n",
    "print(f\"    dataset_info.txt\")\n",
    "\n",
    "# Read class names\n",
    "with open(dataset_root / 'class_names.txt', 'r') as f:\n",
    "    class_names = [line.strip() for line in f]\n",
    "\n",
    "print(f\"\\nClasses ({len(class_names)}):\")\n",
    "for i, name in enumerate(class_names):\n",
    "    print(f\"  {i}: {name}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load-dataset"
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"LOADING SUN RGB-D 15-CATEGORY DATASET (2-STREAM: RGB + DEPTH)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Dataset configuration\n",
    "DATASET_CONFIG = {\n",
    "    'data_root': LOCAL_DATASET_PATH,\n",
    "    'batch_size': 128,  # Good balance for A100\n",
    "    'num_workers': 4,\n",
    "    'target_size': (416, 544),  \n",
    "    'num_classes': 15,  # SUN RGB-D merged to 15 categories (labels 0-14)\n",
    "    'seed': SEED  # For reproducible data loading\n",
    "}\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "for key, value in DATASET_CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(f\"\\nLoading dataset from: {DATASET_CONFIG['data_root']}\")\n",
    "\n",
    "# Create reproducible dataloaders\n",
    "train_loader, val_loader = get_sunrgbd_dataloaders(\n",
    "    data_root=DATASET_CONFIG['data_root'],\n",
    "    batch_size=DATASET_CONFIG['batch_size'],\n",
    "    num_workers=DATASET_CONFIG['num_workers'],\n",
    "    target_size=DATASET_CONFIG['target_size'],\n",
    "    seed=DATASET_CONFIG['seed']  # Pass seed for reproducibility\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Dataset loaded successfully!\")\n",
    "print(f\"\\nDataset Statistics:\")\n",
    "print(f\"  Train batches: {len(train_loader)}\")\n",
    "print(f\"  Val batches: {len(val_loader)}\")\n",
    "print(f\"  Train samples: {len(train_loader.dataset)}\")\n",
    "print(f\"  Val samples: {len(val_loader.dataset)}\")\n",
    "print(f\"  Batch size: {DATASET_CONFIG['batch_size']}\")\n",
    "\n",
    "# Test loading a batch\n",
    "print(f\"\\nTesting batch loading...\")\n",
    "rgb_batch, depth_batch, label_batch = next(iter(train_loader))\n",
    "print(f\"  RGB shape: {rgb_batch.shape}\")\n",
    "print(f\"  Depth shape: {depth_batch.shape}\")\n",
    "print(f\"  Labels shape: {label_batch.shape}\")\n",
    "print(f\"  Labels min: {label_batch.min().item()}, max: {label_batch.max().item()}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-8"
   },
   "source": [
    "## 8. Visualize Sample Data\n",
    "\n",
    "Shows RGB images, depth maps, and scene labels from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "visualize-data"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Visualize some samples from TRAINING set (WITH augmentation)\n",
    "# This shows what the model actually sees during training\n",
    "print(\"Loading samples from TRAINING set (with augmentation)...\")\n",
    "rgb_batch, depth_batch, label_batch = next(iter(train_loader))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Creating visualization...\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(14, 7))\n",
    "\n",
    "for i in range(4):\n",
    "    rgb = rgb_batch[i].cpu()\n",
    "    depth = depth_batch[i].cpu()\n",
    "    label = label_batch[i].item()\n",
    "    \n",
    "    # Denormalize RGB from training statistics\n",
    "    # RGB stats: mean=[0.4906, 0.4564, 0.4311], std=[0.2794, 0.2869, 0.2922]\n",
    "    rgb_mean = torch.tensor([0.4906, 0.4564, 0.4311]).view(3, 1, 1)\n",
    "    rgb_std = torch.tensor([0.2794, 0.2869, 0.2922]).view(3, 1, 1)\n",
    "    rgb_vis = rgb * rgb_std + rgb_mean\n",
    "    \n",
    "    # Denormalize Depth from training statistics\n",
    "    # Depth stats: mean=[0.2912], std=[0.1472]\n",
    "    depth_vis = depth * 0.1472 + 0.2912\n",
    "    \n",
    "    # Clamp to valid range\n",
    "    rgb_vis = torch.clamp(rgb_vis, 0, 1)\n",
    "    depth_vis = torch.clamp(depth_vis, 0, 1)\n",
    "    \n",
    "    # Plot RGB\n",
    "    axes[0, i].imshow(rgb_vis.permute(1, 2, 0))\n",
    "    axes[0, i].set_title(f\"RGB - Class {label}\", fontsize=10)\n",
    "    axes[0, i].axis('off')\n",
    "    \n",
    "    # Plot Depth\n",
    "    axes[1, i].imshow(depth_vis.squeeze(), cmap='viridis')\n",
    "    axes[1, i].set_title(f\"Depth - Class {label}\", fontsize=10)\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "plt.suptitle('SUN RGB-D Training Data (RGB + Depth)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Sample visualization complete!\")\n",
    "print(\"\\nNote: These are from the TRAINING set (with augmentation).\")\n",
    "print(\"Augmentations include: random flip, crop, color jitter, blur, and random erasing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8b. Hyperparameter Tuning with Ray Tune (Optional)\n",
    "\n",
    "Perform a wide search for optimal hyperparameters using Ray Tune.\n",
    "- **Parallel Trials:** Run multiple configurations simultaneously\n",
    "- **Data Subset:** Use 50% of data for fast iteration\n",
    "- **Short Duration:** Train for 10 epochs per trial\n",
    "- **ASHA Scheduler:** Early stopping for bad trials\n",
    "- **Uses fit() method:** Ensures consistency with main training (no custom training loop!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import ray\nfrom ray import tune\nfrom ray.tune.schedulers import ASHAScheduler\nimport torch\nfrom src.models.direct_mixing_conv.dm_net import dm_net18, dm_net50\nfrom src.training.optimizers import create_stream_optimizer\nfrom src.training.schedulers import setup_scheduler\nfrom src.data_utils.sunrgbd_dataset import get_sunrgbd_dataloaders\nfrom src.utils.seed import set_seed, WorkerInitFn\n\nEPOCHS = 10\n\n\nclass RayTuneReporter:\n    \"\"\"Callback for reporting metrics to Ray Tune during training.\"\"\"\n    \n    def on_epoch_end(self, epoch, logs):\n        \"\"\"Called at the end of each epoch to report metrics to Ray Tune.\"\"\"\n        tune.report({\n            \"loss\": logs['val_loss'],\n            \"accuracy\": logs['val_accuracy'],\n            \"train_loss\": logs['train_loss'],\n            \"train_accuracy\": logs['train_accuracy']\n        })\n\n\ndef train_dmnet_tune(config, train_dataset=None, val_dataset=None):\n    \"\"\"\n    Trainable function for Ray Tune using fit() method.\n    \n    Benefits of using fit() instead of custom loop:\n    - Reuses all battle-tested training logic from dm_net.py\n    - Consistent behavior between main training and hyperparameter tuning\n    - Proper scheduler stepping, AMP handling, gradient clipping\n    - Single source of truth - no code duplication\n    \n    Args:\n        config: Ray Tune configuration dict with hyperparameters\n        train_dataset: Pre-loaded training dataset (passed via tune.with_parameters)\n        val_dataset: Pre-loaded validation dataset (passed via tune.with_parameters)\n    \"\"\"\n    # Set seed for reproducibility (fast mode for hyperparameter search)\n    set_seed(42, deterministic=False)\n    \n    # Create 50% subset for faster hyperparameter evaluation\n    subset_fraction = 0.5\n    train_indices = torch.randperm(len(train_dataset))[:int(len(train_dataset) * subset_fraction)]\n    val_indices = torch.randperm(len(val_dataset))[:int(len(val_dataset) * subset_fraction)]\n    \n    train_subset = torch.utils.data.Subset(train_dataset, train_indices)\n    val_subset = torch.utils.data.Subset(val_dataset, val_indices)\n    \n    # Create reproducible subset dataloaders using picklable WorkerInitFn class\n    worker_init_fn = WorkerInitFn(42)\n    \n    train_loader = torch.utils.data.DataLoader(\n        train_subset,\n        batch_size=config[\"batch_size\"],\n        shuffle=True,\n        num_workers=2,\n        pin_memory=True,\n        worker_init_fn=worker_init_fn,\n        generator=torch.Generator().manual_seed(42)\n    )\n    val_loader = torch.utils.data.DataLoader(\n        val_subset,\n        batch_size=config[\"batch_size\"],\n        shuffle=False,\n        num_workers=1,\n        pin_memory=True,\n        worker_init_fn=worker_init_fn,\n        generator=torch.Generator().manual_seed(42)\n    )\n    \n    # Create Model (2-stream: RGB + Depth)\n    model = dm_net18(\n        num_classes=15,\n        stream_input_channels=[3, 1],\n        dropout_p=config[\"dropout_p\"],\n        device=\"cuda\",\n        use_amp=True\n    )\n    \n    # Create Optimizer with stream-specific learning rates\n    optimizer = create_stream_optimizer(\n        model,\n        optimizer_type=config[\"optimizer_type\"],\n        stream_lrs=[config[\"lr_rgb\"], config[\"lr_depth\"]],\n        stream_weight_decays=[config[\"wd_rgb\"], config[\"wd_depth\"]],\n        shared_lr=config[\"lr_shared\"],\n        shared_weight_decay=config[\"wd_shared\"]\n    )\n    \n    # Create Scheduler\n    scheduler_kwargs = {}\n    if config[\"scheduler_type\"] == 'cosine':\n        scheduler_kwargs['eta_min'] = config[\"eta_min\"]\n    elif config[\"scheduler_type\"] == 'plateau':\n        scheduler_kwargs['patience'] = config.get(\"patience\", 3)\n    \n    warmup_epochs = 1 if config[\"scheduler_type\"] != 'plateau' else 0\n    \n    scheduler = setup_scheduler(\n        optimizer,\n        scheduler_type=config[\"scheduler_type\"],\n        epochs=EPOCHS,\n        train_loader_len=len(train_loader),\n        warmup_epochs=warmup_epochs,\n        **scheduler_kwargs\n    )\n    \n    # Compile model\n    model.compile(\n        optimizer=optimizer,\n        scheduler=scheduler,\n        loss='cross_entropy',\n        label_smoothing=config[\"label_smoothing\"]\n    )\n    \n    # Train using fit() with Ray Tune callback\n    model.fit(\n        train_loader=train_loader,\n        val_loader=val_loader,\n        epochs=EPOCHS,\n        grad_clip_norm=config[\"grad_clip_norm\"],\n        callbacks=[RayTuneReporter()]\n    )"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Ray\n",
    "ray.shutdown()  # Clean shutdown of any previous Ray instance\n",
    "ray.init(ignore_reinit_error=True)\n",
    "\n",
    "# STAGE 2: REFINED SEARCH SPACE (Based on Top 3 from Stage 1)\n",
    "# Narrowing ranges around top-performing configurations\n",
    "search_space = {\n",
    "    # Learning rates - tighter ranges based on top 3\n",
    "    \"lr_rgb\": tune.loguniform(1.5e-4, 2.8e-4),      # Top 3: 1.49e-04 to 2.79e-04\n",
    "    \"lr_depth\": tune.loguniform(1.0e-4, 4.5e-4),    # Top 3: 1.05e-04 to 4.53e-04\n",
    "    \"lr_shared\": tune.loguniform(5.0e-4, 8.0e-3),   # Top 3: 5.20e-04 to 7.71e-03 (wide range)\n",
    "    \n",
    "    # Weight decay - tighter ranges based on top 3\n",
    "    \"wd_rgb\": tune.loguniform(2.5e-5, 9.0e-5),      # Top 3: 2.52e-05 to 8.79e-05\n",
    "    \"wd_depth\": tune.loguniform(1.2e-5, 6.0e-5),    # Top 3: 1.20e-05 to 5.89e-05\n",
    "    \"wd_shared\": tune.loguniform(1.0e-5, 5.0e-5),   # Top 3: 1.04e-05 to 5.08e-05\n",
    "    \n",
    "    # Optimizer - focus on top performers\n",
    "    \"optimizer_type\": tune.choice(['adam', 'adamw']),  # Both showed good results\n",
    "    \n",
    "    # Scheduler - all used cosine\n",
    "    \"scheduler_type\": tune.choice(['cosine']),\n",
    "    \"eta_min\": tune.loguniform(1.0e-7, 6.0e-6),     # Top 3: 1.16e-07 to 6.13e-06\n",
    "    \n",
    "    # Regularization - explore around top performers\n",
    "    \"dropout_p\": tune.uniform(0.05, 0.35),           # Top 3: 0.068 to 0.342\n",
    "    \"label_smoothing\": tune.uniform(0.0, 0.2),\n",
    "    \"grad_clip_norm\": tune.choice([0.5, 1.0]),       # Top 3: 0.5 and 1.0\n",
    "    \n",
    "    # Batch size - all top 3 used 64\n",
    "    \"batch_size\": tune.choice([64]),\n",
    "}\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STAGE 2: REFINED SEARCH SPACE (2-STREAM: RGB + DEPTH)\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nPer-Stream Learning Rates (narrowed):\")\n",
    "print(\"  lr_rgb:    log-uniform [1.5e-4, 2.8e-4]  (Top 3: 1.49e-04 to 2.79e-04)\")\n",
    "print(\"  lr_depth:  log-uniform [1.0e-4, 4.5e-4]  (Top 3: 1.05e-04 to 4.53e-04)\")\n",
    "print(\"\\nShared/Integrated:\")\n",
    "print(\"  lr_shared: log-uniform [5.0e-4, 8.0e-3]  (Top 3: 5.20e-04 to 7.71e-03)\")\n",
    "print(\"\\nPer-Stream Weight Decay (narrowed):\")\n",
    "print(\"  wd_rgb:    log-uniform [2.5e-5, 9.0e-5]  (Top 3: 2.52e-05 to 8.79e-05)\")\n",
    "print(\"  wd_depth:  log-uniform [1.2e-5, 6.0e-5]  (Top 3: 1.20e-05 to 5.89e-05)\")\n",
    "print(\"  wd_shared: log-uniform [1.0e-5, 5.0e-5]  (Top 3: 1.04e-05 to 5.08e-05)\")\n",
    "print(\"\\nOptimizer & Scheduler:\")\n",
    "print(\"  optimizer_type:   ['adam', 'adamw']       (Locked - both top performers)\")\n",
    "print(\"  scheduler_type:   ['cosine']              (Locked - all top 3 used this)\")\n",
    "print(\"  eta_min:          log-uniform [1.0e-7, 6.0e-6]  (Top 3: 1.16e-07 to 6.13e-06)\")\n",
    "print(\"\\nRegularization:\")\n",
    "print(\"  dropout_p:        uniform [0.05, 0.35]    (Top 3: 0.068 to 0.342)\")\n",
    "print(\"  label_smoothing:  uniform [0.0, 0.2]\")\n",
    "print(\"  grad_clip_norm:   [0.5, 1.0]              (Locked - top 3 used these)\")\n",
    "print(\"\\nBatch Size:\")\n",
    "print(\"  batch_size:       [64]                    (Locked - all top 3 used 64)\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "print(\"\\nüöÄ OPTIMIZATION: Creating datasets ONCE instead of 500 times\")\n",
    "print(\"   - Shared across all trials for efficiency\")\n",
    "print(\"   - Avoids redundant I/O and preprocessing\\n\")\n",
    "\n",
    "# Create datasets ONCE before tuning starts\n",
    "# These will be shared across all trials via tune.with_parameters()\n",
    "train_loader_shared, val_loader_shared = get_sunrgbd_dataloaders(\n",
    "    data_root=LOCAL_DATASET_PATH,\n",
    "    batch_size=search_space[\"batch_size\"],  \n",
    "    num_workers=2,\n",
    "    target_size=(416, 544),\n",
    "    seed=SEED  # For reproducible data loading\n",
    ")\n",
    "\n",
    "# Extract the underlying datasets (not the DataLoaders)\n",
    "train_dataset_shared = train_loader_shared.dataset\n",
    "val_dataset_shared = val_loader_shared.dataset\n",
    "\n",
    "print(f\"‚úÖ Datasets created successfully!\")\n",
    "print(f\"   Training samples: {len(train_dataset_shared):,}\")\n",
    "print(f\"   Validation samples: {len(val_dataset_shared):,}\")\n",
    "print(f\"   These will be reused across all trials\\n\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STARTING HYPERPARAMETER TUNING\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nüí° Running 20 trials in parallel for maximum GPU utilization\")\n",
    "print(f\"   Each trial: ~4 GB GPU memory\")\n",
    "print(f\"   Total: ~80 GB / 80 GB available\")\n",
    "print(f\"   20x speedup compared to sequential execution!\")\n",
    "print(f\"   Total trials: 500\")\n",
    "print(f\"   Estimated time: ~5-8 hours with early stopping\\n\")\n",
    "\n",
    "# Configure Tuner\n",
    "# Pass shared datasets to ALL trials via tune.with_parameters()\n",
    "# This avoids creating datasets 500 times (once per trial)\n",
    "tuner = tune.Tuner(\n",
    "    tune.with_resources(\n",
    "        tune.with_parameters(\n",
    "            train_dmnet_tune,\n",
    "            train_dataset=train_dataset_shared,  # Shared dataset\n",
    "            val_dataset=val_dataset_shared        # Shared dataset\n",
    "        ),\n",
    "        resources={\"cpu\": 0.5, \"gpu\": 0.05}  # Each trial gets 1/20 GPU\n",
    "    ),\n",
    "    param_space=search_space,\n",
    "    tune_config=tune.TuneConfig(\n",
    "        metric=\"accuracy\",\n",
    "        mode=\"max\",\n",
    "        scheduler=ASHAScheduler(\n",
    "            max_t=EPOCHS,  \n",
    "            grace_period=2, \n",
    "            reduction_factor=2\n",
    "        ),\n",
    "        num_samples=500,  # 500 trials for thorough search\n",
    "        max_concurrent_trials=20  # PARALLEL: Run 20 trials at once for 20x speedup!\n",
    "    )\n",
    ")\n",
    "\n",
    "# Run Tuning with custom progress reporter\n",
    "results = tuner.fit()\n",
    "\n",
    "# Get Best Result\n",
    "best_result = results.get_best_result(\"accuracy\", \"max\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TUNING COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Best Trial Config: {best_result.config}\")\n",
    "print(f\"Best Trial Accuracy: {best_result.metrics['accuracy']:.4f}\")\n",
    "print(f\"Best Trial Loss: {best_result.metrics['loss']:.4f}\")\n",
    "\n",
    "print(\"\\nüìã COPY THIS CONFIGURATION TO THE NEXT CELL:\")\n",
    "print(\"-\" * 40)\n",
    "print(\"STREAM_SPECIFIC_CONFIG = {\")\n",
    "print(f\"    'stream_lrs': [{best_result.config['lr_rgb']:.2e}, {best_result.config['lr_depth']:.2e}],\")\n",
    "print(f\"    'stream_weight_decays': [{best_result.config['wd_rgb']:.2e}, {best_result.config['wd_depth']:.2e}],\")\n",
    "print(f\"    'shared_lr': {best_result.config['lr_shared']:.2e},\")\n",
    "print(f\"    'shared_weight_decay': {best_result.config['wd_shared']:.2e}\")\n",
    "print(\"}\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Recommended Optimizer: {best_result.config['optimizer_type']}\")\n",
    "print(f\"Recommended Scheduler: {best_result.config['scheduler_type']}\")\n",
    "if best_result.config['scheduler_type'] == 'cosine':\n",
    "    print(f\"Recommended Eta Min: {best_result.config['eta_min']:.2e}\")\n",
    "elif best_result.config['scheduler_type'] == 'plateau':\n",
    "    print(f\"Recommended Patience: {best_result.config['patience']}\")\n",
    "print(f\"Recommended Grad Clip: {best_result.config['grad_clip_norm']}\")\n",
    "print(f\"Recommended Label Smoothing: {best_result.config['label_smoothing']}\")\n",
    "print(f\"Recommended Dropout: {best_result.config['dropout_p']}\")\n",
    "print(f\"Recommended Batch Size: {best_result.config['batch_size']}\")\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è IMPORTANT: Please also update MODEL_CONFIG (dropout), DATASET_CONFIG (batch_size),\")\n",
    "print(\"   and TRAIN_CONFIG (grad_clip_norm) with the recommended values above!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze Top 10 Trials from Ray Tune\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"TOP 10 TRIALS BY ACCURACY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Get all trials and convert to DataFrame\n",
    "df = results.get_dataframe()\n",
    "\n",
    "# Sort by accuracy (descending)\n",
    "df_sorted = df.sort_values('accuracy', ascending=False)\n",
    "\n",
    "# Select relevant columns for display (2-stream version)\n",
    "display_cols = [\n",
    "    'accuracy', 'loss', \n",
    "    'config/lr_rgb', 'config/lr_depth', 'config/lr_shared',\n",
    "    'config/wd_rgb', 'config/wd_depth', 'config/wd_shared',\n",
    "    'config/optimizer_type', 'config/scheduler_type', \n",
    "    'config/dropout_p', 'config/batch_size', 'config/grad_clip_norm', 'config/eta_min'\n",
    "]\n",
    "\n",
    "# Get top 10 trials\n",
    "top_10 = df_sorted[display_cols].head(10)\n",
    "\n",
    "# Format for better display\n",
    "top_10_formatted = top_10.copy()\n",
    "top_10_formatted['accuracy'] = top_10_formatted['accuracy'].apply(lambda x: f\"{x*100:.2f}%\")\n",
    "top_10_formatted['loss'] = top_10_formatted['loss'].apply(lambda x: f\"{x:.4f}\")\n",
    "top_10_formatted['config/lr_rgb'] = top_10_formatted['config/lr_rgb'].apply(lambda x: f\"{x:.2e}\")\n",
    "top_10_formatted['config/lr_depth'] = top_10_formatted['config/lr_depth'].apply(lambda x: f\"{x:.2e}\")\n",
    "top_10_formatted['config/lr_shared'] = top_10_formatted['config/lr_shared'].apply(lambda x: f\"{x:.2e}\")\n",
    "top_10_formatted['config/wd_rgb'] = top_10_formatted['config/wd_rgb'].apply(lambda x: f\"{x:.2e}\")\n",
    "top_10_formatted['config/wd_depth'] = top_10_formatted['config/wd_depth'].apply(lambda x: f\"{x:.2e}\")\n",
    "top_10_formatted['config/wd_shared'] = top_10_formatted['config/wd_shared'].apply(lambda x: f\"{x:.2e}\")\n",
    "top_10_formatted['config/eta_min'] = top_10_formatted['config/eta_min'].apply(lambda x: f\"{x:.2e}\")\n",
    "\n",
    "print(top_10_formatted.to_string(index=False))\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-9"
   },
   "source": [
    "## 9. Create & Compile DMNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create-model"
   },
   "outputs": [],
   "source": [
    "from src.models.direct_mixing_conv.dm_net import dm_net18, dm_net50\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"MODEL CREATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Model configuration\n",
    "MODEL_CONFIG = {\n",
    "    'architecture': 'resnet18',  # or 'resnet50' for better accuracy\n",
    "    'num_classes': 15,  # SUN RGB-D has 15 merged categories (labels 0-14)\n",
    "    'stream_input_channels': [3, 1],  # RGB=3, Depth=1 (2-stream)\n",
    "    'dropout_p': 0.5,  # Dropout for regularization\n",
    "    'device': 'cuda',\n",
    "    'use_amp': True  # Automatic Mixed Precision (2x faster on A100)\n",
    "}\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "for key, value in MODEL_CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Create model\n",
    "print(f\"\\nCreating DMNet-{MODEL_CONFIG['architecture'].upper()} (Direct Mixing ResNet)...\")\n",
    "\n",
    "if MODEL_CONFIG['architecture'] == 'resnet18':\n",
    "    model = dm_net18(\n",
    "        num_classes=MODEL_CONFIG['num_classes'],\n",
    "        stream_input_channels=MODEL_CONFIG['stream_input_channels'],  # [RGB, Depth]\n",
    "        dropout_p=MODEL_CONFIG['dropout_p'],\n",
    "        device=MODEL_CONFIG['device'],\n",
    "        use_amp=MODEL_CONFIG['use_amp']\n",
    "    )\n",
    "elif MODEL_CONFIG['architecture'] == 'resnet50':\n",
    "    model = dm_net50(\n",
    "        num_classes=MODEL_CONFIG['num_classes'],\n",
    "        stream_input_channels=MODEL_CONFIG['stream_input_channels'],  # [RGB, Depth]\n",
    "        dropout_p=MODEL_CONFIG['dropout_p'],\n",
    "        device=MODEL_CONFIG['device'],\n",
    "        use_amp=MODEL_CONFIG['use_amp']\n",
    "    )\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Count integration-specific parameters (scalars for direct mixing)\n",
    "integration_params = 0\n",
    "for name, param in model.named_parameters():\n",
    "    if 'alpha' in name or 'gamma' in name:  # DMNet uses alpha and gamma scalars\n",
    "        integration_params += param.numel()\n",
    "\n",
    "print(f\"\\n‚úÖ Model created successfully!\")\n",
    "print(f\"\\nModel Statistics:\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"  Mixing scalar parameters: {integration_params:,}\")\n",
    "print(f\"  Model size: {total_params * 4 / 1024**2:.2f} MB (FP32)\")\n",
    "print(f\"  Architecture: {len(MODEL_CONFIG['stream_input_channels'])}-stream Direct Mixing (DMNet)\")\n",
    "print(f\"  Device: {MODEL_CONFIG['device']}\")\n",
    "print(f\"  AMP enabled: {MODEL_CONFIG['use_amp']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "compile-model"
   },
   "source": [
    "## 9b. Model Compilation (Keras-Style API with Warmup)\n",
    "\n",
    "**Create optimizer and scheduler as objects, then pass to compile()**\n",
    "\n",
    "**NEW:** Learning rate warmup support! The scheduler will linearly increase the learning rate from a lower starting point to the target LR over the first few epochs, helping stabilize early training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model with stream-specific optimization\n",
    "print(\"=\" * 60)\n",
    "print(\"MODEL COMPILATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Import optimizer and scheduler utilities\n",
    "from src.training.optimizers import create_stream_optimizer\n",
    "from src.training.schedulers import setup_scheduler\n",
    "\n",
    "# Stream-specific configuration for optimal RGB/Depth balance (2-stream)\n",
    "STREAM_SPECIFIC_CONFIG = {\n",
    "    # Stream-specific learning rates (adjusted based on research):\n",
    "    'stream_lrs': [3e-5, 1e-4],  # [RGB, Depth] - 2-stream format!\n",
    "    'stream_weight_decays': [5e-4, 1e-4],  # [RGB, Depth]\n",
    "    'shared_lr': 7e-5,  # Shared params: base LR\n",
    "    'shared_weight_decay': 2e-4,  # Shared params: base WD\n",
    "}\n",
    "\n",
    "# Scheduler configuration (with warmup support!)\n",
    "SCHEDULER_CONFIG = {\n",
    "    'scheduler_type': 'cosine',\n",
    "    't_max': 80,  # Will be updated to match epochs in training config\n",
    "    'eta_min': 1e-6,\n",
    "    'warmup_epochs': 5,  # Warmup: linearly increase LR for first 5 epochs\n",
    "    'warmup_start_factor': 0.1  # Start at 10% of target LR during warmup\n",
    "}\n",
    "\n",
    "print(f\"Stream-Specific Configuration:\")\n",
    "for key, value in STREAM_SPECIFIC_CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(f\"\\nScheduler Configuration (with warmup):\")\n",
    "for key, value in SCHEDULER_CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Step 1: Create optimizer with stream-specific learning rates\n",
    "print(\"\\n[Step 1] Creating stream-specific optimizer...\")\n",
    "optimizer = create_stream_optimizer(\n",
    "    model,\n",
    "    optimizer_type='adamw',\n",
    "    stream_lrs=STREAM_SPECIFIC_CONFIG['stream_lrs'],\n",
    "    stream_weight_decays=STREAM_SPECIFIC_CONFIG['stream_weight_decays'],\n",
    "    shared_lr=STREAM_SPECIFIC_CONFIG['shared_lr'],\n",
    "    shared_weight_decay=STREAM_SPECIFIC_CONFIG['shared_weight_decay']\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Optimizer created: {optimizer.__class__.__name__}\")\n",
    "print(f\"   Parameter groups: {len(optimizer.param_groups)}\")\n",
    "for i, group in enumerate(optimizer.param_groups):\n",
    "    num_params = sum(p.numel() for p in group['params'])\n",
    "    print(f\"   Group {i+1}: lr={group['lr']:.2e}, wd={group['weight_decay']:.2e}, params={num_params:,}\")\n",
    "\n",
    "# Step 2: Create scheduler with warmup support\n",
    "print(\"\\n[Step 2] Creating learning rate scheduler with warmup...\")\n",
    "scheduler = setup_scheduler(\n",
    "    optimizer,\n",
    "    scheduler_type=SCHEDULER_CONFIG['scheduler_type'],\n",
    "    epochs=80,  # Placeholder - will match TRAIN_CONFIG['epochs']\n",
    "    train_loader_len=len(train_loader),\n",
    "    t_max=SCHEDULER_CONFIG['t_max'],\n",
    "    eta_min=SCHEDULER_CONFIG['eta_min'],\n",
    "    warmup_epochs=SCHEDULER_CONFIG['warmup_epochs'],\n",
    "    warmup_start_factor=SCHEDULER_CONFIG['warmup_start_factor']\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Scheduler created: {scheduler.__class__.__name__}\")\n",
    "print(f\"   Warmup: {SCHEDULER_CONFIG['warmup_epochs']} epochs (LR: {SCHEDULER_CONFIG['warmup_start_factor']*100:.0f}% ‚Üí 100%)\")\n",
    "print(f\"   Main scheduler: {SCHEDULER_CONFIG['scheduler_type']} annealing\")\n",
    "\n",
    "# Step 3: Compile model with optimizer and scheduler objects (Keras-style!)\n",
    "print(\"\\n[Step 3] Compiling model with optimizer and scheduler objects...\")\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    loss='cross_entropy',\n",
    "    label_smoothing=0.1\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Model compiled successfully (Keras-style)!\")\n",
    "print(\"\\nüí° Learning rate warmup enabled!\")\n",
    "print(f\"   First {SCHEDULER_CONFIG['warmup_epochs']} epochs: LR increases from {SCHEDULER_CONFIG['warmup_start_factor']*100:.0f}% to 100%\")\n",
    "print(f\"   Remaining epochs: {SCHEDULER_CONFIG['scheduler_type']} annealing from 100% to {SCHEDULER_CONFIG['eta_min']:.0e}\")\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-10"
   },
   "source": [
    "## 10. Test Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "test-forward"
   },
   "outputs": [],
   "source": [
    "# Test forward pass with detailed debugging\n",
    "print(\"Testing forward pass with CUDA_LAUNCH_BLOCKING for better error messages...\")\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'  # Synchronous CUDA for better error messages\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    rgb_test, depth_test, labels_test = next(iter(train_loader))\n",
    "    \n",
    "    print(f\"\\nInput validation:\")\n",
    "    print(f\"  RGB shape: {rgb_test.shape}, dtype: {rgb_test.dtype}\")\n",
    "    print(f\"  RGB min: {rgb_test.min():.4f}, max: {rgb_test.max():.4f}\")\n",
    "    print(f\"  RGB has NaN: {torch.isnan(rgb_test).any()}\")\n",
    "    print(f\"  RGB has Inf: {torch.isinf(rgb_test).any()}\")\n",
    "    \n",
    "    print(f\"\\n  Depth shape: {depth_test.shape}, dtype: {depth_test.dtype}\")\n",
    "    print(f\"  Depth min: {depth_test.min():.4f}, max: {depth_test.max():.4f}\")\n",
    "    print(f\"  Depth has NaN: {torch.isnan(depth_test).any()}\")\n",
    "    print(f\"  Depth has Inf: {torch.isinf(depth_test).any()}\")\n",
    "    \n",
    "    print(f\"\\n  Labels shape: {labels_test.shape}, dtype: {labels_test.dtype}\")\n",
    "    print(f\"  Labels min: {labels_test.min()}, max: {labels_test.max()}\")\n",
    "    print(f\"  Labels unique: {torch.unique(labels_test).tolist()}\")\n",
    "    \n",
    "    print(\"\\nRunning forward pass...\")\n",
    "    rgb_cuda = rgb_test.to('cuda')\n",
    "    depth_cuda = depth_test.to('cuda')\n",
    "    \n",
    "    try:\n",
    "        # Pass list of streams for 2-stream model\n",
    "        outputs = model([rgb_cuda, depth_cuda])\n",
    "        print(f\"  ‚úÖ Forward pass successful!\")\n",
    "        print(f\"  Output shape: {outputs.shape}\")\n",
    "        print(f\"  Output min: {outputs.min():.4f}, max: {outputs.max():.4f}\")\n",
    "        \n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        print(f\"\\nSample predictions: {predictions.cpu().numpy()[:10]}\")\n",
    "        print(f\"Ground truth: {labels_test.numpy()[:10]}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Forward pass failed!\")\n",
    "        print(f\"Error: {e}\")\n",
    "        print(f\"\\nThis is likely a model architecture issue, not a data issue.\")\n",
    "        print(f\"Possible causes:\")\n",
    "        print(f\"  1. BatchNorm running stats issue\")\n",
    "        print(f\"  2. Invalid tensor operations in model\")\n",
    "        print(f\"  3. Memory corruption\")\n",
    "        raise\n",
    "\n",
    "print(\"\\n‚úÖ Forward pass test complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-11"
   },
   "source": [
    "## 11. Setup Checkpoint Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"CHECKPOINT DIRECTORY SETUP\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create checkpoint directory on Google Drive (persistent storage)\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "checkpoint_dir = f\"/content/drive/MyDrive/dmnet_checkpoints/run_{timestamp}\"\n",
    "\n",
    "# Create directory\n",
    "Path(checkpoint_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"\\n‚úÖ Checkpoint directory created:\")\n",
    "print(f\"   {checkpoint_dir}\")\n",
    "print(f\"\\nAll training artifacts will be saved here:\")\n",
    "print(f\"  ‚Ä¢ Best model weights\")\n",
    "print(f\"  ‚Ä¢ Training history\")\n",
    "print(f\"  ‚Ä¢ Monitoring metrics\")\n",
    "print(f\"  ‚Ä¢ Visualizations\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Train the Model üöÄ\n",
    "\n",
    "**Expected time:** ~2-3 hours for 90 epochs on A100\n",
    "\n",
    "**All optimizations enabled:**\n",
    "- ‚úÖ Automatic Mixed Precision (2x faster)\n",
    "- ‚úÖ Gradient Clipping (stability)\n",
    "- ‚úÖ Cosine Annealing LR\n",
    "- ‚úÖ Early Stopping\n",
    "- ‚úÖ Best Model Checkpointing\n",
    "- ‚úÖ Local disk I/O (10-20x faster than Drive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup-checkpoints"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Suppress PyTorch SequentialLR deprecation warning (internal PyTorch issue, not our code)\n",
    "warnings.filterwarnings(\n",
    "    'ignore',\n",
    "    message='The epoch parameter in `scheduler.step\\\\(\\\\)` was not necessary',\n",
    "    category=UserWarning\n",
    ")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TRAINING WITH STREAM MONITORING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Training configuration\n",
    "TRAIN_CONFIG = {\n",
    "    'epochs': 80,\n",
    "    'grad_clip_norm': 5.0,\n",
    "    'early_stopping': True,\n",
    "    'patience': 12,\n",
    "    'min_delta': 0.001,\n",
    "    'monitor': 'val_accuracy',\n",
    "    'restore_best_weights': True,\n",
    "    'save_path': f\"{checkpoint_dir}/best_model.pt\",\n",
    "    'stream_monitoring': True,  # Built-in stream monitoring!\n",
    "}\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "for key, value in TRAIN_CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"Training will take approximately 2-3 hours on A100\")\n",
    "print(f\"Stream monitoring active - detailed per-stream metrics shown each epoch\")\n",
    "print(\"==\" * 60 + \"\\n\")\n",
    "\n",
    "# Train using built-in fit() method with stream monitoring\n",
    "# NOTE: No scheduler_kwargs needed! Scheduler was already created and passed to compile()\n",
    "history = model.fit(\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    epochs=TRAIN_CONFIG['epochs'],\n",
    "    verbose=True,\n",
    "    save_path=TRAIN_CONFIG['save_path'],\n",
    "    early_stopping=TRAIN_CONFIG['early_stopping'],\n",
    "    patience=TRAIN_CONFIG['patience'],\n",
    "    min_delta=TRAIN_CONFIG['min_delta'],\n",
    "    monitor=TRAIN_CONFIG['monitor'],\n",
    "    restore_best_weights=TRAIN_CONFIG['restore_best_weights'],\n",
    "    grad_clip_norm=TRAIN_CONFIG['grad_clip_norm'],\n",
    "    stream_monitoring=TRAIN_CONFIG['stream_monitoring']  # Enable built-in monitoring\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üéâ TRAINING COMPLETE!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-13"
   },
   "source": [
    "## 13. Evaluate Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"FINAL EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Evaluate on validation set\n",
    "results = model.evaluate(data_loader=val_loader)\n",
    "\n",
    "print(f\"\\nFinal Validation Results:\")\n",
    "print(f\"  Loss: {results['loss']:.4f}\")\n",
    "print(f\"  Overall Accuracy: {results['accuracy']*100:.2f}%\")\n",
    "\n",
    "# Show per-stream accuracies (always available with stream_monitoring=True)\n",
    "print(f\"\\nStream-Specific Performance:\")\n",
    "for i in range(len(MODEL_CONFIG['stream_input_channels'])):\n",
    "    print(f\"  Stream{i} Accuracy: {results[f'stream{i}_accuracy']*100:.2f}%\")\n",
    "\n",
    "print(f\"\\nTraining Summary:\")\n",
    "print(f\"  Initial train loss: {history['train_loss'][0]:.4f}\")\n",
    "print(f\"  Final train loss: {history['train_loss'][-1]:.4f}\")\n",
    "print(f\"  Best val loss: {min(history['val_loss']):.4f}\")\n",
    "print(f\"  Initial train acc: {history['train_accuracy'][0]*100:.2f}%\")\n",
    "print(f\"  Final train acc: {history['train_accuracy'][-1]*100:.2f}%\")\n",
    "print(f\"  Best val acc: {max(history['val_accuracy'])*100:.2f}%\")\n",
    "print(f\"  Total epochs: {len(history['train_loss'])}\")\n",
    "\n",
    "if 'early_stopping' in history:\n",
    "    print(f\"\\nEarly Stopping Info:\")\n",
    "    print(f\"  Stopped early: {history['early_stopping']['stopped_early']}\")\n",
    "    print(f\"  Best epoch: {history['early_stopping']['best_epoch']}\")\n",
    "    print(f\"  Best {history['early_stopping']['monitor']}: {history['early_stopping']['best_metric']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-14"
   },
   "source": [
    "## 14. Plot Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "plot-curves"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Loss curve\n",
    "axes[0].plot(history['train_loss'], label='Train Loss', linewidth=2)\n",
    "axes[0].plot(history['val_loss'], label='Val Loss', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Loss', fontsize=12)\n",
    "axes[0].set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy curve with per-stream curves\n",
    "axes[1].plot([acc*100 for acc in history['train_accuracy']], label='Full Model Train', linewidth=2, color='green')\n",
    "axes[1].plot([acc*100 for acc in history['val_accuracy']], label='Full Model Val', linewidth=2, color='darkgreen')\n",
    "\n",
    "# Add per-stream curves (always available with stream_monitoring=True)\n",
    "stream_train_colors = ['skyblue', 'lightcoral', 'gold', 'lightgreen', 'plum']\n",
    "stream_val_colors = ['blue', 'red', 'orange', 'green', 'purple']\n",
    "for i in range(len(MODEL_CONFIG['stream_input_channels'])):\n",
    "    color_idx = i % len(stream_train_colors)\n",
    "    axes[1].plot([acc*100 for acc in history[f'stream_{i}_train_acc']], \n",
    "                label=f'Stream{i} Train', linewidth=1, alpha=0.6, linestyle='--', color=stream_train_colors[color_idx])\n",
    "    axes[1].plot([acc*100 for acc in history[f'stream_{i}_val_acc']], \n",
    "                label=f'Stream{i} Val', linewidth=1, alpha=0.6, linestyle='--', \n",
    "                color=stream_val_colors[color_idx])\n",
    "\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Accuracy (%)', fontsize=12)\n",
    "axes[1].set_title('Training and Validation Accuracy\\n(Full Model = Integrated Stream)', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=9, loc='lower right')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Learning rate curve with per-stream LRs\n",
    "sampled_lrs = history['learning_rates'][::max(1, len(history['learning_rates'])//100)]\n",
    "axes[2].plot(sampled_lrs, linewidth=2, color='green', label='Base LR')\n",
    "\n",
    "# Add per-stream LRs (always available with stream_monitoring=True)\n",
    "lr_colors = ['blue', 'red', 'orange', 'purple', 'brown']\n",
    "for i in range(len(MODEL_CONFIG['stream_input_channels'])):\n",
    "    color_idx = i % len(lr_colors)\n",
    "    axes[2].plot(history[f'stream_{i}_lr'], linewidth=1, alpha=0.7, linestyle='--', \n",
    "                color=lr_colors[color_idx], label=f'Stream{i} LR')\n",
    "\n",
    "axes[2].set_xlabel('Epoch', fontsize=12)\n",
    "axes[2].set_ylabel('Learning Rate', fontsize=12)\n",
    "axes[2].set_title('Learning Rate Schedule', fontsize=14, fontweight='bold')\n",
    "axes[2].legend(fontsize=9, loc='upper right')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{checkpoint_dir}/training_curves.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úÖ Training curves saved to: {checkpoint_dir}/training_curves.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-15"
   },
   "source": [
    "## 15. Pathway Analysis (RGB/Depth/Integrated Contributions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pathway-analysis"
   },
   "outputs": [],
   "source": "print(\"=\" * 60)\nprint(f\"PATHWAY ANALYSIS ({len(MODEL_CONFIG['stream_input_channels'])}-STREAM DMNET)\")\nprint(\"=\" * 60)\nprint(f\"\\nAnalyzing {len(MODEL_CONFIG['stream_input_channels'])} stream pathways and integrated pathway contributions...\")\nprint(\"This may take a few minutes...\\n\")\n\n# Analyze pathways\npathway_analysis = model.analyze_pathways(data_loader=val_loader)\n\nprint(\"\\nAccuracy Metrics:\")\nprint(f\"  Full model (Integrated): {pathway_analysis['accuracy']['full_model']*100:.2f}%\")\nfor i in range(len(MODEL_CONFIG['stream_input_channels'])):\n    print(f\"  Stream{i} only: {pathway_analysis['accuracy'][f'stream{i}_only']*100:.2f}%\")\nprint(f\"  Integrated only: {pathway_analysis['accuracy']['integrated_only']*100:.2f}%\")\nprint()\nfor i in range(len(MODEL_CONFIG['stream_input_channels'])):\n    print(f\"  Stream{i} contribution: {pathway_analysis['accuracy'][f'stream{i}_contribution']*100:.2f}%\")\nprint(f\"  Integrated contribution: {pathway_analysis['accuracy']['integrated_contribution']*100:.2f}%\")\n\nprint(\"\\nLoss Metrics:\")\nprint(f\"  Full model: {pathway_analysis['loss']['full_model']:.4f}\")\nfor i in range(len(MODEL_CONFIG['stream_input_channels'])):\n    print(f\"  Stream{i} only: {pathway_analysis['loss'][f'stream{i}_only']:.4f}\")\nprint(f\"  Integrated only: {pathway_analysis['loss']['integrated_only']:.4f}\")\nprint()\nfor i in range(len(MODEL_CONFIG['stream_input_channels'])):\n    print(f\"  Stream{i} contribution: {pathway_analysis['loss'][f'stream{i}_contribution']:.4f}\")\nprint(f\"  Integrated contribution: {pathway_analysis['loss']['integrated_contribution']:.4f}\")\n\nprint(\"\\nFeature Norms:\")\nfor i in range(len(MODEL_CONFIG['stream_input_channels'])):\n    print(f\"  Stream{i}: mean={pathway_analysis['feature_norms'][f'stream{i}_mean']:.2f}, std={pathway_analysis['feature_norms'][f'stream{i}_std']:.2f}\")\nprint(f\"  Integrated: mean={pathway_analysis['feature_norms']['integrated_mean']:.2f}, std={pathway_analysis['feature_norms']['integrated_std']:.2f}\")\n\nprint(\"\\n\" + \"=\" * 60)\n\n# Save pathway analysis results\nimport json\nwith open(f\"{checkpoint_dir}/pathway_analysis.json\", 'w') as f:\n    # Convert numpy floats to Python floats for JSON serialization\n    json_safe = {}\n    for metric_type, metrics in pathway_analysis.items():\n        if metric_type == 'samples_analyzed':\n            json_safe[metric_type] = int(metrics)\n        else:\n            json_safe[metric_type] = {k: float(v) for k, v in metrics.items()}\n    json.dump(json_safe, f, indent=2)\n\nprint(f\"‚úÖ Pathway analysis saved to: {checkpoint_dir}/pathway_analysis.json\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-16"
   },
   "source": [
    "## 16. Save Results & Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save-results"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SAVING RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Save training history as JSON\n",
    "history_path = f\"{checkpoint_dir}/training_history.json\"\n",
    "with open(history_path, 'w') as f:\n",
    "    json_history = {\n",
    "        'train_loss': [float(x) for x in history['train_loss']],\n",
    "        'val_loss': [float(x) for x in history['val_loss']],\n",
    "        'train_accuracy': [float(x) for x in history['train_accuracy']],\n",
    "        'val_accuracy': [float(x) for x in history['val_accuracy']],\n",
    "        'learning_rates': [float(x) for x in history['learning_rates']],\n",
    "        'model_config': MODEL_CONFIG,\n",
    "        'dataset_config': DATASET_CONFIG,\n",
    "        'stream_specific_config': STREAM_SPECIFIC_CONFIG,\n",
    "        'scheduler_config': SCHEDULER_CONFIG,\n",
    "        'training_config': TRAIN_CONFIG,\n",
    "        'final_results': {\n",
    "            'val_loss': float(results['loss']),\n",
    "            'val_accuracy': float(results['accuracy'])\n",
    "        },\n",
    "        'pathway_analysis': {\n",
    "            'full_model_accuracy': float(pathway_analysis['accuracy']['full_model']),\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Add per-stream pathway analysis results\n",
    "    for i in range(len(MODEL_CONFIG['stream_input_channels'])):\n",
    "        json_history['pathway_analysis'][f'stream{i}_only_accuracy'] = float(pathway_analysis['accuracy'][f'stream{i}_only'])\n",
    "    json_history['pathway_analysis']['integrated_only_accuracy'] = float(pathway_analysis['accuracy']['integrated_only'])\n",
    "    \n",
    "    if 'early_stopping' in history:\n",
    "        json_history['early_stopping'] = history['early_stopping']\n",
    "    \n",
    "    json.dump(json_history, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Training history saved: {history_path}\")\n",
    "\n",
    "# Save final model (in addition to best model)\n",
    "final_model_path = f\"{checkpoint_dir}/final_model.pt\"\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': model.optimizer.state_dict(),\n",
    "    'scheduler_state_dict': model.scheduler.state_dict() if model.scheduler else None,\n",
    "    'config': MODEL_CONFIG,\n",
    "    'stream_specific_config': STREAM_SPECIFIC_CONFIG,\n",
    "    'scheduler_config': SCHEDULER_CONFIG,\n",
    "    'history': history,\n",
    "    'val_accuracy': results['accuracy']\n",
    "}, final_model_path)\n",
    "\n",
    "print(f\"‚úÖ Final model saved: {final_model_path}\")\n",
    "\n",
    "# Save summary report\n",
    "summary_path = f\"{checkpoint_dir}/summary.txt\"\n",
    "with open(summary_path, 'w') as f:\n",
    "    f.write(\"=\" * 60 + \"\\n\")\n",
    "    f.write(\"LINet Training Summary - SUN RGB-D\\n\")\n",
    "    f.write(\"=\" * 60 + \"\\n\\n\")\n",
    "    \n",
    "    # Model Configuration\n",
    "    f.write(\"Model Configuration:\\n\")\n",
    "    f.write(f\"  Architecture: LINet-{MODEL_CONFIG['architecture'].upper()} ({len(MODEL_CONFIG['stream_input_channels'])}-stream Linear Integration)\\n\")\n",
    "    f.write(f\"  Num Classes: {MODEL_CONFIG['num_classes']}\\n\")\n",
    "    f.write(f\"  Stream Input Channels: {MODEL_CONFIG['stream_input_channels']}\\n\")\n",
    "    f.write(f\"  Dropout: {MODEL_CONFIG['dropout_p']}\\n\")\n",
    "    f.write(f\"  Device: {MODEL_CONFIG['device']}\\n\")\n",
    "    f.write(f\"  AMP Enabled: {MODEL_CONFIG['use_amp']}\\n\")\n",
    "    f.write(f\"  Total Parameters: {total_params:,}\\n\")\n",
    "    f.write(f\"  Trainable Parameters: {trainable_params:,}\\n\")\n",
    "    f.write(f\"  Integration Parameters: {integration_params:,}\\n\")\n",
    "    \n",
    "    # Dataset Configuration\n",
    "    f.write(f\"\\nDataset Configuration:\\n\")\n",
    "    f.write(f\"  Dataset: SUN RGB-D 15-category (Scene Classification)\\n\")\n",
    "    f.write(f\"  Training Samples: {len(train_loader.dataset)}\\n\")\n",
    "    f.write(f\"  Validation Samples: {len(val_loader.dataset)}\\n\")\n",
    "    f.write(f\"  Batch Size: {DATASET_CONFIG['batch_size']}\\n\")\n",
    "    f.write(f\"  Num Workers: {DATASET_CONFIG['num_workers']}\\n\")\n",
    "    f.write(f\"  Input Size: {DATASET_CONFIG['target_size']}\\n\")\n",
    "    \n",
    "    # Optimization Configuration\n",
    "    f.write(f\"\\nOptimization Configuration (Keras-Style API):\\n\")\n",
    "    f.write(f\"  Optimizer: AdamW (stream-specific)\\n\")\n",
    "    f.write(f\"  Loss Function: cross_entropy\\n\")\n",
    "    f.write(f\"  Label Smoothing: 0.1\\n\")\n",
    "    f.write(f\"  Scheduler: {SCHEDULER_CONFIG['scheduler_type']}\\n\")\n",
    "    f.write(f\"  Scheduler t_max: {SCHEDULER_CONFIG['t_max']}\\n\")\n",
    "    f.write(f\"  Scheduler eta_min: {SCHEDULER_CONFIG['eta_min']}\\n\")\n",
    "    f.write(f\"  Gradient Clipping: {TRAIN_CONFIG['grad_clip_norm']}\\n\")\n",
    "    \n",
    "    # Stream-Specific Settings\n",
    "    f.write(f\"\\nStream-Specific Settings:\\n\")\n",
    "    for i in range(len(MODEL_CONFIG['stream_input_channels'])):\n",
    "        lr = STREAM_SPECIFIC_CONFIG['stream_lrs'][i]\n",
    "        wd = STREAM_SPECIFIC_CONFIG['stream_weight_decays'][i]\n",
    "        f.write(f\"  Stream{i}:\\n\")\n",
    "        f.write(f\"    Learning Rate: {lr}\\n\")\n",
    "        f.write(f\"    Weight Decay: {wd}\\n\")\n",
    "    f.write(f\"  Shared (Fusion/Classifier):\\n\")\n",
    "    f.write(f\"    Learning Rate: {STREAM_SPECIFIC_CONFIG['shared_lr']}\\n\")\n",
    "    f.write(f\"    Weight Decay: {STREAM_SPECIFIC_CONFIG['shared_weight_decay']}\\n\")\n",
    "    \n",
    "    # Training Configuration\n",
    "    f.write(f\"\\nTraining Configuration:\\n\")\n",
    "    f.write(f\"  Total Epochs: {len(history['train_loss'])}\\n\")\n",
    "    f.write(f\"  Stream Monitoring: {TRAIN_CONFIG['stream_monitoring']}\\n\")\n",
    "    f.write(f\"  Early Stopping: {TRAIN_CONFIG['early_stopping']}\\n\")\n",
    "    if TRAIN_CONFIG['early_stopping']:\n",
    "        f.write(f\"    Monitor: {TRAIN_CONFIG['monitor']}\\n\")\n",
    "        f.write(f\"    Patience: {TRAIN_CONFIG['patience']}\\n\")\n",
    "        f.write(f\"    Min Delta: {TRAIN_CONFIG['min_delta']}\\n\")\n",
    "        f.write(f\"    Restore Best Weights: {TRAIN_CONFIG['restore_best_weights']}\\n\")\n",
    "    \n",
    "    # Results\n",
    "    f.write(f\"\\nFinal Results:\\n\")\n",
    "    f.write(f\"  Val Loss: {results['loss']:.4f}\\n\")\n",
    "    f.write(f\"  Val Accuracy: {results['accuracy']*100:.2f}%\\n\")\n",
    "    f.write(f\"  Best Val Accuracy: {max(history['val_accuracy'])*100:.2f}%\\n\")\n",
    "    f.write(f\"  Initial Train Loss: {history['train_loss'][0]:.4f}\\n\")\n",
    "    f.write(f\"  Final Train Loss: {history['train_loss'][-1]:.4f}\\n\")\n",
    "    f.write(f\"  Best Val Loss: {min(history['val_loss']):.4f}\\n\")\n",
    "    \n",
    "    # Pathway Analysis\n",
    "    f.write(f\"\\nPathway Analysis ({len(MODEL_CONFIG['stream_input_channels'])}-stream LINet):\\n\")\n",
    "    f.write(f\"  Full Model (Integrated): {pathway_analysis['accuracy']['full_model']*100:.2f}%\\n\")\n",
    "    for i in range(len(MODEL_CONFIG['stream_input_channels'])):\n",
    "        f.write(f\"  Stream{i} Only: {pathway_analysis['accuracy'][f'stream{i}_only']*100:.2f}%\\n\")\n",
    "    f.write(f\"  Integrated Only: {pathway_analysis['accuracy']['integrated_only']*100:.2f}%\\n\")\n",
    "    for i in range(len(MODEL_CONFIG['stream_input_channels'])):\n",
    "        f.write(f\"  Stream{i} Contribution: {pathway_analysis['accuracy'][f'stream{i}_contribution']*100:.2f}%\\n\")\n",
    "    f.write(f\"  Integrated Contribution: {pathway_analysis['accuracy']['integrated_contribution']*100:.2f}%\\n\")\n",
    "\n",
    "print(f\"‚úÖ Summary report saved: {summary_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"All results saved to: {checkpoint_dir}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# List saved files\n",
    "print(\"\\nSaved files:\")\n",
    "!ls -lh {checkpoint_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-17"
   },
   "source": [
    "## 17. Summary & Next Steps\n",
    "\n",
    "### üéâ Training Complete!\n",
    "\n",
    "**What we accomplished:**\n",
    "- ‚úÖ Trained LINet (N-stream Linear Integration ResNet) on SUN RGB-D dataset (15 categories)\n",
    "- ‚úÖ Used **Keras-style API** with explicit optimizer and scheduler creation\n",
    "- ‚úÖ Used A100 GPU with AMP (2x speedup)\n",
    "- ‚úÖ Saved all checkpoints to Google Drive\n",
    "- ‚úÖ Analyzed per-stream and integrated pathway contributions\n",
    "- ‚úÖ Generated training curves and visualizations\n",
    "- ‚úÖ Comprehensive stream monitoring with overfitting detection\n",
    "\n",
    "**Results are saved to:** Check the output above for the checkpoint directory path\n",
    "\n",
    "### üìä Expected Performance:\n",
    "\n",
    "For **SUN RGB-D Scene Classification (15 categories, 10,335 images)**:\n",
    "- **Good:** 65-75% validation accuracy\n",
    "- **Very Good:** 75-80% validation accuracy\n",
    "- **Excellent:** 80-85% validation accuracy\n",
    "\n",
    "**Much better than NYU Depth V2 due to:**\n",
    "- 6.9x more training samples (8,041 vs 1,159)\n",
    "- 22.6x better class balance (8.5x vs 192x)\n",
    "- Higher quality, more diverse dataset\n",
    "\n",
    "### üîÑ New Keras-Style API Used:\n",
    "\n",
    "This notebook uses the **refactored Keras-style N-stream API**:\n",
    "\n",
    "```python\n",
    "# 1. Create optimizer with stream-specific LRs (N-stream format)\n",
    "optimizer = create_stream_optimizer(\n",
    "    model, \n",
    "    stream_lrs=[3e-5, 1e-4],  # [RGB, Depth]\n",
    "    stream_weight_decays=[5e-4, 1e-4],  # [RGB, Depth]\n",
    "    shared_lr=7e-5\n",
    ")\n",
    "\n",
    "# 2. Create scheduler\n",
    "scheduler = setup_scheduler(optimizer, 'cosine', epochs=80, ...)\n",
    "\n",
    "# 3. Compile with objects (not strings!)\n",
    "model.compile(optimizer=optimizer, scheduler=scheduler, loss='cross_entropy')\n",
    "\n",
    "# 4. Train (no scheduler_kwargs needed!)\n",
    "model.fit(train_loader, val_loader, epochs=80, stream_monitoring=True)\n",
    "```\n",
    "\n",
    "**Benefits:**\n",
    "- ‚úÖ Explicit optimizer/scheduler creation\n",
    "- ‚úÖ Clear separation: compile() = config, fit() = execution\n",
    "- ‚úÖ Easy to customize and experiment\n",
    "- ‚úÖ N-stream support - works with any number of streams!\n",
    "\n",
    "### üß† LINet Architecture Highlights:\n",
    "\n",
    "**N-Stream Linear Integration:**\n",
    "- Multiple input streams (e.g., RGB, Depth, etc.)\n",
    "- Integrated stream combines all inputs through learned linear weights at every layer\n",
    "- Neuron-level integration rather than late fusion\n",
    "\n",
    "**Weight Matrices per LIConv2d:**\n",
    "- Per-stream weights (full kernels for each input stream)\n",
    "- Integrated weight (1√ó1 channel-wise for integrated features)\n",
    "- Integration weights from each stream (1√ó1 convolutions)\n",
    "\n",
    "This allows the network to learn optimal integration strategies at every layer!\n",
    "\n",
    "### üîç Next Steps:\n",
    "\n",
    "1. **Review Results:**\n",
    "   - Check training curves above\n",
    "   - Review pathway analysis\n",
    "   - Compare per-stream and integrated contributions\n",
    "   - Analyze stream monitoring plots\n",
    "\n",
    "2. **Download Results:**\n",
    "   - All files are saved to your Google Drive\n",
    "   - Download checkpoints for local inference\n",
    "\n",
    "3. **Experiment:**\n",
    "   - Try ResNet50 for better accuracy (change `architecture` in Model Config)\n",
    "   - Adjust stream-specific learning rates if monitoring shows imbalance\n",
    "   - Train longer if early stopping triggered\n",
    "   - Analyze integration weights to understand learned strategies\n",
    "\n",
    "4. **Deploy:**\n",
    "   - Use the best model for inference\n",
    "   - Test on new RGB-D images\n",
    "   - Integrate into your application\n",
    "\n",
    "---\n",
    "\n",
    "**Questions or issues?** Check the training summary and pathway analysis above!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "final-summary"
   },
   "outputs": [],
   "source": [
    "# Print final summary\n",
    "print(\"=\" * 60)\n",
    "print(\"FINAL SUMMARY - LINET\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n‚úÖ Training Complete!\")\n",
    "print(f\"\\nFinal Validation Accuracy: {results['accuracy']*100:.2f}%\")\n",
    "print(f\"Best Validation Accuracy: {max(history['val_accuracy'])*100:.2f}%\")\n",
    "\n",
    "print(f\"\\nPathway Performance:\")\n",
    "for i in range(len(MODEL_CONFIG['stream_input_channels'])):\n",
    "    print(f\"  Stream{i} Pathway: {pathway_analysis['accuracy'][f'stream{i}_only']*100:.2f}%\")\n",
    "print(f\"  Integrated Pathway: {pathway_analysis['accuracy']['integrated_only']*100:.2f}%\")\n",
    "print(f\"  Combined (Full Model): {pathway_analysis['accuracy']['full_model']*100:.2f}%\")\n",
    "\n",
    "print(f\"\\nTotal Training Epochs: {len(history['train_loss'])}\")\n",
    "print(f\"Total Parameters: {total_params:,}\")\n",
    "print(f\"Integration Parameters: {integration_params:,}\")\n",
    "print(f\"\\nCheckpoints saved to: {checkpoint_dir}\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üéâ All done! Check Google Drive for saved models and results.\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}