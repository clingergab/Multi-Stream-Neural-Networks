{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# LINet Training on SUN RGB-D - Google Colab\n",
    "\n",
    "**Complete end-to-end training pipeline for Linear Integration ResNet (LINet) on Google Colab with A100 GPU**\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Checklist Before Running:\n",
    "\n",
    "- [ ] **Enable A100 GPU:** Runtime ‚Üí Change runtime type ‚Üí Hardware accelerator: GPU ‚Üí GPU type: A100\n",
    "- [ ] **Mount Google Drive:** Your code and dataset will be stored on Drive\n",
    "- [ ] **Upload dataset to Drive:** `MyDrive/datasets/sunrgbd_15/` (preprocessed 15-category dataset)\n",
    "- [ ] **Expected Runtime:** ~2-3 hours for training\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ What This Notebook Does:\n",
    "\n",
    "1. ‚úÖ Verify A100 GPU is available\n",
    "2. ‚úÖ Mount Google Drive\n",
    "3. ‚úÖ Clone your repository to local disk (fast I/O)\n",
    "4. ‚úÖ Copy SUN RGB-D dataset to local disk (10-20x faster than Drive)\n",
    "5. ‚úÖ Install dependencies\n",
    "6. ‚úÖ Train LINet (N-stream Linear Integration ResNet) with all optimizations\n",
    "7. ‚úÖ Save checkpoints to Drive (persistent storage)\n",
    "8. ‚úÖ Generate training curves and analysis\n",
    "\n",
    "---\n",
    "\n",
    "## üß† About LINet:\n",
    "\n",
    "**LINet** (Linear Integration Network) is an N-stream neural network architecture where:\n",
    "- **Multiple input streams** process different modalities (e.g., RGB, Depth)\n",
    "- **Integrated Stream** combines all streams using learned linear integration weights at every layer\n",
    "\n",
    "Unlike traditional fusion methods, LINet performs integration **at the neuron level** through weight matrices per convolution:\n",
    "- Per-stream weights (full kernels for each input modality)\n",
    "- Integrated weight (1√ó1 channel-wise for integrated features)\n",
    "- Integration weights from each stream (1√ó1 convolutions from each stream to integrated)\n",
    "\n",
    "This allows the network to learn optimal integration strategies at every layer!\n",
    "\n",
    "---\n",
    "\n",
    "**Let's get started!** üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-1"
   },
   "source": [
    "## 1. Environment Setup & GPU Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "check-gpu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "GPU VERIFICATION\n",
      "============================================================\n",
      "PyTorch version: 2.7.1\n",
      "CUDA available: False\n",
      "\n",
      "‚ùå NO GPU DETECTED!\n",
      "Please enable GPU: Runtime ‚Üí Change runtime type ‚Üí Hardware accelerator: GPU\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "GPU is required for training",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 31\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m‚ùå NO GPU DETECTED!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease enable GPU: Runtime ‚Üí Change runtime type ‚Üí Hardware accelerator: GPU\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 31\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPU is required for training\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m60\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: GPU is required for training"
     ]
    }
   ],
   "source": [
    "# Check GPU availability and specs\n",
    "import torch\n",
    "import subprocess\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"GPU VERIFICATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check PyTorch and CUDA\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "    \n",
    "    # Check if it's A100\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    if 'A100' in gpu_name:\n",
    "        print(\"\\n‚úÖ A100 GPU detected - PERFECT for training!\")\n",
    "    elif 'V100' in gpu_name:\n",
    "        print(\"\\n‚úÖ V100 GPU detected - Good for training (slower than A100)\")\n",
    "    elif 'T4' in gpu_name:\n",
    "        print(\"\\n‚ö†Ô∏è  T4 GPU detected - Will be slower, consider upgrading to A100\")\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è  GPU: {gpu_name} - Consider using A100 for best performance\")\n",
    "else:\n",
    "    print(\"\\n‚ùå NO GPU DETECTED!\")\n",
    "    print(\"Please enable GPU: Runtime ‚Üí Change runtime type ‚Üí Hardware accelerator: GPU\")\n",
    "    raise RuntimeError(\"GPU is required for training\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nvidia-smi"
   },
   "outputs": [],
   "source": [
    "# Detailed GPU info\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-2"
   },
   "source": [
    "## 2. Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mount-drive"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "print(\"\\n‚úÖ Google Drive mounted successfully!\")\n",
    "print(f\"\\nDrive contents:\")\n",
    "!ls -la /content/drive/MyDrive/ | head -20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-3"
   },
   "source": [
    "## 3. Clone Repository to Local Disk (Fast I/O)\n",
    "\n",
    "**Important:** We clone to `/content/` (local SSD) instead of Drive for 10-20x faster I/O\n",
    "\n",
    "**Default:** Clone from GitHub (recommended - always gets latest code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clone-repo"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Configuration\n",
    "PROJECT_NAME = \"Multi-Stream-Neural-Networks\"\n",
    "GITHUB_REPO = \"https://github.com/clingergab/Multi-Stream-Neural-Networks.git\"  # UPDATE THIS\n",
    "LOCAL_REPO_PATH = f\"/content/{PROJECT_NAME}\"  # Local copy for fast I/O\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"REPOSITORY SETUP\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Ensure we're in a valid directory\n",
    "os.chdir('/content')\n",
    "print(f\"Starting in: {os.getcwd()}\")\n",
    "\n",
    "# Check if repo already exists (same session, rerunning cell)\n",
    "if Path(LOCAL_REPO_PATH).exists() and Path(f\"{LOCAL_REPO_PATH}/.git\").exists():\n",
    "    print(f\"\\nüìÅ Repo already exists: {LOCAL_REPO_PATH}\")\n",
    "    print(f\"üîÑ Pulling latest changes...\")\n",
    "    \n",
    "    os.chdir(LOCAL_REPO_PATH)\n",
    "    !git pull\n",
    "    print(\"‚úÖ Repo updated\")\n",
    "\n",
    "# Clone from GitHub (first run)\n",
    "else:\n",
    "    # Remove old incomplete copy if exists\n",
    "    if Path(LOCAL_REPO_PATH).exists():\n",
    "        print(f\"\\nüóëÔ∏è  Removing incomplete repo copy...\")\n",
    "        !rm -rf {LOCAL_REPO_PATH}\n",
    "    \n",
    "    print(f\"\\nüîÑ Cloning from GitHub...\")\n",
    "    print(f\"   Repo: {GITHUB_REPO}\")\n",
    "    print(f\"   Destination: {LOCAL_REPO_PATH}\")\n",
    "    \n",
    "    !git clone {GITHUB_REPO} {LOCAL_REPO_PATH}\n",
    "    \n",
    "    # Verify clone succeeded\n",
    "    if not Path(LOCAL_REPO_PATH).exists():\n",
    "        raise RuntimeError(f\"Failed to clone repository to {LOCAL_REPO_PATH}\")\n",
    "    \n",
    "    print(\"‚úÖ Repo cloned successfully\")\n",
    "    os.chdir(LOCAL_REPO_PATH)\n",
    "\n",
    "# Verify repo structure\n",
    "print(f\"\\nüìÇ Repository structure:\")\n",
    "!ls -la {LOCAL_REPO_PATH}\n",
    "\n",
    "print(f\"\\n‚úÖ Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-4"
   },
   "source": [
    "## 4. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install-deps"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "print(\"Installing dependencies...\")\n",
    "\n",
    "!pip install -q h5py tqdm matplotlib seaborn ray[tune]\n",
    "\n",
    "# Verify installations\n",
    "import h5py\n",
    "import tqdm\n",
    "import matplotlib\n",
    "import seaborn\n",
    "import ray\n",
    "\n",
    "print(\"‚úÖ All dependencies installed!\")\n",
    "print(f\"   h5py: {h5py.__version__}\")\n",
    "print(f\"   matplotlib: {matplotlib.__version__}\")\n",
    "print(f\"   ray: {ray.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-5"
   },
   "source": [
    "## 5. Copy SUN RGB-D Dataset to Local Disk\n",
    "\n",
    "**Performance Note:** Local disk I/O is ~10-20x faster than Drive!\n",
    "\n",
    "**Dataset:** SUN RGB-D 15-category preprocessed dataset (~3.5 GB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download-dataset"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Paths\n",
    "DRIVE_DATASET_TAR = \"/content/drive/MyDrive/datasets/sunrgbd3_15.tar.gz\"  # Compressed file (3-stream)\n",
    "LOCAL_DATASET_PATH = \"/content/data/sunrgbd_15\"  # Extracted location\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SUN RGB-D 15-CATEGORY DATASET SETUP (3-STREAM)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check if already on local disk\n",
    "if Path(LOCAL_DATASET_PATH).exists():\n",
    "    print(f\"‚úÖ Dataset already on local disk: {LOCAL_DATASET_PATH}\")\n",
    "    \n",
    "    # Verify structure\n",
    "    train_rgb_count = len(list(Path(f\"{LOCAL_DATASET_PATH}/train/rgb\").glob(\"*.png\")))\n",
    "    val_rgb_count = len(list(Path(f\"{LOCAL_DATASET_PATH}/val/rgb\").glob(\"*.png\")))\n",
    "    print(f\"   Train samples: {train_rgb_count}\")\n",
    "    print(f\"   Val samples: {val_rgb_count}\")\n",
    "\n",
    "# Copy and extract from Drive\n",
    "elif Path(DRIVE_DATASET_TAR).exists():\n",
    "    print(f\"üìÅ Found compressed dataset on Drive: {DRIVE_DATASET_TAR}\")\n",
    "    print(f\"üì• Copying compressed file to local disk...\")\n",
    "    \n",
    "    # Create parent directory\n",
    "    !mkdir -p /content/data\n",
    "    \n",
    "    # Copy compressed file with progress\n",
    "    print(f\"\\nCopying compressed archive...\")\n",
    "    !rsync -ah --info=progress2 {DRIVE_DATASET_TAR} /content/data/sunrgbd3_15.tar.gz\n",
    "    \n",
    "    # Extract to local disk (suppress macOS metadata warnings)\n",
    "    print(f\"\\nüì¶ Extracting dataset to local disk...\")\n",
    "    !tar -xzf /content/data/sunrgbd3_15.tar.gz -C /content/data/ 2>&1 | grep -v \"Ignoring unknown extended header\"\n",
    "    \n",
    "    # Remove tar file to save space\n",
    "    !rm /content/data/sunrgbd3_15.tar.gz\n",
    "    \n",
    "    print(f\"\\n‚úÖ Dataset extracted to local disk\")\n",
    "    \n",
    "    # Verify extraction\n",
    "    train_rgb_count = len(list(Path(f\"{LOCAL_DATASET_PATH}/train/rgb\").glob(\"*.png\")))\n",
    "    val_rgb_count = len(list(Path(f\"{LOCAL_DATASET_PATH}/val/rgb\").glob(\"*.png\")))\n",
    "    print(f\"   Train samples: {train_rgb_count}\")\n",
    "    print(f\"   Val samples: {val_rgb_count}\")\n",
    "\n",
    "else:\n",
    "    print(f\"‚ùå Compressed dataset not found on Drive!\")\n",
    "    print(f\"   Expected location: {DRIVE_DATASET_TAR}\")\n",
    "    print(f\"\\nüìã To fix this:\")\n",
    "    print(f\"   1. Run: tar -czf sunrgbd3_15.tar.gz -C data sunrgbd_15\")\n",
    "    print(f\"   2. Upload sunrgbd3_15.tar.gz to Google Drive\")\n",
    "    print(f\"   3. Place it at: {DRIVE_DATASET_TAR}\")\n",
    "    raise FileNotFoundError(f\"Compressed dataset not found at {DRIVE_DATASET_TAR}\")\n",
    "\n",
    "# ==================================================================================\n",
    "# CHECK FOR ORTHOGONAL DATA (Required for 3-Stream)\n",
    "# ==================================================================================\n",
    "orth_train_path = Path(f\"{LOCAL_DATASET_PATH}/train/orth\")\n",
    "if not orth_train_path.exists():\n",
    "    print(f\"\\n‚ö†Ô∏è  WARNING: 'orth' directory missing from dataset!\")\n",
    "    print(f\"   The 3-stream model requires RGB, Depth, and Orthogonal data.\")\n",
    "    print(f\"   Please ensure your 'sunrgbd3_15.tar.gz' includes the 'orth' folders.\")\n",
    "    raise FileNotFoundError(f\"Orthogonal data missing at {orth_train_path}\")\n",
    "else:\n",
    "    print(f\"‚úÖ Orthogonal data found at {orth_train_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"Dataset ready at: {LOCAL_DATASET_PATH}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-6"
   },
   "source": [
    "## 6. Setup Python Path & Import LINet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup-imports"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Remove cached modules\n",
    "modules_to_reload = [k for k in sys.modules.keys() if k.startswith('src.')]\n",
    "for module in modules_to_reload:\n",
    "    del sys.modules[module]\n",
    "    \n",
    "# Add project to Python path\n",
    "project_root = '/content/Multi-Stream-Neural-Networks'\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "# Verify project structure\n",
    "print(\"Project structure:\")\n",
    "!ls -la {project_root}/src/models/\n",
    "\n",
    "# Import LINet and SUN RGB-D dataloader\n",
    "print(\"\\nImporting LINet and dataloaders...\")\n",
    "from src.models.linear_integration.li_net3.li_net import li_resnet18, li_resnet50\n",
    "from src.data_utils.sunrgbd_3stream_dataset import get_sunrgbd_3stream_dataloaders\n",
    "\n",
    "# Import Ray Tune\n",
    "from ray import train, tune\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "\n",
    "print(\"‚úÖ LINet, dataloaders, and Ray Tune imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-7"
   },
   "source": [
    "## 7. Load SUN RGB-D Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify dataset structure\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DATASET STRUCTURE VERIFICATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "dataset_root = Path(LOCAL_DATASET_PATH)\n",
    "\n",
    "print(\"\\nDirectory structure:\")\n",
    "print(f\"  {dataset_root}/\")\n",
    "print(f\"    train/\")\n",
    "print(f\"      rgb/ - {len(list((dataset_root / 'train' / 'rgb').glob('*.png')))} images\")\n",
    "print(f\"      depth/ - {len(list((dataset_root / 'train' / 'depth').glob('*.png')))} images\")\n",
    "print(f\"      orth/ - {len(list((dataset_root / 'train' / 'orth').glob('*.png')))} images\")\n",
    "print(f\"      labels.txt\")\n",
    "print(f\"    val/\")\n",
    "print(f\"      rgb/ - {len(list((dataset_root / 'val' / 'rgb').glob('*.png')))} images\")\n",
    "print(f\"      depth/ - {len(list((dataset_root / 'val' / 'depth').glob('*.png')))} images\")\n",
    "print(f\"      orth/ - {len(list((dataset_root / 'val' / 'orth').glob('*.png')))} images\")\n",
    "print(f\"      labels.txt\")\n",
    "print(f\"    class_names.txt\")\n",
    "print(f\"    dataset_info.txt\")\n",
    "\n",
    "# Read class names\n",
    "with open(dataset_root / 'class_names.txt', 'r') as f:\n",
    "    class_names = [line.strip() for line in f]\n",
    "\n",
    "print(f\"\\nClasses ({len(class_names)}):\")\n",
    "for i, name in enumerate(class_names):\n",
    "    print(f\"  {i}: {name}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load-dataset"
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"LOADING SUN RGB-D 15-CATEGORY DATASET (3-STREAM)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Dataset configuration\n",
    "DATASET_CONFIG = {\n",
    "    'data_root': LOCAL_DATASET_PATH,\n",
    "    'batch_size': 128,  # Good balance for A100\n",
    "    'num_workers': 4,\n",
    "    'target_size': (416, 544),  \n",
    "    'num_classes': 15   # SUN RGB-D merged to 15 categories (labels 0-14)\n",
    "}\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "for key, value in DATASET_CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(f\"\\nLoading dataset from: {DATASET_CONFIG['data_root']}\")\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader, val_loader = get_sunrgbd_3stream_dataloaders(\n",
    "    data_root=DATASET_CONFIG['data_root'],\n",
    "    batch_size=DATASET_CONFIG['batch_size'],\n",
    "    num_workers=DATASET_CONFIG['num_workers'],\n",
    "    target_size=DATASET_CONFIG['target_size']\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Dataset loaded successfully!\")\n",
    "print(f\"\\nDataset Statistics:\")\n",
    "print(f\"  Train batches: {len(train_loader)}\")\n",
    "print(f\"  Val batches: {len(val_loader)}\")\n",
    "print(f\"  Train samples: {len(train_loader.dataset)}\")\n",
    "print(f\"  Val samples: {len(val_loader.dataset)}\")\n",
    "print(f\"  Batch size: {DATASET_CONFIG['batch_size']}\")\n",
    "\n",
    "# Test loading a batch\n",
    "print(f\"\\nTesting batch loading...\")\n",
    "rgb_batch, depth_batch, orth_batch, label_batch = next(iter(train_loader))\n",
    "print(f\"  RGB shape: {rgb_batch.shape}\")\n",
    "print(f\"  Depth shape: {depth_batch.shape}\")\n",
    "print(f\"  Orth shape: {orth_batch.shape}\")\n",
    "print(f\"  Labels shape: {label_batch.shape}\")\n",
    "print(f\"  Labels min: {label_batch.min().item()}, max: {label_batch.max().item()}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-8"
   },
   "source": [
    "## 8. Visualize Sample Data\n",
    "\n",
    "Shows RGB images, depth maps, and scene labels from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "visualize-data"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Visualize some samples from TRAINING set (WITH augmentation)\n",
    "# This shows what the model actually sees during training\n",
    "print(\"Loading samples from TRAINING set (with augmentation)...\")\n",
    "rgb_batch, depth_batch, orth_batch, label_batch = next(iter(train_loader))\n",
    "\n",
    "# DEBUGGING: Check for problematic samples\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DEBUGGING: Checking batch for issues\")\n",
    "print(\"=\"*60)\n",
    "for i in range(min(4, len(orth_batch))):\n",
    "    rgb = rgb_batch[i]\n",
    "    depth = depth_batch[i]\n",
    "    orth = orth_batch[i]\n",
    "    \n",
    "    print(f\"\\nSample {i}:\")\n",
    "    print(f\"  RGB shape: {rgb.shape}\")\n",
    "    print(f\"  Depth shape: {depth.shape}\")\n",
    "    print(f\"  Orth shape: {orth.shape}\")\n",
    "    print(f\"  Orth min: {orth.min():.4f}, max: {orth.max():.4f}\")\n",
    "    \n",
    "    # Check for black regions after denormalization\n",
    "    orth_vis = orth * 0.5 + 0.5\n",
    "    orth_vis = torch.clamp(orth_vis, 0, 1)\n",
    "    near_black = (orth_vis < 0.1).sum().item()\n",
    "    total = orth_vis.numel()\n",
    "    print(f\"  Near-black pixels (<0.1): {near_black}/{total} ({100*near_black/total:.1f}%)\")\n",
    "    \n",
    "    if near_black > 0.5 * total:\n",
    "        print(f\"  ‚ö†Ô∏è  WARNING: More than 50% black pixels detected!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Creating visualization...\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "fig, axes = plt.subplots(3, 4, figsize=(14, 10))\n",
    "\n",
    "for i in range(4):\n",
    "    rgb = rgb_batch[i].cpu()\n",
    "    depth = depth_batch[i].cpu()\n",
    "    orth = orth_batch[i].cpu()\n",
    "    label = label_batch[i].item()\n",
    "    \n",
    "    # Denormalize from [-1, 1] to [0, 1]\n",
    "    # Dataset uses mean=0.5, std=0.5 for all modalities\n",
    "    rgb_vis = rgb * 0.5 + 0.5\n",
    "    depth_vis = depth * 0.5 + 0.5\n",
    "    orth_vis = orth * 0.5 + 0.5\n",
    "    \n",
    "    # Clamp to valid range\n",
    "    rgb_vis = torch.clamp(rgb_vis, 0, 1)\n",
    "    depth_vis = torch.clamp(depth_vis, 0, 1)\n",
    "    orth_vis = torch.clamp(orth_vis, 0, 1)\n",
    "    \n",
    "    # Plot RGB\n",
    "    axes[0, i].imshow(rgb_vis.permute(1, 2, 0))\n",
    "    axes[0, i].set_title(f\"RGB - Class {label}\", fontsize=10)\n",
    "    axes[0, i].axis('off')\n",
    "    \n",
    "    # Plot Depth\n",
    "    axes[1, i].imshow(depth_vis.squeeze(), cmap='viridis')\n",
    "    axes[1, i].set_title(f\"Depth - Class {label}\", fontsize=10)\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "    # Plot Orthogonal (with per-image auto-scaling to reveal structure)\n",
    "    # Note: Orth has a very narrow value range (0.34-0.67), so we auto-scale\n",
    "    # each image to use the full colormap range for better visualization\n",
    "    orth_np = orth_vis.squeeze().numpy()\n",
    "    axes[2, i].imshow(orth_np, cmap='magma', vmin=orth_np.min(), vmax=orth_np.max())\n",
    "    axes[2, i].set_title(f\"Orth - Class {label}\", fontsize=10)\n",
    "    axes[2, i].axis('off')\n",
    "\n",
    "plt.suptitle('SUN RGB-D Training Data (RGB + Depth + Orthogonal)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Sample visualization complete!\")\n",
    "print(\"\\nNote: These are from the TRAINING set (with augmentation).\")\n",
    "print(\"Augmentations include: random flip, crop, color jitter, and random erasing.\")\n",
    "print(\"\\nOrthogonal images are auto-scaled per image to show structure (narrow value range).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8b. Hyperparameter Tuning with Ray Tune (Optional)\n",
    "\n",
    "Perform a wide search for optimal hyperparameters using Ray Tune.\n",
    "- **Parallel Trials:** Run multiple configurations simultaneously\n",
    "- **Data Subset:** Use 50% of data for fast iteration\n",
    "- **Short Duration:** Train for 10 epochs per trial\n",
    "- **ASHA Scheduler:** Early stopping for bad trials\n",
    "- **Uses fit() method:** Ensures consistency with main training (no custom training loop!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "import torch\n",
    "from src.models.linear_integration.li_net3.li_net import li_resnet18\n",
    "from src.training.optimizers import create_stream_optimizer\n",
    "from src.training.schedulers import setup_scheduler\n",
    "from src.data_utils.sunrgbd_3stream_dataset import get_sunrgbd_3stream_dataloaders\n",
    "\n",
    "EPOCHS = 10\n",
    "class RayTuneReporter:\n",
    "    \"\"\"Callback for reporting metrics to Ray Tune during training.\"\"\"\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        \"\"\"Called at the end of each epoch to report metrics to Ray Tune.\"\"\"\n",
    "        # Use tune.report() not train.report() when running in Ray Tune!\n",
    "        tune.report({\n",
    "            \"loss\": logs['val_loss'],\n",
    "            \"accuracy\": logs['val_accuracy'],\n",
    "            \"train_loss\": logs['train_loss'],\n",
    "            \"train_accuracy\": logs['train_accuracy']\n",
    "        })\n",
    "\n",
    "\n",
    "def train_linet_tune(config, train_dataset=None, val_dataset=None):\n",
    "    \"\"\"\n",
    "    Trainable function for Ray Tune using fit() method.\n",
    "    \n",
    "    Benefits of using fit() instead of custom loop:\n",
    "    - Reuses all battle-tested training logic from li_net.py\n",
    "    - Consistent behavior between main training and hyperparameter tuning\n",
    "    - Proper scheduler stepping, AMP handling, gradient clipping\n",
    "    - Single source of truth - no code duplication\n",
    "    \n",
    "    Args:\n",
    "        config: Ray Tune configuration dict with hyperparameters\n",
    "        train_dataset: Pre-loaded training dataset (passed via tune.with_parameters)\n",
    "        val_dataset: Pre-loaded validation dataset (passed via tune.with_parameters)\n",
    "    \"\"\"\n",
    "    # 1. Create dataloaders from pre-loaded datasets\n",
    "    # Datasets are created ONCE before tuning starts, then shared across all trials\n",
    "    # This avoids redundant I/O and preprocessing for each trial\n",
    "    \n",
    "    # Create 50% subset for faster hyperparameter evaluation\n",
    "    subset_fraction = 0.5\n",
    "    train_indices = torch.randperm(len(train_dataset))[:int(len(train_dataset) * subset_fraction)]\n",
    "    val_indices = torch.randperm(len(val_dataset))[:int(len(val_dataset) * subset_fraction)]\n",
    "    \n",
    "    train_subset = torch.utils.data.Subset(train_dataset, train_indices)\n",
    "    val_subset = torch.utils.data.Subset(val_dataset, val_indices)\n",
    "    \n",
    "    # Create subset dataloaders\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_subset,\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        shuffle=True,\n",
    "        num_workers=2,  # Reduced for parallel trials\n",
    "        pin_memory=True\n",
    "    )\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_subset,\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        shuffle=False,\n",
    "        num_workers=1,  # Reduced for parallel trials\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # 2. Create Model\n",
    "    model = li_resnet18(\n",
    "        num_classes=15,\n",
    "        stream_input_channels=[3, 1, 1],\n",
    "        dropout_p=config[\"dropout_p\"],\n",
    "        device=\"cuda\",\n",
    "        use_amp=True\n",
    "    )\n",
    "    \n",
    "    # 3. Create Optimizer with stream-specific learning rates\n",
    "    optimizer = create_stream_optimizer(\n",
    "        model,\n",
    "        optimizer_type=config[\"optimizer_type\"],\n",
    "        stream_lrs=[config[\"lr_rgb\"], config[\"lr_depth\"], config[\"lr_orth\"]],\n",
    "        stream_weight_decays=[config[\"wd_rgb\"], config[\"wd_depth\"], config[\"wd_orth\"]],\n",
    "        shared_lr=config[\"lr_shared\"],\n",
    "        shared_weight_decay=config[\"wd_shared\"]\n",
    "    )\n",
    "    \n",
    "    # 4. Create Scheduler\n",
    "    scheduler_kwargs = {}\n",
    "    if config[\"scheduler_type\"] == 'cosine':\n",
    "        scheduler_kwargs['eta_min'] = config[\"eta_min\"]\n",
    "    elif config[\"scheduler_type\"] == 'plateau':\n",
    "        scheduler_kwargs['patience'] = config.get(\"patience\", 3)\n",
    "    \n",
    "    # Disable warmup for Plateau scheduler (incompatible with SequentialLR)\n",
    "    warmup_epochs = 1 if config[\"scheduler_type\"] != 'plateau' else 0\n",
    "    \n",
    "    scheduler = setup_scheduler(\n",
    "        optimizer,\n",
    "        scheduler_type=config[\"scheduler_type\"],\n",
    "        epochs=EPOCHS,  # Match max_t in ASHA scheduler\n",
    "        train_loader_len=len(train_loader),\n",
    "        warmup_epochs=warmup_epochs,\n",
    "        **scheduler_kwargs\n",
    "    )\n",
    "    \n",
    "    # 5. Compile model (Keras-style API)\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        loss='cross_entropy',\n",
    "        label_smoothing=config[\"label_smoothing\"]\n",
    "    )\n",
    "    \n",
    "    # 6. Train using fit() with Ray Tune callback\n",
    "    # This reuses ALL the training logic from li_net.py:\n",
    "    # - Proper scheduler stepping (OneCycleLR, Plateau, etc.)\n",
    "    # - AMP handling with scaler\n",
    "    # - Gradient clipping\n",
    "    # - Progress tracking\n",
    "    # - Consistent with main training\n",
    "    model.fit(\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        epochs=EPOCHS,  # Match max_t in ASHA scheduler\n",
    "        grad_clip_norm=config.get(\"grad_clip_norm\"),\n",
    "        stream_monitoring=False,  # Disable for speed during tuning\n",
    "        callbacks=[RayTuneReporter()],  # Report to Ray Tune after each epoch\n",
    "        verbose=False  # Disable tqdm for cleaner Ray Tune output\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"‚úÖ Tuning function defined successfully!\")\n",
    "print(\"\\nüí° Now using fit() method instead of custom training loop:\")\n",
    "print(\"   - Reuses all battle-tested training logic\")\n",
    "print(\"   - Consistent with main training\")\n",
    "print(\"   - Proper scheduler/AMP/gradient handling\")\n",
    "print(\"   - Single source of truth (no code duplication)\")\n",
    "print(\"   - Uses tune.report() for Ray Tune compatibility\")\n",
    "print(\"\\nüöÄ OPTIMIZATION: Datasets are passed as parameters (created once)\")\n",
    "print(\"   - Avoids redundant I/O and preprocessing for each trial\")\n",
    "print(\"   - Shared across all 100 trials for efficiency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reinitialize Ray to configure for parallel trials\n",
    "import ray\n",
    "ray.shutdown()\n",
    "ray.init(ignore_reinit_error=True, num_gpus=1)\n",
    "\n",
    "# Reduce output verbosity with environment variables\n",
    "import os\n",
    "os.environ[\"TUNE_MAX_PENDING_TRIALS_PG\"] = \"1\"\n",
    "os.environ[\"TUNE_WARN_THRESHOLD_S\"] = \"120\"\n",
    "\n",
    "# STAGE 4: HIGHLY REFINED SEARCH SPACE (Based on Top 10 from Stage 3)\n",
    "# Narrowed ranges around successful hyperparameters\n",
    "search_space = {\n",
    "    # Learning rates - further tightened around top 10 ranges\n",
    "    \"lr_rgb\": tune.loguniform(1e-5, 1.2e-4),      # Top 10: 1.10e-05 to 1.17e-04\n",
    "    \"lr_depth\": tune.loguniform(2.5e-5, 2.7e-4),  # Top 10: 2.53e-05 to 2.63e-04\n",
    "    \"lr_orth\": tune.loguniform(1e-5, 5e-4),       # Top 10: 1.17e-05 to 4.96e-04\n",
    "    \"lr_shared\": tune.loguniform(2e-5, 2.2e-4),   # Top 10: 2.47e-05 to 2.13e-04\n",
    "    \n",
    "    # Weight decay - tightened to top 10 ranges\n",
    "    \"wd_rgb\": tune.loguniform(3e-6, 3e-4),        # Top 10: 3.18e-06 to 3.00e-04\n",
    "    \"wd_depth\": tune.loguniform(1.5e-6, 4.3e-4),  # Top 10: 1.62e-06 to 4.22e-04\n",
    "    \"wd_orth\": tune.loguniform(1.5e-6, 5.2e-5),   # Top 10: 1.58e-06 to 5.11e-05\n",
    "    \"wd_shared\": tune.loguniform(1e-6, 2.7e-4),   # Top 10: 1.31e-06 to 2.64e-04\n",
    "    \n",
    "    \"optimizer_type\": tune.choice(['adamw']),\n",
    "    \"scheduler_type\": tune.choice(['cosine']),\n",
    "    \"label_smoothing\": tune.uniform(0.1, 0.3),\n",
    "    \n",
    "    \"grad_clip_norm\": tune.uniform(1.2, 3.0),     # Top 10: 1.30 to 2.97\n",
    "    \n",
    "    \"eta_min\": tune.loguniform(3e-7, 2e-6),       # Top 10: 3.26e-07 to 1.60e-06\n",
    "    \n",
    "    \"dropout_p\": tune.choice([0.2, 0.3]),         # Top 10: 0.2 (4x), 0.3 (6x)\n",
    "    \"batch_size\": tune.choice([32, 64])               # Top 10: 32 (7x), 64 (3x) - lock to 32\n",
    "}\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"CREATING SHARED DATASETS FOR ALL TRIALS\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nüöÄ OPTIMIZATION: Creating datasets ONCE instead of 100 times\")\n",
    "print(\"   - Shared across all trials for efficiency\")\n",
    "print(\"   - Avoids redundant I/O and preprocessing\\n\")\n",
    "\n",
    "# Create datasets ONCE before tuning starts\n",
    "# These will be shared across all trials via tune.with_parameters()\n",
    "train_loader_shared, val_loader_shared = get_sunrgbd_3stream_dataloaders(\n",
    "    data_root=LOCAL_DATASET_PATH,\n",
    "    batch_size=search_space[\"batch_size\"],  \n",
    "    num_workers=2,\n",
    "    target_size=(416, 544)\n",
    ")\n",
    "\n",
    "# Extract the underlying datasets (not the DataLoaders)\n",
    "train_dataset_shared = train_loader_shared.dataset\n",
    "val_dataset_shared = val_loader_shared.dataset\n",
    "\n",
    "print(f\"‚úÖ Datasets created successfully!\")\n",
    "print(f\"   Training samples: {len(train_dataset_shared):,}\")\n",
    "print(f\"   Validation samples: {len(val_dataset_shared):,}\")\n",
    "print(f\"   These will be reused across all {100} trials\\n\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STARTING HYPERPARAMETER TUNING\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nüí° Running 5 trials in parallel for maximum GPU utilization\")\n",
    "print(f\"   Each trial: ~16 GB GPU memory\")\n",
    "print(f\"   Total: ~48 GB / 80 GB available\")\n",
    "print(f\"   5x speedup compared to sequential execution!\\n\")\n",
    "\n",
    "# Configure Tuner\n",
    "# Pass shared datasets to ALL trials via tune.with_parameters()\n",
    "# This avoids creating datasets 100 times (once per trial)\n",
    "tuner = tune.Tuner(\n",
    "    tune.with_resources(\n",
    "        tune.with_parameters(\n",
    "            train_linet_tune,\n",
    "            train_dataset=train_dataset_shared,  # Shared dataset\n",
    "            val_dataset=val_dataset_shared        # Shared dataset\n",
    "        ),\n",
    "        resources={\"cpu\": 2, \"gpu\": 0.2}  # Each trial gets 1/5 GPU\n",
    "    ),\n",
    "    param_space=search_space,\n",
    "    tune_config=tune.TuneConfig(\n",
    "        metric=\"accuracy\",\n",
    "        mode=\"max\",\n",
    "        scheduler=ASHAScheduler(\n",
    "            max_t=EPOCHS,  \n",
    "            grace_period=2, \n",
    "            reduction_factor=2\n",
    "        ),\n",
    "        num_samples=300,  # 300 trials for broad search\n",
    "        max_concurrent_trials=5  # PARALLEL: Run 5 trials at once for 5x speedup!\n",
    "    )\n",
    ")\n",
    "\n",
    "# Run Tuning with custom progress reporter\n",
    "results = tuner.fit()\n",
    "\n",
    "# Get Best Result\n",
    "best_result = results.get_best_result(\"accuracy\", \"max\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TUNING COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Best Trial Config: {best_result.config}\")\n",
    "print(f\"Best Trial Accuracy: {best_result.metrics['accuracy']:.4f}\")\n",
    "print(f\"Best Trial Loss: {best_result.metrics['loss']:.4f}\")\n",
    "\n",
    "print(\"\\nüìã COPY THIS CONFIGURATION TO THE NEXT CELL:\")\n",
    "print(\"-\" * 40)\n",
    "print(\"STREAM_SPECIFIC_CONFIG = {\")\n",
    "print(f\"    'stream_lrs': [{best_result.config['lr_rgb']:.2e}, {best_result.config['lr_depth']:.2e}, {best_result.config['lr_orth']:.2e}],\")\n",
    "print(f\"    'stream_weight_decays': [{best_result.config['wd_rgb']:.2e}, {best_result.config['wd_depth']:.2e}, {best_result.config['wd_orth']:.2e}],\")\n",
    "print(f\"    'shared_lr': {best_result.config['lr_shared']:.2e},\")\n",
    "print(f\"    'shared_weight_decay': {best_result.config['wd_shared']:.2e}\")\n",
    "print(\"}\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Recommended Optimizer: {best_result.config['optimizer_type']}\")\n",
    "print(f\"Recommended Scheduler: {best_result.config['scheduler_type']}\")\n",
    "if best_result.config['scheduler_type'] == 'cosine':\n",
    "    print(f\"Recommended Eta Min: {best_result.config['eta_min']:.2e}\")\n",
    "elif best_result.config['scheduler_type'] == 'plateau':\n",
    "    print(f\"Recommended Patience: {best_result.config['patience']}\")\n",
    "print(f\"Recommended Grad Clip: {best_result.config['grad_clip_norm']}\")\n",
    "print(f\"Recommended Label Smoothing: {best_result.config['label_smoothing']}\")\n",
    "print(f\"Recommended Dropout: {best_result.config['dropout_p']}\")\n",
    "print(f\"Recommended Batch Size: {best_result.config['batch_size']}\")\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è IMPORTANT: Please also update MODEL_CONFIG (dropout), DATASET_CONFIG (batch_size),\")\n",
    "print(\"   and TRAIN_CONFIG (grad_clip_norm) with the recommended values above!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze Top 10 Trials from Ray Tune\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"TOP 10 TRIALS BY ACCURACY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Get all trials and convert to DataFrame\n",
    "df = results.get_dataframe()\n",
    "\n",
    "# Sort by accuracy (descending)\n",
    "df_sorted = df.sort_values('accuracy', ascending=False)\n",
    "\n",
    "# Select relevant columns for display\n",
    "display_cols = [\n",
    "    'accuracy', 'loss', \n",
    "    'config/lr_rgb', 'config/lr_depth', 'config/lr_orth', 'config/lr_shared',\n",
    "    'config/wd_rgb', 'config/wd_depth', 'config/wd_orth', 'config/wd_shared',\n",
    "    'config/optimizer_type', 'config/scheduler_type', \n",
    "    'config/dropout_p', 'config/batch_size', 'config/grad_clip_norm', 'config/eta_min'\n",
    "]\n",
    "\n",
    "# Get top 10 trials\n",
    "top_10 = df_sorted[display_cols].head(10)\n",
    "\n",
    "# Format for better display\n",
    "top_10_formatted = top_10.copy()\n",
    "top_10_formatted['accuracy'] = top_10_formatted['accuracy'].apply(lambda x: f\"{x*100:.2f}%\")\n",
    "top_10_formatted['loss'] = top_10_formatted['loss'].apply(lambda x: f\"{x:.4f}\")\n",
    "top_10_formatted['config/lr_rgb'] = top_10_formatted['config/lr_rgb'].apply(lambda x: f\"{x:.2e}\")\n",
    "top_10_formatted['config/lr_depth'] = top_10_formatted['config/lr_depth'].apply(lambda x: f\"{x:.2e}\")\n",
    "top_10_formatted['config/lr_orth'] = top_10_formatted['config/lr_orth'].apply(lambda x: f\"{x:.2e}\")\n",
    "top_10_formatted['config/lr_shared'] = top_10_formatted['config/lr_shared'].apply(lambda x: f\"{x:.2e}\")\n",
    "top_10_formatted['config/wd_rgb'] = top_10_formatted['config/wd_rgb'].apply(lambda x: f\"{x:.2e}\")\n",
    "top_10_formatted['config/wd_depth'] = top_10_formatted['config/wd_depth'].apply(lambda x: f\"{x:.2e}\")\n",
    "top_10_formatted['config/wd_orth'] = top_10_formatted['config/wd_orth'].apply(lambda x: f\"{x:.2e}\")\n",
    "top_10_formatted['config/wd_shared'] = top_10_formatted['config/wd_shared'].apply(lambda x: f\"{x:.2e}\")\n",
    "top_10_formatted['config/eta_min'] = top_10_formatted['config/eta_min'].apply(lambda x: f\"{x:.2e}\")\n",
    "\n",
    "# Rename columns for cleaner display\n",
    "top_10_formatted.columns = [\n",
    "    'Accuracy', 'Loss',\n",
    "    'LR RGB', 'LR Depth', 'LR Orth', 'LR Shared',\n",
    "    'WD RGB', 'WD Depth', 'WD Orth', 'WD Shared',\n",
    "    'Optimizer', 'Scheduler',\n",
    "    'Dropout', 'Batch Size', 'Grad Clip', 'Eta Min'\n",
    "]\n",
    "\n",
    "print(\"\\n\")\n",
    "print(top_10_formatted.to_string(index=True))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"BEST TRIAL CONFIGURATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "best_trial = df_sorted.iloc[0]\n",
    "print(f\"\\nRank: #1\")\n",
    "print(f\"Accuracy: {best_trial['accuracy']*100:.2f}%\")\n",
    "print(f\"Loss: {best_trial['loss']:.4f}\")\n",
    "print(f\"\\nLearning Rates:\")\n",
    "print(f\"  RGB:    {best_trial['config/lr_rgb']:.2e}\")\n",
    "print(f\"  Depth:  {best_trial['config/lr_depth']:.2e}\")\n",
    "print(f\"  Orth:   {best_trial['config/lr_orth']:.2e}\")\n",
    "print(f\"  Shared: {best_trial['config/lr_shared']:.2e}\")\n",
    "print(f\"\\nWeight Decays:\")\n",
    "print(f\"  RGB:    {best_trial['config/wd_rgb']:.2e}\")\n",
    "print(f\"  Depth:  {best_trial['config/wd_depth']:.2e}\")\n",
    "print(f\"  Orth:   {best_trial['config/wd_orth']:.2e}\")\n",
    "print(f\"  Shared: {best_trial['config/wd_shared']:.2e}\")\n",
    "print(f\"\\nOptimization:\")\n",
    "print(f\"  Optimizer: {best_trial['config/optimizer_type']}\")\n",
    "print(f\"  Scheduler: {best_trial['config/scheduler_type']}\")\n",
    "print(f\"  Grad Clip: {best_trial['config/grad_clip_norm']}\")\n",
    "print(f\"  Label Smoothing: {best_trial['config/label_smoothing']}\")\n",
    "print(f\"  Dropout: {best_trial['config/dropout_p']}\")\n",
    "print(f\"  Batch Size: {best_trial['config/batch_size']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-9"
   },
   "source": [
    "## 9. Create & Compile LINet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create-model"
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"MODEL CREATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Model configuration\n",
    "MODEL_CONFIG = {\n",
    "    'architecture': 'resnet18',  # or 'resnet50' for better accuracy\n",
    "    'num_classes': 15,  # SUN RGB-D has 15 merged categories (labels 0-14)\n",
    "    'stream_input_channels': [3, 1, 1],  # Stream 0: RGB (3), Stream 1: Depth (1), Stream 2: Orth (1)\n",
    "    'dropout_p': 0.5,  # Dropout for regularization\n",
    "    'device': 'cuda',\n",
    "    'use_amp': True  # Automatic Mixed Precision (2x faster on A100)\n",
    "}\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "for key, value in MODEL_CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Create model\n",
    "print(f\"\\nCreating LINet-{MODEL_CONFIG['architecture'].upper()} (N-stream Linear Integration ResNet)...\")\n",
    "\n",
    "if MODEL_CONFIG['architecture'] == 'resnet18':\n",
    "    model = li_resnet18(\n",
    "        num_classes=MODEL_CONFIG['num_classes'],\n",
    "        stream_input_channels=MODEL_CONFIG['stream_input_channels'],  # N-stream API!\n",
    "        dropout_p=MODEL_CONFIG['dropout_p'],\n",
    "        device=MODEL_CONFIG['device'],\n",
    "        use_amp=MODEL_CONFIG['use_amp']\n",
    "    )\n",
    "elif MODEL_CONFIG['architecture'] == 'resnet50':\n",
    "    model = li_resnet50(\n",
    "        num_classes=MODEL_CONFIG['num_classes'],\n",
    "        stream_input_channels=MODEL_CONFIG['stream_input_channels'],  # N-stream API!\n",
    "        dropout_p=MODEL_CONFIG['dropout_p'],\n",
    "        device=MODEL_CONFIG['device'],\n",
    "        use_amp=MODEL_CONFIG['use_amp']\n",
    "    )\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Count integration-specific parameters\n",
    "integration_params = 0\n",
    "for name, param in model.named_parameters():\n",
    "    if 'integration' in name or 'integrated_weight' in name:\n",
    "        integration_params += param.numel()\n",
    "\n",
    "print(f\"\\n‚úÖ Model created successfully!\")\n",
    "print(f\"\\nModel Statistics:\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"  Integration parameters: {integration_params:,}\")\n",
    "print(f\"  Model size: {total_params * 4 / 1024**2:.2f} MB (FP32)\")\n",
    "print(f\"  Architecture: {len(MODEL_CONFIG['stream_input_channels'])}-stream Linear Integration (LINet)\")\n",
    "print(f\"  Device: {MODEL_CONFIG['device']}\")\n",
    "print(f\"  AMP enabled: {MODEL_CONFIG['use_amp']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "compile-model"
   },
   "source": [
    "## 9b. Model Compilation (Keras-Style API with Warmup)\n",
    "\n",
    "**Create optimizer and scheduler as objects, then pass to compile()**\n",
    "\n",
    "**NEW:** Learning rate warmup support! The scheduler will linearly increase the learning rate from a lower starting point to the target LR over the first few epochs, helping stabilize early training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model with stream-specific optimization\n",
    "print(\"=\" * 60)\n",
    "print(\"MODEL COMPILATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Import optimizer and scheduler utilities\n",
    "from src.training.optimizers import create_stream_optimizer\n",
    "from src.training.schedulers import setup_scheduler\n",
    "\n",
    "# Stream-specific configuration for optimal RGB/Depth/Orth balance\n",
    "STREAM_SPECIFIC_CONFIG = {\n",
    "    # Stream-specific learning rates (adjusted based on research):\n",
    "    'stream_lrs': [3e-5, 1e-4, 1e-4],  # [RGB, Depth, Orth] - N-stream format!\n",
    "    'stream_weight_decays': [5e-4, 1e-4, 1e-4],  # [RGB, Depth, Orth]\n",
    "    'shared_lr': 7e-5,  # Shared params: base LR\n",
    "    'shared_weight_decay': 2e-4,  # Shared params: base WD\n",
    "}\n",
    "\n",
    "# Scheduler configuration (with warmup support!)\n",
    "SCHEDULER_CONFIG = {\n",
    "    'scheduler_type': 'cosine',\n",
    "    't_max': 80,  # Will be updated to match epochs in training config\n",
    "    'eta_min': 1e-6,\n",
    "    'warmup_epochs': 5,  # Warmup: linearly increase LR for first 5 epochs\n",
    "    'warmup_start_factor': 0.1  # Start at 10% of target LR during warmup\n",
    "}\n",
    "\n",
    "print(f\"Stream-Specific Configuration:\")\n",
    "for key, value in STREAM_SPECIFIC_CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(f\"\\nScheduler Configuration (with warmup):\")\n",
    "for key, value in SCHEDULER_CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Step 1: Create optimizer with stream-specific learning rates\n",
    "print(\"\\n[Step 1] Creating stream-specific optimizer...\")\n",
    "optimizer = create_stream_optimizer(\n",
    "    model,\n",
    "    optimizer_type='adamw',\n",
    "    stream_lrs=STREAM_SPECIFIC_CONFIG['stream_lrs'],\n",
    "    stream_weight_decays=STREAM_SPECIFIC_CONFIG['stream_weight_decays'],\n",
    "    shared_lr=STREAM_SPECIFIC_CONFIG['shared_lr'],\n",
    "    shared_weight_decay=STREAM_SPECIFIC_CONFIG['shared_weight_decay']\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Optimizer created: {optimizer.__class__.__name__}\")\n",
    "print(f\"   Parameter groups: {len(optimizer.param_groups)}\")\n",
    "for i, group in enumerate(optimizer.param_groups):\n",
    "    num_params = sum(p.numel() for p in group['params'])\n",
    "    print(f\"   Group {i+1}: lr={group['lr']:.2e}, wd={group['weight_decay']:.2e}, params={num_params:,}\")\n",
    "\n",
    "# Step 2: Create scheduler with warmup support\n",
    "print(\"\\n[Step 2] Creating learning rate scheduler with warmup...\")\n",
    "scheduler = setup_scheduler(\n",
    "    optimizer,\n",
    "    scheduler_type=SCHEDULER_CONFIG['scheduler_type'],\n",
    "    epochs=80,  # Placeholder - will match TRAIN_CONFIG['epochs']\n",
    "    train_loader_len=len(train_loader),\n",
    "    t_max=SCHEDULER_CONFIG['t_max'],\n",
    "    eta_min=SCHEDULER_CONFIG['eta_min'],\n",
    "    warmup_epochs=SCHEDULER_CONFIG['warmup_epochs'],\n",
    "    warmup_start_factor=SCHEDULER_CONFIG['warmup_start_factor']\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Scheduler created: {scheduler.__class__.__name__}\")\n",
    "print(f\"   Warmup: {SCHEDULER_CONFIG['warmup_epochs']} epochs (LR: {SCHEDULER_CONFIG['warmup_start_factor']*100:.0f}% ‚Üí 100%)\")\n",
    "print(f\"   Main scheduler: {SCHEDULER_CONFIG['scheduler_type']} annealing\")\n",
    "\n",
    "# Step 3: Compile model with optimizer and scheduler objects (Keras-style!)\n",
    "print(\"\\n[Step 3] Compiling model with optimizer and scheduler objects...\")\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    loss='cross_entropy',\n",
    "    label_smoothing=0.1\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Model compiled successfully (Keras-style)!\")\n",
    "print(\"\\nüí° Learning rate warmup enabled!\")\n",
    "print(f\"   First {SCHEDULER_CONFIG['warmup_epochs']} epochs: LR increases from {SCHEDULER_CONFIG['warmup_start_factor']*100:.0f}% to 100%\")\n",
    "print(f\"   Remaining epochs: {SCHEDULER_CONFIG['scheduler_type']} annealing from 100% to {SCHEDULER_CONFIG['eta_min']:.0e}\")\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-10"
   },
   "source": [
    "## 10. Test Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "test-forward"
   },
   "outputs": [],
   "source": [
    "# Test forward pass with detailed debugging\n",
    "print(\"Testing forward pass with CUDA_LAUNCH_BLOCKING for better error messages...\")\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'  # Synchronous CUDA for better error messages\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    rgb_test, depth_test, orth_test, labels_test = next(iter(train_loader))\n",
    "    \n",
    "    print(f\"\\nInput validation:\")\n",
    "    print(f\"  RGB shape: {rgb_test.shape}, dtype: {rgb_test.dtype}\")\n",
    "    print(f\"  RGB min: {rgb_test.min():.4f}, max: {rgb_test.max():.4f}\")\n",
    "    print(f\"  RGB has NaN: {torch.isnan(rgb_test).any()}\")\n",
    "    print(f\"  RGB has Inf: {torch.isinf(rgb_test).any()}\")\n",
    "    \n",
    "    print(f\"\\n  Depth shape: {depth_test.shape}, dtype: {depth_test.dtype}\")\n",
    "    print(f\"  Depth min: {depth_test.min():.4f}, max: {depth_test.max():.4f}\")\n",
    "    print(f\"  Depth has NaN: {torch.isnan(depth_test).any()}\")\n",
    "    print(f\"  Depth has Inf: {torch.isinf(depth_test).any()}\")\n",
    "\n",
    "    print(f\"\\n  Orth shape: {orth_test.shape}, dtype: {orth_test.dtype}\")\n",
    "    print(f\"  Orth min: {orth_test.min():.4f}, max: {orth_test.max():.4f}\")\n",
    "    print(f\"  Orth has NaN: {torch.isnan(orth_test).any()}\")\n",
    "    print(f\"  Orth has Inf: {torch.isinf(orth_test).any()}\")\n",
    "    \n",
    "    print(f\"\\n  Labels shape: {labels_test.shape}, dtype: {labels_test.dtype}\")\n",
    "    print(f\"  Labels min: {labels_test.min()}, max: {labels_test.max()}\")\n",
    "    print(f\"  Labels unique: {torch.unique(labels_test).tolist()}\")\n",
    "    \n",
    "    print(\"\\nRunning forward pass...\")\n",
    "    rgb_cuda = rgb_test.to('cuda')\n",
    "    depth_cuda = depth_test.to('cuda')\n",
    "    orth_cuda = orth_test.to('cuda')\n",
    "    \n",
    "    try:\n",
    "        # Pass list of streams for N-stream model\n",
    "        outputs = model([rgb_cuda, depth_cuda, orth_cuda])\n",
    "        print(f\"  ‚úÖ Forward pass successful!\")\n",
    "        print(f\"  Output shape: {outputs.shape}\")\n",
    "        print(f\"  Output min: {outputs.min():.4f}, max: {outputs.max():.4f}\")\n",
    "        \n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        print(f\"\\nSample predictions: {predictions.cpu().numpy()[:10]}\")\n",
    "        print(f\"Ground truth: {labels_test.numpy()[:10]}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Forward pass failed!\")\n",
    "        print(f\"Error: {e}\")\n",
    "        print(f\"\\nThis is likely a model architecture issue, not a data issue.\")\n",
    "        print(f\"Possible causes:\")\n",
    "        print(f\"  1. BatchNorm running stats issue\")\n",
    "        print(f\"  2. Invalid tensor operations in model\")\n",
    "        print(f\"  3. Memory corruption\")\n",
    "        raise\n",
    "\n",
    "print(\"\\n‚úÖ Forward pass test complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-11"
   },
   "source": [
    "## 11. Setup Checkpoint Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"CHECKPOINT DIRECTORY SETUP\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create checkpoint directory on Google Drive (persistent storage)\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "checkpoint_dir = f\"/content/drive/MyDrive/linet_checkpoints/run_{timestamp}\"\n",
    "\n",
    "# Create directory\n",
    "Path(checkpoint_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"\\n‚úÖ Checkpoint directory created:\")\n",
    "print(f\"   {checkpoint_dir}\")\n",
    "print(f\"\\nAll training artifacts will be saved here:\")\n",
    "print(f\"  ‚Ä¢ Best model weights\")\n",
    "print(f\"  ‚Ä¢ Training history\")\n",
    "print(f\"  ‚Ä¢ Monitoring metrics\")\n",
    "print(f\"  ‚Ä¢ Visualizations\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Train the Model üöÄ\n",
    "\n",
    "**Expected time:** ~2-3 hours for 90 epochs on A100\n",
    "\n",
    "**All optimizations enabled:**\n",
    "- ‚úÖ Automatic Mixed Precision (2x faster)\n",
    "- ‚úÖ Gradient Clipping (stability)\n",
    "- ‚úÖ Cosine Annealing LR\n",
    "- ‚úÖ Early Stopping\n",
    "- ‚úÖ Best Model Checkpointing\n",
    "- ‚úÖ Local disk I/O (10-20x faster than Drive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup-checkpoints"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Suppress PyTorch SequentialLR deprecation warning (internal PyTorch issue, not our code)\n",
    "warnings.filterwarnings(\n",
    "    'ignore',\n",
    "    message='The epoch parameter in `scheduler.step\\\\(\\\\)` was not necessary',\n",
    "    category=UserWarning\n",
    ")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TRAINING WITH STREAM MONITORING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Training configuration\n",
    "TRAIN_CONFIG = {\n",
    "    'epochs': 80,\n",
    "    'grad_clip_norm': 5.0,\n",
    "    'early_stopping': True,\n",
    "    'patience': 12,\n",
    "    'min_delta': 0.001,\n",
    "    'monitor': 'val_accuracy',\n",
    "    'restore_best_weights': True,\n",
    "    'save_path': f\"{checkpoint_dir}/best_model.pt\",\n",
    "    'stream_monitoring': True,  # Built-in stream monitoring!\n",
    "}\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "for key, value in TRAIN_CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"Training will take approximately 2-3 hours on A100\")\n",
    "print(f\"Stream monitoring active - detailed per-stream metrics shown each epoch\")\n",
    "print(\"==\" * 60 + \"\\n\")\n",
    "\n",
    "# Train using built-in fit() method with stream monitoring\n",
    "# NOTE: No scheduler_kwargs needed! Scheduler was already created and passed to compile()\n",
    "history = model.fit(\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    epochs=TRAIN_CONFIG['epochs'],\n",
    "    verbose=True,\n",
    "    save_path=TRAIN_CONFIG['save_path'],\n",
    "    early_stopping=TRAIN_CONFIG['early_stopping'],\n",
    "    patience=TRAIN_CONFIG['patience'],\n",
    "    min_delta=TRAIN_CONFIG['min_delta'],\n",
    "    monitor=TRAIN_CONFIG['monitor'],\n",
    "    restore_best_weights=TRAIN_CONFIG['restore_best_weights'],\n",
    "    grad_clip_norm=TRAIN_CONFIG['grad_clip_norm'],\n",
    "    stream_monitoring=TRAIN_CONFIG['stream_monitoring']  # Enable built-in monitoring\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üéâ TRAINING COMPLETE!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-13"
   },
   "source": [
    "## 13. Evaluate Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"FINAL EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Evaluate on validation set\n",
    "results = model.evaluate(data_loader=val_loader)\n",
    "\n",
    "print(f\"\\nFinal Validation Results:\")\n",
    "print(f\"  Loss: {results['loss']:.4f}\")\n",
    "print(f\"  Overall Accuracy: {results['accuracy']*100:.2f}%\")\n",
    "\n",
    "# Show per-stream accuracies (always available with stream_monitoring=True)\n",
    "print(f\"\\nStream-Specific Performance:\")\n",
    "for i in range(len(MODEL_CONFIG['stream_input_channels'])):\n",
    "    print(f\"  Stream{i} Accuracy: {results[f'stream{i}_accuracy']*100:.2f}%\")\n",
    "\n",
    "print(f\"\\nTraining Summary:\")\n",
    "print(f\"  Initial train loss: {history['train_loss'][0]:.4f}\")\n",
    "print(f\"  Final train loss: {history['train_loss'][-1]:.4f}\")\n",
    "print(f\"  Best val loss: {min(history['val_loss']):.4f}\")\n",
    "print(f\"  Initial train acc: {history['train_accuracy'][0]*100:.2f}%\")\n",
    "print(f\"  Final train acc: {history['train_accuracy'][-1]*100:.2f}%\")\n",
    "print(f\"  Best val acc: {max(history['val_accuracy'])*100:.2f}%\")\n",
    "print(f\"  Total epochs: {len(history['train_loss'])}\")\n",
    "\n",
    "if 'early_stopping' in history:\n",
    "    print(f\"\\nEarly Stopping Info:\")\n",
    "    print(f\"  Stopped early: {history['early_stopping']['stopped_early']}\")\n",
    "    print(f\"  Best epoch: {history['early_stopping']['best_epoch']}\")\n",
    "    print(f\"  Best {history['early_stopping']['monitor']}: {history['early_stopping']['best_metric']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-14"
   },
   "source": [
    "## 14. Plot Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "plot-curves"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Loss curve\n",
    "axes[0].plot(history['train_loss'], label='Train Loss', linewidth=2)\n",
    "axes[0].plot(history['val_loss'], label='Val Loss', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Loss', fontsize=12)\n",
    "axes[0].set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy curve with per-stream curves\n",
    "axes[1].plot([acc*100 for acc in history['train_accuracy']], label='Full Model Train', linewidth=2, color='green')\n",
    "axes[1].plot([acc*100 for acc in history['val_accuracy']], label='Full Model Val', linewidth=2, color='darkgreen')\n",
    "\n",
    "# Add per-stream curves (always available with stream_monitoring=True)\n",
    "stream_train_colors = ['skyblue', 'lightcoral', 'gold', 'lightgreen', 'plum']\n",
    "stream_val_colors = ['blue', 'red', 'orange', 'green', 'purple']\n",
    "for i in range(len(MODEL_CONFIG['stream_input_channels'])):\n",
    "    color_idx = i % len(stream_train_colors)\n",
    "    axes[1].plot([acc*100 for acc in history[f'stream_{i}_train_acc']], \n",
    "                label=f'Stream{i} Train', linewidth=1, alpha=0.6, linestyle='--', color=stream_train_colors[color_idx])\n",
    "    axes[1].plot([acc*100 for acc in history[f'stream_{i}_val_acc']], \n",
    "                label=f'Stream{i} Val', linewidth=1, alpha=0.6, linestyle='--', \n",
    "                color=stream_val_colors[color_idx])\n",
    "\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Accuracy (%)', fontsize=12)\n",
    "axes[1].set_title('Training and Validation Accuracy\\n(Full Model = Integrated Stream)', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=9, loc='lower right')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Learning rate curve with per-stream LRs\n",
    "sampled_lrs = history['learning_rates'][::max(1, len(history['learning_rates'])//100)]\n",
    "axes[2].plot(sampled_lrs, linewidth=2, color='green', label='Base LR')\n",
    "\n",
    "# Add per-stream LRs (always available with stream_monitoring=True)\n",
    "lr_colors = ['blue', 'red', 'orange', 'purple', 'brown']\n",
    "for i in range(len(MODEL_CONFIG['stream_input_channels'])):\n",
    "    color_idx = i % len(lr_colors)\n",
    "    axes[2].plot(history[f'stream_{i}_lr'], linewidth=1, alpha=0.7, linestyle='--', \n",
    "                color=lr_colors[color_idx], label=f'Stream{i} LR')\n",
    "\n",
    "axes[2].set_xlabel('Epoch', fontsize=12)\n",
    "axes[2].set_ylabel('Learning Rate', fontsize=12)\n",
    "axes[2].set_title('Learning Rate Schedule', fontsize=14, fontweight='bold')\n",
    "axes[2].legend(fontsize=9, loc='upper right')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{checkpoint_dir}/training_curves.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úÖ Training curves saved to: {checkpoint_dir}/training_curves.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-15"
   },
   "source": [
    "## 15. Pathway Analysis (Stream1/Stream2/Integrated Contributions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pathway-analysis"
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(f\"PATHWAY ANALYSIS ({len(MODEL_CONFIG['stream_input_channels'])}-STREAM LINET)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nAnalyzing {len(MODEL_CONFIG['stream_input_channels'])} stream pathways and integrated pathway contributions...\")\n",
    "print(\"This may take a few minutes...\\n\")\n",
    "\n",
    "# Analyze pathways\n",
    "pathway_analysis = model.analyze_pathways(data_loader=val_loader)\n",
    "\n",
    "print(\"\\nAccuracy Metrics:\")\n",
    "print(f\"  Full model (Integrated): {pathway_analysis['accuracy']['full_model']*100:.2f}%\")\n",
    "for i in range(len(MODEL_CONFIG['stream_input_channels'])):\n",
    "    print(f\"  Stream{i} only: {pathway_analysis['accuracy'][f'stream{i}_only']*100:.2f}%\")\n",
    "print(f\"  Integrated only: {pathway_analysis['accuracy']['integrated_only']*100:.2f}%\")\n",
    "print()\n",
    "for i in range(len(MODEL_CONFIG['stream_input_channels'])):\n",
    "    print(f\"  Stream{i} contribution: {pathway_analysis['accuracy'][f'stream{i}_contribution']*100:.2f}%\")\n",
    "print(f\"  Integrated contribution: {pathway_analysis['accuracy']['integrated_contribution']*100:.2f}%\")\n",
    "\n",
    "print(\"\\nLoss Metrics:\")\n",
    "print(f\"  Full model: {pathway_analysis['loss']['full_model']:.4f}\")\n",
    "for i in range(len(MODEL_CONFIG['stream_input_channels'])):\n",
    "    print(f\"  Stream{i} only: {pathway_analysis['loss'][f'stream{i}_only']:.4f}\")\n",
    "print(f\"  Integrated only: {pathway_analysis['loss']['integrated_only']:.4f}\")\n",
    "\n",
    "print(\"\\nFeature Norm Statistics:\")\n",
    "for i in range(len(MODEL_CONFIG['stream_input_channels'])):\n",
    "    print(f\"  Stream{i} mean: {pathway_analysis['feature_norms'][f'stream{i}_mean']:.4f}\")\n",
    "    print(f\"  Stream{i} std: {pathway_analysis['feature_norms'][f'stream{i}_std']:.4f}\")\n",
    "print(f\"  Integrated mean: {pathway_analysis['feature_norms']['integrated_mean']:.4f}\")\n",
    "print(f\"  Integrated std: {pathway_analysis['feature_norms']['integrated_std']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"INTEGRATION WEIGHT ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nAnalyzing integration weight magnitudes...\")\n",
    "print(\"(Measures how much the architecture favors each stream)\\n\")\n",
    "\n",
    "# Calculate stream contributions to integration\n",
    "integration_contributions = model.calculate_stream_contributions_to_integration()\n",
    "\n",
    "print(\"Integration Weight Contributions:\")\n",
    "for i in range(len(MODEL_CONFIG['stream_input_channels'])):\n",
    "    print(f\"  Stream{i}: {integration_contributions['interpretation'][f'stream{i}_percentage']}\")\n",
    "\n",
    "print(\"\\nRaw Integration Weight Norms:\")\n",
    "for i in range(len(MODEL_CONFIG['stream_input_channels'])):\n",
    "    print(f\"  Stream{i} integration weights: {integration_contributions['raw_norms'][f'stream{i}_integration_weights']:.4f}\")\n",
    "print(f\"  Total: {integration_contributions['raw_norms']['total']:.4f}\")\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "# Find the dominant stream\n",
    "contribs = [integration_contributions[f'stream{i}_contribution'] for i in range(len(MODEL_CONFIG['stream_input_channels']))]\n",
    "max_contrib = max(contribs)\n",
    "if max_contrib > 0.55:\n",
    "    dominant_idx = contribs.index(max_contrib)\n",
    "    print(f\"  ‚Üí Architecture favors Stream{dominant_idx} - larger integration weights\")\n",
    "else:\n",
    "    print(\"  ‚Üí Architecture uses all streams fairly equally\")\n",
    "\n",
    "print(f\"\\nNote: {integration_contributions['note']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ Pathway analysis complete!\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Visualize pathway contributions\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Accuracy comparison\n",
    "pathway_labels = ['Full Model\\n(Integrated)'] + [f'Stream{i}' for i in range(len(MODEL_CONFIG['stream_input_channels']))]\n",
    "accuracies = [pathway_analysis['accuracy']['full_model'] * 100]\n",
    "accuracies += [pathway_analysis['accuracy'][f'stream{i}_only'] * 100 for i in range(len(MODEL_CONFIG['stream_input_channels']))]\n",
    "colors = ['green'] + ['blue', 'orange', 'purple', 'brown', 'pink'][:len(MODEL_CONFIG['stream_input_channels'])]\n",
    "\n",
    "axes[0].bar(pathway_labels, accuracies, color=colors, alpha=0.7)\n",
    "axes[0].set_ylabel('Accuracy (%)', fontsize=12)\n",
    "axes[0].set_title('Pathway Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "for i, v in enumerate(accuracies):\n",
    "    axes[0].text(i, v + 1, f'{v:.1f}%', ha='center', fontweight='bold')\n",
    "\n",
    "# Feature norm comparison\n",
    "norm_labels = [f'Stream{i}' for i in range(len(MODEL_CONFIG['stream_input_channels']))] + ['Integrated']\n",
    "norm_values = [pathway_analysis['feature_norms'][f'stream{i}_mean'] for i in range(len(MODEL_CONFIG['stream_input_channels']))]\n",
    "norm_values.append(pathway_analysis['feature_norms']['integrated_mean'])\n",
    "norm_colors = ['blue', 'orange', 'purple', 'brown', 'pink'][:len(MODEL_CONFIG['stream_input_channels'])] + ['darkgreen']\n",
    "\n",
    "axes[1].bar(norm_labels, norm_values, color=norm_colors, alpha=0.7)\n",
    "axes[1].set_ylabel('Feature Norm (Mean)', fontsize=12)\n",
    "axes[1].set_title('Runtime Feature Magnitude', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "for i, v in enumerate(norm_values):\n",
    "    axes[1].text(i, v + 0.1, f'{v:.2f}', ha='center', fontweight='bold')\n",
    "\n",
    "# Integration weight contributions (input streams only)\n",
    "integration_labels = [f'Stream{i}' for i in range(len(MODEL_CONFIG['stream_input_channels']))]\n",
    "integration_values = [integration_contributions[f'stream{i}_contribution'] * 100 for i in range(len(MODEL_CONFIG['stream_input_channels']))]\n",
    "integration_colors = ['blue', 'orange', 'purple', 'brown', 'pink'][:len(MODEL_CONFIG['stream_input_channels'])]\n",
    "\n",
    "axes[2].bar(integration_labels, integration_values, color=integration_colors, alpha=0.7)\n",
    "axes[2].set_ylabel('Contribution (%)', fontsize=12)\n",
    "axes[2].set_title('Integration Weight Contribution', fontsize=14, fontweight='bold')\n",
    "axes[2].grid(True, alpha=0.3, axis='y')\n",
    "for i, v in enumerate(integration_values):\n",
    "    axes[2].text(i, v + 1, f'{v:.1f}%', ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{checkpoint_dir}/pathway_analysis.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úÖ Pathway analysis plot saved to: {checkpoint_dir}/pathway_analysis.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-16"
   },
   "source": [
    "## 16. Save Results & Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save-results"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SAVING RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Save training history as JSON\n",
    "history_path = f\"{checkpoint_dir}/training_history.json\"\n",
    "with open(history_path, 'w') as f:\n",
    "    json_history = {\n",
    "        'train_loss': [float(x) for x in history['train_loss']],\n",
    "        'val_loss': [float(x) for x in history['val_loss']],\n",
    "        'train_accuracy': [float(x) for x in history['train_accuracy']],\n",
    "        'val_accuracy': [float(x) for x in history['val_accuracy']],\n",
    "        'learning_rates': [float(x) for x in history['learning_rates']],\n",
    "        'model_config': MODEL_CONFIG,\n",
    "        'dataset_config': DATASET_CONFIG,\n",
    "        'stream_specific_config': STREAM_SPECIFIC_CONFIG,\n",
    "        'scheduler_config': SCHEDULER_CONFIG,\n",
    "        'training_config': TRAIN_CONFIG,\n",
    "        'final_results': {\n",
    "            'val_loss': float(results['loss']),\n",
    "            'val_accuracy': float(results['accuracy'])\n",
    "        },\n",
    "        'pathway_analysis': {\n",
    "            'full_model_accuracy': float(pathway_analysis['accuracy']['full_model']),\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Add per-stream pathway analysis results\n",
    "    for i in range(len(MODEL_CONFIG['stream_input_channels'])):\n",
    "        json_history['pathway_analysis'][f'stream{i}_only_accuracy'] = float(pathway_analysis['accuracy'][f'stream{i}_only'])\n",
    "    json_history['pathway_analysis']['integrated_only_accuracy'] = float(pathway_analysis['accuracy']['integrated_only'])\n",
    "    \n",
    "    if 'early_stopping' in history:\n",
    "        json_history['early_stopping'] = history['early_stopping']\n",
    "    \n",
    "    json.dump(json_history, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Training history saved: {history_path}\")\n",
    "\n",
    "# Save final model (in addition to best model)\n",
    "final_model_path = f\"{checkpoint_dir}/final_model.pt\"\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': model.optimizer.state_dict(),\n",
    "    'scheduler_state_dict': model.scheduler.state_dict() if model.scheduler else None,\n",
    "    'config': MODEL_CONFIG,\n",
    "    'stream_specific_config': STREAM_SPECIFIC_CONFIG,\n",
    "    'scheduler_config': SCHEDULER_CONFIG,\n",
    "    'history': history,\n",
    "    'val_accuracy': results['accuracy']\n",
    "}, final_model_path)\n",
    "\n",
    "print(f\"‚úÖ Final model saved: {final_model_path}\")\n",
    "\n",
    "# Save summary report\n",
    "summary_path = f\"{checkpoint_dir}/summary.txt\"\n",
    "with open(summary_path, 'w') as f:\n",
    "    f.write(\"=\" * 60 + \"\\n\")\n",
    "    f.write(\"LINet Training Summary - SUN RGB-D\\n\")\n",
    "    f.write(\"=\" * 60 + \"\\n\\n\")\n",
    "    \n",
    "    # Model Configuration\n",
    "    f.write(\"Model Configuration:\\n\")\n",
    "    f.write(f\"  Architecture: LINet-{MODEL_CONFIG['architecture'].upper()} ({len(MODEL_CONFIG['stream_input_channels'])}-stream Linear Integration)\\n\")\n",
    "    f.write(f\"  Num Classes: {MODEL_CONFIG['num_classes']}\\n\")\n",
    "    f.write(f\"  Stream Input Channels: {MODEL_CONFIG['stream_input_channels']}\\n\")\n",
    "    f.write(f\"  Dropout: {MODEL_CONFIG['dropout_p']}\\n\")\n",
    "    f.write(f\"  Device: {MODEL_CONFIG['device']}\\n\")\n",
    "    f.write(f\"  AMP Enabled: {MODEL_CONFIG['use_amp']}\\n\")\n",
    "    f.write(f\"  Total Parameters: {total_params:,}\\n\")\n",
    "    f.write(f\"  Trainable Parameters: {trainable_params:,}\\n\")\n",
    "    f.write(f\"  Integration Parameters: {integration_params:,}\\n\")\n",
    "    \n",
    "    # Dataset Configuration\n",
    "    f.write(f\"\\nDataset Configuration:\\n\")\n",
    "    f.write(f\"  Dataset: SUN RGB-D 15-category (Scene Classification)\\n\")\n",
    "    f.write(f\"  Training Samples: {len(train_loader.dataset)}\\n\")\n",
    "    f.write(f\"  Validation Samples: {len(val_loader.dataset)}\\n\")\n",
    "    f.write(f\"  Batch Size: {DATASET_CONFIG['batch_size']}\\n\")\n",
    "    f.write(f\"  Num Workers: {DATASET_CONFIG['num_workers']}\\n\")\n",
    "    f.write(f\"  Input Size: {DATASET_CONFIG['target_size']}\\n\")\n",
    "    \n",
    "    # Optimization Configuration\n",
    "    f.write(f\"\\nOptimization Configuration (Keras-Style API):\\n\")\n",
    "    f.write(f\"  Optimizer: AdamW (stream-specific)\\n\")\n",
    "    f.write(f\"  Loss Function: cross_entropy\\n\")\n",
    "    f.write(f\"  Label Smoothing: 0.1\\n\")\n",
    "    f.write(f\"  Scheduler: {SCHEDULER_CONFIG['scheduler_type']}\\n\")\n",
    "    f.write(f\"  Scheduler t_max: {SCHEDULER_CONFIG['t_max']}\\n\")\n",
    "    f.write(f\"  Scheduler eta_min: {SCHEDULER_CONFIG['eta_min']}\\n\")\n",
    "    f.write(f\"  Gradient Clipping: {TRAIN_CONFIG['grad_clip_norm']}\\n\")\n",
    "    \n",
    "    # Stream-Specific Settings\n",
    "    f.write(f\"\\nStream-Specific Settings:\\n\")\n",
    "    for i in range(len(MODEL_CONFIG['stream_input_channels'])):\n",
    "        lr = STREAM_SPECIFIC_CONFIG['stream_lrs'][i]\n",
    "        wd = STREAM_SPECIFIC_CONFIG['stream_weight_decays'][i]\n",
    "        f.write(f\"  Stream{i}:\\n\")\n",
    "        f.write(f\"    Learning Rate: {lr}\\n\")\n",
    "        f.write(f\"    Weight Decay: {wd}\\n\")\n",
    "    f.write(f\"  Shared (Fusion/Classifier):\\n\")\n",
    "    f.write(f\"    Learning Rate: {STREAM_SPECIFIC_CONFIG['shared_lr']}\\n\")\n",
    "    f.write(f\"    Weight Decay: {STREAM_SPECIFIC_CONFIG['shared_weight_decay']}\\n\")\n",
    "    \n",
    "    # Training Configuration\n",
    "    f.write(f\"\\nTraining Configuration:\\n\")\n",
    "    f.write(f\"  Total Epochs: {len(history['train_loss'])}\\n\")\n",
    "    f.write(f\"  Stream Monitoring: {TRAIN_CONFIG['stream_monitoring']}\\n\")\n",
    "    f.write(f\"  Early Stopping: {TRAIN_CONFIG['early_stopping']}\\n\")\n",
    "    if TRAIN_CONFIG['early_stopping']:\n",
    "        f.write(f\"    Monitor: {TRAIN_CONFIG['monitor']}\\n\")\n",
    "        f.write(f\"    Patience: {TRAIN_CONFIG['patience']}\\n\")\n",
    "        f.write(f\"    Min Delta: {TRAIN_CONFIG['min_delta']}\\n\")\n",
    "        f.write(f\"    Restore Best Weights: {TRAIN_CONFIG['restore_best_weights']}\\n\")\n",
    "    \n",
    "    # Results\n",
    "    f.write(f\"\\nFinal Results:\\n\")\n",
    "    f.write(f\"  Val Loss: {results['loss']:.4f}\\n\")\n",
    "    f.write(f\"  Val Accuracy: {results['accuracy']*100:.2f}%\\n\")\n",
    "    f.write(f\"  Best Val Accuracy: {max(history['val_accuracy'])*100:.2f}%\\n\")\n",
    "    f.write(f\"  Initial Train Loss: {history['train_loss'][0]:.4f}\\n\")\n",
    "    f.write(f\"  Final Train Loss: {history['train_loss'][-1]:.4f}\\n\")\n",
    "    f.write(f\"  Best Val Loss: {min(history['val_loss']):.4f}\\n\")\n",
    "    \n",
    "    # Pathway Analysis\n",
    "    f.write(f\"\\nPathway Analysis ({len(MODEL_CONFIG['stream_input_channels'])}-stream LINet):\\n\")\n",
    "    f.write(f\"  Full Model (Integrated): {pathway_analysis['accuracy']['full_model']*100:.2f}%\\n\")\n",
    "    for i in range(len(MODEL_CONFIG['stream_input_channels'])):\n",
    "        f.write(f\"  Stream{i} Only: {pathway_analysis['accuracy'][f'stream{i}_only']*100:.2f}%\\n\")\n",
    "    f.write(f\"  Integrated Only: {pathway_analysis['accuracy']['integrated_only']*100:.2f}%\\n\")\n",
    "    for i in range(len(MODEL_CONFIG['stream_input_channels'])):\n",
    "        f.write(f\"  Stream{i} Contribution: {pathway_analysis['accuracy'][f'stream{i}_contribution']*100:.2f}%\\n\")\n",
    "    f.write(f\"  Integrated Contribution: {pathway_analysis['accuracy']['integrated_contribution']*100:.2f}%\\n\")\n",
    "\n",
    "print(f\"‚úÖ Summary report saved: {summary_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"All results saved to: {checkpoint_dir}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# List saved files\n",
    "print(\"\\nSaved files:\")\n",
    "!ls -lh {checkpoint_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-17"
   },
   "source": [
    "## 17. Summary & Next Steps\n",
    "\n",
    "### üéâ Training Complete!\n",
    "\n",
    "**What we accomplished:**\n",
    "- ‚úÖ Trained LINet (N-stream Linear Integration ResNet) on SUN RGB-D dataset (15 categories)\n",
    "- ‚úÖ Used **Keras-style API** with explicit optimizer and scheduler creation\n",
    "- ‚úÖ Used A100 GPU with AMP (2x speedup)\n",
    "- ‚úÖ Saved all checkpoints to Google Drive\n",
    "- ‚úÖ Analyzed per-stream and integrated pathway contributions\n",
    "- ‚úÖ Generated training curves and visualizations\n",
    "- ‚úÖ Comprehensive stream monitoring with overfitting detection\n",
    "\n",
    "**Results are saved to:** Check the output above for the checkpoint directory path\n",
    "\n",
    "### üìä Expected Performance:\n",
    "\n",
    "For **SUN RGB-D Scene Classification (15 categories, 10,335 images)**:\n",
    "- **Good:** 65-75% validation accuracy\n",
    "- **Very Good:** 75-80% validation accuracy\n",
    "- **Excellent:** 80-85% validation accuracy\n",
    "\n",
    "**Much better than NYU Depth V2 due to:**\n",
    "- 6.9x more training samples (8,041 vs 1,159)\n",
    "- 22.6x better class balance (8.5x vs 192x)\n",
    "- Higher quality, more diverse dataset\n",
    "\n",
    "### üîÑ New Keras-Style API Used:\n",
    "\n",
    "This notebook uses the **refactored Keras-style N-stream API**:\n",
    "\n",
    "```python\n",
    "# 1. Create optimizer with stream-specific LRs (N-stream format)\n",
    "optimizer = create_stream_optimizer(\n",
    "    model, \n",
    "    stream_lrs=[3e-5, 1e-4],  # [RGB, Depth]\n",
    "    stream_weight_decays=[5e-4, 1e-4],  # [RGB, Depth]\n",
    "    shared_lr=7e-5\n",
    ")\n",
    "\n",
    "# 2. Create scheduler\n",
    "scheduler = setup_scheduler(optimizer, 'cosine', epochs=80, ...)\n",
    "\n",
    "# 3. Compile with objects (not strings!)\n",
    "model.compile(optimizer=optimizer, scheduler=scheduler, loss='cross_entropy')\n",
    "\n",
    "# 4. Train (no scheduler_kwargs needed!)\n",
    "model.fit(train_loader, val_loader, epochs=80, stream_monitoring=True)\n",
    "```\n",
    "\n",
    "**Benefits:**\n",
    "- ‚úÖ Explicit optimizer/scheduler creation\n",
    "- ‚úÖ Clear separation: compile() = config, fit() = execution\n",
    "- ‚úÖ Easy to customize and experiment\n",
    "- ‚úÖ N-stream support - works with any number of streams!\n",
    "\n",
    "### üß† LINet Architecture Highlights:\n",
    "\n",
    "**N-Stream Linear Integration:**\n",
    "- Multiple input streams (e.g., RGB, Depth, etc.)\n",
    "- Integrated stream combines all inputs through learned linear weights at every layer\n",
    "- Neuron-level integration rather than late fusion\n",
    "\n",
    "**Weight Matrices per LIConv2d:**\n",
    "- Per-stream weights (full kernels for each input stream)\n",
    "- Integrated weight (1√ó1 channel-wise for integrated features)\n",
    "- Integration weights from each stream (1√ó1 convolutions)\n",
    "\n",
    "This allows the network to learn optimal integration strategies at every layer!\n",
    "\n",
    "### üîç Next Steps:\n",
    "\n",
    "1. **Review Results:**\n",
    "   - Check training curves above\n",
    "   - Review pathway analysis\n",
    "   - Compare per-stream and integrated contributions\n",
    "   - Analyze stream monitoring plots\n",
    "\n",
    "2. **Download Results:**\n",
    "   - All files are saved to your Google Drive\n",
    "   - Download checkpoints for local inference\n",
    "\n",
    "3. **Experiment:**\n",
    "   - Try ResNet50 for better accuracy (change `architecture` in Model Config)\n",
    "   - Adjust stream-specific learning rates if monitoring shows imbalance\n",
    "   - Train longer if early stopping triggered\n",
    "   - Analyze integration weights to understand learned strategies\n",
    "\n",
    "4. **Deploy:**\n",
    "   - Use the best model for inference\n",
    "   - Test on new RGB-D images\n",
    "   - Integrate into your application\n",
    "\n",
    "---\n",
    "\n",
    "**Questions or issues?** Check the training summary and pathway analysis above!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "final-summary"
   },
   "outputs": [],
   "source": [
    "# Print final summary\n",
    "print(\"=\" * 60)\n",
    "print(\"FINAL SUMMARY - LINET\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n‚úÖ Training Complete!\")\n",
    "print(f\"\\nFinal Validation Accuracy: {results['accuracy']*100:.2f}%\")\n",
    "print(f\"Best Validation Accuracy: {max(history['val_accuracy'])*100:.2f}%\")\n",
    "\n",
    "print(f\"\\nPathway Performance:\")\n",
    "for i in range(len(MODEL_CONFIG['stream_input_channels'])):\n",
    "    print(f\"  Stream{i} Pathway: {pathway_analysis['accuracy'][f'stream{i}_only']*100:.2f}%\")\n",
    "print(f\"  Integrated Pathway: {pathway_analysis['accuracy']['integrated_only']*100:.2f}%\")\n",
    "print(f\"  Combined (Full Model): {pathway_analysis['accuracy']['full_model']*100:.2f}%\")\n",
    "\n",
    "print(f\"\\nTotal Training Epochs: {len(history['train_loss'])}\")\n",
    "print(f\"Total Parameters: {total_params:,}\")\n",
    "print(f\"Integration Parameters: {integration_params:,}\")\n",
    "print(f\"\\nCheckpoints saved to: {checkpoint_dir}\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üéâ All done! Check Google Drive for saved models and results.\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
