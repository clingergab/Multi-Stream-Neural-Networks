{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# LINet Training on SUN RGB-D - Google Colab\n",
    "\n",
    "**Complete end-to-end training pipeline for Linear Integration ResNet (LINet) on Google Colab with A100 GPU**\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83d\udccb Checklist Before Running:\n",
    "\n",
    "- [ ] **Enable A100 GPU:** Runtime \u2192 Change runtime type \u2192 Hardware accelerator: GPU \u2192 GPU type: A100\n",
    "- [ ] **Mount Google Drive:** Your code and dataset will be stored on Drive\n",
    "- [ ] **Upload dataset to Drive:** `MyDrive/datasets/sunrgbd_15/` (preprocessed 15-category dataset)\n",
    "- [ ] **Expected Runtime:** ~2-3 hours for training\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83c\udfaf What This Notebook Does:\n",
    "\n",
    "1. \u2705 Verify A100 GPU is available\n",
    "2. \u2705 Mount Google Drive\n",
    "3. \u2705 Clone your repository to local disk (fast I/O)\n",
    "4. \u2705 Copy SUN RGB-D dataset to local disk (10-20x faster than Drive)\n",
    "5. \u2705 Install dependencies\n",
    "6. \u2705 Train LINet (3-stream Linear Integration ResNet) with all optimizations\n",
    "7. \u2705 Save checkpoints to Drive (persistent storage)\n",
    "8. \u2705 Generate training curves and analysis\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83e\udde0 About LINet:\n",
    "\n",
    "**LINet** (Linear Integration Network) is a 3-stream neural network architecture where:\n",
    "- **Stream1** processes RGB images\n",
    "- **Stream2** processes Depth maps\n",
    "- **Integrated Stream** combines both streams using learned linear integration weights\n",
    "\n",
    "Unlike traditional fusion methods, LINet performs integration **at the neuron level** through 5 weight matrices per convolution:\n",
    "- `stream1_weight` (full kernel for RGB)\n",
    "- `stream2_weight` (full kernel for Depth)\n",
    "- `integrated_weight` (1\u00d71 channel-wise for integrated features)\n",
    "- `integration_from_stream1` (1\u00d71 integration from RGB)\n",
    "- `integration_from_stream2` (1\u00d71 integration from Depth)\n",
    "\n",
    "This allows the network to learn optimal integration strategies at every layer!\n",
    "\n",
    "---\n",
    "\n",
    "**Let's get started!** \ud83d\ude80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-1"
   },
   "source": [
    "## 1. Environment Setup & GPU Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check-gpu"
   },
   "outputs": [],
   "source": [
    "# Check GPU availability and specs\n",
    "import torch\n",
    "import subprocess\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"GPU VERIFICATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check PyTorch and CUDA\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "    \n",
    "    # Check if it's A100\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    if 'A100' in gpu_name:\n",
    "        print(\"\\n\u2705 A100 GPU detected - PERFECT for training!\")\n",
    "    elif 'V100' in gpu_name:\n",
    "        print(\"\\n\u2705 V100 GPU detected - Good for training (slower than A100)\")\n",
    "    elif 'T4' in gpu_name:\n",
    "        print(\"\\n\u26a0\ufe0f  T4 GPU detected - Will be slower, consider upgrading to A100\")\n",
    "    else:\n",
    "        print(f\"\\n\u26a0\ufe0f  GPU: {gpu_name} - Consider using A100 for best performance\")\n",
    "else:\n",
    "    print(\"\\n\u274c NO GPU DETECTED!\")\n",
    "    print(\"Please enable GPU: Runtime \u2192 Change runtime type \u2192 Hardware accelerator: GPU\")\n",
    "    raise RuntimeError(\"GPU is required for training\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nvidia-smi"
   },
   "outputs": [],
   "source": [
    "# Detailed GPU info\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-2"
   },
   "source": [
    "## 2. Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mount-drive"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "print(\"\\n\u2705 Google Drive mounted successfully!\")\n",
    "print(f\"\\nDrive contents:\")\n",
    "!ls -la /content/drive/MyDrive/ | head -20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-3"
   },
   "source": [
    "## 3. Clone Repository to Local Disk (Fast I/O)\n",
    "\n",
    "**Important:** We clone to `/content/` (local SSD) instead of Drive for 10-20x faster I/O\n",
    "\n",
    "**Default:** Clone from GitHub (recommended - always gets latest code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clone-repo"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Configuration\n",
    "PROJECT_NAME = \"Multi-Stream-Neural-Networks\"\n",
    "GITHUB_REPO = \"https://github.com/clingergab/Multi-Stream-Neural-Networks.git\"  # UPDATE THIS\n",
    "LOCAL_REPO_PATH = f\"/content/{PROJECT_NAME}\"  # Local copy for fast I/O\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"REPOSITORY SETUP\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Ensure we're in a valid directory\n",
    "os.chdir('/content')\n",
    "print(f\"Starting in: {os.getcwd()}\")\n",
    "\n",
    "# Check if repo already exists (same session, rerunning cell)\n",
    "if Path(LOCAL_REPO_PATH).exists() and Path(f\"{LOCAL_REPO_PATH}/.git\").exists():\n",
    "    print(f\"\\n\ud83d\udcc1 Repo already exists: {LOCAL_REPO_PATH}\")\n",
    "    print(f\"\ud83d\udd04 Pulling latest changes...\")\n",
    "    \n",
    "    os.chdir(LOCAL_REPO_PATH)\n",
    "    !git pull\n",
    "    print(\"\u2705 Repo updated\")\n",
    "\n",
    "# Clone from GitHub (first run)\n",
    "else:\n",
    "    # Remove old incomplete copy if exists\n",
    "    if Path(LOCAL_REPO_PATH).exists():\n",
    "        print(f\"\\n\ud83d\uddd1\ufe0f  Removing incomplete repo copy...\")\n",
    "        !rm -rf {LOCAL_REPO_PATH}\n",
    "    \n",
    "    print(f\"\\n\ud83d\udd04 Cloning from GitHub...\")\n",
    "    print(f\"   Repo: {GITHUB_REPO}\")\n",
    "    print(f\"   Destination: {LOCAL_REPO_PATH}\")\n",
    "    \n",
    "    !git clone {GITHUB_REPO} {LOCAL_REPO_PATH}\n",
    "    \n",
    "    # Verify clone succeeded\n",
    "    if not Path(LOCAL_REPO_PATH).exists():\n",
    "        raise RuntimeError(f\"Failed to clone repository to {LOCAL_REPO_PATH}\")\n",
    "    \n",
    "    print(\"\u2705 Repo cloned successfully\")\n",
    "    os.chdir(LOCAL_REPO_PATH)\n",
    "\n",
    "# Verify repo structure\n",
    "print(f\"\\n\ud83d\udcc2 Repository structure:\")\n",
    "!ls -la {LOCAL_REPO_PATH}\n",
    "\n",
    "print(f\"\\n\u2705 Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-4"
   },
   "source": [
    "## 4. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install-deps"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "print(\"Installing dependencies...\")\n",
    "\n",
    "!pip install -q h5py tqdm matplotlib seaborn\n",
    "\n",
    "# Verify installations\n",
    "import h5py\n",
    "import tqdm\n",
    "import matplotlib\n",
    "import seaborn\n",
    "\n",
    "print(\"\u2705 All dependencies installed!\")\n",
    "print(f\"   h5py: {h5py.__version__}\")\n",
    "print(f\"   matplotlib: {matplotlib.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-5"
   },
   "source": [
    "## 5. Copy SUN RGB-D Dataset to Local Disk\n",
    "\n",
    "**Performance Note:** Local disk I/O is ~10-20x faster than Drive!\n",
    "\n",
    "**Dataset:** SUN RGB-D 15-category preprocessed dataset (~3.5 GB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download-dataset"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Paths\n",
    "DRIVE_DATASET_TAR = \"/content/drive/MyDrive/datasets/sunrgbd_15.tar.gz\"  # Compressed file\n",
    "LOCAL_DATASET_PATH = \"/content/data/sunrgbd_15\"  # Extracted location\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SUN RGB-D 15-CATEGORY DATASET SETUP\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check if already on local disk\n",
    "if Path(LOCAL_DATASET_PATH).exists():\n",
    "    print(f\"\u2705 Dataset already on local disk: {LOCAL_DATASET_PATH}\")\n",
    "    \n",
    "    # Verify structure\n",
    "    train_rgb_count = len(list(Path(f\"{LOCAL_DATASET_PATH}/train/rgb\").glob(\"*.png\")))\n",
    "    val_rgb_count = len(list(Path(f\"{LOCAL_DATASET_PATH}/val/rgb\").glob(\"*.png\")))\n",
    "    print(f\"   Train samples: {train_rgb_count}\")\n",
    "    print(f\"   Val samples: {val_rgb_count}\")\n",
    "\n",
    "# Copy and extract from Drive\n",
    "elif Path(DRIVE_DATASET_TAR).exists():\n",
    "    print(f\"\ud83d\udcc1 Found compressed dataset on Drive: {DRIVE_DATASET_TAR}\")\n",
    "    print(f\"\ud83d\udce5 Copying 4.2GB compressed file to local disk...\")\n",
    "    print(f\"   \u23f1\ufe0f  This takes ~3-5 minutes (much faster than 20k individual files!)\")\n",
    "    \n",
    "    # Create parent directory\n",
    "    !mkdir -p /content/data\n",
    "    \n",
    "    # Copy compressed file with progress\n",
    "    print(f\"\\nCopying compressed archive...\")\n",
    "    !rsync -ah --info=progress2 {DRIVE_DATASET_TAR} /content/data/sunrgbd_15.tar.gz\n",
    "    \n",
    "    # Extract to local disk (suppress macOS metadata warnings)\n",
    "    print(f\"\\n\ud83d\udce6 Extracting dataset to local disk...\")\n",
    "    !tar -xzf /content/data/sunrgbd_15.tar.gz -C /content/data/ 2>&1 | grep -v \"Ignoring unknown extended header\"\n",
    "    \n",
    "    # Remove tar file to save space\n",
    "    !rm /content/data/sunrgbd_15.tar.gz\n",
    "    \n",
    "    print(f\"\\n\u2705 Dataset extracted to local disk\")\n",
    "    \n",
    "    # Verify extraction\n",
    "    train_rgb_count = len(list(Path(f\"{LOCAL_DATASET_PATH}/train/rgb\").glob(\"*.png\")))\n",
    "    val_rgb_count = len(list(Path(f\"{LOCAL_DATASET_PATH}/val/rgb\").glob(\"*.png\")))\n",
    "    print(f\"   Train samples: {train_rgb_count}\")\n",
    "    print(f\"   Val samples: {val_rgb_count}\")\n",
    "\n",
    "else:\n",
    "    print(f\"\u274c Compressed dataset not found on Drive!\")\n",
    "    print(f\"   Expected location: {DRIVE_DATASET_TAR}\")\n",
    "    print(f\"\\n\ud83d\udccb To fix this:\")\n",
    "    print(f\"   1. Run: COPYFILE_DISABLE=1 tar -czf sunrgbd_15.tar.gz sunrgbd_15/\")\n",
    "    print(f\"   2. Upload sunrgbd_15.tar.gz to Google Drive\")\n",
    "    print(f\"   3. Place it at: {DRIVE_DATASET_TAR}\")\n",
    "    raise FileNotFoundError(f\"Compressed dataset not found at {DRIVE_DATASET_TAR}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"Dataset ready at: {LOCAL_DATASET_PATH}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-6"
   },
   "source": [
    "## 6. Setup Python Path & Import LINet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup-imports"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Remove cached modules\n",
    "modules_to_reload = [k for k in sys.modules.keys() if k.startswith('src.')]\n",
    "for module in modules_to_reload:\n",
    "    del sys.modules[module]\n",
    "    \n",
    "# Add project to Python path\n",
    "project_root = '/content/Multi-Stream-Neural-Networks'\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "# Verify project structure\n",
    "print(\"Project structure:\")\n",
    "!ls -la {project_root}/src/models/\n",
    "\n",
    "# Import LINet and SUN RGB-D dataloader\n",
    "print(\"\\nImporting LINet and dataloaders...\")\n",
    "from src.models.linear_integration.li_net import li_resnet18, li_resnet50\n",
    "from src.data_utils.sunrgbd_dataset import get_sunrgbd_dataloaders\n",
    "\n",
    "print(\"\u2705 LINet and dataloaders imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-7"
   },
   "source": [
    "## 7. Load SUN RGB-D Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify dataset structure\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DATASET STRUCTURE VERIFICATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "dataset_root = Path(LOCAL_DATASET_PATH)\n",
    "\n",
    "print(\"\\nDirectory structure:\")\n",
    "print(f\"  {dataset_root}/\")\n",
    "print(f\"    train/\")\n",
    "print(f\"      rgb/ - {len(list((dataset_root / 'train' / 'rgb').glob('*.png')))} images\")\n",
    "print(f\"      depth/ - {len(list((dataset_root / 'train' / 'depth').glob('*.png')))} images\")\n",
    "print(f\"      labels.txt\")\n",
    "print(f\"    val/\")\n",
    "print(f\"      rgb/ - {len(list((dataset_root / 'val' / 'rgb').glob('*.png')))} images\")\n",
    "print(f\"      depth/ - {len(list((dataset_root / 'val' / 'depth').glob('*.png')))} images\")\n",
    "print(f\"      labels.txt\")\n",
    "print(f\"    class_names.txt\")\n",
    "print(f\"    dataset_info.txt\")\n",
    "\n",
    "# Read class names\n",
    "with open(dataset_root / 'class_names.txt', 'r') as f:\n",
    "    class_names = [line.strip() for line in f]\n",
    "\n",
    "print(f\"\\nClasses ({len(class_names)}):\")\n",
    "for i, name in enumerate(class_names):\n",
    "    print(f\"  {i}: {name}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load-dataset"
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"LOADING SUN RGB-D 15-CATEGORY DATASET\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Dataset configuration\n",
    "DATASET_CONFIG = {\n",
    "    'data_root': LOCAL_DATASET_PATH,\n",
    "    'batch_size': 128,  # Good balance for A100\n",
    "    'num_workers': 4,\n",
    "    'target_size': (416, 544),  \n",
    "    'num_classes': 15   # SUN RGB-D merged to 15 categories (labels 0-14)\n",
    "}\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "for key, value in DATASET_CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(f\"\\nLoading dataset from: {DATASET_CONFIG['data_root']}\")\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader, val_loader = get_sunrgbd_dataloaders(\n",
    "    data_root=DATASET_CONFIG['data_root'],\n",
    "    batch_size=DATASET_CONFIG['batch_size'],\n",
    "    num_workers=DATASET_CONFIG['num_workers'],\n",
    "    target_size=DATASET_CONFIG['target_size']\n",
    ")\n",
    "\n",
    "print(f\"\\n\u2705 Dataset loaded successfully!\")\n",
    "print(f\"\\nDataset Statistics:\")\n",
    "print(f\"  Train batches: {len(train_loader)}\")\n",
    "print(f\"  Val batches: {len(val_loader)}\")\n",
    "print(f\"  Train samples: {len(train_loader.dataset)}\")\n",
    "print(f\"  Val samples: {len(val_loader.dataset)}\")\n",
    "print(f\"  Batch size: {DATASET_CONFIG['batch_size']}\")\n",
    "\n",
    "# Test loading a batch\n",
    "print(f\"\\nTesting batch loading...\")\n",
    "rgb_batch, depth_batch, label_batch = next(iter(train_loader))\n",
    "print(f\"  RGB shape: {rgb_batch.shape}\")\n",
    "print(f\"  Depth shape: {depth_batch.shape}\")\n",
    "print(f\"  Labels shape: {label_batch.shape}\")\n",
    "print(f\"  Labels min: {label_batch.min().item()}, max: {label_batch.max().item()}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-8"
   },
   "source": [
    "## 8. Visualize Sample Data\n",
    "\n",
    "Shows RGB images, depth maps, and scene labels from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "visualize-data"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Visualize some samples\n",
    "fig, axes = plt.subplots(2, 4, figsize=(14, 7))\n",
    "\n",
    "for i in range(4):\n",
    "    rgb = rgb_batch[i].cpu()\n",
    "    depth = depth_batch[i].cpu()\n",
    "    label = label_batch[i].item()\n",
    "    \n",
    "    # Denormalize for visualization\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "    rgb_vis = rgb * std + mean\n",
    "    rgb_vis = torch.clamp(rgb_vis, 0, 1)\n",
    "    \n",
    "    # Plot RGB\n",
    "    axes[0, i].imshow(rgb_vis.permute(1, 2, 0))\n",
    "    axes[0, i].set_title(f\"RGB - Class {label}\", fontsize=10)\n",
    "    axes[0, i].axis('off')\n",
    "    \n",
    "    # Plot Depth\n",
    "    axes[1, i].imshow(depth.squeeze(), cmap='viridis')\n",
    "    axes[1, i].set_title(f\"Depth - Class {label}\", fontsize=10)\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "plt.suptitle('SUN RGB-D Sample Data (RGB + Depth)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\u2705 Sample visualization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-9"
   },
   "source": [
    "## 9. Create & Compile LINet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create-model"
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"MODEL CREATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Model configuration\n",
    "MODEL_CONFIG = {\n",
    "    'architecture': 'resnet18',  # or 'resnet50' for better accuracy\n",
    "    'num_classes': 15,  # SUN RGB-D has 15 merged categories (labels 0-14)\n",
    "    'stream1_channels': 3,  # RGB\n",
    "    'stream2_channels': 1,  # Depth\n",
    "    'dropout_p': 0.5,  # Dropout for regularization\n",
    "    'device': 'cuda',\n",
    "    'use_amp': True  # Automatic Mixed Precision (2x faster on A100)\n",
    "}\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "for key, value in MODEL_CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Create model\n",
    "print(f\"\\nCreating LINet-{MODEL_CONFIG['architecture'].upper()} (3-stream Linear Integration ResNet)...\")\n",
    "\n",
    "if MODEL_CONFIG['architecture'] == 'resnet18':\n",
    "    model = li_resnet18(\n",
    "        num_classes=MODEL_CONFIG['num_classes'],\n",
    "        stream1_input_channels=MODEL_CONFIG['stream1_channels'],\n",
    "        stream2_input_channels=MODEL_CONFIG['stream2_channels'],\n",
    "        dropout_p=MODEL_CONFIG['dropout_p'],\n",
    "        device=MODEL_CONFIG['device'],\n",
    "        use_amp=MODEL_CONFIG['use_amp']\n",
    "    )\n",
    "elif MODEL_CONFIG['architecture'] == 'resnet50':\n",
    "    model = li_resnet50(\n",
    "        num_classes=MODEL_CONFIG['num_classes'],\n",
    "        stream1_input_channels=MODEL_CONFIG['stream1_channels'],\n",
    "        stream2_input_channels=MODEL_CONFIG['stream2_channels'],\n",
    "        dropout_p=MODEL_CONFIG['dropout_p'],\n",
    "        device=MODEL_CONFIG['device'],\n",
    "        use_amp=MODEL_CONFIG['use_amp']\n",
    "    )\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Count integration-specific parameters\n",
    "integration_params = 0\n",
    "for name, param in model.named_parameters():\n",
    "    if 'integration' in name or 'integrated_weight' in name:\n",
    "        integration_params += param.numel()\n",
    "\n",
    "print(f\"\\n\u2705 Model created successfully!\")\n",
    "print(f\"\\nModel Statistics:\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"  Integration parameters: {integration_params:,}\")\n",
    "print(f\"  Model size: {total_params * 4 / 1024**2:.2f} MB (FP32)\")\n",
    "print(f\"  Architecture: 3-stream Linear Integration (LINet)\")\n",
    "print(f\"  Device: {MODEL_CONFIG['device']}\")\n",
    "print(f\"  AMP enabled: {MODEL_CONFIG['use_amp']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "compile-model"
   },
   "source": [
    "## 9b. Model Compilation Options\n",
    "\n",
    "**Choose your optimization strategy below (cell-22)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model with stream-specific optimization\n",
    "print(\"=\" * 60)\n",
    "print(\"MODEL COMPILATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Stream-specific configuration for optimal RGB/Depth balance\n",
    "STREAM_SPECIFIC_CONFIG = {\n",
    "    'optimizer': 'adamw',\n",
    "    'learning_rate': 7e-5,           # Base LR for shared params (fusion, classifier)\n",
    "    'weight_decay': 2e-4,             # Base weight decay\n",
    "\n",
    "    # Stream-specific settings (adjusted based on research):\n",
    "    'stream1_lr': 3e-5,               # RGB stream: lower LR (more regularization)\n",
    "    'stream1_weight_decay': 5e-4,     # RGB stream: higher WD (prevent overfitting)\n",
    "    'stream2_lr': 1e-4,               # Depth stream: higher LR (needs more learning)\n",
    "    'stream2_weight_decay': 1e-4,     # Depth stream: lighter WD (less regularization)\n",
    "\n",
    "    'loss': 'cross_entropy',\n",
    "    'scheduler': 'cosine',\n",
    "    'label_smoothing': 0.1\n",
    "}\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "for key, value in STREAM_SPECIFIC_CONFIG.items():\n",
    "    if value is not None:\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "# Compile\n",
    "model.compile(**STREAM_SPECIFIC_CONFIG)\n",
    "\n",
    "print(\"\\n\u2705 Model compiled successfully!\")\n",
    "\n",
    "# Show parameter groups\n",
    "if hasattr(model.optimizer, 'param_groups'):\n",
    "    print(f\"\\nParameter groups created: {len(model.optimizer.param_groups)}\")\n",
    "    for i, group in enumerate(model.optimizer.param_groups):\n",
    "        num_params = sum(p.numel() for p in group['params'])\n",
    "        print(f\"  Group {i}: LR={group['lr']:.2e}, WD={group['weight_decay']:.2e}, Params={num_params:,}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-10"
   },
   "source": [
    "## 10. Test Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "test-forward"
   },
   "outputs": [],
   "source": [
    "# Test forward pass with detailed debugging\n",
    "print(\"Testing forward pass with CUDA_LAUNCH_BLOCKING for better error messages...\")\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'  # Synchronous CUDA for better error messages\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    rgb_test, depth_test, labels_test = next(iter(train_loader))\n",
    "    \n",
    "    print(f\"\\nInput validation:\")\n",
    "    print(f\"  RGB shape: {rgb_test.shape}, dtype: {rgb_test.dtype}\")\n",
    "    print(f\"  RGB min: {rgb_test.min():.4f}, max: {rgb_test.max():.4f}\")\n",
    "    print(f\"  RGB has NaN: {torch.isnan(rgb_test).any()}\")\n",
    "    print(f\"  RGB has Inf: {torch.isinf(rgb_test).any()}\")\n",
    "    \n",
    "    print(f\"\\n  Depth shape: {depth_test.shape}, dtype: {depth_test.dtype}\")\n",
    "    print(f\"  Depth min: {depth_test.min():.4f}, max: {depth_test.max():.4f}\")\n",
    "    print(f\"  Depth has NaN: {torch.isnan(depth_test).any()}\")\n",
    "    print(f\"  Depth has Inf: {torch.isinf(depth_test).any()}\")\n",
    "    \n",
    "    print(f\"\\n  Labels shape: {labels_test.shape}, dtype: {labels_test.dtype}\")\n",
    "    print(f\"  Labels min: {labels_test.min()}, max: {labels_test.max()}\")\n",
    "    print(f\"  Labels unique: {torch.unique(labels_test).tolist()}\")\n",
    "    \n",
    "    print(\"\\nRunning forward pass...\")\n",
    "    rgb_cuda = rgb_test.to('cuda')\n",
    "    depth_cuda = depth_test.to('cuda')\n",
    "    \n",
    "    try:\n",
    "        outputs = model(rgb_cuda, depth_cuda)\n",
    "        print(f\"  \u2705 Forward pass successful!\")\n",
    "        print(f\"  Output shape: {outputs.shape}\")\n",
    "        print(f\"  Output min: {outputs.min():.4f}, max: {outputs.max():.4f}\")\n",
    "        \n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        print(f\"\\nSample predictions: {predictions.cpu().numpy()[:10]}\")\n",
    "        print(f\"Ground truth: {labels_test.numpy()[:10]}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n\u274c Forward pass failed!\")\n",
    "        print(f\"Error: {e}\")\n",
    "        print(f\"\\nThis is likely a model architecture issue, not a data issue.\")\n",
    "        print(f\"Possible causes:\")\n",
    "        print(f\"  1. BatchNorm running stats issue\")\n",
    "        print(f\"  2. Invalid tensor operations in model\")\n",
    "        print(f\"  3. Memory corruption\")\n",
    "        raise\n",
    "\n",
    "print(\"\\n\u2705 Forward pass test complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-11"
   },
   "source": [
    "## 11. Setup Checkpoint Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"CHECKPOINT DIRECTORY SETUP\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create checkpoint directory on Google Drive (persistent storage)\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "checkpoint_dir = f\"/content/drive/MyDrive/linet_checkpoints/run_{timestamp}\"\n",
    "\n",
    "# Create directory\n",
    "Path(checkpoint_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"\\n\u2705 Checkpoint directory created:\")\n",
    "print(f\"   {checkpoint_dir}\")\n",
    "print(f\"\\nAll training artifacts will be saved here:\")\n",
    "print(f\"  \u2022 Best model weights\")\n",
    "print(f\"  \u2022 Training history\")\n",
    "print(f\"  \u2022 Monitoring metrics\")\n",
    "print(f\"  \u2022 Visualizations\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Train the Model \ud83d\ude80\n",
    "\n",
    "**Expected time:** ~2-3 hours for 90 epochs on A100\n",
    "\n",
    "**All optimizations enabled:**\n",
    "- \u2705 Automatic Mixed Precision (2x faster)\n",
    "- \u2705 Gradient Clipping (stability)\n",
    "- \u2705 Cosine Annealing LR\n",
    "- \u2705 Early Stopping\n",
    "- \u2705 Best Model Checkpointing\n",
    "- \u2705 Local disk I/O (10-20x faster than Drive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup-checkpoints"
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"TRAINING WITH STREAM MONITORING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Training configuration\n",
    "TRAIN_CONFIG = {\n",
    "    'epochs': 80,\n",
    "    'grad_clip_norm': 5.0,\n",
    "    'early_stopping': True,\n",
    "    'patience': 12,\n",
    "    'min_delta': 0.001,\n",
    "    'monitor': 'val_accuracy',\n",
    "    'restore_best_weights': True,\n",
    "    'save_path': f\"{checkpoint_dir}/best_model.pt\",\n",
    "    'stream_monitoring': True,  # Built-in stream monitoring!\n",
    "    # 'scheduler_patience': 2,      # Number of epochs with no improvement before reducing LR\n",
    "    # 'factor': 0.7,                 # Multiply LR by this factor when reducing (new_lr = lr * 0.5)\n",
    "    # 'mode': 'max',                 # 'min' for loss (default), 'max' for accuracy\n",
    "    # 'min_lr': 1e-6                # Minimum learning rate floor\n",
    "    't_0': 25,\n",
    "    't_mult': 1,\n",
    "    'restart_decay': 0.08,\n",
    "    'eta_min': 1e-6\n",
    "}\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "for key, value in TRAIN_CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"Training will take approximately 2-3 hours on A100\")\n",
    "print(f\"Stream monitoring active - detailed per-stream metrics shown each epoch\")\n",
    "print(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "# Train using built-in fit() method with stream monitoring\n",
    "history = model.fit(\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    epochs=TRAIN_CONFIG['epochs'],\n",
    "    verbose=True,\n",
    "    save_path=TRAIN_CONFIG['save_path'],\n",
    "    early_stopping=TRAIN_CONFIG['early_stopping'],\n",
    "    patience=TRAIN_CONFIG['patience'],\n",
    "    min_delta=TRAIN_CONFIG['min_delta'],\n",
    "    monitor=TRAIN_CONFIG['monitor'],\n",
    "    restore_best_weights=TRAIN_CONFIG['restore_best_weights'],\n",
    "    grad_clip_norm=TRAIN_CONFIG['grad_clip_norm'],\n",
    "    stream_monitoring=TRAIN_CONFIG['stream_monitoring'],  # Enable built-in monitoring\n",
    "    # scheduler_patience=TRAIN_CONFIG['scheduler_patience'],      # Number of epochs with no improvement before reducing LR\n",
    "    # factor=TRAIN_CONFIG['factor'],                 # Multiply LR by this factor when reducing (new_lr = lr * 0.5)\n",
    "    # mode='max',                 # 'min' for loss (default), 'max' for accuracy\n",
    "    # min_lr=TRAIN_CONFIG['min_lr'],                # Minimum learning rate floor\n",
    "    t_0=TRAIN_CONFIG['t_0'],\n",
    "    t_mult=TRAIN_CONFIG['t_mult'],\n",
    "    restart_decay=TRAIN_CONFIG['restart_decay'],\n",
    "    eta_min=TRAIN_CONFIG['eta_min'],\n",
    "    stream_early_stopping=True\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"\ud83c\udf89 TRAINING COMPLETE!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-13"
   },
   "source": [
    "## 13. Evaluate Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"FINAL EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Evaluate on validation set\n",
    "results = model.evaluate(data_loader=val_loader)\n",
    "\n",
    "print(f\"\\nFinal Validation Results:\")\n",
    "print(f\"  Loss: {results['loss']:.4f}\")\n",
    "print(f\"  Overall Accuracy: {results['accuracy']*100:.2f}%\")\n",
    "\n",
    "# Show stream-specific accuracies if available\n",
    "if 'stream1_accuracy' in results:\n",
    "    print(f\"\\nStream-Specific Performance:\")\n",
    "    print(f\"  Stream1 (RGB) Accuracy: {results['stream1_accuracy']*100:.2f}%\")\n",
    "    print(f\"  Stream2 (Depth) Accuracy: {results['stream2_accuracy']*100:.2f}%\")\n",
    "\n",
    "print(f\"\\nTraining Summary:\")\n",
    "print(f\"  Initial train loss: {history['train_loss'][0]:.4f}\")\n",
    "print(f\"  Final train loss: {history['train_loss'][-1]:.4f}\")\n",
    "print(f\"  Best val loss: {min(history['val_loss']):.4f}\")\n",
    "print(f\"  Initial train acc: {history['train_accuracy'][0]*100:.2f}%\")\n",
    "print(f\"  Final train acc: {history['train_accuracy'][-1]*100:.2f}%\")\n",
    "print(f\"  Best val acc: {max(history['val_accuracy'])*100:.2f}%\")\n",
    "print(f\"  Total epochs: {len(history['train_loss'])}\")\n",
    "\n",
    "if 'early_stopping' in history:\n",
    "    print(f\"\\nEarly Stopping Info:\")\n",
    "    print(f\"  Stopped early: {history['early_stopping']['stopped_early']}\")\n",
    "    print(f\"  Best epoch: {history['early_stopping']['best_epoch']}\")\n",
    "    print(f\"  Best {history['early_stopping']['monitor']}: {history['early_stopping']['best_metric']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-14"
   },
   "source": [
    "## 14. Plot Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "plot-curves"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Loss curve\n",
    "axes[0].plot(history['train_loss'], label='Train Loss', linewidth=2)\n",
    "axes[0].plot(history['val_loss'], label='Val Loss', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Loss', fontsize=12)\n",
    "axes[0].set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy curve with stream-specific curves\n",
    "# Note: train_accuracy and val_accuracy are the full model (integrated stream) accuracies\n",
    "axes[1].plot([acc*100 for acc in history['train_accuracy']], label='Full Model Train', linewidth=2, color='green')\n",
    "axes[1].plot([acc*100 for acc in history['val_accuracy']], label='Full Model Val', linewidth=2, color='darkgreen')\n",
    "\n",
    "# Add stream-specific curves if available (stream1 and stream2 only)\n",
    "if len(history.get('stream1_train_acc', [])) > 0:\n",
    "    axes[1].plot([acc*100 for acc in history['stream1_train_acc']], \n",
    "                label='Stream1 (RGB) Train', linewidth=1, alpha=0.6, linestyle='--', color='skyblue')\n",
    "    axes[1].plot([acc*100 for acc in history['stream1_val_acc']], \n",
    "                label='Stream1 (RGB) Val', linewidth=1, alpha=0.6, linestyle='--', color='blue')\n",
    "    axes[1].plot([acc*100 for acc in history['stream2_train_acc']], \n",
    "                label='Stream2 (Depth) Train', linewidth=1, alpha=0.6, linestyle='--', color='lightcoral')\n",
    "    axes[1].plot([acc*100 for acc in history['stream2_val_acc']], \n",
    "                label='Stream2 (Depth) Val', linewidth=1, alpha=0.6, linestyle='--', color='red')\n",
    "\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Accuracy (%)', fontsize=12)\n",
    "axes[1].set_title('Training and Validation Accuracy\\n(Full Model = Integrated Stream)', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=9, loc='lower right')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Learning rate curve with stream-specific LRs\n",
    "if len(history['learning_rates']) > 0:\n",
    "    # Sample learning rates (they're recorded per step, not per epoch)\n",
    "    sampled_lrs = history['learning_rates'][::max(1, len(history['learning_rates'])//100)]\n",
    "    axes[2].plot(sampled_lrs, linewidth=2, color='green', label='Base LR')\n",
    "    \n",
    "    # Add stream-specific LRs if available (recorded per epoch)\n",
    "    if len(history.get('stream1_lr', [])) > 0:\n",
    "        axes[2].plot(history['stream1_lr'], linewidth=1, alpha=0.7, linestyle='--', \n",
    "                    color='blue', label='Stream1 (RGB) LR')\n",
    "        axes[2].plot(history['stream2_lr'], linewidth=1, alpha=0.7, linestyle='--', \n",
    "                    color='red', label='Stream2 (Depth) LR')\n",
    "    \n",
    "    axes[2].set_xlabel('Epoch' if len(history.get('stream1_lr', [])) > 0 else 'Training Step (sampled)', fontsize=12)\n",
    "    axes[2].set_ylabel('Learning Rate', fontsize=12)\n",
    "    axes[2].set_title('Learning Rate Schedule', fontsize=14, fontweight='bold')\n",
    "    axes[2].legend(fontsize=9, loc='upper right')\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    # axes[2].set_yscale('log')  # Removed: Linear scale shows scheduler shape better\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{checkpoint_dir}/training_curves.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\u2705 Training curves saved to: {checkpoint_dir}/training_curves.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-15"
   },
   "source": [
    "## 15. Pathway Analysis (Stream1/Stream2/Integrated Contributions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pathway-analysis"
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"PATHWAY ANALYSIS (3-STREAM LINET)\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nAnalyzing Stream1, Stream2, and Integrated pathway contributions...\")\n",
    "print(\"This may take a few minutes...\\n\")\n",
    "\n",
    "# Analyze pathways (LINet has 3 streams: stream1, stream2, integrated)\n",
    "pathway_analysis = model.analyze_pathways(\n",
    "    data_loader=val_loader\n",
    ")\n",
    "\n",
    "print(\"\\nAccuracy Metrics:\")\n",
    "print(f\"  Full model (Integrated): {pathway_analysis['accuracy']['full_model']*100:.2f}%\")\n",
    "print(f\"  Stream1 only (RGB): {pathway_analysis['accuracy']['stream1_only']*100:.2f}%\")\n",
    "print(f\"  Stream2 only (Depth): {pathway_analysis['accuracy']['stream2_only']*100:.2f}%\")\n",
    "print(f\"  Integrated only: {pathway_analysis['accuracy']['integrated_only']*100:.2f}%\")\n",
    "print(f\"\\n  Stream1 contribution: {pathway_analysis['accuracy']['stream1_contribution']*100:.2f}%\")\n",
    "print(f\"  Stream2 contribution: {pathway_analysis['accuracy']['stream2_contribution']*100:.2f}%\")\n",
    "print(f\"  Integrated contribution: {pathway_analysis['accuracy']['integrated_contribution']*100:.2f}%\")\n",
    "\n",
    "print(\"\\nLoss Metrics:\")\n",
    "print(f\"  Full model: {pathway_analysis['loss']['full_model']:.4f}\")\n",
    "print(f\"  Stream1 only: {pathway_analysis['loss']['stream1_only']:.4f}\")\n",
    "print(f\"  Stream2 only: {pathway_analysis['loss']['stream2_only']:.4f}\")\n",
    "print(f\"  Integrated only: {pathway_analysis['loss']['integrated_only']:.4f}\")\n",
    "\n",
    "print(\"\\nFeature Norm Statistics:\")\n",
    "print(f\"  Stream1 mean: {pathway_analysis['feature_norms']['stream1_mean']:.4f}\")\n",
    "print(f\"  Stream1 std: {pathway_analysis['feature_norms']['stream1_std']:.4f}\")\n",
    "print(f\"  Stream2 mean: {pathway_analysis['feature_norms']['stream2_mean']:.4f}\")\n",
    "print(f\"  Stream2 std: {pathway_analysis['feature_norms']['stream2_std']:.4f}\")\n",
    "print(f\"  Integrated mean: {pathway_analysis['feature_norms']['integrated_mean']:.4f}\")\n",
    "print(f\"  Integrated std: {pathway_analysis['feature_norms']['integrated_std']:.4f}\")\n",
    "print(f\"  Stream1/Stream2 ratio: {pathway_analysis['feature_norms']['stream1_to_stream2_ratio']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STREAM CONTRIBUTION ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nCalculating importance of each stream to final predictions...\")\n",
    "print(\"(Based on individual stream performance in ablation study)\\n\")\n",
    "\n",
    "# Calculate stream contributions - shows importance of each stream\n",
    "stream_contributions = model.calculate_stream_contributions(\n",
    "    data_loader=val_loader\n",
    ")\n",
    "\n",
    "print(\"Stream Importance Scores:\")\n",
    "print(f\"  Stream1 (RGB) importance: {stream_contributions['stream1_importance']*100:.1f}%\")\n",
    "print(f\"  Stream2 (Depth) importance: {stream_contributions['stream2_importance']*100:.1f}%\")\n",
    "print(f\"  Integrated importance: {stream_contributions['integrated_importance']*100:.1f}%\")\n",
    "\n",
    "print(\"\\nIndividual Stream Accuracies:\")\n",
    "print(f\"  Full model: {stream_contributions['individual_accuracies']['full_model']*100:.2f}%\")\n",
    "print(f\"  Stream1 only: {stream_contributions['individual_accuracies']['stream1_only']*100:.2f}%\")\n",
    "print(f\"  Stream2 only: {stream_contributions['individual_accuracies']['stream2_only']*100:.2f}%\")\n",
    "print(f\"  Integrated only: {stream_contributions['individual_accuracies']['integrated_only']*100:.2f}%\")\n",
    "\n",
    "print(\"\\nRelative Performance (vs Full Model):\")\n",
    "print(f\"  Stream1: {stream_contributions['relative_to_full_model']['stream1_ratio']*100:.1f}%\")\n",
    "print(f\"  Stream2: {stream_contributions['relative_to_full_model']['stream2_ratio']*100:.1f}%\")\n",
    "print(f\"  Integrated: {stream_contributions['relative_to_full_model']['integrated_ratio']*100:.1f}%\")\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "if stream_contributions['stream1_importance'] > 0.45:\n",
    "    print(\"  \u2192 Model relies more heavily on Stream1 (RGB)\")\n",
    "elif stream_contributions['stream2_importance'] > 0.45:\n",
    "    print(\"  \u2192 Model relies more heavily on Stream2 (Depth)\")\n",
    "elif stream_contributions['integrated_importance'] > 0.5:\n",
    "    print(\"  \u2192 Model relies most on Integrated stream (learned fusion)\")\n",
    "else:\n",
    "    print(\"  \u2192 Model uses all streams fairly equally\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"\u2705 Pathway analysis complete!\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Visualize pathway contributions\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Accuracy comparison (4 pathways for LINet)\n",
    "pathways = ['Full Model\\n(Integrated)', 'Stream1\\nOnly', 'Stream2\\nOnly']\n",
    "accuracies = [\n",
    "    pathway_analysis['accuracy']['full_model'] * 100,\n",
    "    pathway_analysis['accuracy']['stream1_only'] * 100,\n",
    "    pathway_analysis['accuracy']['stream2_only'] * 100\n",
    "]\n",
    "colors = ['green', 'blue', 'orange', 'purple']\n",
    "\n",
    "axes[0].bar(pathways, accuracies, color=colors, alpha=0.7)\n",
    "axes[0].set_ylabel('Accuracy (%)', fontsize=12)\n",
    "axes[0].set_title('Pathway Accuracy Comparison (LINet)', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "for i, v in enumerate(accuracies):\n",
    "    axes[0].text(i, v + 1, f'{v:.1f}%', ha='center', fontweight='bold')\n",
    "\n",
    "# Feature norm comparison (3 streams)\n",
    "norms = ['Stream1\\nFeatures', 'Stream2\\nFeatures', 'Integrated\\nFeatures']\n",
    "norm_values = [\n",
    "    pathway_analysis['feature_norms']['stream1_mean'],\n",
    "    pathway_analysis['feature_norms']['stream2_mean'],\n",
    "    pathway_analysis['feature_norms']['integrated_mean']\n",
    "]\n",
    "axes[1].bar(norms, norm_values, color=['blue', 'orange', 'purple'], alpha=0.7)\n",
    "axes[1].set_ylabel('Feature Norm (Mean)', fontsize=12)\n",
    "axes[1].set_title('Feature Magnitude Comparison', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "for i, v in enumerate(norm_values):\n",
    "    axes[1].text(i, v + 0.1, f'{v:.2f}', ha='center', fontweight='bold')\n",
    "\n",
    "# Stream importance (3 streams including integrated)\n",
    "streams = ['Stream1\\n(RGB)', 'Stream2\\n(Depth)', 'Integrated\\n(Fusion)']\n",
    "importance_values = [\n",
    "    stream_contributions['stream1_importance'] * 100,\n",
    "    stream_contributions['stream2_importance'] * 100,\n",
    "    stream_contributions['integrated_importance'] * 100\n",
    "]\n",
    "axes[2].bar(streams, importance_values, color=['blue', 'orange', 'purple'], alpha=0.7)\n",
    "axes[2].set_ylabel('Importance (%)', fontsize=12)\n",
    "axes[2].set_title('Stream Importance to Predictions', fontsize=14, fontweight='bold')\n",
    "axes[2].grid(True, alpha=0.3, axis='y')\n",
    "for i, v in enumerate(importance_values):\n",
    "    axes[2].text(i, v + 1, f'{v:.1f}%', ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{checkpoint_dir}/pathway_analysis.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\u2705 Pathway analysis plot saved to: {checkpoint_dir}/pathway_analysis.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-16"
   },
   "source": [
    "## 16. Save Results & Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save-results"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SAVING RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Save training history as JSON\n",
    "history_path = f\"{checkpoint_dir}/training_history.json\"\n",
    "with open(history_path, 'w') as f:\n",
    "    json_history = {\n",
    "        'train_loss': [float(x) for x in history['train_loss']],\n",
    "        'val_loss': [float(x) for x in history['val_loss']],\n",
    "        'train_accuracy': [float(x) for x in history['train_accuracy']],\n",
    "        'val_accuracy': [float(x) for x in history['val_accuracy']],\n",
    "        'learning_rates': [float(x) for x in history['learning_rates']],\n",
    "        'model_config': MODEL_CONFIG,\n",
    "        'dataset_config': DATASET_CONFIG,\n",
    "        'stream_specific_config': STREAM_SPECIFIC_CONFIG,\n",
    "        'training_config': TRAIN_CONFIG,\n",
    "        'scheduler_kwargs': history.get('scheduler_kwargs', {}),  # Scheduler-specific parameters\n",
    "        'final_results': {\n",
    "            'val_loss': float(results['loss']),\n",
    "            'val_accuracy': float(results['accuracy'])\n",
    "        },\n",
    "        'pathway_analysis': {\n",
    "            'full_model_accuracy': float(pathway_analysis['accuracy']['full_model']),\n",
    "            'stream1_only_accuracy': float(pathway_analysis['accuracy']['stream1_only']),\n",
    "            'stream2_only_accuracy': float(pathway_analysis['accuracy']['stream2_only']),\n",
    "            'integrated_only_accuracy': float(pathway_analysis['accuracy']['integrated_only'])\n",
    "        }\n",
    "    }\n",
    "    if 'early_stopping' in history:\n",
    "        json_history['early_stopping'] = history['early_stopping']\n",
    "    \n",
    "    json.dump(json_history, f, indent=2)\n",
    "\n",
    "print(f\"\u2705 Training history saved: {history_path}\")\n",
    "\n",
    "# Save final model (in addition to best model)\n",
    "final_model_path = f\"{checkpoint_dir}/final_model.pt\"\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': model.optimizer.state_dict(),\n",
    "    'config': MODEL_CONFIG,\n",
    "    'stream_specific_config': STREAM_SPECIFIC_CONFIG,\n",
    "    'scheduler_kwargs': history.get('scheduler_kwargs', {}),  # Scheduler-specific parameters\n",
    "    'history': history,\n",
    "    'val_accuracy': results['accuracy']\n",
    "}, final_model_path)\n",
    "\n",
    "print(f\"\u2705 Final model saved: {final_model_path}\")\n",
    "\n",
    "# Save summary report\n",
    "summary_path = f\"{checkpoint_dir}/summary.txt\"\n",
    "with open(summary_path, 'w') as f:\n",
    "    f.write(\"=\" * 60 + \"\\n\")\n",
    "    f.write(\"LINet Training Summary - SUN RGB-D\\n\")\n",
    "    f.write(\"=\" * 60 + \"\\n\\n\")\n",
    "    \n",
    "    # Model Configuration\n",
    "    f.write(\"Model Configuration:\\n\")\n",
    "    f.write(f\"  Architecture: LINet-{MODEL_CONFIG['architecture'].upper()} (3-stream Linear Integration)\\n\")\n",
    "    f.write(f\"  Num Classes: {MODEL_CONFIG['num_classes']}\\n\")\n",
    "    f.write(f\"  Stream1 Channels: {MODEL_CONFIG['stream1_channels']} (RGB)\\n\")\n",
    "    f.write(f\"  Stream2 Channels: {MODEL_CONFIG['stream2_channels']} (Depth)\\n\")\n",
    "    f.write(f\"  Dropout: {MODEL_CONFIG['dropout_p']}\\n\")\n",
    "    f.write(f\"  Device: {MODEL_CONFIG['device']}\\n\")\n",
    "    f.write(f\"  AMP Enabled: {MODEL_CONFIG['use_amp']}\\n\")\n",
    "    f.write(f\"  Total Parameters: {total_params:,}\\n\")\n",
    "    f.write(f\"  Trainable Parameters: {trainable_params:,}\\n\")\n",
    "    f.write(f\"  Integration Parameters: {integration_params:,}\\n\")\n",
    "    \n",
    "    # Dataset Configuration\n",
    "    f.write(f\"\\nDataset Configuration:\\n\")\n",
    "    f.write(f\"  Dataset: SUN RGB-D 15-category (Scene Classification)\\n\")\n",
    "    f.write(f\"  Training Samples: {len(train_loader.dataset)}\\n\")\n",
    "    f.write(f\"  Validation Samples: {len(val_loader.dataset)}\\n\")\n",
    "    f.write(f\"  Batch Size: {DATASET_CONFIG['batch_size']}\\n\")\n",
    "    f.write(f\"  Num Workers: {DATASET_CONFIG['num_workers']}\\n\")\n",
    "    f.write(f\"  Input Size: {DATASET_CONFIG['target_size']}\\n\")\n",
    "    \n",
    "    # Optimization Configuration\n",
    "    f.write(f\"\\nOptimization Configuration:\\n\")\n",
    "    f.write(f\"  Optimizer: {STREAM_SPECIFIC_CONFIG['optimizer']}\\n\")\n",
    "    f.write(f\"  Loss Function: {STREAM_SPECIFIC_CONFIG['loss']}\\n\")\n",
    "    \n",
    "    # Label smoothing if present\n",
    "    if 'label_smoothing' in STREAM_SPECIFIC_CONFIG and STREAM_SPECIFIC_CONFIG['label_smoothing'] > 0:\n",
    "        f.write(f\"  Label Smoothing: {STREAM_SPECIFIC_CONFIG['label_smoothing']}\\n\")\n",
    "    \n",
    "    f.write(f\"  Scheduler: {STREAM_SPECIFIC_CONFIG['scheduler']}\\n\")\n",
    "    \n",
    "    # Scheduler-specific parameters\n",
    "    scheduler_kwargs = history.get('scheduler_kwargs', {})\n",
    "    if scheduler_kwargs:\n",
    "        f.write(f\"  Scheduler Parameters:\\n\")\n",
    "        for key, value in scheduler_kwargs.items():\n",
    "            f.write(f\"    {key}: {value}\\n\")\n",
    "    \n",
    "    f.write(f\"  Base LR: {STREAM_SPECIFIC_CONFIG['learning_rate']}\\n\")\n",
    "    f.write(f\"  Base Weight Decay: {STREAM_SPECIFIC_CONFIG['weight_decay']}\\n\")\n",
    "    f.write(f\"  Gradient Clipping: {TRAIN_CONFIG['grad_clip_norm']}\\n\")\n",
    "    \n",
    "    # Stream-Specific Settings\n",
    "    f.write(f\"\\nStream-Specific Settings:\\n\")\n",
    "    f.write(f\"  Stream1 (RGB):\\n\")\n",
    "    f.write(f\"    Learning Rate: {STREAM_SPECIFIC_CONFIG['stream1_lr']}\\n\")\n",
    "    f.write(f\"    Weight Decay: {STREAM_SPECIFIC_CONFIG['stream1_weight_decay']}\\n\")\n",
    "    f.write(f\"  Stream2 (Depth):\\n\")\n",
    "    f.write(f\"    Learning Rate: {STREAM_SPECIFIC_CONFIG['stream2_lr']}\\n\")\n",
    "    f.write(f\"    Weight Decay: {STREAM_SPECIFIC_CONFIG['stream2_weight_decay']}\\n\")\n",
    "    \n",
    "    # Training Configuration\n",
    "    f.write(f\"\\nTraining Configuration:\\n\")\n",
    "    f.write(f\"  Total Epochs: {len(history['train_loss'])}\\n\")\n",
    "    f.write(f\"  Stream Monitoring: {TRAIN_CONFIG['stream_monitoring']}\\n\")\n",
    "    f.write(f\"  Early Stopping: {TRAIN_CONFIG['early_stopping']}\\n\")\n",
    "    if TRAIN_CONFIG['early_stopping']:\n",
    "        f.write(f\"    Monitor: {TRAIN_CONFIG['monitor']}\\n\")\n",
    "        f.write(f\"    Patience: {TRAIN_CONFIG['patience']}\\n\")\n",
    "        f.write(f\"    Min Delta: {TRAIN_CONFIG['min_delta']}\\n\")\n",
    "        f.write(f\"    Restore Best Weights: {TRAIN_CONFIG['restore_best_weights']}\\n\")\n",
    "    \n",
    "    # Results\n",
    "    f.write(f\"\\nFinal Results:\\n\")\n",
    "    f.write(f\"  Val Loss: {results['loss']:.4f}\\n\")\n",
    "    f.write(f\"  Val Accuracy: {results['accuracy']*100:.2f}%\\n\")\n",
    "    f.write(f\"  Best Val Accuracy: {max(history['val_accuracy'])*100:.2f}%\\n\")\n",
    "    f.write(f\"  Initial Train Loss: {history['train_loss'][0]:.4f}\\n\")\n",
    "    f.write(f\"  Final Train Loss: {history['train_loss'][-1]:.4f}\\n\")\n",
    "    f.write(f\"  Best Val Loss: {min(history['val_loss']):.4f}\\n\")\n",
    "    \n",
    "    # Pathway Analysis (3 streams for LINet)\n",
    "    f.write(f\"\\nPathway Analysis (3-stream LINet):\\n\")\n",
    "    f.write(f\"  Full Model (Integrated): {pathway_analysis['accuracy']['full_model']*100:.2f}%\\n\")\n",
    "    f.write(f\"  Stream1 Only (RGB): {pathway_analysis['accuracy']['stream1_only']*100:.2f}%\\n\")\n",
    "    f.write(f\"  Stream2 Only (Depth): {pathway_analysis['accuracy']['stream2_only']*100:.2f}%\\n\")\n",
    "    f.write(f\"  Integrated Only: {pathway_analysis['accuracy']['integrated_only']*100:.2f}%\\n\")\n",
    "    f.write(f\"  Stream1 Contribution: {pathway_analysis['accuracy']['stream1_contribution']*100:.2f}%\\n\")\n",
    "    f.write(f\"  Stream2 Contribution: {pathway_analysis['accuracy']['stream2_contribution']*100:.2f}%\\n\")\n",
    "    f.write(f\"  Integrated Contribution: {pathway_analysis['accuracy']['integrated_contribution']*100:.2f}%\\n\")\n",
    "\n",
    "print(f\"\u2705 Summary report saved: {summary_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"All results saved to: {checkpoint_dir}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# List saved files\n",
    "print(\"\\nSaved files:\")\n",
    "!ls -lh {checkpoint_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-17"
   },
   "source": [
    "## 17. Summary & Next Steps\n",
    "\n",
    "### \ud83c\udf89 Training Complete!\n",
    "\n",
    "**What we accomplished:**\n",
    "- \u2705 Trained LINet (3-stream Linear Integration ResNet) on SUN RGB-D dataset (15 categories)\n",
    "- \u2705 Used A100 GPU with AMP (2x speedup)\n",
    "- \u2705 Saved all checkpoints to Google Drive\n",
    "- \u2705 Analyzed Stream1, Stream2, and Integrated pathway contributions\n",
    "- \u2705 Generated training curves and visualizations\n",
    "- \u2705 Comprehensive stream monitoring with overfitting detection\n",
    "\n",
    "**Results are saved to:** Check the output above for the checkpoint directory path\n",
    "\n",
    "### \ud83d\udcca Expected Performance:\n",
    "\n",
    "For **SUN RGB-D Scene Classification (15 categories, 10,335 images)**:\n",
    "- **Good:** 65-75% validation accuracy\n",
    "- **Very Good:** 75-80% validation accuracy\n",
    "- **Excellent:** 80-85% validation accuracy\n",
    "\n",
    "**Much better than NYU Depth V2 due to:**\n",
    "- 6.9x more training samples (8,041 vs 1,159)\n",
    "- 22.6x better class balance (8.5x vs 192x)\n",
    "- Higher quality, more diverse dataset\n",
    "\n",
    "### \ud83e\udde0 LINet Architecture Highlights:\n",
    "\n",
    "**3-Stream Linear Integration:**\n",
    "- Stream1 processes RGB\n",
    "- Stream2 processes Depth\n",
    "- Integrated stream combines both through learned linear weights at every layer\n",
    "\n",
    "**5 Weight Matrices per LIConv2d:**\n",
    "- `stream1_weight` (full kernel for RGB)\n",
    "- `stream2_weight` (full kernel for Depth)\n",
    "- `integrated_weight` (1\u00d71 channel-wise)\n",
    "- `integration_from_stream1` (1\u00d71)\n",
    "- `integration_from_stream2` (1\u00d71)\n",
    "\n",
    "This allows **neuron-level integration** rather than late fusion!\n",
    "\n",
    "### \ud83d\udd0d Next Steps:\n",
    "\n",
    "1. **Review Results:**\n",
    "   - Check training curves above\n",
    "   - Review pathway analysis (3 streams)\n",
    "   - Compare Stream1/Stream2/Integrated contributions\n",
    "   - Analyze stream monitoring plots\n",
    "\n",
    "2. **Download Results:**\n",
    "   - All files are saved to your Google Drive\n",
    "   - Download checkpoints for local inference\n",
    "\n",
    "3. **Experiment:**\n",
    "   - Try ResNet50 for better accuracy (change `architecture` in Model Config)\n",
    "   - Use stream-specific optimization if monitoring shows imbalance\n",
    "   - Train longer if early stopping triggered\n",
    "   - Analyze integration weights to understand learned strategies\n",
    "\n",
    "4. **Deploy:**\n",
    "   - Use the best model for inference\n",
    "   - Test on new RGB-D images\n",
    "   - Integrate into your application\n",
    "\n",
    "---\n",
    "\n",
    "**Questions or issues?** Check the training summary and pathway analysis above!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "final-summary"
   },
   "outputs": [],
   "source": [
    "# Print final summary\n",
    "print(\"=\" * 60)\n",
    "print(\"FINAL SUMMARY - LINET\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n\u2705 Training Complete!\")\n",
    "print(f\"\\nFinal Validation Accuracy: {results['accuracy']*100:.2f}%\")\n",
    "print(f\"Best Validation Accuracy: {max(history['val_accuracy'])*100:.2f}%\")\n",
    "print(f\"\\nStream1 Pathway (RGB): {pathway_analysis['accuracy']['stream1_only']*100:.2f}%\")\n",
    "print(f\"Stream2 Pathway (Depth): {pathway_analysis['accuracy']['stream2_only']*100:.2f}%\")\n",
    "print(f\"Integrated Pathway: {pathway_analysis['accuracy']['integrated_only']*100:.2f}%\")\n",
    "print(f\"Combined (Full Model): {pathway_analysis['accuracy']['full_model']*100:.2f}%\")\n",
    "print(f\"\\nTotal Training Epochs: {len(history['train_loss'])}\")\n",
    "print(f\"Total Parameters: {total_params:,}\")\n",
    "print(f\"Integration Parameters: {integration_params:,}\")\n",
    "print(f\"\\nCheckpoints saved to: {checkpoint_dir}\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"\ud83c\udf89 All done! Check Google Drive for saved models and results.\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}