{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# LINet Training on SUN RGB-D - Google Colab\n",
    "\n",
    "**Complete end-to-end training pipeline for Linear Integration ResNet (LINet) on Google Colab with A100 GPU**\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Checklist Before Running:\n",
    "\n",
    "- [ ] **Enable A100 GPU:** Runtime ‚Üí Change runtime type ‚Üí Hardware accelerator: GPU ‚Üí GPU type: A100\n",
    "- [ ] **Mount Google Drive:** Your code and dataset will be stored on Drive\n",
    "- [ ] **Upload dataset to Drive:** `MyDrive/datasets/sunrgbd_15/` (preprocessed 15-category dataset)\n",
    "- [ ] **Expected Runtime:** ~2-3 hours for training\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ What This Notebook Does:\n",
    "\n",
    "1. ‚úÖ Verify A100 GPU is available\n",
    "2. ‚úÖ Mount Google Drive\n",
    "3. ‚úÖ Clone your repository to local disk (fast I/O)\n",
    "4. ‚úÖ Copy SUN RGB-D dataset to local disk (10-20x faster than Drive)\n",
    "5. ‚úÖ Install dependencies\n",
    "6. ‚úÖ Train LINet (3-stream Linear Integration ResNet) with all optimizations\n",
    "7. ‚úÖ Save checkpoints to Drive (persistent storage)\n",
    "8. ‚úÖ Generate training curves and analysis\n",
    "\n",
    "---\n",
    "\n",
    "## üß† About LINet:\n",
    "\n",
    "**LINet** (Linear Integration Network) is a 3-stream neural network architecture where:\n",
    "- **Stream1** processes RGB images\n",
    "- **Stream2** processes Depth maps\n",
    "- **Integrated Stream** combines both streams using learned linear integration weights\n",
    "\n",
    "Unlike traditional fusion methods, LINet performs integration **at the neuron level** through 5 weight matrices per convolution:\n",
    "- `stream1_weight` (full kernel for RGB)\n",
    "- `stream2_weight` (full kernel for Depth)\n",
    "- `integrated_weight` (1√ó1 channel-wise for integrated features)\n",
    "- `integration_from_stream1` (1√ó1 integration from RGB)\n",
    "- `integration_from_stream2` (1√ó1 integration from Depth)\n",
    "\n",
    "This allows the network to learn optimal integration strategies at every layer!\n",
    "\n",
    "---\n",
    "\n",
    "**Let's get started!** üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-1"
   },
   "source": [
    "## 1. Environment Setup & GPU Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check-gpu"
   },
   "outputs": [],
   "source": [
    "# Check GPU availability and specs\n",
    "import torch\n",
    "import subprocess\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"GPU VERIFICATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check PyTorch and CUDA\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "    \n",
    "    # Check if it's A100\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    if 'A100' in gpu_name:\n",
    "        print(\"\\n‚úÖ A100 GPU detected - PERFECT for training!\")\n",
    "    elif 'V100' in gpu_name:\n",
    "        print(\"\\n‚úÖ V100 GPU detected - Good for training (slower than A100)\")\n",
    "    elif 'T4' in gpu_name:\n",
    "        print(\"\\n‚ö†Ô∏è  T4 GPU detected - Will be slower, consider upgrading to A100\")\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è  GPU: {gpu_name} - Consider using A100 for best performance\")\n",
    "else:\n",
    "    print(\"\\n‚ùå NO GPU DETECTED!\")\n",
    "    print(\"Please enable GPU: Runtime ‚Üí Change runtime type ‚Üí Hardware accelerator: GPU\")\n",
    "    raise RuntimeError(\"GPU is required for training\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nvidia-smi"
   },
   "outputs": [],
   "source": [
    "# Detailed GPU info\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-2"
   },
   "source": [
    "## 2. Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mount-drive"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "print(\"\\n‚úÖ Google Drive mounted successfully!\")\n",
    "print(f\"\\nDrive contents:\")\n",
    "!ls -la /content/drive/MyDrive/ | head -20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-3"
   },
   "source": [
    "## 3. Clone Repository to Local Disk (Fast I/O)\n",
    "\n",
    "**Important:** We clone to `/content/` (local SSD) instead of Drive for 10-20x faster I/O\n",
    "\n",
    "**Default:** Clone from GitHub (recommended - always gets latest code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clone-repo"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Configuration\n",
    "PROJECT_NAME = \"Multi-Stream-Neural-Networks\"\n",
    "GITHUB_REPO = \"https://github.com/clingergab/Multi-Stream-Neural-Networks.git\"  # UPDATE THIS\n",
    "LOCAL_REPO_PATH = f\"/content/{PROJECT_NAME}\"  # Local copy for fast I/O\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"REPOSITORY SETUP\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Ensure we're in a valid directory\n",
    "os.chdir('/content')\n",
    "print(f\"Starting in: {os.getcwd()}\")\n",
    "\n",
    "# Check if repo already exists (same session, rerunning cell)\n",
    "if Path(LOCAL_REPO_PATH).exists() and Path(f\"{LOCAL_REPO_PATH}/.git\").exists():\n",
    "    print(f\"\\nüìÅ Repo already exists: {LOCAL_REPO_PATH}\")\n",
    "    print(f\"üîÑ Pulling latest changes...\")\n",
    "    \n",
    "    os.chdir(LOCAL_REPO_PATH)\n",
    "    !git pull\n",
    "    print(\"‚úÖ Repo updated\")\n",
    "\n",
    "# Clone from GitHub (first run)\n",
    "else:\n",
    "    # Remove old incomplete copy if exists\n",
    "    if Path(LOCAL_REPO_PATH).exists():\n",
    "        print(f\"\\nüóëÔ∏è  Removing incomplete repo copy...\")\n",
    "        !rm -rf {LOCAL_REPO_PATH}\n",
    "    \n",
    "    print(f\"\\nüîÑ Cloning from GitHub...\")\n",
    "    print(f\"   Repo: {GITHUB_REPO}\")\n",
    "    print(f\"   Destination: {LOCAL_REPO_PATH}\")\n",
    "    \n",
    "    !git clone {GITHUB_REPO} {LOCAL_REPO_PATH}\n",
    "    \n",
    "    # Verify clone succeeded\n",
    "    if not Path(LOCAL_REPO_PATH).exists():\n",
    "        raise RuntimeError(f\"Failed to clone repository to {LOCAL_REPO_PATH}\")\n",
    "    \n",
    "    print(\"‚úÖ Repo cloned successfully\")\n",
    "    os.chdir(LOCAL_REPO_PATH)\n",
    "\n",
    "# Verify repo structure\n",
    "print(f\"\\nüìÇ Repository structure:\")\n",
    "!ls -la {LOCAL_REPO_PATH}\n",
    "\n",
    "print(f\"\\n‚úÖ Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-4"
   },
   "source": [
    "## 4. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install-deps"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "print(\"Installing dependencies...\")\n",
    "\n",
    "!pip install -q h5py tqdm matplotlib seaborn\n",
    "\n",
    "# Verify installations\n",
    "import h5py\n",
    "import tqdm\n",
    "import matplotlib\n",
    "import seaborn\n",
    "\n",
    "print(\"‚úÖ All dependencies installed!\")\n",
    "print(f\"   h5py: {h5py.__version__}\")\n",
    "print(f\"   matplotlib: {matplotlib.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-5"
   },
   "source": [
    "## 5. Copy SUN RGB-D Dataset to Local Disk\n",
    "\n",
    "**Performance Note:** Local disk I/O is ~10-20x faster than Drive!\n",
    "\n",
    "**Dataset:** SUN RGB-D 15-category preprocessed dataset (~3.5 GB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download-dataset"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Paths\n",
    "DRIVE_DATASET_TAR = \"/content/drive/MyDrive/datasets/sunrgbd_15.tar.gz\"  # Compressed file\n",
    "LOCAL_DATASET_PATH = \"/content/data/sunrgbd_15\"  # Extracted location\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SUN RGB-D 15-CATEGORY DATASET SETUP\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check if already on local disk\n",
    "if Path(LOCAL_DATASET_PATH).exists():\n",
    "    print(f\"‚úÖ Dataset already on local disk: {LOCAL_DATASET_PATH}\")\n",
    "    \n",
    "    # Verify structure\n",
    "    train_rgb_count = len(list(Path(f\"{LOCAL_DATASET_PATH}/train/rgb\").glob(\"*.png\")))\n",
    "    val_rgb_count = len(list(Path(f\"{LOCAL_DATASET_PATH}/val/rgb\").glob(\"*.png\")))\n",
    "    print(f\"   Train samples: {train_rgb_count}\")\n",
    "    print(f\"   Val samples: {val_rgb_count}\")\n",
    "\n",
    "# Copy and extract from Drive\n",
    "elif Path(DRIVE_DATASET_TAR).exists():\n",
    "    print(f\"üìÅ Found compressed dataset on Drive: {DRIVE_DATASET_TAR}\")\n",
    "    print(f\"üì• Copying 4.2GB compressed file to local disk...\")\n",
    "    print(f\"   ‚è±Ô∏è  This takes ~3-5 minutes (much faster than 20k individual files!)\")\n",
    "    \n",
    "    # Create parent directory\n",
    "    !mkdir -p /content/data\n",
    "    \n",
    "    # Copy compressed file with progress\n",
    "    print(f\"\\nCopying compressed archive...\")\n",
    "    !rsync -ah --info=progress2 {DRIVE_DATASET_TAR} /content/data/sunrgbd_15.tar.gz\n",
    "    \n",
    "    # Extract to local disk (suppress macOS metadata warnings)\n",
    "    print(f\"\\nüì¶ Extracting dataset to local disk...\")\n",
    "    !tar -xzf /content/data/sunrgbd_15.tar.gz -C /content/data/ 2>&1 | grep -v \"Ignoring unknown extended header\"\n",
    "    \n",
    "    # Remove tar file to save space\n",
    "    !rm /content/data/sunrgbd_15.tar.gz\n",
    "    \n",
    "    print(f\"\\n‚úÖ Dataset extracted to local disk\")\n",
    "    \n",
    "    # Verify extraction\n",
    "    train_rgb_count = len(list(Path(f\"{LOCAL_DATASET_PATH}/train/rgb\").glob(\"*.png\")))\n",
    "    val_rgb_count = len(list(Path(f\"{LOCAL_DATASET_PATH}/val/rgb\").glob(\"*.png\")))\n",
    "    print(f\"   Train samples: {train_rgb_count}\")\n",
    "    print(f\"   Val samples: {val_rgb_count}\")\n",
    "\n",
    "else:\n",
    "    print(f\"‚ùå Compressed dataset not found on Drive!\")\n",
    "    print(f\"   Expected location: {DRIVE_DATASET_TAR}\")\n",
    "    print(f\"\\nüìã To fix this:\")\n",
    "    print(f\"   1. Run: COPYFILE_DISABLE=1 tar -czf sunrgbd_15.tar.gz sunrgbd_15/\")\n",
    "    print(f\"   2. Upload sunrgbd_15.tar.gz to Google Drive\")\n",
    "    print(f\"   3. Place it at: {DRIVE_DATASET_TAR}\")\n",
    "    raise FileNotFoundError(f\"Compressed dataset not found at {DRIVE_DATASET_TAR}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"Dataset ready at: {LOCAL_DATASET_PATH}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-6"
   },
   "source": [
    "## 6. Setup Python Path & Import LINet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup-imports"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Remove cached modules\n",
    "modules_to_reload = [k for k in sys.modules.keys() if k.startswith('src.')]\n",
    "for module in modules_to_reload:\n",
    "    del sys.modules[module]\n",
    "    \n",
    "# Add project to Python path\n",
    "project_root = '/content/Multi-Stream-Neural-Networks'\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "# Verify project structure\n",
    "print(\"Project structure:\")\n",
    "!ls -la {project_root}/src/models/\n",
    "\n",
    "# Import LINet and SUN RGB-D dataloader\n",
    "print(\"\\nImporting LINet and dataloaders...\")\n",
    "from src.models.linear_integration.li_net import li_resnet18, li_resnet50\n",
    "from src.data_utils.sunrgbd_dataset import get_sunrgbd_dataloaders\n",
    "\n",
    "print(\"‚úÖ LINet and dataloaders imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-7"
   },
   "source": [
    "## 7. Load SUN RGB-D Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify dataset structure\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DATASET STRUCTURE VERIFICATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "dataset_root = Path(LOCAL_DATASET_PATH)\n",
    "\n",
    "print(\"\\nDirectory structure:\")\n",
    "print(f\"  {dataset_root}/\")\n",
    "print(f\"    train/\")\n",
    "print(f\"      rgb/ - {len(list((dataset_root / 'train' / 'rgb').glob('*.png')))} images\")\n",
    "print(f\"      depth/ - {len(list((dataset_root / 'train' / 'depth').glob('*.png')))} images\")\n",
    "print(f\"      labels.txt\")\n",
    "print(f\"    val/\")\n",
    "print(f\"      rgb/ - {len(list((dataset_root / 'val' / 'rgb').glob('*.png')))} images\")\n",
    "print(f\"      depth/ - {len(list((dataset_root / 'val' / 'depth').glob('*.png')))} images\")\n",
    "print(f\"      labels.txt\")\n",
    "print(f\"    class_names.txt\")\n",
    "print(f\"    dataset_info.txt\")\n",
    "\n",
    "# Read class names\n",
    "with open(dataset_root / 'class_names.txt', 'r') as f:\n",
    "    class_names = [line.strip() for line in f]\n",
    "\n",
    "print(f\"\\nClasses ({len(class_names)}):\")\n",
    "for i, name in enumerate(class_names):\n",
    "    print(f\"  {i}: {name}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load-dataset"
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"LOADING SUN RGB-D 15-CATEGORY DATASET\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Dataset configuration\n",
    "DATASET_CONFIG = {\n",
    "    'data_root': LOCAL_DATASET_PATH,\n",
    "    'batch_size': 128,  # Good balance for A100\n",
    "    'num_workers': 4,\n",
    "    'target_size': (416, 544),  \n",
    "    'num_classes': 15   # SUN RGB-D merged to 15 categories (labels 0-14)\n",
    "}\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "for key, value in DATASET_CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(f\"\\nLoading dataset from: {DATASET_CONFIG['data_root']}\")\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader, val_loader = get_sunrgbd_dataloaders(\n",
    "    data_root=DATASET_CONFIG['data_root'],\n",
    "    batch_size=DATASET_CONFIG['batch_size'],\n",
    "    num_workers=DATASET_CONFIG['num_workers'],\n",
    "    target_size=DATASET_CONFIG['target_size']\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Dataset loaded successfully!\")\n",
    "print(f\"\\nDataset Statistics:\")\n",
    "print(f\"  Train batches: {len(train_loader)}\")\n",
    "print(f\"  Val batches: {len(val_loader)}\")\n",
    "print(f\"  Train samples: {len(train_loader.dataset)}\")\n",
    "print(f\"  Val samples: {len(val_loader.dataset)}\")\n",
    "print(f\"  Batch size: {DATASET_CONFIG['batch_size']}\")\n",
    "\n",
    "# Test loading a batch\n",
    "print(f\"\\nTesting batch loading...\")\n",
    "rgb_batch, depth_batch, label_batch = next(iter(train_loader))\n",
    "print(f\"  RGB shape: {rgb_batch.shape}\")\n",
    "print(f\"  Depth shape: {depth_batch.shape}\")\n",
    "print(f\"  Labels shape: {label_batch.shape}\")\n",
    "print(f\"  Labels min: {label_batch.min().item()}, max: {label_batch.max().item()}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-8"
   },
   "source": [
    "## 8. Visualize Sample Data\n",
    "\n",
    "Shows RGB images, depth maps, and scene labels from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "visualize-data"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Visualize some samples\n",
    "fig, axes = plt.subplots(2, 4, figsize=(14, 7))\n",
    "\n",
    "for i in range(4):\n",
    "    rgb = rgb_batch[i].cpu()\n",
    "    depth = depth_batch[i].cpu()\n",
    "    label = label_batch[i].item()\n",
    "    \n",
    "    # Denormalize for visualization\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "    rgb_vis = rgb * std + mean\n",
    "    rgb_vis = torch.clamp(rgb_vis, 0, 1)\n",
    "    \n",
    "    # Plot RGB\n",
    "    axes[0, i].imshow(rgb_vis.permute(1, 2, 0))\n",
    "    axes[0, i].set_title(f\"RGB - Class {label}\", fontsize=10)\n",
    "    axes[0, i].axis('off')\n",
    "    \n",
    "    # Plot Depth\n",
    "    axes[1, i].imshow(depth.squeeze(), cmap='viridis')\n",
    "    axes[1, i].set_title(f\"Depth - Class {label}\", fontsize=10)\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "plt.suptitle('SUN RGB-D Sample Data (RGB + Depth)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Sample visualization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-9"
   },
   "source": [
    "## 9. Create & Compile LINet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create-model"
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"MODEL CREATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Model configuration\n",
    "MODEL_CONFIG = {\n",
    "    'architecture': 'resnet18',  # or 'resnet50' for better accuracy\n",
    "    'num_classes': 15,  # SUN RGB-D has 15 merged categories (labels 0-14)\n",
    "    'stream1_channels': 3,  # RGB\n",
    "    'stream2_channels': 1,  # Depth\n",
    "    'dropout_p': 0.5,  # Dropout for regularization\n",
    "    'device': 'cuda',\n",
    "    'use_amp': True  # Automatic Mixed Precision (2x faster on A100)\n",
    "}\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "for key, value in MODEL_CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Create model\n",
    "print(f\"\\nCreating LINet-{MODEL_CONFIG['architecture'].upper()} (3-stream Linear Integration ResNet)...\")\n",
    "\n",
    "if MODEL_CONFIG['architecture'] == 'resnet18':\n",
    "    model = li_resnet18(\n",
    "        num_classes=MODEL_CONFIG['num_classes'],\n",
    "        stream1_input_channels=MODEL_CONFIG['stream1_channels'],\n",
    "        stream2_input_channels=MODEL_CONFIG['stream2_channels'],\n",
    "        dropout_p=MODEL_CONFIG['dropout_p'],\n",
    "        device=MODEL_CONFIG['device'],\n",
    "        use_amp=MODEL_CONFIG['use_amp']\n",
    "    )\n",
    "elif MODEL_CONFIG['architecture'] == 'resnet50':\n",
    "    model = li_resnet50(\n",
    "        num_classes=MODEL_CONFIG['num_classes'],\n",
    "        stream1_input_channels=MODEL_CONFIG['stream1_channels'],\n",
    "        stream2_input_channels=MODEL_CONFIG['stream2_channels'],\n",
    "        dropout_p=MODEL_CONFIG['dropout_p'],\n",
    "        device=MODEL_CONFIG['device'],\n",
    "        use_amp=MODEL_CONFIG['use_amp']\n",
    "    )\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Count integration-specific parameters\n",
    "integration_params = 0\n",
    "for name, param in model.named_parameters():\n",
    "    if 'integration' in name or 'integrated_weight' in name:\n",
    "        integration_params += param.numel()\n",
    "\n",
    "print(f\"\\n‚úÖ Model created successfully!\")\n",
    "print(f\"\\nModel Statistics:\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"  Integration parameters: {integration_params:,}\")\n",
    "print(f\"  Model size: {total_params * 4 / 1024**2:.2f} MB (FP32)\")\n",
    "print(f\"  Architecture: 3-stream Linear Integration (LINet)\")\n",
    "print(f\"  Device: {MODEL_CONFIG['device']}\")\n",
    "print(f\"  AMP enabled: {MODEL_CONFIG['use_amp']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "compile-model"
   },
   "source": "## 9b. Model Compilation (Keras-Style API with Warmup)\n\n**Create optimizer and scheduler as objects, then pass to compile()**\n\n**NEW:** Learning rate warmup support! The scheduler will linearly increase the learning rate from a lower starting point to the target LR over the first few epochs, helping stabilize early training."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Compile model with stream-specific optimization\nprint(\"=\" * 60)\nprint(\"MODEL COMPILATION\")\nprint(\"=\" * 60)\n\n# Import optimizer and scheduler utilities\nfrom src.training.optimizers import create_stream_optimizer\nfrom src.training.schedulers import setup_scheduler\n\n# Stream-specific configuration for optimal RGB/Depth balance\nSTREAM_SPECIFIC_CONFIG = {\n    # Stream-specific learning rates (adjusted based on research):\n    'stream1_lr': 3e-5,               # RGB stream: lower LR (more regularization)\n    'stream2_lr': 1e-4,               # Depth stream: higher LR (needs more learning)\n    'shared_lr': 7e-5,                # Shared params: base LR\n\n    # Stream-specific weight decay:\n    'stream1_weight_decay': 5e-4,     # RGB stream: higher WD (prevent overfitting)\n    'stream2_weight_decay': 1e-4,     # Depth stream: lighter WD (less regularization)\n    'shared_weight_decay': 2e-4,      # Shared params: base WD\n}\n\n# Scheduler configuration (with warmup support!)\nSCHEDULER_CONFIG = {\n    'scheduler_type': 'cosine',\n    't_max': 80,  # Will be updated to match epochs in training config\n    'eta_min': 1e-6,\n    'warmup_epochs': 5,  # Warmup: linearly increase LR for first 5 epochs\n    'warmup_start_factor': 0.1  # Start at 10% of target LR during warmup\n}\n\nprint(f\"Stream-Specific Configuration:\")\nfor key, value in STREAM_SPECIFIC_CONFIG.items():\n    print(f\"  {key}: {value}\")\n\nprint(f\"\\nScheduler Configuration (with warmup):\")\nfor key, value in SCHEDULER_CONFIG.items():\n    print(f\"  {key}: {value}\")\n\n# Step 1: Create optimizer with stream-specific learning rates\nprint(\"\\n[Step 1] Creating stream-specific optimizer...\")\noptimizer = create_stream_optimizer(\n    model,\n    optimizer_type='adamw',\n    stream1_lr=STREAM_SPECIFIC_CONFIG['stream1_lr'],\n    stream2_lr=STREAM_SPECIFIC_CONFIG['stream2_lr'],\n    shared_lr=STREAM_SPECIFIC_CONFIG['shared_lr'],\n    stream1_weight_decay=STREAM_SPECIFIC_CONFIG['stream1_weight_decay'],\n    stream2_weight_decay=STREAM_SPECIFIC_CONFIG['stream2_weight_decay'],\n    shared_weight_decay=STREAM_SPECIFIC_CONFIG['shared_weight_decay']\n)\n\nprint(f\"‚úÖ Optimizer created: {optimizer.__class__.__name__}\")\nprint(f\"   Parameter groups: {len(optimizer.param_groups)}\")\nfor i, group in enumerate(optimizer.param_groups):\n    num_params = sum(p.numel() for p in group['params'])\n    print(f\"   Group {i+1}: lr={group['lr']:.2e}, wd={group['weight_decay']:.2e}, params={num_params:,}\")\n\n# Step 2: Create scheduler with warmup support\nprint(\"\\n[Step 2] Creating learning rate scheduler with warmup...\")\nscheduler = setup_scheduler(\n    optimizer,\n    scheduler_type=SCHEDULER_CONFIG['scheduler_type'],\n    epochs=80,  # Placeholder - will match TRAIN_CONFIG['epochs']\n    train_loader_len=len(train_loader),\n    t_max=SCHEDULER_CONFIG['t_max'],\n    eta_min=SCHEDULER_CONFIG['eta_min'],\n    warmup_epochs=SCHEDULER_CONFIG['warmup_epochs'],\n    warmup_start_factor=SCHEDULER_CONFIG['warmup_start_factor']\n)\n\nprint(f\"‚úÖ Scheduler created: {scheduler.__class__.__name__}\")\nprint(f\"   Warmup: {SCHEDULER_CONFIG['warmup_epochs']} epochs (LR: {SCHEDULER_CONFIG['warmup_start_factor']*100:.0f}% ‚Üí 100%)\")\nprint(f\"   Main scheduler: {SCHEDULER_CONFIG['scheduler_type']} annealing\")\n\n# Step 3: Compile model with optimizer and scheduler objects (Keras-style!)\nprint(\"\\n[Step 3] Compiling model with optimizer and scheduler objects...\")\nmodel.compile(\n    optimizer=optimizer,\n    scheduler=scheduler,\n    loss='cross_entropy',\n    label_smoothing=0.1\n)\n\nprint(\"\\n‚úÖ Model compiled successfully (Keras-style)!\")\nprint(\"\\nüí° Learning rate warmup enabled!\")\nprint(f\"   First {SCHEDULER_CONFIG['warmup_epochs']} epochs: LR increases from {SCHEDULER_CONFIG['warmup_start_factor']*100:.0f}% to 100%\")\nprint(f\"   Remaining epochs: {SCHEDULER_CONFIG['scheduler_type']} annealing from 100% to {SCHEDULER_CONFIG['eta_min']:.0e}\")\nprint(\"\\n\" + \"=\" * 60)"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-10"
   },
   "source": [
    "## 10. Test Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "test-forward"
   },
   "outputs": [],
   "source": [
    "# Test forward pass with detailed debugging\n",
    "print(\"Testing forward pass with CUDA_LAUNCH_BLOCKING for better error messages...\")\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'  # Synchronous CUDA for better error messages\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    rgb_test, depth_test, labels_test = next(iter(train_loader))\n",
    "    \n",
    "    print(f\"\\nInput validation:\")\n",
    "    print(f\"  RGB shape: {rgb_test.shape}, dtype: {rgb_test.dtype}\")\n",
    "    print(f\"  RGB min: {rgb_test.min():.4f}, max: {rgb_test.max():.4f}\")\n",
    "    print(f\"  RGB has NaN: {torch.isnan(rgb_test).any()}\")\n",
    "    print(f\"  RGB has Inf: {torch.isinf(rgb_test).any()}\")\n",
    "    \n",
    "    print(f\"\\n  Depth shape: {depth_test.shape}, dtype: {depth_test.dtype}\")\n",
    "    print(f\"  Depth min: {depth_test.min():.4f}, max: {depth_test.max():.4f}\")\n",
    "    print(f\"  Depth has NaN: {torch.isnan(depth_test).any()}\")\n",
    "    print(f\"  Depth has Inf: {torch.isinf(depth_test).any()}\")\n",
    "    \n",
    "    print(f\"\\n  Labels shape: {labels_test.shape}, dtype: {labels_test.dtype}\")\n",
    "    print(f\"  Labels min: {labels_test.min()}, max: {labels_test.max()}\")\n",
    "    print(f\"  Labels unique: {torch.unique(labels_test).tolist()}\")\n",
    "    \n",
    "    print(\"\\nRunning forward pass...\")\n",
    "    rgb_cuda = rgb_test.to('cuda')\n",
    "    depth_cuda = depth_test.to('cuda')\n",
    "    \n",
    "    try:\n",
    "        outputs = model(rgb_cuda, depth_cuda)\n",
    "        print(f\"  ‚úÖ Forward pass successful!\")\n",
    "        print(f\"  Output shape: {outputs.shape}\")\n",
    "        print(f\"  Output min: {outputs.min():.4f}, max: {outputs.max():.4f}\")\n",
    "        \n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        print(f\"\\nSample predictions: {predictions.cpu().numpy()[:10]}\")\n",
    "        print(f\"Ground truth: {labels_test.numpy()[:10]}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Forward pass failed!\")\n",
    "        print(f\"Error: {e}\")\n",
    "        print(f\"\\nThis is likely a model architecture issue, not a data issue.\")\n",
    "        print(f\"Possible causes:\")\n",
    "        print(f\"  1. BatchNorm running stats issue\")\n",
    "        print(f\"  2. Invalid tensor operations in model\")\n",
    "        print(f\"  3. Memory corruption\")\n",
    "        raise\n",
    "\n",
    "print(\"\\n‚úÖ Forward pass test complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-11"
   },
   "source": [
    "## 11. Setup Checkpoint Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"CHECKPOINT DIRECTORY SETUP\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create checkpoint directory on Google Drive (persistent storage)\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "checkpoint_dir = f\"/content/drive/MyDrive/linet_checkpoints/run_{timestamp}\"\n",
    "\n",
    "# Create directory\n",
    "Path(checkpoint_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"\\n‚úÖ Checkpoint directory created:\")\n",
    "print(f\"   {checkpoint_dir}\")\n",
    "print(f\"\\nAll training artifacts will be saved here:\")\n",
    "print(f\"  ‚Ä¢ Best model weights\")\n",
    "print(f\"  ‚Ä¢ Training history\")\n",
    "print(f\"  ‚Ä¢ Monitoring metrics\")\n",
    "print(f\"  ‚Ä¢ Visualizations\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Train the Model üöÄ\n",
    "\n",
    "**Expected time:** ~2-3 hours for 90 epochs on A100\n",
    "\n",
    "**All optimizations enabled:**\n",
    "- ‚úÖ Automatic Mixed Precision (2x faster)\n",
    "- ‚úÖ Gradient Clipping (stability)\n",
    "- ‚úÖ Cosine Annealing LR\n",
    "- ‚úÖ Early Stopping\n",
    "- ‚úÖ Best Model Checkpointing\n",
    "- ‚úÖ Local disk I/O (10-20x faster than Drive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup-checkpoints"
   },
   "outputs": [],
   "source": "import warnings\n\n# Suppress PyTorch SequentialLR deprecation warning (internal PyTorch issue, not our code)\nwarnings.filterwarnings(\n    'ignore',\n    message='The epoch parameter in `scheduler.step\\\\(\\\\)` was not necessary',\n    category=UserWarning\n)\n\nprint(\"=\" * 60)\nprint(\"TRAINING WITH STREAM MONITORING\")\nprint(\"=\" * 60)\n\n# Training configuration\nTRAIN_CONFIG = {\n    'epochs': 80,\n    'grad_clip_norm': 5.0,\n    'early_stopping': True,\n    'patience': 12,\n    'min_delta': 0.001,\n    'monitor': 'val_accuracy',\n    'restore_best_weights': True,\n    'save_path': f\"{checkpoint_dir}/best_model.pt\",\n    'stream_monitoring': True,  # Built-in stream monitoring!\n}\n\nprint(f\"Configuration:\")\nfor key, value in TRAIN_CONFIG.items():\n    print(f\"  {key}: {value}\")\n\nprint(\"\\n\" + \"=\" * 60)\nprint(f\"Training will take approximately 2-3 hours on A100\")\nprint(f\"Stream monitoring active - detailed per-stream metrics shown each epoch\")\nprint(\"==\" * 60 + \"\\n\")\n\n# Train using built-in fit() method with stream monitoring\n# NOTE: No scheduler_kwargs needed! Scheduler was already created and passed to compile()\nhistory = model.fit(\n    train_loader=train_loader,\n    val_loader=val_loader,\n    epochs=TRAIN_CONFIG['epochs'],\n    verbose=True,\n    save_path=TRAIN_CONFIG['save_path'],\n    early_stopping=TRAIN_CONFIG['early_stopping'],\n    patience=TRAIN_CONFIG['patience'],\n    min_delta=TRAIN_CONFIG['min_delta'],\n    monitor=TRAIN_CONFIG['monitor'],\n    restore_best_weights=TRAIN_CONFIG['restore_best_weights'],\n    grad_clip_norm=TRAIN_CONFIG['grad_clip_norm'],\n    stream_monitoring=TRAIN_CONFIG['stream_monitoring']  # Enable built-in monitoring\n)\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"üéâ TRAINING COMPLETE!\")\nprint(\"=\" * 60)"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-13"
   },
   "source": [
    "## 13. Evaluate Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"FINAL EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Evaluate on validation set\n",
    "results = model.evaluate(data_loader=val_loader)\n",
    "\n",
    "print(f\"\\nFinal Validation Results:\")\n",
    "print(f\"  Loss: {results['loss']:.4f}\")\n",
    "print(f\"  Overall Accuracy: {results['accuracy']*100:.2f}%\")\n",
    "\n",
    "# Show stream-specific accuracies if available\n",
    "if 'stream1_accuracy' in results:\n",
    "    print(f\"\\nStream-Specific Performance:\")\n",
    "    print(f\"  Stream1 (RGB) Accuracy: {results['stream1_accuracy']*100:.2f}%\")\n",
    "    print(f\"  Stream2 (Depth) Accuracy: {results['stream2_accuracy']*100:.2f}%\")\n",
    "\n",
    "print(f\"\\nTraining Summary:\")\n",
    "print(f\"  Initial train loss: {history['train_loss'][0]:.4f}\")\n",
    "print(f\"  Final train loss: {history['train_loss'][-1]:.4f}\")\n",
    "print(f\"  Best val loss: {min(history['val_loss']):.4f}\")\n",
    "print(f\"  Initial train acc: {history['train_accuracy'][0]*100:.2f}%\")\n",
    "print(f\"  Final train acc: {history['train_accuracy'][-1]*100:.2f}%\")\n",
    "print(f\"  Best val acc: {max(history['val_accuracy'])*100:.2f}%\")\n",
    "print(f\"  Total epochs: {len(history['train_loss'])}\")\n",
    "\n",
    "if 'early_stopping' in history:\n",
    "    print(f\"\\nEarly Stopping Info:\")\n",
    "    print(f\"  Stopped early: {history['early_stopping']['stopped_early']}\")\n",
    "    print(f\"  Best epoch: {history['early_stopping']['best_epoch']}\")\n",
    "    print(f\"  Best {history['early_stopping']['monitor']}: {history['early_stopping']['best_metric']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-14"
   },
   "source": [
    "## 14. Plot Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "plot-curves"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Loss curve\n",
    "axes[0].plot(history['train_loss'], label='Train Loss', linewidth=2)\n",
    "axes[0].plot(history['val_loss'], label='Val Loss', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Loss', fontsize=12)\n",
    "axes[0].set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy curve with stream-specific curves\n",
    "# Note: train_accuracy and val_accuracy are the full model (integrated stream) accuracies\n",
    "axes[1].plot([acc*100 for acc in history['train_accuracy']], label='Full Model Train', linewidth=2, color='green')\n",
    "axes[1].plot([acc*100 for acc in history['val_accuracy']], label='Full Model Val', linewidth=2, color='darkgreen')\n",
    "\n",
    "# Add stream-specific curves if available (stream1 and stream2 only)\n",
    "if len(history.get('stream1_train_acc', [])) > 0:\n",
    "    axes[1].plot([acc*100 for acc in history['stream1_train_acc']], \n",
    "                label='Stream1 (RGB) Train', linewidth=1, alpha=0.6, linestyle='--', color='skyblue')\n",
    "    axes[1].plot([acc*100 for acc in history['stream1_val_acc']], \n",
    "                label='Stream1 (RGB) Val', linewidth=1, alpha=0.6, linestyle='--', color='blue')\n",
    "    axes[1].plot([acc*100 for acc in history['stream2_train_acc']], \n",
    "                label='Stream2 (Depth) Train', linewidth=1, alpha=0.6, linestyle='--', color='lightcoral')\n",
    "    axes[1].plot([acc*100 for acc in history['stream2_val_acc']], \n",
    "                label='Stream2 (Depth) Val', linewidth=1, alpha=0.6, linestyle='--', color='red')\n",
    "\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Accuracy (%)', fontsize=12)\n",
    "axes[1].set_title('Training and Validation Accuracy\\n(Full Model = Integrated Stream)', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=9, loc='lower right')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Learning rate curve with stream-specific LRs\n",
    "if len(history['learning_rates']) > 0:\n",
    "    # Sample learning rates (they're recorded per step, not per epoch)\n",
    "    sampled_lrs = history['learning_rates'][::max(1, len(history['learning_rates'])//100)]\n",
    "    axes[2].plot(sampled_lrs, linewidth=2, color='green', label='Base LR')\n",
    "    \n",
    "    # Add stream-specific LRs if available (recorded per epoch)\n",
    "    if len(history.get('stream1_lr', [])) > 0:\n",
    "        axes[2].plot(history['stream1_lr'], linewidth=1, alpha=0.7, linestyle='--', \n",
    "                    color='blue', label='Stream1 (RGB) LR')\n",
    "        axes[2].plot(history['stream2_lr'], linewidth=1, alpha=0.7, linestyle='--', \n",
    "                    color='red', label='Stream2 (Depth) LR')\n",
    "    \n",
    "    axes[2].set_xlabel('Epoch' if len(history.get('stream1_lr', [])) > 0 else 'Training Step (sampled)', fontsize=12)\n",
    "    axes[2].set_ylabel('Learning Rate', fontsize=12)\n",
    "    axes[2].set_title('Learning Rate Schedule', fontsize=14, fontweight='bold')\n",
    "    axes[2].legend(fontsize=9, loc='upper right')\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    # axes[2].set_yscale('log')  # Removed: Linear scale shows scheduler shape better\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{checkpoint_dir}/training_curves.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úÖ Training curves saved to: {checkpoint_dir}/training_curves.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-15"
   },
   "source": [
    "## 15. Pathway Analysis (Stream1/Stream2/Integrated Contributions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pathway-analysis"
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"PATHWAY ANALYSIS (3-STREAM LINET)\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nAnalyzing Stream1, Stream2, and Integrated pathway contributions...\")\n",
    "print(\"This may take a few minutes...\\n\")\n",
    "\n",
    "# Analyze pathways (LINet has 3 streams: stream1, stream2, integrated)\n",
    "pathway_analysis = model.analyze_pathways(\n",
    "    data_loader=val_loader\n",
    ")\n",
    "\n",
    "print(\"\\nAccuracy Metrics:\")\n",
    "print(f\"  Full model (Integrated): {pathway_analysis['accuracy']['full_model']*100:.2f}%\")\n",
    "print(f\"  Stream1 only (RGB): {pathway_analysis['accuracy']['stream1_only']*100:.2f}%\")\n",
    "print(f\"  Stream2 only (Depth): {pathway_analysis['accuracy']['stream2_only']*100:.2f}%\")\n",
    "print(f\"  Integrated only: {pathway_analysis['accuracy']['integrated_only']*100:.2f}%\")\n",
    "print(f\"\\n  Stream1 contribution: {pathway_analysis['accuracy']['stream1_contribution']*100:.2f}%\")\n",
    "print(f\"  Stream2 contribution: {pathway_analysis['accuracy']['stream2_contribution']*100:.2f}%\")\n",
    "print(f\"  Integrated contribution: {pathway_analysis['accuracy']['integrated_contribution']*100:.2f}%\")\n",
    "\n",
    "print(\"\\nLoss Metrics:\")\n",
    "print(f\"  Full model: {pathway_analysis['loss']['full_model']:.4f}\")\n",
    "print(f\"  Stream1 only: {pathway_analysis['loss']['stream1_only']:.4f}\")\n",
    "print(f\"  Stream2 only: {pathway_analysis['loss']['stream2_only']:.4f}\")\n",
    "print(f\"  Integrated only: {pathway_analysis['loss']['integrated_only']:.4f}\")\n",
    "\n",
    "print(\"\\nFeature Norm Statistics:\")\n",
    "print(f\"  Stream1 mean: {pathway_analysis['feature_norms']['stream1_mean']:.4f}\")\n",
    "print(f\"  Stream1 std: {pathway_analysis['feature_norms']['stream1_std']:.4f}\")\n",
    "print(f\"  Stream2 mean: {pathway_analysis['feature_norms']['stream2_mean']:.4f}\")\n",
    "print(f\"  Stream2 std: {pathway_analysis['feature_norms']['stream2_std']:.4f}\")\n",
    "print(f\"  Integrated mean: {pathway_analysis['feature_norms']['integrated_mean']:.4f}\")\n",
    "print(f\"  Integrated std: {pathway_analysis['feature_norms']['integrated_std']:.4f}\")\n",
    "print(f\"  Stream1/Stream2 ratio: {pathway_analysis['feature_norms']['stream1_to_stream2_ratio']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"INTEGRATION WEIGHT ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nAnalyzing integration weight magnitudes...\")\n",
    "print(\"(Measures how much the architecture favors each stream)\\n\")\n",
    "\n",
    "# Calculate stream contributions to integration - NEW SIMPLIFIED METHOD\n",
    "integration_contributions = model.calculate_stream_contributions_to_integration()\n",
    "\n",
    "print(\"Integration Weight Contributions:\")\n",
    "print(f\"  Stream1 (RGB): {integration_contributions['interpretation']['stream1_percentage']}\")\n",
    "print(f\"  Stream2 (Depth): {integration_contributions['interpretation']['stream2_percentage']}\")\n",
    "\n",
    "print(\"\\nRaw Integration Weight Norms:\")\n",
    "print(f\"  Stream1 integration weights: {integration_contributions['raw_norms']['stream1_integration_weights']:.4f}\")\n",
    "print(f\"  Stream2 integration weights: {integration_contributions['raw_norms']['stream2_integration_weights']:.4f}\")\n",
    "print(f\"  Total: {integration_contributions['raw_norms']['total']:.4f}\")\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "s1_contrib = integration_contributions['stream1_contribution']\n",
    "s2_contrib = integration_contributions['stream2_contribution']\n",
    "if s1_contrib > 0.55:\n",
    "    print(\"  ‚Üí Architecture favors Stream1 (RGB) - larger integration weights\")\n",
    "elif s2_contrib > 0.55:\n",
    "    print(\"  ‚Üí Architecture favors Stream2 (Depth) - larger integration weights\")\n",
    "else:\n",
    "    print(\"  ‚Üí Architecture uses both streams fairly equally\")\n",
    "\n",
    "print(f\"\\nNote: {integration_contributions['note']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ Pathway analysis complete!\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Visualize pathway contributions\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Accuracy comparison (3 pathways for LINet)\n",
    "pathways = ['Full Model\\n(Integrated)', 'Stream1\\nOnly', 'Stream2\\nOnly']\n",
    "accuracies = [\n",
    "    pathway_analysis['accuracy']['full_model'] * 100,\n",
    "    pathway_analysis['accuracy']['stream1_only'] * 100,\n",
    "    pathway_analysis['accuracy']['stream2_only'] * 100\n",
    "]\n",
    "colors = ['green', 'blue', 'orange']\n",
    "\n",
    "axes[0].bar(pathways, accuracies, color=colors, alpha=0.7)\n",
    "axes[0].set_ylabel('Accuracy (%)', fontsize=12)\n",
    "axes[0].set_title('Pathway Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "for i, v in enumerate(accuracies):\n",
    "    axes[0].text(i, v + 1, f'{v:.1f}%', ha='center', fontweight='bold')\n",
    "\n",
    "# Feature norm comparison (3 streams)\n",
    "norms = ['Stream1\\nFeatures', 'Stream2\\nFeatures', 'Integrated\\nFeatures']\n",
    "norm_values = [\n",
    "    pathway_analysis['feature_norms']['stream1_mean'],\n",
    "    pathway_analysis['feature_norms']['stream2_mean'],\n",
    "    pathway_analysis['feature_norms']['integrated_mean']\n",
    "]\n",
    "axes[1].bar(norms, norm_values, color=['blue', 'orange', 'purple'], alpha=0.7)\n",
    "axes[1].set_ylabel('Feature Norm (Mean)', fontsize=12)\n",
    "axes[1].set_title('Runtime Feature Magnitude', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "for i, v in enumerate(norm_values):\n",
    "    axes[1].text(i, v + 0.1, f'{v:.2f}', ha='center', fontweight='bold')\n",
    "\n",
    "# Integration weight contributions (2 input streams only - NOT integrated!)\n",
    "streams = ['Stream1\\n(RGB)', 'Stream2\\n(Depth)']\n",
    "integration_values = [\n",
    "    integration_contributions['stream1_contribution'] * 100,\n",
    "    integration_contributions['stream2_contribution'] * 100\n",
    "]\n",
    "axes[2].bar(streams, integration_values, color=['blue', 'orange'], alpha=0.7)\n",
    "axes[2].set_ylabel('Contribution (%)', fontsize=12)\n",
    "axes[2].set_title('Integration Weight Contribution', fontsize=14, fontweight='bold')\n",
    "axes[2].grid(True, alpha=0.3, axis='y')\n",
    "for i, v in enumerate(integration_values):\n",
    "    axes[2].text(i, v + 1, f'{v:.1f}%', ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{checkpoint_dir}/pathway_analysis.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úÖ Pathway analysis plot saved to: {checkpoint_dir}/pathway_analysis.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-16"
   },
   "source": [
    "## 16. Save Results & Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save-results"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SAVING RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Save training history as JSON\n",
    "history_path = f\"{checkpoint_dir}/training_history.json\"\n",
    "with open(history_path, 'w') as f:\n",
    "    json_history = {\n",
    "        'train_loss': [float(x) for x in history['train_loss']],\n",
    "        'val_loss': [float(x) for x in history['val_loss']],\n",
    "        'train_accuracy': [float(x) for x in history['train_accuracy']],\n",
    "        'val_accuracy': [float(x) for x in history['val_accuracy']],\n",
    "        'learning_rates': [float(x) for x in history['learning_rates']],\n",
    "        'model_config': MODEL_CONFIG,\n",
    "        'dataset_config': DATASET_CONFIG,\n",
    "        'stream_specific_config': STREAM_SPECIFIC_CONFIG,\n",
    "        'scheduler_config': SCHEDULER_CONFIG,\n",
    "        'training_config': TRAIN_CONFIG,\n",
    "        'final_results': {\n",
    "            'val_loss': float(results['loss']),\n",
    "            'val_accuracy': float(results['accuracy'])\n",
    "        },\n",
    "        'pathway_analysis': {\n",
    "            'full_model_accuracy': float(pathway_analysis['accuracy']['full_model']),\n",
    "            'stream1_only_accuracy': float(pathway_analysis['accuracy']['stream1_only']),\n",
    "            'stream2_only_accuracy': float(pathway_analysis['accuracy']['stream2_only']),\n",
    "            'integrated_only_accuracy': float(pathway_analysis['accuracy']['integrated_only'])\n",
    "        }\n",
    "    }\n",
    "    if 'early_stopping' in history:\n",
    "        json_history['early_stopping'] = history['early_stopping']\n",
    "    \n",
    "    json.dump(json_history, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Training history saved: {history_path}\")\n",
    "\n",
    "# Save final model (in addition to best model)\n",
    "final_model_path = f\"{checkpoint_dir}/final_model.pt\"\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': model.optimizer.state_dict(),\n",
    "    'scheduler_state_dict': model.scheduler.state_dict() if model.scheduler else None,\n",
    "    'config': MODEL_CONFIG,\n",
    "    'stream_specific_config': STREAM_SPECIFIC_CONFIG,\n",
    "    'scheduler_config': SCHEDULER_CONFIG,\n",
    "    'history': history,\n",
    "    'val_accuracy': results['accuracy']\n",
    "}, final_model_path)\n",
    "\n",
    "print(f\"‚úÖ Final model saved: {final_model_path}\")\n",
    "\n",
    "# Save summary report\n",
    "summary_path = f\"{checkpoint_dir}/summary.txt\"\n",
    "with open(summary_path, 'w') as f:\n",
    "    f.write(\"=\" * 60 + \"\\n\")\n",
    "    f.write(\"LINet Training Summary - SUN RGB-D\\n\")\n",
    "    f.write(\"=\" * 60 + \"\\n\\n\")\n",
    "    \n",
    "    # Model Configuration\n",
    "    f.write(\"Model Configuration:\\n\")\n",
    "    f.write(f\"  Architecture: LINet-{MODEL_CONFIG['architecture'].upper()} (3-stream Linear Integration)\\n\")\n",
    "    f.write(f\"  Num Classes: {MODEL_CONFIG['num_classes']}\\n\")\n",
    "    f.write(f\"  Stream1 Channels: {MODEL_CONFIG['stream1_channels']} (RGB)\\n\")\n",
    "    f.write(f\"  Stream2 Channels: {MODEL_CONFIG['stream2_channels']} (Depth)\\n\")\n",
    "    f.write(f\"  Dropout: {MODEL_CONFIG['dropout_p']}\\n\")\n",
    "    f.write(f\"  Device: {MODEL_CONFIG['device']}\\n\")\n",
    "    f.write(f\"  AMP Enabled: {MODEL_CONFIG['use_amp']}\\n\")\n",
    "    f.write(f\"  Total Parameters: {total_params:,}\\n\")\n",
    "    f.write(f\"  Trainable Parameters: {trainable_params:,}\\n\")\n",
    "    f.write(f\"  Integration Parameters: {integration_params:,}\\n\")\n",
    "    \n",
    "    # Dataset Configuration\n",
    "    f.write(f\"\\nDataset Configuration:\\n\")\n",
    "    f.write(f\"  Dataset: SUN RGB-D 15-category (Scene Classification)\\n\")\n",
    "    f.write(f\"  Training Samples: {len(train_loader.dataset)}\\n\")\n",
    "    f.write(f\"  Validation Samples: {len(val_loader.dataset)}\\n\")\n",
    "    f.write(f\"  Batch Size: {DATASET_CONFIG['batch_size']}\\n\")\n",
    "    f.write(f\"  Num Workers: {DATASET_CONFIG['num_workers']}\\n\")\n",
    "    f.write(f\"  Input Size: {DATASET_CONFIG['target_size']}\\n\")\n",
    "    \n",
    "    # Optimization Configuration\n",
    "    f.write(f\"\\nOptimization Configuration (Keras-Style API):\\n\")\n",
    "    f.write(f\"  Optimizer: AdamW (stream-specific)\\n\")\n",
    "    f.write(f\"  Loss Function: cross_entropy\\n\")\n",
    "    f.write(f\"  Label Smoothing: 0.1\\n\")\n",
    "    f.write(f\"  Scheduler: {SCHEDULER_CONFIG['scheduler_type']}\\n\")\n",
    "    f.write(f\"  Scheduler t_max: {SCHEDULER_CONFIG['t_max']}\\n\")\n",
    "    f.write(f\"  Scheduler eta_min: {SCHEDULER_CONFIG['eta_min']}\\n\")\n",
    "    f.write(f\"  Gradient Clipping: {TRAIN_CONFIG['grad_clip_norm']}\\n\")\n",
    "    \n",
    "    # Stream-Specific Settings\n",
    "    f.write(f\"\\nStream-Specific Settings:\\n\")\n",
    "    f.write(f\"  Stream1 (RGB):\\n\")\n",
    "    f.write(f\"    Learning Rate: {STREAM_SPECIFIC_CONFIG['stream1_lr']}\\n\")\n",
    "    f.write(f\"    Weight Decay: {STREAM_SPECIFIC_CONFIG['stream1_weight_decay']}\\n\")\n",
    "    f.write(f\"  Stream2 (Depth):\\n\")\n",
    "    f.write(f\"    Learning Rate: {STREAM_SPECIFIC_CONFIG['stream2_lr']}\\n\")\n",
    "    f.write(f\"    Weight Decay: {STREAM_SPECIFIC_CONFIG['stream2_weight_decay']}\\n\")\n",
    "    f.write(f\"  Shared (Fusion/Classifier):\\n\")\n",
    "    f.write(f\"    Learning Rate: {STREAM_SPECIFIC_CONFIG['shared_lr']}\\n\")\n",
    "    f.write(f\"    Weight Decay: {STREAM_SPECIFIC_CONFIG['shared_weight_decay']}\\n\")\n",
    "    \n",
    "    # Training Configuration\n",
    "    f.write(f\"\\nTraining Configuration:\\n\")\n",
    "    f.write(f\"  Total Epochs: {len(history['train_loss'])}\\n\")\n",
    "    f.write(f\"  Stream Monitoring: {TRAIN_CONFIG['stream_monitoring']}\\n\")\n",
    "    f.write(f\"  Early Stopping: {TRAIN_CONFIG['early_stopping']}\\n\")\n",
    "    if TRAIN_CONFIG['early_stopping']:\n",
    "        f.write(f\"    Monitor: {TRAIN_CONFIG['monitor']}\\n\")\n",
    "        f.write(f\"    Patience: {TRAIN_CONFIG['patience']}\\n\")\n",
    "        f.write(f\"    Min Delta: {TRAIN_CONFIG['min_delta']}\\n\")\n",
    "        f.write(f\"    Restore Best Weights: {TRAIN_CONFIG['restore_best_weights']}\\n\")\n",
    "    \n",
    "    # Results\n",
    "    f.write(f\"\\nFinal Results:\\n\")\n",
    "    f.write(f\"  Val Loss: {results['loss']:.4f}\\n\")\n",
    "    f.write(f\"  Val Accuracy: {results['accuracy']*100:.2f}%\\n\")\n",
    "    f.write(f\"  Best Val Accuracy: {max(history['val_accuracy'])*100:.2f}%\\n\")\n",
    "    f.write(f\"  Initial Train Loss: {history['train_loss'][0]:.4f}\\n\")\n",
    "    f.write(f\"  Final Train Loss: {history['train_loss'][-1]:.4f}\\n\")\n",
    "    f.write(f\"  Best Val Loss: {min(history['val_loss']):.4f}\\n\")\n",
    "    \n",
    "    # Pathway Analysis (3 streams for LINet)\n",
    "    f.write(f\"\\nPathway Analysis (3-stream LINet):\\n\")\n",
    "    f.write(f\"  Full Model (Integrated): {pathway_analysis['accuracy']['full_model']*100:.2f}%\\n\")\n",
    "    f.write(f\"  Stream1 Only (RGB): {pathway_analysis['accuracy']['stream1_only']*100:.2f}%\\n\")\n",
    "    f.write(f\"  Stream2 Only (Depth): {pathway_analysis['accuracy']['stream2_only']*100:.2f}%\\n\")\n",
    "    f.write(f\"  Integrated Only: {pathway_analysis['accuracy']['integrated_only']*100:.2f}%\\n\")\n",
    "    f.write(f\"  Stream1 Contribution: {pathway_analysis['accuracy']['stream1_contribution']*100:.2f}%\\n\")\n",
    "    f.write(f\"  Stream2 Contribution: {pathway_analysis['accuracy']['stream2_contribution']*100:.2f}%\\n\")\n",
    "    f.write(f\"  Integrated Contribution: {pathway_analysis['accuracy']['integrated_contribution']*100:.2f}%\\n\")\n",
    "\n",
    "print(f\"‚úÖ Summary report saved: {summary_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"All results saved to: {checkpoint_dir}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# List saved files\n",
    "print(\"\\nSaved files:\")\n",
    "!ls -lh {checkpoint_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-17"
   },
   "source": [
    "## 17. Summary & Next Steps\n",
    "\n",
    "### üéâ Training Complete!\n",
    "\n",
    "**What we accomplished:**\n",
    "- ‚úÖ Trained LINet (3-stream Linear Integration ResNet) on SUN RGB-D dataset (15 categories)\n",
    "- ‚úÖ Used **Keras-style API** with explicit optimizer and scheduler creation\n",
    "- ‚úÖ Used A100 GPU with AMP (2x speedup)\n",
    "- ‚úÖ Saved all checkpoints to Google Drive\n",
    "- ‚úÖ Analyzed Stream1, Stream2, and Integrated pathway contributions\n",
    "- ‚úÖ Generated training curves and visualizations\n",
    "- ‚úÖ Comprehensive stream monitoring with overfitting detection\n",
    "\n",
    "**Results are saved to:** Check the output above for the checkpoint directory path\n",
    "\n",
    "### üìä Expected Performance:\n",
    "\n",
    "For **SUN RGB-D Scene Classification (15 categories, 10,335 images)**:\n",
    "- **Good:** 65-75% validation accuracy\n",
    "- **Very Good:** 75-80% validation accuracy\n",
    "- **Excellent:** 80-85% validation accuracy\n",
    "\n",
    "**Much better than NYU Depth V2 due to:**\n",
    "- 6.9x more training samples (8,041 vs 1,159)\n",
    "- 22.6x better class balance (8.5x vs 192x)\n",
    "- Higher quality, more diverse dataset\n",
    "\n",
    "### üîÑ New Keras-Style API Used:\n",
    "\n",
    "This notebook uses the **refactored Keras-style API**:\n",
    "\n",
    "```python\n",
    "# 1. Create optimizer with stream-specific LRs\n",
    "optimizer = create_stream_optimizer(\n",
    "    model, stream1_lr=3e-5, stream2_lr=1e-4, shared_lr=7e-5, ...\n",
    ")\n",
    "\n",
    "# 2. Create scheduler\n",
    "scheduler = setup_scheduler(optimizer, 'cosine', epochs=80, ...)\n",
    "\n",
    "# 3. Compile with objects (not strings!)\n",
    "model.compile(optimizer=optimizer, scheduler=scheduler, loss='cross_entropy')\n",
    "\n",
    "# 4. Train (no scheduler_kwargs needed!)\n",
    "model.fit(train_loader, val_loader, epochs=80, stream_monitoring=True)\n",
    "```\n",
    "\n",
    "**Benefits:**\n",
    "- ‚úÖ Explicit optimizer/scheduler creation\n",
    "- ‚úÖ Clear separation: compile() = config, fit() = execution\n",
    "- ‚úÖ Easy to customize and experiment\n",
    "- ‚úÖ Stream-specific learning rates still fully supported\n",
    "\n",
    "### üß† LINet Architecture Highlights:\n",
    "\n",
    "**3-Stream Linear Integration:**\n",
    "- Stream1 processes RGB\n",
    "- Stream2 processes Depth\n",
    "- Integrated stream combines both through learned linear weights at every layer\n",
    "\n",
    "**5 Weight Matrices per LIConv2d:**\n",
    "- `stream1_weight` (full kernel for RGB)\n",
    "- `stream2_weight` (full kernel for Depth)\n",
    "- `integrated_weight` (1√ó1 channel-wise)\n",
    "- `integration_from_stream1` (1√ó1)\n",
    "- `integration_from_stream2` (1√ó1)\n",
    "\n",
    "This allows **neuron-level integration** rather than late fusion!\n",
    "\n",
    "### üîç Next Steps:\n",
    "\n",
    "1. **Review Results:**\n",
    "   - Check training curves above\n",
    "   - Review pathway analysis (3 streams)\n",
    "   - Compare Stream1/Stream2/Integrated contributions\n",
    "   - Analyze stream monitoring plots\n",
    "\n",
    "2. **Download Results:**\n",
    "   - All files are saved to your Google Drive\n",
    "   - Download checkpoints for local inference\n",
    "\n",
    "3. **Experiment:**\n",
    "   - Try ResNet50 for better accuracy (change `architecture` in Model Config)\n",
    "   - Adjust stream-specific learning rates if monitoring shows imbalance\n",
    "   - Train longer if early stopping triggered\n",
    "   - Analyze integration weights to understand learned strategies\n",
    "\n",
    "4. **Deploy:**\n",
    "   - Use the best model for inference\n",
    "   - Test on new RGB-D images\n",
    "   - Integrate into your application\n",
    "\n",
    "---\n",
    "\n",
    "**Questions or issues?** Check the training summary and pathway analysis above!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "final-summary"
   },
   "outputs": [],
   "source": [
    "# Print final summary\n",
    "print(\"=\" * 60)\n",
    "print(\"FINAL SUMMARY - LINET\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n‚úÖ Training Complete!\")\n",
    "print(f\"\\nFinal Validation Accuracy: {results['accuracy']*100:.2f}%\")\n",
    "print(f\"Best Validation Accuracy: {max(history['val_accuracy'])*100:.2f}%\")\n",
    "print(f\"\\nStream1 Pathway (RGB): {pathway_analysis['accuracy']['stream1_only']*100:.2f}%\")\n",
    "print(f\"Stream2 Pathway (Depth): {pathway_analysis['accuracy']['stream2_only']*100:.2f}%\")\n",
    "print(f\"Integrated Pathway: {pathway_analysis['accuracy']['integrated_only']*100:.2f}%\")\n",
    "print(f\"Combined (Full Model): {pathway_analysis['accuracy']['full_model']*100:.2f}%\")\n",
    "print(f\"\\nTotal Training Epochs: {len(history['train_loss'])}\")\n",
    "print(f\"Total Parameters: {total_params:,}\")\n",
    "print(f\"Integration Parameters: {integration_params:,}\")\n",
    "print(f\"\\nCheckpoints saved to: {checkpoint_dir}\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üéâ All done! Check Google Drive for saved models and results.\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}