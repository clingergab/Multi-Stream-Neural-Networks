{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# MCResNet Training on SUN RGB-D - Google Colab\n",
    "\n",
    "**Complete end-to-end training pipeline for Google Colab with A100 GPU**\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Checklist Before Running:\n",
    "\n",
    "- [ ] **Enable A100 GPU:** Runtime ‚Üí Change runtime type ‚Üí Hardware accelerator: GPU ‚Üí GPU type: A100\n",
    "- [ ] **Mount Google Drive:** Your code and dataset will be stored on Drive\n",
    "- [ ] **Upload dataset to Drive:** `MyDrive/datasets/sunrgbd_15/` (preprocessed 15-category dataset)\n",
    "- [ ] **Expected Runtime:** ~2-3 hours for training\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ What This Notebook Does:\n",
    "\n",
    "1. ‚úÖ Verify A100 GPU is available\n",
    "2. ‚úÖ Mount Google Drive\n",
    "3. ‚úÖ Clone your repository to local disk (fast I/O)\n",
    "4. ‚úÖ Copy SUN RGB-D dataset to local disk (10-20x faster than Drive)\n",
    "5. ‚úÖ Install dependencies\n",
    "6. ‚úÖ Train MCResNet with all optimizations\n",
    "7. ‚úÖ Save checkpoints to Drive (persistent storage)\n",
    "8. ‚úÖ Generate training curves and analysis\n",
    "\n",
    "---\n",
    "\n",
    "**Let's get started!** üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-1"
   },
   "source": [
    "## 1. Environment Setup & GPU Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check-gpu"
   },
   "outputs": [],
   "source": [
    "# Check GPU availability and specs\n",
    "import torch\n",
    "import subprocess\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"GPU VERIFICATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check PyTorch and CUDA\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "    \n",
    "    # Check if it's A100\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    if 'A100' in gpu_name:\n",
    "        print(\"\\n‚úÖ A100 GPU detected - PERFECT for training!\")\n",
    "    elif 'V100' in gpu_name:\n",
    "        print(\"\\n‚úÖ V100 GPU detected - Good for training (slower than A100)\")\n",
    "    elif 'T4' in gpu_name:\n",
    "        print(\"\\n‚ö†Ô∏è  T4 GPU detected - Will be slower, consider upgrading to A100\")\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è  GPU: {gpu_name} - Consider using A100 for best performance\")\n",
    "else:\n",
    "    print(\"\\n‚ùå NO GPU DETECTED!\")\n",
    "    print(\"Please enable GPU: Runtime ‚Üí Change runtime type ‚Üí Hardware accelerator: GPU\")\n",
    "    raise RuntimeError(\"GPU is required for training\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nvidia-smi"
   },
   "outputs": [],
   "source": [
    "# Detailed GPU info\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-2"
   },
   "source": [
    "## 2. Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mount-drive"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "print(\"\\n‚úÖ Google Drive mounted successfully!\")\n",
    "print(f\"\\nDrive contents:\")\n",
    "!ls -la /content/drive/MyDrive/ | head -20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-3"
   },
   "source": [
    "## 3. Clone Repository to Local Disk (Fast I/O)\n",
    "\n",
    "**Important:** We clone to `/content/` (local SSD) instead of Drive for 10-20x faster I/O\n",
    "\n",
    "**Default:** Clone from GitHub (recommended - always gets latest code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clone-repo"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Configuration\n",
    "PROJECT_NAME = \"Multi-Stream-Neural-Networks\"\n",
    "GITHUB_REPO = \"https://github.com/clingergab/Multi-Stream-Neural-Networks.git\"  # UPDATE THIS\n",
    "LOCAL_REPO_PATH = f\"/content/{PROJECT_NAME}\"  # Local copy for fast I/O\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"REPOSITORY SETUP\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Ensure we're in a valid directory\n",
    "os.chdir('/content')\n",
    "print(f\"Starting in: {os.getcwd()}\")\n",
    "\n",
    "# Check if repo already exists (same session, rerunning cell)\n",
    "if Path(LOCAL_REPO_PATH).exists() and Path(f\"{LOCAL_REPO_PATH}/.git\").exists():\n",
    "    print(f\"\\nüìÅ Repo already exists: {LOCAL_REPO_PATH}\")\n",
    "    print(f\"üîÑ Pulling latest changes...\")\n",
    "    \n",
    "    os.chdir(LOCAL_REPO_PATH)\n",
    "    !git pull\n",
    "    print(\"‚úÖ Repo updated\")\n",
    "\n",
    "# Clone from GitHub (first run)\n",
    "else:\n",
    "    # Remove old incomplete copy if exists\n",
    "    if Path(LOCAL_REPO_PATH).exists():\n",
    "        print(f\"\\nüóëÔ∏è  Removing incomplete repo copy...\")\n",
    "        !rm -rf {LOCAL_REPO_PATH}\n",
    "    \n",
    "    print(f\"\\nüîÑ Cloning from GitHub...\")\n",
    "    print(f\"   Repo: {GITHUB_REPO}\")\n",
    "    print(f\"   Destination: {LOCAL_REPO_PATH}\")\n",
    "    \n",
    "    !git clone {GITHUB_REPO} {LOCAL_REPO_PATH}\n",
    "    \n",
    "    # Verify clone succeeded\n",
    "    if not Path(LOCAL_REPO_PATH).exists():\n",
    "        raise RuntimeError(f\"Failed to clone repository to {LOCAL_REPO_PATH}\")\n",
    "    \n",
    "    print(\"‚úÖ Repo cloned successfully\")\n",
    "    os.chdir(LOCAL_REPO_PATH)\n",
    "\n",
    "# Verify repo structure\n",
    "print(f\"\\nüìÇ Repository structure:\")\n",
    "!ls -la {LOCAL_REPO_PATH}\n",
    "\n",
    "print(f\"\\n‚úÖ Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-4"
   },
   "source": [
    "## 4. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install-deps"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "print(\"Installing dependencies...\")\n",
    "\n",
    "!pip install -q h5py tqdm matplotlib seaborn\n",
    "\n",
    "# Verify installations\n",
    "import h5py\n",
    "import tqdm\n",
    "import matplotlib\n",
    "import seaborn\n",
    "\n",
    "print(\"‚úÖ All dependencies installed!\")\n",
    "print(f\"   h5py: {h5py.__version__}\")\n",
    "print(f\"   matplotlib: {matplotlib.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-5"
   },
   "source": [
    "## 5. Copy SUN RGB-D Dataset to Local Disk\n",
    "\n",
    "**Performance Note:** Local disk I/O is ~10-20x faster than Drive!\n",
    "\n",
    "**Dataset:** SUN RGB-D 15-category preprocessed dataset (~3.5 GB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download-dataset"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Paths\n",
    "DRIVE_DATASET_TAR = \"/content/drive/MyDrive/datasets/sunrgbd_15.tar.gz\"  # Compressed file\n",
    "LOCAL_DATASET_PATH = \"/content/data/sunrgbd_15\"  # Extracted location\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SUN RGB-D 15-CATEGORY DATASET SETUP\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check if already on local disk\n",
    "if Path(LOCAL_DATASET_PATH).exists():\n",
    "    print(f\"‚úÖ Dataset already on local disk: {LOCAL_DATASET_PATH}\")\n",
    "    \n",
    "    # Verify structure\n",
    "    train_rgb_count = len(list(Path(f\"{LOCAL_DATASET_PATH}/train/rgb\").glob(\"*.png\")))\n",
    "    val_rgb_count = len(list(Path(f\"{LOCAL_DATASET_PATH}/val/rgb\").glob(\"*.png\")))\n",
    "    print(f\"   Train samples: {train_rgb_count}\")\n",
    "    print(f\"   Val samples: {val_rgb_count}\")\n",
    "\n",
    "# Copy and extract from Drive\n",
    "elif Path(DRIVE_DATASET_TAR).exists():\n",
    "    print(f\"üìÅ Found compressed dataset on Drive: {DRIVE_DATASET_TAR}\")\n",
    "    print(f\"üì• Copying 4.2GB compressed file to local disk...\")\n",
    "    print(f\"   ‚è±Ô∏è  This takes ~3-5 minutes (much faster than 20k individual files!)\")\n",
    "    \n",
    "    # Create parent directory\n",
    "    !mkdir -p /content/data\n",
    "    \n",
    "    # Copy compressed file with progress\n",
    "    print(f\"\\nCopying compressed archive...\")\n",
    "    !rsync -ah --info=progress2 {DRIVE_DATASET_TAR} /content/data/sunrgbd_15.tar.gz\n",
    "    \n",
    "    # Extract to local disk (suppress macOS metadata warnings)\n",
    "    print(f\"\\nüì¶ Extracting dataset to local disk...\")\n",
    "    !tar -xzf /content/data/sunrgbd_15.tar.gz -C /content/data/ 2>&1 | grep -v \"Ignoring unknown extended header\"\n",
    "    \n",
    "    # Remove tar file to save space\n",
    "    !rm /content/data/sunrgbd_15.tar.gz\n",
    "    \n",
    "    print(f\"\\n‚úÖ Dataset extracted to local disk\")\n",
    "    \n",
    "    # Verify extraction\n",
    "    train_rgb_count = len(list(Path(f\"{LOCAL_DATASET_PATH}/train/rgb\").glob(\"*.png\")))\n",
    "    val_rgb_count = len(list(Path(f\"{LOCAL_DATASET_PATH}/val/rgb\").glob(\"*.png\")))\n",
    "    print(f\"   Train samples: {train_rgb_count}\")\n",
    "    print(f\"   Val samples: {val_rgb_count}\")\n",
    "\n",
    "else:\n",
    "    print(f\"‚ùå Compressed dataset not found on Drive!\")\n",
    "    print(f\"   Expected location: {DRIVE_DATASET_TAR}\")\n",
    "    print(f\"\\nüìã To fix this:\")\n",
    "    print(f\"   1. Run: COPYFILE_DISABLE=1 tar -czf sunrgbd_15.tar.gz sunrgbd_15/\")\n",
    "    print(f\"   2. Upload sunrgbd_15.tar.gz to Google Drive\")\n",
    "    print(f\"   3. Place it at: {DRIVE_DATASET_TAR}\")\n",
    "    raise FileNotFoundError(f\"Compressed dataset not found at {DRIVE_DATASET_TAR}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"Dataset ready at: {LOCAL_DATASET_PATH}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-6"
   },
   "source": [
    "## 6. Setup Python Path & Import MCResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup-imports"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Remove cached modules\n",
    "modules_to_reload = [k for k in sys.modules.keys() if k.startswith('src.')]\n",
    "for module in modules_to_reload:\n",
    "    del sys.modules[module]\n",
    "    \n",
    "# Add project to Python path\n",
    "project_root = '/content/Multi-Stream-Neural-Networks'\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "# Verify project structure\n",
    "print(\"Project structure:\")\n",
    "!ls -la {project_root}/src/models/\n",
    "\n",
    "# Import MCResNet and SUN RGB-D dataloader\n",
    "print(\"\\nImporting MCResNet and dataloaders...\")\n",
    "from src.models.multi_channel.mc_resnet import mc_resnet18, mc_resnet50\n",
    "from src.data_utils.sunrgbd_dataset import get_sunrgbd_dataloaders\n",
    "\n",
    "print(\"‚úÖ MCResNet and dataloaders imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-7"
   },
   "source": [
    "## 7. Load SUN RGB-D Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify dataset structure\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DATASET STRUCTURE VERIFICATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "dataset_root = Path(LOCAL_DATASET_PATH)\n",
    "\n",
    "print(\"\\nDirectory structure:\")\n",
    "print(f\"  {dataset_root}/\")\n",
    "print(f\"    train/\")\n",
    "print(f\"      rgb/ - {len(list((dataset_root / 'train' / 'rgb').glob('*.png')))} images\")\n",
    "print(f\"      depth/ - {len(list((dataset_root / 'train' / 'depth').glob('*.png')))} images\")\n",
    "print(f\"      labels.txt\")\n",
    "print(f\"    val/\")\n",
    "print(f\"      rgb/ - {len(list((dataset_root / 'val' / 'rgb').glob('*.png')))} images\")\n",
    "print(f\"      depth/ - {len(list((dataset_root / 'val' / 'depth').glob('*.png')))} images\")\n",
    "print(f\"      labels.txt\")\n",
    "print(f\"    class_names.txt\")\n",
    "print(f\"    dataset_info.txt\")\n",
    "\n",
    "# Read class names\n",
    "with open(dataset_root / 'class_names.txt', 'r') as f:\n",
    "    class_names = [line.strip() for line in f]\n",
    "\n",
    "print(f\"\\nClasses ({len(class_names)}):\")\n",
    "for i, name in enumerate(class_names):\n",
    "    print(f\"  {i}: {name}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load-dataset"
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"LOADING SUN RGB-D 15-CATEGORY DATASET\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Dataset configuration\n",
    "DATASET_CONFIG = {\n",
    "    'data_root': LOCAL_DATASET_PATH,\n",
    "    'batch_size': 96,  # Good balance for A100\n",
    "    'num_workers': 4,\n",
    "    'target_size': (416, 544),  \n",
    "    'num_classes': 15   # SUN RGB-D merged to 15 categories (labels 0-14)\n",
    "}\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "for key, value in DATASET_CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(f\"\\nLoading dataset from: {DATASET_CONFIG['data_root']}\")\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader, val_loader = get_sunrgbd_dataloaders(\n",
    "    data_root=DATASET_CONFIG['data_root'],\n",
    "    batch_size=DATASET_CONFIG['batch_size'],\n",
    "    num_workers=DATASET_CONFIG['num_workers'],\n",
    "    target_size=DATASET_CONFIG['target_size']\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Dataset loaded successfully!\")\n",
    "print(f\"\\nDataset Statistics:\")\n",
    "print(f\"  Train batches: {len(train_loader)}\")\n",
    "print(f\"  Val batches: {len(val_loader)}\")\n",
    "print(f\"  Train samples: {len(train_loader.dataset)}\")\n",
    "print(f\"  Val samples: {len(val_loader.dataset)}\")\n",
    "print(f\"  Batch size: {DATASET_CONFIG['batch_size']}\")\n",
    "\n",
    "# Test loading a batch\n",
    "print(f\"\\nTesting batch loading...\")\n",
    "rgb_batch, depth_batch, label_batch = next(iter(train_loader))\n",
    "print(f\"  RGB shape: {rgb_batch.shape}\")\n",
    "print(f\"  Depth shape: {depth_batch.shape}\")\n",
    "print(f\"  Labels shape: {label_batch.shape}\")\n",
    "print(f\"  Labels min: {label_batch.min().item()}, max: {label_batch.max().item()}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-8"
   },
   "source": [
    "## 8. Visualize Sample Data\n",
    "\n",
    "Shows RGB images, depth maps, and scene labels from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "visualize-data"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Visualize some samples\n",
    "fig, axes = plt.subplots(2, 4, figsize=(14, 7))\n",
    "\n",
    "for i in range(4):\n",
    "    rgb = rgb_batch[i].cpu()\n",
    "    depth = depth_batch[i].cpu()\n",
    "    label = label_batch[i].item()\n",
    "    \n",
    "    # Denormalize for visualization\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "    rgb_vis = rgb * std + mean\n",
    "    rgb_vis = torch.clamp(rgb_vis, 0, 1)\n",
    "    \n",
    "    # Plot RGB\n",
    "    axes[0, i].imshow(rgb_vis.permute(1, 2, 0))\n",
    "    axes[0, i].set_title(f\"RGB - Class {label}\", fontsize=10)\n",
    "    axes[0, i].axis('off')\n",
    "    \n",
    "    # Plot Depth\n",
    "    axes[1, i].imshow(depth.squeeze(), cmap='viridis')\n",
    "    axes[1, i].set_title(f\"Depth - Class {label}\", fontsize=10)\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "plt.suptitle('SUN RGB-D Sample Data (RGB + Depth)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Sample visualization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-9"
   },
   "source": [
    "## 9. Create & Compile MCResNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create-model"
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"MODEL CREATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Model configuration\n",
    "MODEL_CONFIG = {\n",
    "    'architecture': 'resnet18',  # or 'resnet50' for better accuracy\n",
    "    'num_classes': 15,  # SUN RGB-D has 15 merged categories (labels 0-14)\n",
    "    'stream1_channels': 3,  # RGB\n",
    "    'stream2_channels': 1,  # Depth\n",
    "    'fusion_type': 'concat',  # 'concat', 'weighted', or 'gated'\n",
    "    'dropout_p': 0.5,  # Dropout for regularization\n",
    "    'device': 'cuda',\n",
    "    'use_amp': True  # Automatic Mixed Precision (2x faster on A100)\n",
    "}\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "for key, value in MODEL_CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Create model\n",
    "print(f\"\\nCreating MCResNet-{MODEL_CONFIG['architecture'].upper()}...\")\n",
    "\n",
    "if MODEL_CONFIG['architecture'] == 'resnet18':\n",
    "    model = mc_resnet18(\n",
    "        num_classes=MODEL_CONFIG['num_classes'],\n",
    "        stream1_input_channels=MODEL_CONFIG['stream1_channels'],\n",
    "        stream2_input_channels=MODEL_CONFIG['stream2_channels'],\n",
    "        fusion_type=MODEL_CONFIG['fusion_type'],\n",
    "        dropout_p=MODEL_CONFIG['dropout_p'],\n",
    "        device=MODEL_CONFIG['device'],\n",
    "        use_amp=MODEL_CONFIG['use_amp']\n",
    "    )\n",
    "elif MODEL_CONFIG['architecture'] == 'resnet50':\n",
    "    model = mc_resnet50(\n",
    "        num_classes=MODEL_CONFIG['num_classes'],\n",
    "        stream1_input_channels=MODEL_CONFIG['stream1_channels'],\n",
    "        stream2_input_channels=MODEL_CONFIG['stream2_channels'],\n",
    "        fusion_type=MODEL_CONFIG['fusion_type'],\n",
    "        dropout_p=MODEL_CONFIG['dropout_p'],\n",
    "        device=MODEL_CONFIG['device'],\n",
    "        use_amp=MODEL_CONFIG['use_amp']\n",
    "    )\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "fusion_params = sum(p.numel() for p in model.fusion.parameters())\n",
    "\n",
    "print(f\"\\n‚úÖ Model created successfully!\")\n",
    "print(f\"\\nModel Statistics:\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"  Fusion parameters: {fusion_params:,}\")\n",
    "print(f\"  Model size: {total_params * 4 / 1024**2:.2f} MB (FP32)\")\n",
    "print(f\"  Fusion strategy: {model.fusion_strategy}\")\n",
    "print(f\"  Device: {MODEL_CONFIG['device']}\")\n",
    "print(f\"  AMP enabled: {MODEL_CONFIG['use_amp']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "compile-model"
   },
   "source": [
    "## 9b. Model Compilation Options\n",
    "\n",
    "**Choose your optimization strategy below (cell-22)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model with stream-specific optimization\n",
    "print(\"=\" * 60)\n",
    "print(\"MODEL COMPILATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Stream-specific configuration for optimal RGB/Depth balance\n",
    "STREAM_SPECIFIC_CONFIG = {\n",
    "    'optimizer': 'adamw',\n",
    "    'learning_rate': 7e-5,           # Base LR for shared params (fusion, classifier)\n",
    "    'weight_decay': 2e-4,             # Base weight decay\n",
    "\n",
    "    # Stream-specific settings (adjusted based on research):\n",
    "    'stream1_lr': 3e-5,               # RGB stream: lower LR (more regularization)\n",
    "    'stream1_weight_decay': 5e-4,     # RGB stream: higher WD (prevent overfitting)\n",
    "    'stream2_lr': 1e-4,               # Depth stream: higher LR (needs more learning)\n",
    "    'stream2_weight_decay': 1e-4,     # Depth stream: lighter WD (less regularization)\n",
    "\n",
    "    'loss': 'cross_entropy',\n",
    "    'scheduler': 'cosine'\n",
    "}\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "for key, value in STREAM_SPECIFIC_CONFIG.items():\n",
    "    if value is not None:\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "# Compile\n",
    "model.compile(**STREAM_SPECIFIC_CONFIG)\n",
    "\n",
    "print(\"\\n‚úÖ Model compiled successfully!\")\n",
    "\n",
    "# Show parameter groups\n",
    "if hasattr(model.optimizer, 'param_groups'):\n",
    "    print(f\"\\nParameter groups created: {len(model.optimizer.param_groups)}\")\n",
    "    for i, group in enumerate(model.optimizer.param_groups):\n",
    "        num_params = sum(p.numel() for p in group['params'])\n",
    "        print(f\"  Group {i}: LR={group['lr']:.2e}, WD={group['weight_decay']:.2e}, Params={num_params:,}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-10"
   },
   "source": [
    "## 10. Test Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "test-forward"
   },
   "outputs": [],
   "source": [
    "# Test forward pass with detailed debugging\n",
    "print(\"Testing forward pass with CUDA_LAUNCH_BLOCKING for better error messages...\")\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'  # Synchronous CUDA for better error messages\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    rgb_test, depth_test, labels_test = next(iter(train_loader))\n",
    "    \n",
    "    print(f\"\\nInput validation:\")\n",
    "    print(f\"  RGB shape: {rgb_test.shape}, dtype: {rgb_test.dtype}\")\n",
    "    print(f\"  RGB min: {rgb_test.min():.4f}, max: {rgb_test.max():.4f}\")\n",
    "    print(f\"  RGB has NaN: {torch.isnan(rgb_test).any()}\")\n",
    "    print(f\"  RGB has Inf: {torch.isinf(rgb_test).any()}\")\n",
    "    \n",
    "    print(f\"\\n  Depth shape: {depth_test.shape}, dtype: {depth_test.dtype}\")\n",
    "    print(f\"  Depth min: {depth_test.min():.4f}, max: {depth_test.max():.4f}\")\n",
    "    print(f\"  Depth has NaN: {torch.isnan(depth_test).any()}\")\n",
    "    print(f\"  Depth has Inf: {torch.isinf(depth_test).any()}\")\n",
    "    \n",
    "    print(f\"\\n  Labels shape: {labels_test.shape}, dtype: {labels_test.dtype}\")\n",
    "    print(f\"  Labels min: {labels_test.min()}, max: {labels_test.max()}\")\n",
    "    print(f\"  Labels unique: {torch.unique(labels_test).tolist()}\")\n",
    "    \n",
    "    print(\"\\nRunning forward pass...\")\n",
    "    rgb_cuda = rgb_test.to('cuda')\n",
    "    depth_cuda = depth_test.to('cuda')\n",
    "    \n",
    "    try:\n",
    "        outputs = model(rgb_cuda, depth_cuda)\n",
    "        print(f\"  ‚úÖ Forward pass successful!\")\n",
    "        print(f\"  Output shape: {outputs.shape}\")\n",
    "        print(f\"  Output min: {outputs.min():.4f}, max: {outputs.max():.4f}\")\n",
    "        \n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        print(f\"\\nSample predictions: {predictions.cpu().numpy()[:10]}\")\n",
    "        print(f\"Ground truth: {labels_test.numpy()[:10]}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Forward pass failed!\")\n",
    "        print(f\"Error: {e}\")\n",
    "        print(f\"\\nThis is likely a model architecture issue, not a data issue.\")\n",
    "        print(f\"Possible causes:\")\n",
    "        print(f\"  1. BatchNorm running stats issue\")\n",
    "        print(f\"  2. Invalid tensor operations in model\")\n",
    "        print(f\"  3. Memory corruption\")\n",
    "        raise\n",
    "\n",
    "print(\"\\n‚úÖ Forward pass test complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-11"
   },
   "source": [
    "## 11. Setup Checkpoint Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"CHECKPOINT DIRECTORY SETUP\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create checkpoint directory on Google Drive (persistent storage)\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "checkpoint_dir = f\"/content/drive/MyDrive/mcresnet_checkpoints/run_{timestamp}\"\n",
    "\n",
    "# Create directory\n",
    "Path(checkpoint_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"\\n‚úÖ Checkpoint directory created:\")\n",
    "print(f\"   {checkpoint_dir}\")\n",
    "print(f\"\\nAll training artifacts will be saved here:\")\n",
    "print(f\"  ‚Ä¢ Best model weights\")\n",
    "print(f\"  ‚Ä¢ Training history\")\n",
    "print(f\"  ‚Ä¢ Monitoring metrics\")\n",
    "print(f\"  ‚Ä¢ Visualizations\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Train the Model üöÄ\n",
    "\n",
    "**Expected time:** ~2-3 hours for 90 epochs on A100\n",
    "\n",
    "**All optimizations enabled:**\n",
    "- ‚úÖ Automatic Mixed Precision (2x faster)\n",
    "- ‚úÖ Gradient Clipping (stability)\n",
    "- ‚úÖ Cosine Annealing LR\n",
    "- ‚úÖ Early Stopping\n",
    "- ‚úÖ Best Model Checkpointing\n",
    "- ‚úÖ Local disk I/O (10-20x faster than Drive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup-checkpoints"
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"TRAINING WITH STREAM MONITORING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Training configuration\n",
    "TRAIN_CONFIG = {\n",
    "    'epochs': 90,\n",
    "    'grad_clip_norm': 5.0,\n",
    "    'early_stopping': True,\n",
    "    'patience': 15,\n",
    "    'min_delta': 0.001,\n",
    "    'monitor': 'val_accuracy',\n",
    "    'restore_best_weights': True,\n",
    "    'save_path': f\"{checkpoint_dir}/best_model.pt\",\n",
    "    'stream_monitoring': True  # Built-in stream monitoring!\n",
    "}\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "for key, value in TRAIN_CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"Training will take approximately 2-3 hours on A100\")\n",
    "print(f\"Stream monitoring active - detailed per-stream metrics shown each epoch\")\n",
    "print(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "# Train using built-in fit() method with stream monitoring\n",
    "history = model.fit(\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    epochs=TRAIN_CONFIG['epochs'],\n",
    "    verbose=True,\n",
    "    save_path=TRAIN_CONFIG['save_path'],\n",
    "    early_stopping=TRAIN_CONFIG['early_stopping'],\n",
    "    patience=TRAIN_CONFIG['patience'],\n",
    "    min_delta=TRAIN_CONFIG['min_delta'],\n",
    "    monitor=TRAIN_CONFIG['monitor'],\n",
    "    restore_best_weights=TRAIN_CONFIG['restore_best_weights'],\n",
    "    grad_clip_norm=TRAIN_CONFIG['grad_clip_norm'],\n",
    "    stream_monitoring=TRAIN_CONFIG['stream_monitoring']  # Enable built-in monitoring\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üéâ TRAINING COMPLETE!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-13"
   },
   "source": [
    "## 13. Evaluate Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"FINAL EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Evaluate on validation set\n",
    "results = model.evaluate(data_loader=val_loader)\n",
    "\n",
    "print(f\"\\nFinal Validation Results:\")\n",
    "print(f\"  Loss: {results['loss']:.4f}\")\n",
    "print(f\"  Accuracy: {results['accuracy']*100:.2f}%\")\n",
    "\n",
    "print(f\"\\nTraining Summary:\")\n",
    "print(f\"  Initial train loss: {history['train_loss'][0]:.4f}\")\n",
    "print(f\"  Final train loss: {history['train_loss'][-1]:.4f}\")\n",
    "print(f\"  Best val loss: {min(history['val_loss']):.4f}\")\n",
    "print(f\"  Initial train acc: {history['train_accuracy'][0]*100:.2f}%\")\n",
    "print(f\"  Final train acc: {history['train_accuracy'][-1]*100:.2f}%\")\n",
    "print(f\"  Best val acc: {max(history['val_accuracy'])*100:.2f}%\")\n",
    "print(f\"  Total epochs: {len(history['train_loss'])}\")\n",
    "\n",
    "if 'early_stopping' in history:\n",
    "    print(f\"\\nEarly Stopping Info:\")\n",
    "    print(f\"  Stopped early: {history['early_stopping']['stopped_early']}\")\n",
    "    print(f\"  Best epoch: {history['early_stopping']['best_epoch']}\")\n",
    "    print(f\"  Best {history['early_stopping']['monitor']}: {history['early_stopping']['best_metric']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-14"
   },
   "source": [
    "## 14. Plot Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "plot-curves"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Loss curve\n",
    "axes[0].plot(history['train_loss'], label='Train Loss', linewidth=2)\n",
    "axes[0].plot(history['val_loss'], label='Val Loss', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Loss', fontsize=12)\n",
    "axes[0].set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy curve\n",
    "axes[1].plot([acc*100 for acc in history['train_accuracy']], label='Train Acc', linewidth=2)\n",
    "axes[1].plot([acc*100 for acc in history['val_accuracy']], label='Val Acc', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Accuracy (%)', fontsize=12)\n",
    "axes[1].set_title('Training and Validation Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Learning rate curve\n",
    "if len(history['learning_rates']) > 0:\n",
    "    # Sample learning rates (they're recorded per step, not per epoch)\n",
    "    sampled_lrs = history['learning_rates'][::max(1, len(history['learning_rates'])//100)]\n",
    "    axes[2].plot(sampled_lrs, linewidth=2, color='green')\n",
    "    axes[2].set_xlabel('Training Step (sampled)', fontsize=12)\n",
    "    axes[2].set_ylabel('Learning Rate', fontsize=12)\n",
    "    axes[2].set_title('Learning Rate Schedule', fontsize=14, fontweight='bold')\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    axes[2].set_yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{checkpoint_dir}/training_curves.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úÖ Training curves saved to: {checkpoint_dir}/training_curves.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-15"
   },
   "source": [
    "## 15. Pathway Analysis (RGB vs Depth Contributions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pathway-analysis"
   },
   "outputs": [],
   "source": "print(\"=\" * 60)\nprint(\"PATHWAY ANALYSIS\")\nprint(\"=\" * 60)\nprint(\"\\nAnalyzing RGB and Depth pathway contributions...\")\nprint(\"This may take a few minutes...\\n\")\n\n# Analyze pathways\npathway_analysis = model.analyze_pathways(\n    data_loader=val_loader,\n    num_samples=len(val_loader.dataset)  # Use all validation samples\n)\n\nprint(\"\\nAccuracy Metrics:\")\nprint(f\"  Full model (RGB+Depth): {pathway_analysis['accuracy']['full_model']*100:.2f}%\")\nprint(f\"  RGB only: {pathway_analysis['accuracy']['color_only']*100:.2f}%\")\nprint(f\"  Depth only: {pathway_analysis['accuracy']['brightness_only']*100:.2f}%\")\nprint(f\"\\n  RGB contribution: {pathway_analysis['accuracy']['color_contribution']*100:.2f}%\")\nprint(f\"  Depth contribution: {pathway_analysis['accuracy']['brightness_contribution']*100:.2f}%\")\n\nprint(\"\\nLoss Metrics:\")\nprint(f\"  Full model: {pathway_analysis['loss']['full_model']:.4f}\")\nprint(f\"  RGB only: {pathway_analysis['loss']['color_only']:.4f}\")\nprint(f\"  Depth only: {pathway_analysis['loss']['brightness_only']:.4f}\")\n\nprint(\"\\nFeature Norm Statistics:\")\nprint(f\"  RGB mean: {pathway_analysis['feature_norms']['color_mean']:.4f}\")\nprint(f\"  RGB std: {pathway_analysis['feature_norms']['color_std']:.4f}\")\nprint(f\"  Depth mean: {pathway_analysis['feature_norms']['brightness_mean']:.4f}\")\nprint(f\"  Depth std: {pathway_analysis['feature_norms']['brightness_std']:.4f}\")\nprint(f\"  RGB/Depth ratio: {pathway_analysis['feature_norms']['color_to_brightness_ratio']:.4f}\")\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"STREAM CONTRIBUTION ANALYSIS\")\nprint(\"=\" * 60)\nprint(\"\\nCalculating how much the fusion relies on each stream...\")\nprint(\"(Measures performance drop when each stream is removed)\\n\")\n\n# Calculate stream contributions - shows how much fusion relies on each stream\nstream_contributions = model.calculate_stream_contributions(\n    data_loader=val_loader,\n    batch_size=96\n)\n\nprint(\"Stream Contribution to Final Predictions:\")\nprint(f\"  RGB importance: {stream_contributions['color_importance']*100:.1f}%\")\nprint(f\"  Depth importance: {stream_contributions['brightness_importance']*100:.1f}%\")\n\nprint(\"\\nPerformance Drop Analysis:\")\nprint(f\"  Without RGB: {stream_contributions['performance_drops']['without_color']*100:.2f}% accuracy drop\")\nprint(f\"  Without Depth: {stream_contributions['performance_drops']['without_brightness']*100:.2f}% accuracy drop\")\n\nprint(\"\\nInterpretation:\")\nif stream_contributions['color_importance'] > 0.6:\n    print(\"  ‚Üí Fusion relies heavily on RGB stream\")\nelif stream_contributions['brightness_importance'] > 0.6:\n    print(\"  ‚Üí Fusion relies heavily on Depth stream\")\nelse:\n    print(\"  ‚Üí Fusion uses both streams fairly equally\")\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"‚úÖ Pathway analysis complete!\")\nprint(\"=\" * 60)\n\n# Visualize pathway contributions\nimport matplotlib.pyplot as plt\n\nfig, axes = plt.subplots(1, 3, figsize=(16, 5))\n\n# Accuracy comparison\npathways = ['Full Model\\n(RGB+Depth)', 'RGB Only', 'Depth Only']\naccuracies = [\n    pathway_analysis['accuracy']['full_model'] * 100,\n    pathway_analysis['accuracy']['color_only'] * 100,\n    pathway_analysis['accuracy']['brightness_only'] * 100\n]\ncolors = ['green', 'blue', 'orange']\n\naxes[0].bar(pathways, accuracies, color=colors, alpha=0.7)\naxes[0].set_ylabel('Accuracy (%)', fontsize=12)\naxes[0].set_title('Pathway Accuracy Comparison', fontsize=14, fontweight='bold')\naxes[0].grid(True, alpha=0.3, axis='y')\nfor i, v in enumerate(accuracies):\n    axes[0].text(i, v + 1, f'{v:.1f}%', ha='center', fontweight='bold')\n\n# Feature norm comparison\nnorms = ['RGB Features', 'Depth Features']\nnorm_values = [\n    pathway_analysis['feature_norms']['color_mean'],\n    pathway_analysis['feature_norms']['brightness_mean']\n]\naxes[1].bar(norms, norm_values, color=['blue', 'orange'], alpha=0.7)\naxes[1].set_ylabel('Feature Norm (Mean)', fontsize=12)\naxes[1].set_title('Feature Magnitude Comparison', fontsize=14, fontweight='bold')\naxes[1].grid(True, alpha=0.3, axis='y')\nfor i, v in enumerate(norm_values):\n    axes[1].text(i, v + 0.1, f'{v:.2f}', ha='center', fontweight='bold')\n\n# Stream importance (contribution to predictions)\nstreams = ['RGB Stream', 'Depth Stream']\nimportance_values = [\n    stream_contributions['color_importance'] * 100,\n    stream_contributions['brightness_importance'] * 100\n]\naxes[2].bar(streams, importance_values, color=['blue', 'orange'], alpha=0.7)\naxes[2].set_ylabel('Importance (%)', fontsize=12)\naxes[2].set_title('Stream Contribution to Predictions', fontsize=14, fontweight='bold')\naxes[2].grid(True, alpha=0.3, axis='y')\nfor i, v in enumerate(importance_values):\n    axes[2].text(i, v + 1, f'{v:.1f}%', ha='center', fontweight='bold')\n\nplt.tight_layout()\nplt.savefig(f\"{checkpoint_dir}/pathway_analysis.png\", dpi=150, bbox_inches='tight')\nplt.show()\n\nprint(f\"‚úÖ Pathway analysis plot saved to: {checkpoint_dir}/pathway_analysis.png\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-16"
   },
   "source": [
    "## 16. Save Results & Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save-results"
   },
   "outputs": [],
   "source": "import json\nimport torch\n\nprint(\"=\" * 60)\nprint(\"SAVING RESULTS\")\nprint(\"=\" * 60)\n\n# Save training history as JSON\nhistory_path = f\"{checkpoint_dir}/training_history.json\"\nwith open(history_path, 'w') as f:\n    json_history = {\n        'train_loss': [float(x) for x in history['train_loss']],\n        'val_loss': [float(x) for x in history['val_loss']],\n        'train_accuracy': [float(x) for x in history['train_accuracy']],\n        'val_accuracy': [float(x) for x in history['val_accuracy']],\n        'learning_rates': [float(x) for x in history['learning_rates']],\n        'model_config': MODEL_CONFIG,\n        'dataset_config': DATASET_CONFIG,\n        'stream_specific_config': STREAM_SPECIFIC_CONFIG,\n        'training_config': TRAIN_CONFIG,\n        'scheduler_kwargs': history.get('scheduler_kwargs', {}),  # Scheduler-specific parameters\n        'final_results': {\n            'val_loss': float(results['loss']),\n            'val_accuracy': float(results['accuracy'])\n        },\n        'pathway_analysis': {\n            'full_model_accuracy': float(pathway_analysis['accuracy']['full_model']),\n            'rgb_only_accuracy': float(pathway_analysis['accuracy']['color_only']),\n            'depth_only_accuracy': float(pathway_analysis['accuracy']['brightness_only'])\n        }\n    }\n    if 'early_stopping' in history:\n        json_history['early_stopping'] = history['early_stopping']\n    \n    json.dump(json_history, f, indent=2)\n\nprint(f\"‚úÖ Training history saved: {history_path}\")\n\n# Save final model (in addition to best model)\nfinal_model_path = f\"{checkpoint_dir}/final_model.pt\"\ntorch.save({\n    'model_state_dict': model.state_dict(),\n    'optimizer_state_dict': model.optimizer.state_dict(),\n    'config': MODEL_CONFIG,\n    'stream_specific_config': STREAM_SPECIFIC_CONFIG,\n    'scheduler_kwargs': history.get('scheduler_kwargs', {}),  # Scheduler-specific parameters\n    'history': history,\n    'val_accuracy': results['accuracy']\n}, final_model_path)\n\nprint(f\"‚úÖ Final model saved: {final_model_path}\")\n\n# Save summary report\nsummary_path = f\"{checkpoint_dir}/summary.txt\"\nwith open(summary_path, 'w') as f:\n    f.write(\"=\" * 60 + \"\\n\")\n    f.write(\"MCResNet Training Summary - SUN RGB-D\\n\")\n    f.write(\"=\" * 60 + \"\\n\\n\")\n    \n    # Model Configuration\n    f.write(\"Model Configuration:\\n\")\n    f.write(f\"  Architecture: MCResNet-{MODEL_CONFIG['architecture'].upper()}\\n\")\n    f.write(f\"  Num Classes: {MODEL_CONFIG['num_classes']}\\n\")\n    f.write(f\"  Stream1 Channels: {MODEL_CONFIG['stream1_channels']} (RGB)\\n\")\n    f.write(f\"  Stream2 Channels: {MODEL_CONFIG['stream2_channels']} (Depth)\\n\")\n    f.write(f\"  Fusion Type: {MODEL_CONFIG['fusion_type']}\\n\")\n    f.write(f\"  Dropout: {MODEL_CONFIG['dropout_p']}\\n\")\n    f.write(f\"  Device: {MODEL_CONFIG['device']}\\n\")\n    f.write(f\"  AMP Enabled: {MODEL_CONFIG['use_amp']}\\n\")\n    f.write(f\"  Total Parameters: {total_params:,}\\n\")\n    f.write(f\"  Trainable Parameters: {trainable_params:,}\\n\")\n    f.write(f\"  Fusion Parameters: {fusion_params:,}\\n\")\n    \n    # Dataset Configuration\n    f.write(f\"\\nDataset Configuration:\\n\")\n    f.write(f\"  Dataset: SUN RGB-D 15-category (Scene Classification)\\n\")\n    f.write(f\"  Training Samples: {len(train_loader.dataset)}\\n\")\n    f.write(f\"  Validation Samples: {len(val_loader.dataset)}\\n\")\n    f.write(f\"  Batch Size: {DATASET_CONFIG['batch_size']}\\n\")\n    f.write(f\"  Num Workers: {DATASET_CONFIG['num_workers']}\\n\")\n    f.write(f\"  Input Size: {DATASET_CONFIG['target_size']}\\n\")\n    \n    # Optimization Configuration\n    f.write(f\"\\nOptimization Configuration:\\n\")\n    f.write(f\"  Optimizer: {STREAM_SPECIFIC_CONFIG['optimizer']}\\n\")\n    f.write(f\"  Loss Function: {STREAM_SPECIFIC_CONFIG['loss']}\\n\")\n    \n    # Label smoothing if present\n    if 'label_smoothing' in STREAM_SPECIFIC_CONFIG and STREAM_SPECIFIC_CONFIG['label_smoothing'] > 0:\n        f.write(f\"  Label Smoothing: {STREAM_SPECIFIC_CONFIG['label_smoothing']}\\n\")\n    \n    f.write(f\"  Scheduler: {STREAM_SPECIFIC_CONFIG['scheduler']}\\n\")\n    \n    # Scheduler-specific parameters\n    scheduler_kwargs = history.get('scheduler_kwargs', {})\n    if scheduler_kwargs:\n        f.write(f\"  Scheduler Parameters:\\n\")\n        for key, value in scheduler_kwargs.items():\n            f.write(f\"    {key}: {value}\\n\")\n    \n    f.write(f\"  Base LR: {STREAM_SPECIFIC_CONFIG['learning_rate']}\\n\")\n    f.write(f\"  Base Weight Decay: {STREAM_SPECIFIC_CONFIG['weight_decay']}\\n\")\n    f.write(f\"  Gradient Clipping: {TRAIN_CONFIG['grad_clip_norm']}\\n\")\n    \n    # Stream-Specific Settings\n    f.write(f\"\\nStream-Specific Settings:\\n\")\n    f.write(f\"  Stream1 (RGB):\\n\")\n    f.write(f\"    Learning Rate: {STREAM_SPECIFIC_CONFIG['stream1_lr']}\\n\")\n    f.write(f\"    Weight Decay: {STREAM_SPECIFIC_CONFIG['stream1_weight_decay']}\\n\")\n    f.write(f\"  Stream2 (Depth):\\n\")\n    f.write(f\"    Learning Rate: {STREAM_SPECIFIC_CONFIG['stream2_lr']}\\n\")\n    f.write(f\"    Weight Decay: {STREAM_SPECIFIC_CONFIG['stream2_weight_decay']}\\n\")\n    \n    # Training Configuration\n    f.write(f\"\\nTraining Configuration:\\n\")\n    f.write(f\"  Total Epochs: {len(history['train_loss'])}\\n\")\n    f.write(f\"  Stream Monitoring: {TRAIN_CONFIG['stream_monitoring']}\\n\")\n    f.write(f\"  Early Stopping: {TRAIN_CONFIG['early_stopping']}\\n\")\n    if TRAIN_CONFIG['early_stopping']:\n        f.write(f\"    Monitor: {TRAIN_CONFIG['monitor']}\\n\")\n        f.write(f\"    Patience: {TRAIN_CONFIG['patience']}\\n\")\n        f.write(f\"    Min Delta: {TRAIN_CONFIG['min_delta']}\\n\")\n        f.write(f\"    Restore Best Weights: {TRAIN_CONFIG['restore_best_weights']}\\n\")\n    \n    # Results\n    f.write(f\"\\nFinal Results:\\n\")\n    f.write(f\"  Val Loss: {results['loss']:.4f}\\n\")\n    f.write(f\"  Val Accuracy: {results['accuracy']*100:.2f}%\\n\")\n    f.write(f\"  Best Val Accuracy: {max(history['val_accuracy'])*100:.2f}%\\n\")\n    f.write(f\"  Initial Train Loss: {history['train_loss'][0]:.4f}\\n\")\n    f.write(f\"  Final Train Loss: {history['train_loss'][-1]:.4f}\\n\")\n    f.write(f\"  Best Val Loss: {min(history['val_loss']):.4f}\\n\")\n    \n    # Pathway Analysis\n    f.write(f\"\\nPathway Analysis:\\n\")\n    f.write(f\"  Full Model (RGB+Depth): {pathway_analysis['accuracy']['full_model']*100:.2f}%\\n\")\n    f.write(f\"  RGB Only: {pathway_analysis['accuracy']['color_only']*100:.2f}%\\n\")\n    f.write(f\"  Depth Only: {pathway_analysis['accuracy']['brightness_only']*100:.2f}%\\n\")\n    f.write(f\"  RGB Contribution: {pathway_analysis['accuracy']['color_contribution']*100:.2f}%\\n\")\n    f.write(f\"  Depth Contribution: {pathway_analysis['accuracy']['brightness_contribution']*100:.2f}%\\n\")\n\nprint(f\"‚úÖ Summary report saved: {summary_path}\")\n\nprint(\"\\n\" + \"=\" * 60)\nprint(f\"All results saved to: {checkpoint_dir}\")\nprint(\"=\" * 60)\n\n# List saved files\nprint(\"\\nSaved files:\")\n!ls -lh {checkpoint_dir}"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-17"
   },
   "source": [
    "## 17. Summary & Next Steps\n",
    "\n",
    "### üéâ Training Complete!\n",
    "\n",
    "**What we accomplished:**\n",
    "- ‚úÖ Trained MCResNet on SUN RGB-D dataset (15 categories)\n",
    "- ‚úÖ Used A100 GPU with AMP (2x speedup)\n",
    "- ‚úÖ Saved all checkpoints to Google Drive\n",
    "- ‚úÖ Analyzed RGB and Depth pathway contributions\n",
    "- ‚úÖ Generated training curves and visualizations\n",
    "- ‚úÖ Comprehensive stream monitoring with overfitting detection\n",
    "\n",
    "**Results are saved to:** Check the output above for the checkpoint directory path\n",
    "\n",
    "### üìä Expected Performance:\n",
    "\n",
    "For **SUN RGB-D Scene Classification (15 categories, 10,335 images)**:\n",
    "- **Good:** 65-75% validation accuracy\n",
    "- **Very Good:** 75-80% validation accuracy\n",
    "- **Excellent:** 80-85% validation accuracy\n",
    "\n",
    "**Much better than NYU Depth V2 due to:**\n",
    "- 6.9x more training samples (8,041 vs 1,159)\n",
    "- 22.6x better class balance (8.5x vs 192x)\n",
    "- Higher quality, more diverse dataset\n",
    "\n",
    "### üîç Next Steps:\n",
    "\n",
    "1. **Review Results:**\n",
    "   - Check training curves above\n",
    "   - Review pathway analysis\n",
    "   - Compare RGB vs Depth contributions\n",
    "   - Analyze stream monitoring plots\n",
    "\n",
    "2. **Download Results:**\n",
    "   - All files are saved to your Google Drive\n",
    "   - Download checkpoints for local inference\n",
    "\n",
    "3. **Experiment:**\n",
    "   - Try ResNet50 for better accuracy (change `architecture` in Model Config)\n",
    "   - Use stream-specific optimization if monitoring shows imbalance\n",
    "   - Adjust fusion_type (try 'weighted' or 'gated')\n",
    "   - Train longer if early stopping triggered\n",
    "\n",
    "4. **Deploy:**\n",
    "   - Use the best model for inference\n",
    "   - Test on new RGB-D images\n",
    "   - Integrate into your application\n",
    "\n",
    "---\n",
    "\n",
    "**Questions or issues?** Check the training summary and pathway analysis above!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "final-summary"
   },
   "outputs": [],
   "source": [
    "# Print final summary\n",
    "print(\"=\" * 60)\n",
    "print(\"FINAL SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n‚úÖ Training Complete!\")\n",
    "print(f\"\\nFinal Validation Accuracy: {results['accuracy']*100:.2f}%\")\n",
    "print(f\"Best Validation Accuracy: {max(history['val_accuracy'])*100:.2f}%\")\n",
    "print(f\"\\nRGB Pathway: {pathway_analysis['accuracy']['color_only']*100:.2f}%\")\n",
    "print(f\"Depth Pathway: {pathway_analysis['accuracy']['brightness_only']*100:.2f}%\")\n",
    "print(f\"Combined (RGB+Depth): {pathway_analysis['accuracy']['full_model']*100:.2f}%\")\n",
    "print(f\"\\nTotal Training Epochs: {len(history['train_loss'])}\")\n",
    "print(f\"Total Parameters: {total_params:,}\")\n",
    "print(f\"\\nCheckpoints saved to: {checkpoint_dir}\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üéâ All done! Check Google Drive for saved models and results.\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}