{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# MCResNet Training on SUN RGB-D - Google Colab\n",
    "\n",
    "**Complete end-to-end training pipeline for Google Colab with A100 GPU**\n",
    "\n",
    "---\n",
    "\n",
    "## 📋 Checklist Before Running:\n",
    "\n",
    "- [ ] **Enable A100 GPU:** Runtime → Change runtime type → Hardware accelerator: GPU → GPU type: A100\n",
    "- [ ] **Mount Google Drive:** Your code and dataset will be stored on Drive\n",
    "- [ ] **Upload dataset to Drive:** `MyDrive/datasets/sunrgbd_15/` (preprocessed 15-category dataset)\n",
    "- [ ] **Expected Runtime:** ~2-3 hours for training\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 What This Notebook Does:\n",
    "\n",
    "1. ✅ Verify A100 GPU is available\n",
    "2. ✅ Mount Google Drive\n",
    "3. ✅ Clone your repository to local disk (fast I/O)\n",
    "4. ✅ Copy SUN RGB-D dataset to local disk (10-20x faster than Drive)\n",
    "5. ✅ Install dependencies\n",
    "6. ✅ Train MCResNet with all optimizations\n",
    "7. ✅ Save checkpoints to Drive (persistent storage)\n",
    "8. ✅ Generate training curves and analysis\n",
    "\n",
    "---\n",
    "\n",
    "**Let's get started!** 🚀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-1"
   },
   "source": [
    "## 1. Environment Setup & GPU Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check-gpu"
   },
   "outputs": [],
   "source": [
    "# Check GPU availability and specs\n",
    "import torch\n",
    "import subprocess\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"GPU VERIFICATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check PyTorch and CUDA\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "    \n",
    "    # Check if it's A100\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    if 'A100' in gpu_name:\n",
    "        print(\"\\n✅ A100 GPU detected - PERFECT for training!\")\n",
    "    elif 'V100' in gpu_name:\n",
    "        print(\"\\n✅ V100 GPU detected - Good for training (slower than A100)\")\n",
    "    elif 'T4' in gpu_name:\n",
    "        print(\"\\n⚠️  T4 GPU detected - Will be slower, consider upgrading to A100\")\n",
    "    else:\n",
    "        print(f\"\\n⚠️  GPU: {gpu_name} - Consider using A100 for best performance\")\n",
    "else:\n",
    "    print(\"\\n❌ NO GPU DETECTED!\")\n",
    "    print(\"Please enable GPU: Runtime → Change runtime type → Hardware accelerator: GPU\")\n",
    "    raise RuntimeError(\"GPU is required for training\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nvidia-smi"
   },
   "outputs": [],
   "source": [
    "# Detailed GPU info\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-2"
   },
   "source": [
    "## 2. Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mount-drive"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "print(\"\\n✅ Google Drive mounted successfully!\")\n",
    "print(f\"\\nDrive contents:\")\n",
    "!ls -la /content/drive/MyDrive/ | head -20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-3"
   },
   "source": [
    "## 3. Clone Repository to Local Disk (Fast I/O)\n",
    "\n",
    "**Important:** We clone to `/content/` (local SSD) instead of Drive for 10-20x faster I/O\n",
    "\n",
    "**Default:** Clone from GitHub (recommended - always gets latest code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clone-repo"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Configuration\n",
    "PROJECT_NAME = \"Multi-Stream-Neural-Networks\"\n",
    "GITHUB_REPO = \"https://github.com/clingergab/Multi-Stream-Neural-Networks.git\"  # UPDATE THIS\n",
    "LOCAL_REPO_PATH = f\"/content/{PROJECT_NAME}\"  # Local copy for fast I/O\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"REPOSITORY SETUP\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Ensure we're in a valid directory\n",
    "os.chdir('/content')\n",
    "print(f\"Starting in: {os.getcwd()}\")\n",
    "\n",
    "# Check if repo already exists (same session, rerunning cell)\n",
    "if Path(LOCAL_REPO_PATH).exists() and Path(f\"{LOCAL_REPO_PATH}/.git\").exists():\n",
    "    print(f\"\\n📁 Repo already exists: {LOCAL_REPO_PATH}\")\n",
    "    print(f\"🔄 Pulling latest changes...\")\n",
    "    \n",
    "    os.chdir(LOCAL_REPO_PATH)\n",
    "    !git pull\n",
    "    print(\"✅ Repo updated\")\n",
    "\n",
    "# Clone from GitHub (first run)\n",
    "else:\n",
    "    # Remove old incomplete copy if exists\n",
    "    if Path(LOCAL_REPO_PATH).exists():\n",
    "        print(f\"\\n🗑️  Removing incomplete repo copy...\")\n",
    "        !rm -rf {LOCAL_REPO_PATH}\n",
    "    \n",
    "    print(f\"\\n🔄 Cloning from GitHub...\")\n",
    "    print(f\"   Repo: {GITHUB_REPO}\")\n",
    "    print(f\"   Destination: {LOCAL_REPO_PATH}\")\n",
    "    \n",
    "    !git clone {GITHUB_REPO} {LOCAL_REPO_PATH}\n",
    "    \n",
    "    # Verify clone succeeded\n",
    "    if not Path(LOCAL_REPO_PATH).exists():\n",
    "        raise RuntimeError(f\"Failed to clone repository to {LOCAL_REPO_PATH}\")\n",
    "    \n",
    "    print(\"✅ Repo cloned successfully\")\n",
    "    os.chdir(LOCAL_REPO_PATH)\n",
    "\n",
    "# Verify repo structure\n",
    "print(f\"\\n📂 Repository structure:\")\n",
    "!ls -la {LOCAL_REPO_PATH}\n",
    "\n",
    "print(f\"\\n✅ Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-4"
   },
   "source": [
    "## 4. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install-deps"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "print(\"Installing dependencies...\")\n",
    "\n",
    "!pip install -q h5py tqdm matplotlib seaborn\n",
    "\n",
    "# Verify installations\n",
    "import h5py\n",
    "import tqdm\n",
    "import matplotlib\n",
    "import seaborn\n",
    "\n",
    "print(\"✅ All dependencies installed!\")\n",
    "print(f\"   h5py: {h5py.__version__}\")\n",
    "print(f\"   matplotlib: {matplotlib.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-5"
   },
   "source": [
    "## 5. Copy SUN RGB-D Dataset to Local Disk\n",
    "\n",
    "**Performance Note:** Local disk I/O is ~10-20x faster than Drive!\n",
    "\n",
    "**Dataset:** SUN RGB-D 15-category preprocessed dataset (~3.5 GB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download-dataset"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Paths\n",
    "DRIVE_DATASET_TAR = \"/content/drive/MyDrive/datasets/sunrgbd_15.tar.gz\"  # Compressed file\n",
    "LOCAL_DATASET_PATH = \"/content/data/sunrgbd_15\"  # Extracted location\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SUN RGB-D 15-CATEGORY DATASET SETUP\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check if already on local disk\n",
    "if Path(LOCAL_DATASET_PATH).exists():\n",
    "    print(f\"✅ Dataset already on local disk: {LOCAL_DATASET_PATH}\")\n",
    "    \n",
    "    # Verify structure\n",
    "    train_rgb_count = len(list(Path(f\"{LOCAL_DATASET_PATH}/train/rgb\").glob(\"*.png\")))\n",
    "    val_rgb_count = len(list(Path(f\"{LOCAL_DATASET_PATH}/val/rgb\").glob(\"*.png\")))\n",
    "    print(f\"   Train samples: {train_rgb_count}\")\n",
    "    print(f\"   Val samples: {val_rgb_count}\")\n",
    "\n",
    "# Copy and extract from Drive\n",
    "elif Path(DRIVE_DATASET_TAR).exists():\n",
    "    print(f\"📁 Found compressed dataset on Drive: {DRIVE_DATASET_TAR}\")\n",
    "    print(f\"📥 Copying 4.2GB compressed file to local disk...\")\n",
    "    print(f\"   ⏱️  This takes ~3-5 minutes (much faster than 20k individual files!)\")\n",
    "    \n",
    "    # Create parent directory\n",
    "    !mkdir -p /content/data\n",
    "    \n",
    "    # Copy compressed file with progress\n",
    "    print(f\"\\nCopying compressed archive...\")\n",
    "    !rsync -ah --info=progress2 {DRIVE_DATASET_TAR} /content/data/sunrgbd_15.tar.gz\n",
    "    \n",
    "    # Extract to local disk (suppress macOS metadata warnings)\n",
    "    print(f\"\\n📦 Extracting dataset to local disk...\")\n",
    "    !tar -xzf /content/data/sunrgbd_15.tar.gz -C /content/data/ 2>&1 | grep -v \"Ignoring unknown extended header\"\n",
    "    \n",
    "    # Remove tar file to save space\n",
    "    !rm /content/data/sunrgbd_15.tar.gz\n",
    "    \n",
    "    print(f\"\\n✅ Dataset extracted to local disk\")\n",
    "    \n",
    "    # Verify extraction\n",
    "    train_rgb_count = len(list(Path(f\"{LOCAL_DATASET_PATH}/train/rgb\").glob(\"*.png\")))\n",
    "    val_rgb_count = len(list(Path(f\"{LOCAL_DATASET_PATH}/val/rgb\").glob(\"*.png\")))\n",
    "    print(f\"   Train samples: {train_rgb_count}\")\n",
    "    print(f\"   Val samples: {val_rgb_count}\")\n",
    "\n",
    "else:\n",
    "    print(f\"❌ Compressed dataset not found on Drive!\")\n",
    "    print(f\"   Expected location: {DRIVE_DATASET_TAR}\")\n",
    "    print(f\"\\n📋 To fix this:\")\n",
    "    print(f\"   1. Run: COPYFILE_DISABLE=1 tar -czf sunrgbd_15.tar.gz sunrgbd_15/\")\n",
    "    print(f\"   2. Upload sunrgbd_15.tar.gz to Google Drive\")\n",
    "    print(f\"   3. Place it at: {DRIVE_DATASET_TAR}\")\n",
    "    raise FileNotFoundError(f\"Compressed dataset not found at {DRIVE_DATASET_TAR}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"Dataset ready at: {LOCAL_DATASET_PATH}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-6"
   },
   "source": [
    "## 6. Setup Python Path & Import MCResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup-imports"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Remove cached modules\n",
    "modules_to_reload = [k for k in sys.modules.keys() if k.startswith('src.')]\n",
    "for module in modules_to_reload:\n",
    "    del sys.modules[module]\n",
    "    \n",
    "# Add project to Python path\n",
    "project_root = '/content/Multi-Stream-Neural-Networks'\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "# Verify project structure\n",
    "print(\"Project structure:\")\n",
    "!ls -la {project_root}/src/models/\n",
    "\n",
    "# Import MCResNet and SUN RGB-D dataloader\n",
    "print(\"\\nImporting MCResNet and dataloaders...\")\n",
    "from src.models.multi_channel.mc_resnet import mc_resnet18, mc_resnet50\n",
    "from src.data_utils.sunrgbd_dataset import get_sunrgbd_dataloaders\n",
    "\n",
    "print(\"✅ MCResNet and dataloaders imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-7"
   },
   "source": [
    "## 7. Load SUN RGB-D Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify dataset structure\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DATASET STRUCTURE VERIFICATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "dataset_root = Path(LOCAL_DATASET_PATH)\n",
    "\n",
    "print(\"\\nDirectory structure:\")\n",
    "print(f\"  {dataset_root}/\")\n",
    "print(f\"    train/\")\n",
    "print(f\"      rgb/ - {len(list((dataset_root / 'train' / 'rgb').glob('*.png')))} images\")\n",
    "print(f\"      depth/ - {len(list((dataset_root / 'train' / 'depth').glob('*.png')))} images\")\n",
    "print(f\"      labels.txt\")\n",
    "print(f\"    val/\")\n",
    "print(f\"      rgb/ - {len(list((dataset_root / 'val' / 'rgb').glob('*.png')))} images\")\n",
    "print(f\"      depth/ - {len(list((dataset_root / 'val' / 'depth').glob('*.png')))} images\")\n",
    "print(f\"      labels.txt\")\n",
    "print(f\"    class_names.txt\")\n",
    "print(f\"    dataset_info.txt\")\n",
    "\n",
    "# Read class names\n",
    "with open(dataset_root / 'class_names.txt', 'r') as f:\n",
    "    class_names = [line.strip() for line in f]\n",
    "\n",
    "print(f\"\\nClasses ({len(class_names)}):\")\n",
    "for i, name in enumerate(class_names):\n",
    "    print(f\"  {i}: {name}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load-dataset"
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"LOADING SUN RGB-D 15-CATEGORY DATASET\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Dataset configuration\n",
    "DATASET_CONFIG = {\n",
    "    'data_root': LOCAL_DATASET_PATH,\n",
    "    'batch_size': 96,  # Good balance for A100\n",
    "    'num_workers': 4,\n",
    "    'target_size': (416, 544),  \n",
    "    'num_classes': 15   # SUN RGB-D merged to 15 categories (labels 0-14)\n",
    "}\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "for key, value in DATASET_CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(f\"\\nLoading dataset from: {DATASET_CONFIG['data_root']}\")\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader, val_loader = get_sunrgbd_dataloaders(\n",
    "    data_root=DATASET_CONFIG['data_root'],\n",
    "    batch_size=DATASET_CONFIG['batch_size'],\n",
    "    num_workers=DATASET_CONFIG['num_workers'],\n",
    "    target_size=DATASET_CONFIG['target_size']\n",
    ")\n",
    "\n",
    "print(f\"\\n✅ Dataset loaded successfully!\")\n",
    "print(f\"\\nDataset Statistics:\")\n",
    "print(f\"  Train batches: {len(train_loader)}\")\n",
    "print(f\"  Val batches: {len(val_loader)}\")\n",
    "print(f\"  Train samples: {len(train_loader.dataset)}\")\n",
    "print(f\"  Val samples: {len(val_loader.dataset)}\")\n",
    "print(f\"  Batch size: {DATASET_CONFIG['batch_size']}\")\n",
    "\n",
    "# Test loading a batch\n",
    "print(f\"\\nTesting batch loading...\")\n",
    "rgb_batch, depth_batch, label_batch = next(iter(train_loader))\n",
    "print(f\"  RGB shape: {rgb_batch.shape}\")\n",
    "print(f\"  Depth shape: {depth_batch.shape}\")\n",
    "print(f\"  Labels shape: {label_batch.shape}\")\n",
    "print(f\"  Labels min: {label_batch.min().item()}, max: {label_batch.max().item()}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-8"
   },
   "source": [
    "## 8. Visualize Sample Data\n",
    "\n",
    "Shows RGB images, depth maps, and scene labels from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "visualize-data"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Visualize some samples\n",
    "fig, axes = plt.subplots(2, 4, figsize=(14, 7))\n",
    "\n",
    "for i in range(4):\n",
    "    rgb = rgb_batch[i].cpu()\n",
    "    depth = depth_batch[i].cpu()\n",
    "    label = label_batch[i].item()\n",
    "    \n",
    "    # Denormalize for visualization\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "    rgb_vis = rgb * std + mean\n",
    "    rgb_vis = torch.clamp(rgb_vis, 0, 1)\n",
    "    \n",
    "    # Plot RGB\n",
    "    axes[0, i].imshow(rgb_vis.permute(1, 2, 0))\n",
    "    axes[0, i].set_title(f\"RGB - Class {label}\", fontsize=10)\n",
    "    axes[0, i].axis('off')\n",
    "    \n",
    "    # Plot Depth\n",
    "    axes[1, i].imshow(depth.squeeze(), cmap='viridis')\n",
    "    axes[1, i].set_title(f\"Depth - Class {label}\", fontsize=10)\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "plt.suptitle('SUN RGB-D Sample Data (RGB + Depth)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"✅ Sample visualization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-9"
   },
   "source": [
    "## 9. Create & Compile MCResNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create-model"
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"MODEL CREATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Model configuration\n",
    "MODEL_CONFIG = {\n",
    "    'architecture': 'resnet18',  # or 'resnet50' for better accuracy\n",
    "    'num_classes': 15,  # SUN RGB-D has 15 merged categories (labels 0-14)\n",
    "    'stream1_channels': 3,  # RGB\n",
    "    'stream2_channels': 1,  # Depth\n",
    "    'fusion_type': 'concat',  # 'concat', 'weighted', or 'gated'\n",
    "    'dropout_p': 0.5,  # Dropout for regularization\n",
    "    'device': 'cuda',\n",
    "    'use_amp': True  # Automatic Mixed Precision (2x faster on A100)\n",
    "}\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "for key, value in MODEL_CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Create model\n",
    "print(f\"\\nCreating MCResNet-{MODEL_CONFIG['architecture'].upper()}...\")\n",
    "\n",
    "if MODEL_CONFIG['architecture'] == 'resnet18':\n",
    "    model = mc_resnet18(\n",
    "        num_classes=MODEL_CONFIG['num_classes'],\n",
    "        stream1_input_channels=MODEL_CONFIG['stream1_channels'],\n",
    "        stream2_input_channels=MODEL_CONFIG['stream2_channels'],\n",
    "        fusion_type=MODEL_CONFIG['fusion_type'],\n",
    "        dropout_p=MODEL_CONFIG['dropout_p'],\n",
    "        device=MODEL_CONFIG['device'],\n",
    "        use_amp=MODEL_CONFIG['use_amp']\n",
    "    )\n",
    "elif MODEL_CONFIG['architecture'] == 'resnet50':\n",
    "    model = mc_resnet50(\n",
    "        num_classes=MODEL_CONFIG['num_classes'],\n",
    "        stream1_input_channels=MODEL_CONFIG['stream1_channels'],\n",
    "        stream2_input_channels=MODEL_CONFIG['stream2_channels'],\n",
    "        fusion_type=MODEL_CONFIG['fusion_type'],\n",
    "        dropout_p=MODEL_CONFIG['dropout_p'],\n",
    "        device=MODEL_CONFIG['device'],\n",
    "        use_amp=MODEL_CONFIG['use_amp']\n",
    "    )\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "fusion_params = sum(p.numel() for p in model.fusion.parameters())\n",
    "\n",
    "print(f\"\\n✅ Model created successfully!\")\n",
    "print(f\"\\nModel Statistics:\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"  Fusion parameters: {fusion_params:,}\")\n",
    "print(f\"  Model size: {total_params * 4 / 1024**2:.2f} MB (FP32)\")\n",
    "print(f\"  Fusion strategy: {model.fusion_strategy}\")\n",
    "print(f\"  Device: {MODEL_CONFIG['device']}\")\n",
    "print(f\"  AMP enabled: {MODEL_CONFIG['use_amp']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "compile-model"
   },
   "source": [
    "## 9b. Model Compilation Options\n",
    "\n",
    "**Choose your optimization strategy below (cell-22)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model with stream-specific optimization\n",
    "print(\"=\" * 60)\n",
    "print(\"MODEL COMPILATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Stream-specific configuration for optimal RGB/Depth balance\n",
    "STREAM_SPECIFIC_CONFIG = {\n",
    "    'optimizer': 'adamw',\n",
    "    'learning_rate': 7e-5,           # Base LR for shared params (fusion, classifier)\n",
    "    'weight_decay': 2e-4,             # Base weight decay\n",
    "\n",
    "    # Stream-specific settings (adjusted based on research):\n",
    "    'stream1_lr': 3e-5,               # RGB stream: lower LR (more regularization)\n",
    "    'stream1_weight_decay': 5e-4,     # RGB stream: higher WD (prevent overfitting)\n",
    "    'stream2_lr': 1e-4,               # Depth stream: higher LR (needs more learning)\n",
    "    'stream2_weight_decay': 1e-4,     # Depth stream: lighter WD (less regularization)\n",
    "\n",
    "    'loss': 'cross_entropy',\n",
    "    'scheduler': 'cosine'\n",
    "}\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "for key, value in STREAM_SPECIFIC_CONFIG.items():\n",
    "    if value is not None:\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "# Compile\n",
    "model.compile(**STREAM_SPECIFIC_CONFIG)\n",
    "\n",
    "print(\"\\n✅ Model compiled successfully!\")\n",
    "\n",
    "# Show parameter groups\n",
    "if hasattr(model.optimizer, 'param_groups'):\n",
    "    print(f\"\\nParameter groups created: {len(model.optimizer.param_groups)}\")\n",
    "    for i, group in enumerate(model.optimizer.param_groups):\n",
    "        num_params = sum(p.numel() for p in group['params'])\n",
    "        print(f\"  Group {i}: LR={group['lr']:.2e}, WD={group['weight_decay']:.2e}, Params={num_params:,}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-10"
   },
   "source": [
    "## 10. Test Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "test-forward"
   },
   "outputs": [],
   "source": [
    "# Test forward pass with detailed debugging\n",
    "print(\"Testing forward pass with CUDA_LAUNCH_BLOCKING for better error messages...\")\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'  # Synchronous CUDA for better error messages\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    rgb_test, depth_test, labels_test = next(iter(train_loader))\n",
    "    \n",
    "    print(f\"\\nInput validation:\")\n",
    "    print(f\"  RGB shape: {rgb_test.shape}, dtype: {rgb_test.dtype}\")\n",
    "    print(f\"  RGB min: {rgb_test.min():.4f}, max: {rgb_test.max():.4f}\")\n",
    "    print(f\"  RGB has NaN: {torch.isnan(rgb_test).any()}\")\n",
    "    print(f\"  RGB has Inf: {torch.isinf(rgb_test).any()}\")\n",
    "    \n",
    "    print(f\"\\n  Depth shape: {depth_test.shape}, dtype: {depth_test.dtype}\")\n",
    "    print(f\"  Depth min: {depth_test.min():.4f}, max: {depth_test.max():.4f}\")\n",
    "    print(f\"  Depth has NaN: {torch.isnan(depth_test).any()}\")\n",
    "    print(f\"  Depth has Inf: {torch.isinf(depth_test).any()}\")\n",
    "    \n",
    "    print(f\"\\n  Labels shape: {labels_test.shape}, dtype: {labels_test.dtype}\")\n",
    "    print(f\"  Labels min: {labels_test.min()}, max: {labels_test.max()}\")\n",
    "    print(f\"  Labels unique: {torch.unique(labels_test).tolist()}\")\n",
    "    \n",
    "    print(\"\\nRunning forward pass...\")\n",
    "    rgb_cuda = rgb_test.to('cuda')\n",
    "    depth_cuda = depth_test.to('cuda')\n",
    "    \n",
    "    try:\n",
    "        outputs = model(rgb_cuda, depth_cuda)\n",
    "        print(f\"  ✅ Forward pass successful!\")\n",
    "        print(f\"  Output shape: {outputs.shape}\")\n",
    "        print(f\"  Output min: {outputs.min():.4f}, max: {outputs.max():.4f}\")\n",
    "        \n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        print(f\"\\nSample predictions: {predictions.cpu().numpy()[:10]}\")\n",
    "        print(f\"Ground truth: {labels_test.numpy()[:10]}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Forward pass failed!\")\n",
    "        print(f\"Error: {e}\")\n",
    "        print(f\"\\nThis is likely a model architecture issue, not a data issue.\")\n",
    "        print(f\"Possible causes:\")\n",
    "        print(f\"  1. BatchNorm running stats issue\")\n",
    "        print(f\"  2. Invalid tensor operations in model\")\n",
    "        print(f\"  3. Memory corruption\")\n",
    "        raise\n",
    "\n",
    "print(\"\\n✅ Forward pass test complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-11"
   },
   "source": [
    "## 11. Setup Checkpoint Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"CHECKPOINT DIRECTORY SETUP\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create checkpoint directory on Google Drive (persistent storage)\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "checkpoint_dir = f\"/content/drive/MyDrive/mcresnet_checkpoints/run_{timestamp}\"\n",
    "\n",
    "# Create directory\n",
    "Path(checkpoint_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"\\n✅ Checkpoint directory created:\")\n",
    "print(f\"   {checkpoint_dir}\")\n",
    "print(f\"\\nAll training artifacts will be saved here:\")\n",
    "print(f\"  • Best model weights\")\n",
    "print(f\"  • Training history\")\n",
    "print(f\"  • Monitoring metrics\")\n",
    "print(f\"  • Visualizations\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Train the Model 🚀\n",
    "\n",
    "**Expected time:** ~2-3 hours for 90 epochs on A100\n",
    "\n",
    "**All optimizations enabled:**\n",
    "- ✅ Automatic Mixed Precision (2x faster)\n",
    "- ✅ Gradient Clipping (stability)\n",
    "- ✅ Cosine Annealing LR\n",
    "- ✅ Early Stopping\n",
    "- ✅ Best Model Checkpointing\n",
    "- ✅ Local disk I/O (10-20x faster than Drive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup-checkpoints"
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"TRAINING WITH STREAM MONITORING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Training configuration\n",
    "TRAIN_CONFIG = {\n",
    "    'epochs': 90,\n",
    "    'grad_clip_norm': 5.0,\n",
    "    'early_stopping': True,\n",
    "    'patience': 15,\n",
    "    'min_delta': 0.001,\n",
    "    'monitor': 'val_accuracy',\n",
    "    'restore_best_weights': True,\n",
    "    'save_path': f\"{checkpoint_dir}/best_model.pt\",\n",
    "    'stream_monitoring': True  # Built-in stream monitoring!\n",
    "}\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "for key, value in TRAIN_CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"Training will take approximately 2-3 hours on A100\")\n",
    "print(f\"Stream monitoring active - detailed per-stream metrics shown each epoch\")\n",
    "print(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "# Train using built-in fit() method with stream monitoring\n",
    "history = model.fit(\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    epochs=TRAIN_CONFIG['epochs'],\n",
    "    verbose=True,\n",
    "    save_path=TRAIN_CONFIG['save_path'],\n",
    "    early_stopping=TRAIN_CONFIG['early_stopping'],\n",
    "    patience=TRAIN_CONFIG['patience'],\n",
    "    min_delta=TRAIN_CONFIG['min_delta'],\n",
    "    monitor=TRAIN_CONFIG['monitor'],\n",
    "    restore_best_weights=TRAIN_CONFIG['restore_best_weights'],\n",
    "    grad_clip_norm=TRAIN_CONFIG['grad_clip_norm'],\n",
    "    stream_monitoring=TRAIN_CONFIG['stream_monitoring']  # Enable built-in monitoring\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"🎉 TRAINING COMPLETE!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-13"
   },
   "source": [
    "## 13. Evaluate Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"FINAL EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Evaluate on validation set\n",
    "results = model.evaluate(data_loader=val_loader)\n",
    "\n",
    "print(f\"\\nFinal Validation Results:\")\n",
    "print(f\"  Loss: {results['loss']:.4f}\")\n",
    "print(f\"  Accuracy: {results['accuracy']*100:.2f}%\")\n",
    "\n",
    "print(f\"\\nTraining Summary:\")\n",
    "print(f\"  Initial train loss: {history['train_loss'][0]:.4f}\")\n",
    "print(f\"  Final train loss: {history['train_loss'][-1]:.4f}\")\n",
    "print(f\"  Best val loss: {min(history['val_loss']):.4f}\")\n",
    "print(f\"  Initial train acc: {history['train_accuracy'][0]*100:.2f}%\")\n",
    "print(f\"  Final train acc: {history['train_accuracy'][-1]*100:.2f}%\")\n",
    "print(f\"  Best val acc: {max(history['val_accuracy'])*100:.2f}%\")\n",
    "print(f\"  Total epochs: {len(history['train_loss'])}\")\n",
    "\n",
    "if 'early_stopping' in history:\n",
    "    print(f\"\\nEarly Stopping Info:\")\n",
    "    print(f\"  Stopped early: {history['early_stopping']['stopped_early']}\")\n",
    "    print(f\"  Best epoch: {history['early_stopping']['best_epoch']}\")\n",
    "    print(f\"  Best {history['early_stopping']['monitor']}: {history['early_stopping']['best_metric']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-14"
   },
   "source": [
    "## 14. Plot Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "plot-curves"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Loss curve\n",
    "axes[0].plot(history['train_loss'], label='Train Loss', linewidth=2)\n",
    "axes[0].plot(history['val_loss'], label='Val Loss', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Loss', fontsize=12)\n",
    "axes[0].set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy curve\n",
    "axes[1].plot([acc*100 for acc in history['train_accuracy']], label='Train Acc', linewidth=2)\n",
    "axes[1].plot([acc*100 for acc in history['val_accuracy']], label='Val Acc', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Accuracy (%)', fontsize=12)\n",
    "axes[1].set_title('Training and Validation Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Learning rate curve\n",
    "if len(history['learning_rates']) > 0:\n",
    "    # Sample learning rates (they're recorded per step, not per epoch)\n",
    "    sampled_lrs = history['learning_rates'][::max(1, len(history['learning_rates'])//100)]\n",
    "    axes[2].plot(sampled_lrs, linewidth=2, color='green')\n",
    "    axes[2].set_xlabel('Training Step (sampled)', fontsize=12)\n",
    "    axes[2].set_ylabel('Learning Rate', fontsize=12)\n",
    "    axes[2].set_title('Learning Rate Schedule', fontsize=14, fontweight='bold')\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    axes[2].set_yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{checkpoint_dir}/training_curves.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"✅ Training curves saved to: {checkpoint_dir}/training_curves.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-15"
   },
   "source": [
    "## 15. Pathway Analysis (RGB vs Depth Contributions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pathway-analysis"
   },
   "outputs": [],
   "source": "print(\"=\" * 60)\nprint(\"PATHWAY ANALYSIS\")\nprint(\"=\" * 60)\nprint(\"\\nAnalyzing RGB and Depth pathway contributions...\")\nprint(\"This may take a few minutes...\\n\")\n\n# Analyze pathways\npathway_analysis = model.analyze_pathways(\n    data_loader=val_loader,\n    num_samples=len(val_loader.dataset)  # Use all validation samples\n)\n\nprint(\"\\nAccuracy Metrics:\")\nprint(f\"  Full model (RGB+Depth): {pathway_analysis['accuracy']['full_model']*100:.2f}%\")\nprint(f\"  RGB only: {pathway_analysis['accuracy']['color_only']*100:.2f}%\")\nprint(f\"  Depth only: {pathway_analysis['accuracy']['brightness_only']*100:.2f}%\")\nprint(f\"\\n  RGB contribution: {pathway_analysis['accuracy']['color_contribution']*100:.2f}%\")\nprint(f\"  Depth contribution: {pathway_analysis['accuracy']['brightness_contribution']*100:.2f}%\")\n\nprint(\"\\nLoss Metrics:\")\nprint(f\"  Full model: {pathway_analysis['loss']['full_model']:.4f}\")\nprint(f\"  RGB only: {pathway_analysis['loss']['color_only']:.4f}\")\nprint(f\"  Depth only: {pathway_analysis['loss']['brightness_only']:.4f}\")\n\nprint(\"\\nFeature Norm Statistics:\")\nprint(f\"  RGB mean: {pathway_analysis['feature_norms']['color_mean']:.4f}\")\nprint(f\"  RGB std: {pathway_analysis['feature_norms']['color_std']:.4f}\")\nprint(f\"  Depth mean: {pathway_analysis['feature_norms']['brightness_mean']:.4f}\")\nprint(f\"  Depth std: {pathway_analysis['feature_norms']['brightness_std']:.4f}\")\nprint(f\"  RGB/Depth ratio: {pathway_analysis['feature_norms']['color_to_brightness_ratio']:.4f}\")\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"STREAM CONTRIBUTION ANALYSIS\")\nprint(\"=\" * 60)\nprint(\"\\nCalculating how much the fusion relies on each stream...\")\nprint(\"(Measures performance drop when each stream is removed)\\n\")\n\n# Calculate stream contributions - shows how much fusion relies on each stream\nstream_contributions = model.calculate_stream_contributions(\n    data_loader=val_loader,\n    batch_size=96\n)\n\nprint(\"Stream Contribution to Final Predictions:\")\nprint(f\"  RGB importance: {stream_contributions['color_importance']*100:.1f}%\")\nprint(f\"  Depth importance: {stream_contributions['brightness_importance']*100:.1f}%\")\n\nprint(\"\\nPerformance Drop Analysis:\")\nprint(f\"  Without RGB: {stream_contributions['performance_drops']['without_color']*100:.2f}% accuracy drop\")\nprint(f\"  Without Depth: {stream_contributions['performance_drops']['without_brightness']*100:.2f}% accuracy drop\")\n\nprint(\"\\nInterpretation:\")\nif stream_contributions['color_importance'] > 0.6:\n    print(\"  → Fusion relies heavily on RGB stream\")\nelif stream_contributions['brightness_importance'] > 0.6:\n    print(\"  → Fusion relies heavily on Depth stream\")\nelse:\n    print(\"  → Fusion uses both streams fairly equally\")\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"✅ Pathway analysis complete!\")\nprint(\"=\" * 60)\n\n# Visualize pathway contributions\nimport matplotlib.pyplot as plt\n\nfig, axes = plt.subplots(1, 3, figsize=(16, 5))\n\n# Accuracy comparison\npathways = ['Full Model\\n(RGB+Depth)', 'RGB Only', 'Depth Only']\naccuracies = [\n    pathway_analysis['accuracy']['full_model'] * 100,\n    pathway_analysis['accuracy']['color_only'] * 100,\n    pathway_analysis['accuracy']['brightness_only'] * 100\n]\ncolors = ['green', 'blue', 'orange']\n\naxes[0].bar(pathways, accuracies, color=colors, alpha=0.7)\naxes[0].set_ylabel('Accuracy (%)', fontsize=12)\naxes[0].set_title('Pathway Accuracy Comparison', fontsize=14, fontweight='bold')\naxes[0].grid(True, alpha=0.3, axis='y')\nfor i, v in enumerate(accuracies):\n    axes[0].text(i, v + 1, f'{v:.1f}%', ha='center', fontweight='bold')\n\n# Feature norm comparison\nnorms = ['RGB Features', 'Depth Features']\nnorm_values = [\n    pathway_analysis['feature_norms']['color_mean'],\n    pathway_analysis['feature_norms']['brightness_mean']\n]\naxes[1].bar(norms, norm_values, color=['blue', 'orange'], alpha=0.7)\naxes[1].set_ylabel('Feature Norm (Mean)', fontsize=12)\naxes[1].set_title('Feature Magnitude Comparison', fontsize=14, fontweight='bold')\naxes[1].grid(True, alpha=0.3, axis='y')\nfor i, v in enumerate(norm_values):\n    axes[1].text(i, v + 0.1, f'{v:.2f}', ha='center', fontweight='bold')\n\n# Stream importance (contribution to predictions)\nstreams = ['RGB Stream', 'Depth Stream']\nimportance_values = [\n    stream_contributions['color_importance'] * 100,\n    stream_contributions['brightness_importance'] * 100\n]\naxes[2].bar(streams, importance_values, color=['blue', 'orange'], alpha=0.7)\naxes[2].set_ylabel('Importance (%)', fontsize=12)\naxes[2].set_title('Stream Contribution to Predictions', fontsize=14, fontweight='bold')\naxes[2].grid(True, alpha=0.3, axis='y')\nfor i, v in enumerate(importance_values):\n    axes[2].text(i, v + 1, f'{v:.1f}%', ha='center', fontweight='bold')\n\nplt.tight_layout()\nplt.savefig(f\"{checkpoint_dir}/pathway_analysis.png\", dpi=150, bbox_inches='tight')\nplt.show()\n\nprint(f\"✅ Pathway analysis plot saved to: {checkpoint_dir}/pathway_analysis.png\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-16"
   },
   "source": [
    "## 16. Save Results & Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save-results"
   },
   "outputs": [],
   "source": "import json\nimport torch\n\nprint(\"=\" * 60)\nprint(\"SAVING RESULTS\")\nprint(\"=\" * 60)\n\n# Save training history as JSON\nhistory_path = f\"{checkpoint_dir}/training_history.json\"\nwith open(history_path, 'w') as f:\n    json_history = {\n        'train_loss': [float(x) for x in history['train_loss']],\n        'val_loss': [float(x) for x in history['val_loss']],\n        'train_accuracy': [float(x) for x in history['train_accuracy']],\n        'val_accuracy': [float(x) for x in history['val_accuracy']],\n        'learning_rates': [float(x) for x in history['learning_rates']],\n        'model_config': MODEL_CONFIG,\n        'dataset_config': DATASET_CONFIG,\n        'stream_specific_config': STREAM_SPECIFIC_CONFIG,\n        'training_config': TRAIN_CONFIG,\n        'scheduler_kwargs': history.get('scheduler_kwargs', {}),  # Scheduler-specific parameters\n        'final_results': {\n            'val_loss': float(results['loss']),\n            'val_accuracy': float(results['accuracy'])\n        },\n        'pathway_analysis': {\n            'full_model_accuracy': float(pathway_analysis['accuracy']['full_model']),\n            'rgb_only_accuracy': float(pathway_analysis['accuracy']['color_only']),\n            'depth_only_accuracy': float(pathway_analysis['accuracy']['brightness_only'])\n        }\n    }\n    if 'early_stopping' in history:\n        json_history['early_stopping'] = history['early_stopping']\n    \n    json.dump(json_history, f, indent=2)\n\nprint(f\"✅ Training history saved: {history_path}\")\n\n# Save final model (in addition to best model)\nfinal_model_path = f\"{checkpoint_dir}/final_model.pt\"\ntorch.save({\n    'model_state_dict': model.state_dict(),\n    'optimizer_state_dict': model.optimizer.state_dict(),\n    'config': MODEL_CONFIG,\n    'stream_specific_config': STREAM_SPECIFIC_CONFIG,\n    'scheduler_kwargs': history.get('scheduler_kwargs', {}),  # Scheduler-specific parameters\n    'history': history,\n    'val_accuracy': results['accuracy']\n}, final_model_path)\n\nprint(f\"✅ Final model saved: {final_model_path}\")\n\n# Save summary report\nsummary_path = f\"{checkpoint_dir}/summary.txt\"\nwith open(summary_path, 'w') as f:\n    f.write(\"=\" * 60 + \"\\n\")\n    f.write(\"MCResNet Training Summary - SUN RGB-D\\n\")\n    f.write(\"=\" * 60 + \"\\n\\n\")\n    \n    # Model Configuration\n    f.write(\"Model Configuration:\\n\")\n    f.write(f\"  Architecture: MCResNet-{MODEL_CONFIG['architecture'].upper()}\\n\")\n    f.write(f\"  Num Classes: {MODEL_CONFIG['num_classes']}\\n\")\n    f.write(f\"  Stream1 Channels: {MODEL_CONFIG['stream1_channels']} (RGB)\\n\")\n    f.write(f\"  Stream2 Channels: {MODEL_CONFIG['stream2_channels']} (Depth)\\n\")\n    f.write(f\"  Fusion Type: {MODEL_CONFIG['fusion_type']}\\n\")\n    f.write(f\"  Dropout: {MODEL_CONFIG['dropout_p']}\\n\")\n    f.write(f\"  Device: {MODEL_CONFIG['device']}\\n\")\n    f.write(f\"  AMP Enabled: {MODEL_CONFIG['use_amp']}\\n\")\n    f.write(f\"  Total Parameters: {total_params:,}\\n\")\n    f.write(f\"  Trainable Parameters: {trainable_params:,}\\n\")\n    f.write(f\"  Fusion Parameters: {fusion_params:,}\\n\")\n    \n    # Dataset Configuration\n    f.write(f\"\\nDataset Configuration:\\n\")\n    f.write(f\"  Dataset: SUN RGB-D 15-category (Scene Classification)\\n\")\n    f.write(f\"  Training Samples: {len(train_loader.dataset)}\\n\")\n    f.write(f\"  Validation Samples: {len(val_loader.dataset)}\\n\")\n    f.write(f\"  Batch Size: {DATASET_CONFIG['batch_size']}\\n\")\n    f.write(f\"  Num Workers: {DATASET_CONFIG['num_workers']}\\n\")\n    f.write(f\"  Input Size: {DATASET_CONFIG['target_size']}\\n\")\n    \n    # Optimization Configuration\n    f.write(f\"\\nOptimization Configuration:\\n\")\n    f.write(f\"  Optimizer: {STREAM_SPECIFIC_CONFIG['optimizer']}\\n\")\n    f.write(f\"  Loss Function: {STREAM_SPECIFIC_CONFIG['loss']}\\n\")\n    \n    # Label smoothing if present\n    if 'label_smoothing' in STREAM_SPECIFIC_CONFIG and STREAM_SPECIFIC_CONFIG['label_smoothing'] > 0:\n        f.write(f\"  Label Smoothing: {STREAM_SPECIFIC_CONFIG['label_smoothing']}\\n\")\n    \n    f.write(f\"  Scheduler: {STREAM_SPECIFIC_CONFIG['scheduler']}\\n\")\n    \n    # Scheduler-specific parameters\n    scheduler_kwargs = history.get('scheduler_kwargs', {})\n    if scheduler_kwargs:\n        f.write(f\"  Scheduler Parameters:\\n\")\n        for key, value in scheduler_kwargs.items():\n            f.write(f\"    {key}: {value}\\n\")\n    \n    f.write(f\"  Base LR: {STREAM_SPECIFIC_CONFIG['learning_rate']}\\n\")\n    f.write(f\"  Base Weight Decay: {STREAM_SPECIFIC_CONFIG['weight_decay']}\\n\")\n    f.write(f\"  Gradient Clipping: {TRAIN_CONFIG['grad_clip_norm']}\\n\")\n    \n    # Stream-Specific Settings\n    f.write(f\"\\nStream-Specific Settings:\\n\")\n    f.write(f\"  Stream1 (RGB):\\n\")\n    f.write(f\"    Learning Rate: {STREAM_SPECIFIC_CONFIG['stream1_lr']}\\n\")\n    f.write(f\"    Weight Decay: {STREAM_SPECIFIC_CONFIG['stream1_weight_decay']}\\n\")\n    f.write(f\"  Stream2 (Depth):\\n\")\n    f.write(f\"    Learning Rate: {STREAM_SPECIFIC_CONFIG['stream2_lr']}\\n\")\n    f.write(f\"    Weight Decay: {STREAM_SPECIFIC_CONFIG['stream2_weight_decay']}\\n\")\n    \n    # Training Configuration\n    f.write(f\"\\nTraining Configuration:\\n\")\n    f.write(f\"  Total Epochs: {len(history['train_loss'])}\\n\")\n    f.write(f\"  Stream Monitoring: {TRAIN_CONFIG['stream_monitoring']}\\n\")\n    f.write(f\"  Early Stopping: {TRAIN_CONFIG['early_stopping']}\\n\")\n    if TRAIN_CONFIG['early_stopping']:\n        f.write(f\"    Monitor: {TRAIN_CONFIG['monitor']}\\n\")\n        f.write(f\"    Patience: {TRAIN_CONFIG['patience']}\\n\")\n        f.write(f\"    Min Delta: {TRAIN_CONFIG['min_delta']}\\n\")\n        f.write(f\"    Restore Best Weights: {TRAIN_CONFIG['restore_best_weights']}\\n\")\n    \n    # Results\n    f.write(f\"\\nFinal Results:\\n\")\n    f.write(f\"  Val Loss: {results['loss']:.4f}\\n\")\n    f.write(f\"  Val Accuracy: {results['accuracy']*100:.2f}%\\n\")\n    f.write(f\"  Best Val Accuracy: {max(history['val_accuracy'])*100:.2f}%\\n\")\n    f.write(f\"  Initial Train Loss: {history['train_loss'][0]:.4f}\\n\")\n    f.write(f\"  Final Train Loss: {history['train_loss'][-1]:.4f}\\n\")\n    f.write(f\"  Best Val Loss: {min(history['val_loss']):.4f}\\n\")\n    \n    # Pathway Analysis\n    f.write(f\"\\nPathway Analysis:\\n\")\n    f.write(f\"  Full Model (RGB+Depth): {pathway_analysis['accuracy']['full_model']*100:.2f}%\\n\")\n    f.write(f\"  RGB Only: {pathway_analysis['accuracy']['color_only']*100:.2f}%\\n\")\n    f.write(f\"  Depth Only: {pathway_analysis['accuracy']['brightness_only']*100:.2f}%\\n\")\n    f.write(f\"  RGB Contribution: {pathway_analysis['accuracy']['color_contribution']*100:.2f}%\\n\")\n    f.write(f\"  Depth Contribution: {pathway_analysis['accuracy']['brightness_contribution']*100:.2f}%\\n\")\n\nprint(f\"✅ Summary report saved: {summary_path}\")\n\nprint(\"\\n\" + \"=\" * 60)\nprint(f\"All results saved to: {checkpoint_dir}\")\nprint(\"=\" * 60)\n\n# List saved files\nprint(\"\\nSaved files:\")\n!ls -lh {checkpoint_dir}"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section-17"
   },
   "source": [
    "## 17. Summary & Next Steps\n",
    "\n",
    "### 🎉 Training Complete!\n",
    "\n",
    "**What we accomplished:**\n",
    "- ✅ Trained MCResNet on SUN RGB-D dataset (15 categories)\n",
    "- ✅ Used A100 GPU with AMP (2x speedup)\n",
    "- ✅ Saved all checkpoints to Google Drive\n",
    "- ✅ Analyzed RGB and Depth pathway contributions\n",
    "- ✅ Generated training curves and visualizations\n",
    "- ✅ Comprehensive stream monitoring with overfitting detection\n",
    "\n",
    "**Results are saved to:** Check the output above for the checkpoint directory path\n",
    "\n",
    "### 📊 Expected Performance:\n",
    "\n",
    "For **SUN RGB-D Scene Classification (15 categories, 10,335 images)**:\n",
    "- **Good:** 65-75% validation accuracy\n",
    "- **Very Good:** 75-80% validation accuracy\n",
    "- **Excellent:** 80-85% validation accuracy\n",
    "\n",
    "**Much better than NYU Depth V2 due to:**\n",
    "- 6.9x more training samples (8,041 vs 1,159)\n",
    "- 22.6x better class balance (8.5x vs 192x)\n",
    "- Higher quality, more diverse dataset\n",
    "\n",
    "### 🔍 Next Steps:\n",
    "\n",
    "1. **Review Results:**\n",
    "   - Check training curves above\n",
    "   - Review pathway analysis\n",
    "   - Compare RGB vs Depth contributions\n",
    "   - Analyze stream monitoring plots\n",
    "\n",
    "2. **Download Results:**\n",
    "   - All files are saved to your Google Drive\n",
    "   - Download checkpoints for local inference\n",
    "\n",
    "3. **Experiment:**\n",
    "   - Try ResNet50 for better accuracy (change `architecture` in Model Config)\n",
    "   - Use stream-specific optimization if monitoring shows imbalance\n",
    "   - Adjust fusion_type (try 'weighted' or 'gated')\n",
    "   - Train longer if early stopping triggered\n",
    "\n",
    "4. **Deploy:**\n",
    "   - Use the best model for inference\n",
    "   - Test on new RGB-D images\n",
    "   - Integrate into your application\n",
    "\n",
    "---\n",
    "\n",
    "**Questions or issues?** Check the training summary and pathway analysis above!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "final-summary"
   },
   "outputs": [],
   "source": [
    "# Print final summary\n",
    "print(\"=\" * 60)\n",
    "print(\"FINAL SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n✅ Training Complete!\")\n",
    "print(f\"\\nFinal Validation Accuracy: {results['accuracy']*100:.2f}%\")\n",
    "print(f\"Best Validation Accuracy: {max(history['val_accuracy'])*100:.2f}%\")\n",
    "print(f\"\\nRGB Pathway: {pathway_analysis['accuracy']['color_only']*100:.2f}%\")\n",
    "print(f\"Depth Pathway: {pathway_analysis['accuracy']['brightness_only']*100:.2f}%\")\n",
    "print(f\"Combined (RGB+Depth): {pathway_analysis['accuracy']['full_model']*100:.2f}%\")\n",
    "print(f\"\\nTotal Training Epochs: {len(history['train_loss'])}\")\n",
    "print(f\"Total Parameters: {total_params:,}\")\n",
    "print(f\"\\nCheckpoints saved to: {checkpoint_dir}\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"🎉 All done! Check Google Drive for saved models and results.\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}