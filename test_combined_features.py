"""
Test combined fusion strategies + stream-specific optimization.

Verifies that both features work together correctly.
"""

import torch
import torch.nn as nn
from src.models.multi_channel import mc_resnet18

def test_combined_features():
    """Test all combinations of fusion + stream optimization."""

    print("=" * 70)
    print("Testing Combined Features: Fusion + Stream Optimization")
    print("=" * 70)

    # Test parameters
    num_classes = 27
    batch_size = 8
    img_size = 224

    # Create sample data
    stream1_data = torch.randn(batch_size, 3, img_size, img_size)
    stream2_data = torch.randn(batch_size, 1, img_size, img_size)
    targets = torch.randint(0, num_classes, (batch_size,))

    fusion_types = ['concat', 'weighted', 'gated']

    for fusion_type in fusion_types:
        print(f"\n{'=' * 70}")
        print(f"Testing: {fusion_type.upper()} Fusion + Stream-Specific Optimization")
        print(f"{'=' * 70}")

        # Create model
        model = mc_resnet18(
            num_classes=num_classes,
            stream1_input_channels=3,
            stream2_input_channels=1,
            fusion_type=fusion_type,
            dropout_p=0.3,
            device='cpu'
        )

        print(f"\n1. Model Configuration")
        print("-" * 70)
        print(f"Fusion type: {model.fusion_strategy}")
        print(f"Fusion params: {sum(p.numel() for p in model.fusion.parameters()):,}")
        print(f"Total params: {sum(p.numel() for p in model.parameters()):,}")

        # Compile with stream-specific optimization
        print(f"\n2. Compile with Stream-Specific Optimization")
        print("-" * 70)
        model.compile(
            optimizer='adamw',
            learning_rate=1e-4,
            weight_decay=2e-2,
            stream1_lr=5e-4,            # Boost RGB
            stream2_lr=5e-5,            # Slow depth
            stream1_weight_decay=1e-3,  # Light regularization
            stream2_weight_decay=5e-2,  # Heavy regularization
            loss='cross_entropy'
        )

        # Verify parameter groups
        assert len(model.optimizer.param_groups) == 3, "Should have 3 param groups"
        print(f"âœ“ Parameter groups: {len(model.optimizer.param_groups)}")

        for i, pg in enumerate(model.optimizer.param_groups):
            param_count = sum(p.numel() for p in pg['params'])
            print(f"  Group {i}: lr={pg['lr']:.1e}, wd={pg['weight_decay']:.1e}, params={param_count:,}")

        # Training step
        print(f"\n3. Training Step")
        print("-" * 70)
        model.train()

        # Get initial fusion weights (if applicable)
        if fusion_type == 'weighted':
            initial_w1 = model.fusion.stream1_weight.clone()
            initial_w2 = model.fusion.stream2_weight.clone()

        # Forward pass
        output = model(stream1_data, stream2_data)
        loss = model.criterion(output, targets)

        # Backward pass
        model.optimizer.zero_grad()
        loss.backward()
        model.optimizer.step()

        print(f"âœ“ Loss: {loss.item():.4f}")
        print(f"âœ“ Output shape: {output.shape}")

        # Verify weights updated
        if fusion_type == 'weighted':
            w1_changed = not torch.allclose(model.fusion.stream1_weight, initial_w1)
            w2_changed = not torch.allclose(model.fusion.stream2_weight, initial_w2)
            assert w1_changed and w2_changed, "Fusion weights should update"
            print(f"âœ“ Fusion weights updated: w1={model.fusion.stream1_weight.item():.4f}, w2={model.fusion.stream2_weight.item():.4f}")

        # Check different learning rates applied
        print(f"\n4. Verify Different Learning Rates Applied")
        print("-" * 70)

        # Get gradients for each stream
        stream1_grad_norm = 0.0
        stream2_grad_norm = 0.0

        for name, param in model.named_parameters():
            if param.grad is not None:
                if 'stream1' in name:
                    stream1_grad_norm += param.grad.norm().item()
                elif 'stream2' in name:
                    stream2_grad_norm += param.grad.norm().item()

        print(f"âœ“ Stream1 grad norm: {stream1_grad_norm:.4f}")
        print(f"âœ“ Stream2 grad norm: {stream2_grad_norm:.4f}")

        # Inference test
        print(f"\n5. Inference Test")
        print("-" * 70)
        model.eval()
        with torch.no_grad():
            output = model(stream1_data, stream2_data)
            probs = torch.softmax(output, dim=1)
            preds = torch.argmax(probs, dim=1)

        print(f"âœ“ Predictions: {preds.tolist()[:5]}...")
        print(f"âœ“ Top prob mean: {probs.max(dim=1)[0].mean():.4f}")

        # Verify no NaN/Inf
        assert not torch.isnan(output).any(), "NaN in output"
        assert not torch.isinf(output).any(), "Inf in output"
        print(f"âœ“ No NaN/Inf values")

        print(f"\nâœ… {fusion_type.upper()} + Stream Optimization: PASSED")

    print(f"\n{'=' * 70}")
    print("Final Combined Test: All Fusion Types with Stream Optimization")
    print(f"{'=' * 70}")

    # Test rapid switching between configurations
    for i, (fusion, s1_lr, s2_lr) in enumerate([
        ('concat', 1e-4, 1e-4),   # Equal LR
        ('weighted', 5e-4, 5e-5), # Boost RGB
        ('gated', 1e-3, 1e-5),    # Extreme boost
    ]):
        model = mc_resnet18(
            num_classes=num_classes,
            fusion_type=fusion,
            device='cpu'
        )

        model.compile(
            optimizer='adamw',
            learning_rate=1e-4,
            stream1_lr=s1_lr,
            stream2_lr=s2_lr,
            loss='cross_entropy'
        )

        model.train()
        output = model(stream1_data, stream2_data)
        loss = model.criterion(output, targets)
        loss.backward()
        model.optimizer.step()

        print(f"âœ“ Config {i+1}: {fusion} fusion, s1_lr={s1_lr:.0e}, s2_lr={s2_lr:.0e} â†’ loss={loss.item():.4f}")

    print(f"\n{'=' * 70}")
    print("âœ… ALL COMBINED TESTS PASSED!")
    print(f"{'=' * 70}")
    print("\nSummary:")
    print("  âœ“ All fusion types work with stream-specific optimization")
    print("  âœ“ Parameter groups created correctly")
    print("  âœ“ Different learning rates applied to each stream")
    print("  âœ“ Fusion parameters update correctly")
    print("  âœ“ Training and inference work as expected")
    print("  âœ“ No NaN/Inf issues")
    print("\nReady for production use! ðŸš€")

if __name__ == "__main__":
    test_combined_features()
